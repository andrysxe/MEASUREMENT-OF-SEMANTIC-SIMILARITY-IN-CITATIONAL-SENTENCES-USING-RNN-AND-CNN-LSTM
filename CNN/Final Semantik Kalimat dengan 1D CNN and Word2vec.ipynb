{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "import tensorflow.keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical,plot_model\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, roc_curve,  roc_auc_score, classification_report\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.layers import Embedding, Flatten, Conv1D, SpatialDropout1D, MaxPooling1D,AveragePooling1D, merge, concatenate, Input, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorflow.random.set_seed(2)\n",
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database is locked')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_case</th>\n",
       "      <th>class</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>Recently, there has been a successful attempt ...</td>\n",
       "      <td>Recently, there has been a successful attempt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>The first one is the WS-353 dataset (Finkelste...</td>\n",
       "      <td>The first one is the WS-353 dataset (Finkelste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract Meaning Representation (AMR) (Banares...</td>\n",
       "      <td>Abstract Meaning Representation (AMR) (Banares...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>We perform bootstrap resampling with bounds es...</td>\n",
       "      <td>We perform bootstrap resampling with bounds es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>It is used to support semantic analyses in HPS...</td>\n",
       "      <td>It is used to support semantic analyses in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1201</td>\n",
       "      <td>4</td>\n",
       "      <td>We use Stanford parser (de Marneffe et al., 20...</td>\n",
       "      <td>We first use a dependency parser (de Marneffe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1202</td>\n",
       "      <td>3</td>\n",
       "      <td>Rooth et al. (1999) propose an Expectation-Max...</td>\n",
       "      <td>Alternatively, Rooth et al. (1999)  propose an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1203</td>\n",
       "      <td>3</td>\n",
       "      <td>The Levenshtein distance (Levenshtein, 1966) b...</td>\n",
       "      <td>The Levenshtein distance gives an indication o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1204</td>\n",
       "      <td>1</td>\n",
       "      <td>We use the Moses toolkit (Koehn et al., 2007) ...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>We perform bootstrap resampling with bounds es...</td>\n",
       "      <td>We perform bootstrap resampling with bounds es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>It is used to support semantic analyses in HPS...</td>\n",
       "      <td>It is used to support semantic analyses in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>It is used to support semantic analyses in the...</td>\n",
       "      <td>It is used to support semantic analyses in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>It is used to support semantic analyses in HPS...</td>\n",
       "      <td>It is used to support semantic analyses in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>It is used to support semantic analyses in the...</td>\n",
       "      <td>It is used to support semantic analyses in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>We build upon our previous Markov Logic based ...</td>\n",
       "      <td>We build upon our previous approach for joint ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>Details about SVM and KFD can be found in (Tay...</td>\n",
       "      <td>Details about SVM and KRR can be found in (Tay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>We learn the parameters using a quasi-Newton p...</td>\n",
       "      <td>We learn the parameters  using a quasi-Newton ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the SCFG decoder cdec (Dyer et al., 201...</td>\n",
       "      <td>For direct translation, we use the SCFG decode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>This is known as the Distributional Hypothesis...</td>\n",
       "      <td>This is known as the Distributional Hypothesis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>All our models , as well as the parser describ...</td>\n",
       "      <td>The models , as well as the parser described i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>For strings, many such kernel functions exist ...</td>\n",
       "      <td>For strings, a lot of such kernel functions ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>The estimation of the semantically Smoothed Pa...</td>\n",
       "      <td>The estimation of the semantically Smoothed Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>We train with the Adam optimizer (Kingma and B...</td>\n",
       "      <td>We train with the Adam optimizer (Kingma and B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>56</td>\n",
       "      <td>3</td>\n",
       "      <td>In our experimental study, we use the freely a...</td>\n",
       "      <td>In our experimental study, we use the freely a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>More recently, (Carpineto and Romano, 2010) sh...</td>\n",
       "      <td>OPTIMSRC: (Carpineto and Romano, 2010) showed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>They are based on the distributional hypothesi...</td>\n",
       "      <td>These methods rely on the distributional hypot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>Word alignment is performed using GIZA++ (Och ...</td>\n",
       "      <td>Word alignment is done using GIZA++ (Och and N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>61</td>\n",
       "      <td>4</td>\n",
       "      <td>Word alignment is performed using GIZA++ (Och ...</td>\n",
       "      <td>Word alignment is done using GIZA++ (Och and N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>We used Mallet software (McCallum, 2002) for C...</td>\n",
       "      <td>We used Mallet software (McCallum, 2002) for S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "      <td>The thesaurus consists of a hierarchy of 2,710...</td>\n",
       "      <td>The ontology, GoiTaikei, consists of a hierarc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>The detailed discussion is provided in the lon...</td>\n",
       "      <td>A detailed discussion on the results is provid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>The first one is the WS- 353 3 dataset (Finkel...</td>\n",
       "      <td>The first one is the WS-353 dataset (Finkelste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>68</td>\n",
       "      <td>3</td>\n",
       "      <td>A framework for human error analysis and error...</td>\n",
       "      <td>A framework for human error analysis has been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>MaxEnt classifier is a good example of this gr...</td>\n",
       "      <td>MaxEnt classifier is an example of this group ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>Among these media, blog is one of the communic...</td>\n",
       "      <td>Blog is one of the crucial, communicative and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>For the gold preprocessing and all 5k settings...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>For preprocessing, we used MADA (Morphological...</td>\n",
       "      <td>For this purpose, we use MADA (Morphological A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "      <td>We trained a 5-gram language model on the Xinh...</td>\n",
       "      <td>Our 5-gram language model was trained on the X...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "      <td>To determine semantic type and subtype, we tra...</td>\n",
       "      <td>To determine semantic types and subtypes, we t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>We use the scikit implementation of Random For...</td>\n",
       "      <td>We use the scikit implementation of SVM (Pedre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>Each term in the input text will be represente...</td>\n",
       "      <td>Each term in the input text is represented by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>79</td>\n",
       "      <td>5</td>\n",
       "      <td>An algorithm, the Kuhn-Munkres method (Kuhn, 1...</td>\n",
       "      <td>An algorithm, the Kuhn-Munkres method (Kuhn, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>We use Collapsed Gibbs Sampling (Griffiths and...</td>\n",
       "      <td>We use collapsed Gibbs sampling (Griffiths and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>82</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Stanford dependency parser (Marneff...</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>Filter weights are initialized using Glorot-Be...</td>\n",
       "      <td>Weights are initialized using Glorot-Bengio st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>Automatic sentence alignment of the training d...</td>\n",
       "      <td>Automatic sentence alignment of the training d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "      <td>We use the AdaGrad optimizer (Duchi et al., 20...</td>\n",
       "      <td>We use AdaGrad (Duchi et al., 2011)  with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>87</td>\n",
       "      <td>5</td>\n",
       "      <td>Then we did word alignment using GIZA++ (Och a...</td>\n",
       "      <td>We performed word alignment using GIZA++ (Och ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>88</td>\n",
       "      <td>4</td>\n",
       "      <td>For example, DIRT (Lin and Pantel, 2001) aims ...</td>\n",
       "      <td>For example, DIRT (Lin and Pantel, 2001) aims ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>89</td>\n",
       "      <td>3</td>\n",
       "      <td>The annotation was performed manually using th...</td>\n",
       "      <td>The annotation was performed using the BRAT 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>91</td>\n",
       "      <td>4</td>\n",
       "      <td>System proposed by (Li et al., 2006 ), uses a ...</td>\n",
       "      <td>A similar semantic similarity measure, propose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "      <td>This corpus contains around 11,000 NPs annotat...</td>\n",
       "      <td>It consists of 50 texts taken from the WSJ por...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>93</td>\n",
       "      <td>4</td>\n",
       "      <td>All modules take as input the corpus documents...</td>\n",
       "      <td>All components take as input the corpus docume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>95</td>\n",
       "      <td>4</td>\n",
       "      <td>From the pioneering work of (Rapp, 1995 ), con...</td>\n",
       "      <td>From the pioneering work of (Rapp, 1995 ), BLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>96</td>\n",
       "      <td>5</td>\n",
       "      <td>Europarl (Koehn, 2005) is a multilingual paral...</td>\n",
       "      <td>We run our experiments on Europarl (Koehn, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>TESLA (Translation Evaluation of Sentences wit...</td>\n",
       "      <td>TESLA-F was called TESLA in Liu et al. (2010) .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>For building the baseline SMT system, we used ...</td>\n",
       "      <td>For building our SMT systems, the open-source ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>102</td>\n",
       "      <td>4</td>\n",
       "      <td>Rhetorical Structure Theory (RST) (Mann and Th...</td>\n",
       "      <td>Rhetorical Structure Theory (RST) (Mann and Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>In addition, the fix-discount method in (Foste...</td>\n",
       "      <td>In addition, the fix-discount method (Foster e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>104</td>\n",
       "      <td>5</td>\n",
       "      <td>Memory-based language processing (Daelemans an...</td>\n",
       "      <td>Memory-based language processing (Daelemans an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>We calculate statistical significance of perfo...</td>\n",
       "      <td>We tested the significance of differences usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>106</td>\n",
       "      <td>4</td>\n",
       "      <td>For instance, machine translation (MT) systems...</td>\n",
       "      <td>In addition, machine translation (MT) systems ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>109</td>\n",
       "      <td>4</td>\n",
       "      <td>1 with 2 -regularization using AdaGrad (Duchi ...</td>\n",
       "      <td>1 -regularization using AdaGrad (Duchi et al.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>111</td>\n",
       "      <td>3</td>\n",
       "      <td>Why does the lr model outperform Berkeley 13 T...</td>\n",
       "      <td>The MUC score (Vilain et al., 1995) counts the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>112</td>\n",
       "      <td>3</td>\n",
       "      <td>In the context of this paper we will be focusi...</td>\n",
       "      <td>We will focus on the syntactic tree kernel des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>113</td>\n",
       "      <td>5</td>\n",
       "      <td>Europarl (Koehn, 2005) is a multilingual paral...</td>\n",
       "      <td>The Europarl corpus (Koehn, 2005) is built fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>114</td>\n",
       "      <td>4</td>\n",
       "      <td>We have used Foma, a free software tool to spe...</td>\n",
       "      <td>The module was implemented using Foma, a free ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>116</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the same test set used in Li et al. (2...</td>\n",
       "      <td>We used the same test data as in Li et al. (20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>117</td>\n",
       "      <td>3</td>\n",
       "      <td>The Penn Discourse Treebank (PDTB, Prasad et a...</td>\n",
       "      <td>Penn Discourse Treebank The Penn Discourse Tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "      <td>We used a GB implementation of the scikit-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>119</td>\n",
       "      <td>3</td>\n",
       "      <td>Since the commonly used word similarity datase...</td>\n",
       "      <td>The second is the MEN dataset (Bruni et al., 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>The training data of the shared task is the NU...</td>\n",
       "      <td>The training data released by the task organiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>All system implementation was done using Pytho...</td>\n",
       "      <td>All of the machine learning was done using sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>123</td>\n",
       "      <td>3</td>\n",
       "      <td>It has been shown that a diverse set of predic...</td>\n",
       "      <td>It has been long identified in NLP that a dive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>124</td>\n",
       "      <td>4</td>\n",
       "      <td>In the 2013 system, we had used SentiStrength ...</td>\n",
       "      <td>In our system, we used the sentiment lexicon p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>125</td>\n",
       "      <td>3</td>\n",
       "      <td>Finally, we also compare the quality of the ca...</td>\n",
       "      <td>We also compared the quality of the candidate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>126</td>\n",
       "      <td>4</td>\n",
       "      <td>These methods are based on the distributional ...</td>\n",
       "      <td>Corpus-based VSMs follow the standard distribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>127</td>\n",
       "      <td>5</td>\n",
       "      <td>All annotations were done using the BRAT rapid...</td>\n",
       "      <td>The annotations were made using the BRAT rapid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>Compared to WordNet (Fellbaum, 1998 ), there a...</td>\n",
       "      <td>Compared to WordNet (Fellbaum, 1998 ), there a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>131</td>\n",
       "      <td>5</td>\n",
       "      <td>For training, we use Adam (Kingma and Ba, 2015...</td>\n",
       "      <td>We use Adam (Kingma and Ba, 2015) for optimiza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>132</td>\n",
       "      <td>3</td>\n",
       "      <td>For our classifier, we use SVMs, specifically ...</td>\n",
       "      <td>Specifically, we use the LIBLINEAR SVM package...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>133</td>\n",
       "      <td>4</td>\n",
       "      <td>The first two experiments concern the predicti...</td>\n",
       "      <td>The first two experiments involve predicting t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "      <td>We used only the non-ensembled left-to-right r...</td>\n",
       "      <td>We used only the non-ensembled left-to-right r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>137</td>\n",
       "      <td>2</td>\n",
       "      <td>We used only the non-ensembled left-to-right r...</td>\n",
       "      <td>We used only the non-ensembled left-to-right r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>138</td>\n",
       "      <td>4</td>\n",
       "      <td>The MSD morphological coding system was develo...</td>\n",
       "      <td>The MSD morphological coding system is a posit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>139</td>\n",
       "      <td>5</td>\n",
       "      <td>The phrase tables were generated by means of s...</td>\n",
       "      <td>The phrase table was generated employing symme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>140</td>\n",
       "      <td>5</td>\n",
       "      <td>Word alignment is performed using GIZA++ (Och ...</td>\n",
       "      <td>Word alignment is performed with Giza++ (Och a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>141</td>\n",
       "      <td>4</td>\n",
       "      <td>We experimented with several levels of cluster...</td>\n",
       "      <td>Following Koo et al. (2008) , we also experime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>143</td>\n",
       "      <td>4</td>\n",
       "      <td>(Raghavan et al. (2007)) measure the benefit f...</td>\n",
       "      <td>Raghavan et al. (2007)  evaluate benefit from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>144</td>\n",
       "      <td>4</td>\n",
       "      <td>We built a modified Kneser-Ney smoothed 5-gram...</td>\n",
       "      <td>Training and querying of a modified Kneser-Ney...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>146</td>\n",
       "      <td>4</td>\n",
       "      <td>A formal PAC-style analysis can be found in (A...</td>\n",
       "      <td>The formal derivation can be found in (Ando an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "      <td>The first model we introduce is based on the r...</td>\n",
       "      <td>The model as described thus far is identical t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>148</td>\n",
       "      <td>4</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>149</td>\n",
       "      <td>3</td>\n",
       "      <td>The classifier experiments were carried out us...</td>\n",
       "      <td>The classifier evaluations were carried out us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>The training data of the shared task is the NU...</td>\n",
       "      <td>The training data for the task is from the NUC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>151</td>\n",
       "      <td>5</td>\n",
       "      <td>SALDO (Borin et al., 2013) is the largest free...</td>\n",
       "      <td>SALDO (Borin et al., 2013) is the most compreh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>152</td>\n",
       "      <td>5</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "      <td>However, those string-to-tree systems run slow...</td>\n",
       "      <td>However such string-to-tree systems run slowly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>154</td>\n",
       "      <td>4</td>\n",
       "      <td>The 5-gram target language model was trained u...</td>\n",
       "      <td>We trained an English 5-gram language model us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>155</td>\n",
       "      <td>5</td>\n",
       "      <td>For all experiments, we used the Moses SMT sys...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>156</td>\n",
       "      <td>4</td>\n",
       "      <td>We evaluate our method on the following data s...</td>\n",
       "      <td>@BULLET OntoNotes-Test: Test set of the OntoNo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Moses phrase-based translation syst...</td>\n",
       "      <td>We use the state-of-the-art phrase-based machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>158</td>\n",
       "      <td>4</td>\n",
       "      <td>But we randomly selected 90% of the training d...</td>\n",
       "      <td>Since the training data used in Li et al. (200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>The BLEU score measures the precision of n-gra...</td>\n",
       "      <td>BLEU score: This score measures the precision ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>160</td>\n",
       "      <td>3</td>\n",
       "      <td>The training data of the shared task is the NU...</td>\n",
       "      <td>The training data provided for the task is a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>161</td>\n",
       "      <td>3</td>\n",
       "      <td>Test data was drawn from the Open American Nat...</td>\n",
       "      <td>We selected the dataset of Jurgens and Klapaft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>162</td>\n",
       "      <td>5</td>\n",
       "      <td>We measure statistical significance using 95% ...</td>\n",
       "      <td>The statistical significance tests using 95% c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>163</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the English portion of the ACE 2005 rel...</td>\n",
       "      <td>We evaluate our relation extraction system on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>164</td>\n",
       "      <td>5</td>\n",
       "      <td>This data was collected for the 2014 SemEval c...</td>\n",
       "      <td>Sentences Involving Compositional Knowledge (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>165</td>\n",
       "      <td>5</td>\n",
       "      <td>The parsing model used for intra-sentential pa...</td>\n",
       "      <td>Our novel parsing model is the Dynamic Conditi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>166</td>\n",
       "      <td>4</td>\n",
       "      <td>Latent Dirichlet Allocation (LDA) is a generat...</td>\n",
       "      <td>Combination of latent topics ent Dirichlet All...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>167</td>\n",
       "      <td>5</td>\n",
       "      <td>Distributional hypothesis theory (Harris, 1954...</td>\n",
       "      <td>It therefore follows the distributional hypoth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>168</td>\n",
       "      <td>5</td>\n",
       "      <td>Distributional hypothesis theory (Harris, 1954...</td>\n",
       "      <td>It therefore follows the distributional hypoth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>169</td>\n",
       "      <td>5</td>\n",
       "      <td>In 2009, Yefang Wang (Wang et al., 2009) used ...</td>\n",
       "      <td>In 2009, Yefang Wang (Wang et al., 2009) used ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>170</td>\n",
       "      <td>3</td>\n",
       "      <td>We built a 5-gram language model on the Englis...</td>\n",
       "      <td>We built a modified Kneser-Ney smoothed 5-gram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>171</td>\n",
       "      <td>3</td>\n",
       "      <td>The remaining three models are all Naive Bayes...</td>\n",
       "      <td>The other models are trained on native English...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>172</td>\n",
       "      <td>4</td>\n",
       "      <td>We apply bootstrapping (Kozareva et al., 2008)...</td>\n",
       "      <td>We then apply bootstrapping (Kozareva et al., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>173</td>\n",
       "      <td>5</td>\n",
       "      <td>These methods are based on the distributional ...</td>\n",
       "      <td>It therefore follows the distributional hypoth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>174</td>\n",
       "      <td>5</td>\n",
       "      <td>These results verify the benefit of using LTAG...</td>\n",
       "      <td>Our hypothesis is that the LTAG based features...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>175</td>\n",
       "      <td>5</td>\n",
       "      <td>RG-65: (Rubenstein and Goodenough, 1965) has 6...</td>\n",
       "      <td>RG-65: (Rubenstein and Goodenough, 1965) is se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>176</td>\n",
       "      <td>5</td>\n",
       "      <td>The significance tests were performed using th...</td>\n",
       "      <td>Statistical significance tests are performed u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>177</td>\n",
       "      <td>5</td>\n",
       "      <td>The default Phrasal search algorithm is cube p...</td>\n",
       "      <td>The search is typically carried out using the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>178</td>\n",
       "      <td>5</td>\n",
       "      <td>In-domain data is mainly used to solve the pro...</td>\n",
       "      <td>In-domain data only solves the problem of data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>179</td>\n",
       "      <td>4</td>\n",
       "      <td>All experiments were carried out using the ope...</td>\n",
       "      <td>All the experiments are carried out in Moses t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "      <td>First, we apply heuristics to determine number...</td>\n",
       "      <td>We then apply heuristics to determine number a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>181</td>\n",
       "      <td>4</td>\n",
       "      <td>Rank SVM (Joachims, 2002) is a method based on...</td>\n",
       "      <td>For this task we use RankSVM (Joachims, 2002) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>182</td>\n",
       "      <td>3</td>\n",
       "      <td>A more detailed description of the task can be...</td>\n",
       "      <td>A precise description of the corpus and metric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "      <td>A Tree Kernel function is a convolution kernel...</td>\n",
       "      <td>Tree Kernel (TK) functions are convolution ker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>185</td>\n",
       "      <td>4</td>\n",
       "      <td>We build upon our previous approach for joint ...</td>\n",
       "      <td>For disambiguation and clustering we build upo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>186</td>\n",
       "      <td>4</td>\n",
       "      <td>ROUGE-2 metric (Lin, 2004) is used for the eva...</td>\n",
       "      <td>We used the ROUGE-1 evaluation metric (Lin, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>187</td>\n",
       "      <td>5</td>\n",
       "      <td>We used Mallet software (McCallum, 2002) for C...</td>\n",
       "      <td>We used Mallet toolkit (McCallum, 2002) for CR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>188</td>\n",
       "      <td>3</td>\n",
       "      <td>We exploit a transition-based framework with g...</td>\n",
       "      <td>Our joint parsing model exploits a transition-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>189</td>\n",
       "      <td>4</td>\n",
       "      <td>The annotation was performed using the BRAT 2 ...</td>\n",
       "      <td>The annotations were made using the BRAT rapid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>190</td>\n",
       "      <td>5</td>\n",
       "      <td>For building the word alignment models we use ...</td>\n",
       "      <td>To build the word alignment models we used the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>191</td>\n",
       "      <td>5</td>\n",
       "      <td>The reliability of the annotation was evaluate...</td>\n",
       "      <td>We evaluated annotation reliability by using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>192</td>\n",
       "      <td>5</td>\n",
       "      <td>It therefore follows the distributional hypoth...</td>\n",
       "      <td>Distributional models of meaning follow the di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>193</td>\n",
       "      <td>4</td>\n",
       "      <td>This dataset is composed of 35 triplets of sen...</td>\n",
       "      <td>This dataset is composed of 32 sentence quadru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "      <td>The significance tests were performed using th...</td>\n",
       "      <td>A statistical significance test was performed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>195</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the standard alignment tool Giza++ (Och...</td>\n",
       "      <td>We use the Giza++ tool (Och and Ney, 2003) to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>196</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the standard alignment tool Giza++ (Och...</td>\n",
       "      <td>We use the Giza++ tool (Och and Ney, 2003) to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>198</td>\n",
       "      <td>3</td>\n",
       "      <td>The kernels are combined using Gaussian proces...</td>\n",
       "      <td>Gaussian process regression (GPR) (Rasmussen a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>199</td>\n",
       "      <td>5</td>\n",
       "      <td>To overcome this, Agirre et al. (2009) used Ma...</td>\n",
       "      <td>In another work, a corpus of roughly 1.6 Teraw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>We measure statistical significance using 95% ...</td>\n",
       "      <td>We calculated significance using paired bootst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>701</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Stanford dependency parser (Chen an...</td>\n",
       "      <td>In this work, we use the Stanford neural depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>702</td>\n",
       "      <td>3</td>\n",
       "      <td>Next, a tweet was tokenized and fed into MADAM...</td>\n",
       "      <td>MADAMIRA (Pasha et al., 2014) is a morphologic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>703</td>\n",
       "      <td>4</td>\n",
       "      <td>(Yarowsky ,(1995)) has proposed a bootstrappin...</td>\n",
       "      <td>Yarowsky (1995) proposed such a method for wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>704</td>\n",
       "      <td>5</td>\n",
       "      <td>We also list the previous state-of-the-art per...</td>\n",
       "      <td>We also list the results from SMT model (Durra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>705</td>\n",
       "      <td>4</td>\n",
       "      <td>We used Adam (Kingma and Ba, 2014) with a lear...</td>\n",
       "      <td>We use the Adam (Kingma and Ba, 2014) algorith...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>706</td>\n",
       "      <td>2</td>\n",
       "      <td>Distributional semantics is based on the idea ...</td>\n",
       "      <td>Word co-occurence statistics\" You shall know a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>707</td>\n",
       "      <td>3</td>\n",
       "      <td>In order to estimate the basic lexical similar...</td>\n",
       "      <td>The co-occurrence Word Space is acquired throu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>709</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "      <td>We used the scikit-learn toolkit to train our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>710</td>\n",
       "      <td>3</td>\n",
       "      <td>The translation model was trained by GIZA++ (O...</td>\n",
       "      <td>The probability p(E) is computed using a simpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>711</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a generalization of the operator Id in...</td>\n",
       "      <td>This is similar to the operator Intro in (Kapl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>712</td>\n",
       "      <td>4</td>\n",
       "      <td>We used standard classifiers available in scik...</td>\n",
       "      <td>We used a GB implementation of the scikit-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>714</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "      <td>For the linear logistic regression implementat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>715</td>\n",
       "      <td>5</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "      <td>Finally, we used Moses toolkit as phrase-based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>716</td>\n",
       "      <td>3</td>\n",
       "      <td>4 Word alignments are created by aligning the ...</td>\n",
       "      <td>Baseline word alignments were obtained by runn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>717</td>\n",
       "      <td>4</td>\n",
       "      <td>The perplexity achieved by the 6- gram NN LM i...</td>\n",
       "      <td>The language model is a 5-gram with interpolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>718</td>\n",
       "      <td>5</td>\n",
       "      <td>We used standard classifiers available in scik...</td>\n",
       "      <td>We used the scikit-learn toolkit to train our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>719</td>\n",
       "      <td>5</td>\n",
       "      <td>Statistical machine translation is typically p...</td>\n",
       "      <td>It is a standard phrase-based machine translat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>720</td>\n",
       "      <td>3</td>\n",
       "      <td>WordNet (Miller et al., 1990) is an on-line hi...</td>\n",
       "      <td>The WordNet on-line lexical database (Miller e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>721</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the scikit implementation of Random For...</td>\n",
       "      <td>We used a GB implementation of the scikit-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>724</td>\n",
       "      <td>4</td>\n",
       "      <td>We lemmatise the head of each constituent with...</td>\n",
       "      <td>We used the Stuttgart TreeTagger (Schmid, 1994...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>725</td>\n",
       "      <td>4</td>\n",
       "      <td>Our text processing uses the Natural Language ...</td>\n",
       "      <td>Part-of-speech tagging was accomplished using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>726</td>\n",
       "      <td>5</td>\n",
       "      <td>They used the Web-based annotation tool brat (...</td>\n",
       "      <td>The annotation was performed manually using th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>729</td>\n",
       "      <td>3</td>\n",
       "      <td>Our system participated in SemEval-2013 Task 2...</td>\n",
       "      <td>We participated in both subtask A and B of Sem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>730</td>\n",
       "      <td>5</td>\n",
       "      <td>The English text was tokenized using the word ...</td>\n",
       "      <td>We tokenise the text using the default tokenis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>731</td>\n",
       "      <td>4</td>\n",
       "      <td>The webpages were parsed using the Stanford Co...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>732</td>\n",
       "      <td>3</td>\n",
       "      <td>Statistical machine translation is typically p...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>733</td>\n",
       "      <td>3</td>\n",
       "      <td>The English side was tokenized using the Moses...</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>734</td>\n",
       "      <td>5</td>\n",
       "      <td>We trained an English 5-gram language model us...</td>\n",
       "      <td>We built a trigram language model with Kneser-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>735</td>\n",
       "      <td>5</td>\n",
       "      <td>All of the text data from Reddit was tokenized...</td>\n",
       "      <td>We tokenise the text using the default tokenis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>736</td>\n",
       "      <td>4</td>\n",
       "      <td>Our machine translation systems are trained us...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>737</td>\n",
       "      <td>5</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>738</td>\n",
       "      <td>5</td>\n",
       "      <td>We build upon our previous approach for joint ...</td>\n",
       "      <td>Scope-ignorant (Disambig.): Our previous MLN-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>739</td>\n",
       "      <td>5</td>\n",
       "      <td>We used Adam (Kingma and Ba, 2014) with a lear...</td>\n",
       "      <td>We use ADAM (Kingma and Ba, 2014) with a learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>740</td>\n",
       "      <td>5</td>\n",
       "      <td>For all experiments, we used the Moses SMT sys...</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>741</td>\n",
       "      <td>4</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>742</td>\n",
       "      <td>5</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>743</td>\n",
       "      <td>5</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "      <td>First, we used the Moses toolkit (Koehn et al....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>744</td>\n",
       "      <td>5</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>745</td>\n",
       "      <td>5</td>\n",
       "      <td>We develop translation models using the phrase...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>746</td>\n",
       "      <td>4</td>\n",
       "      <td>We used TnT (Brants, 2000 ), trained on the Ne...</td>\n",
       "      <td>We employed the TnT tagger (Brants, 2000) whic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>747</td>\n",
       "      <td>3</td>\n",
       "      <td>The webpages were parsed using the Stanford Co...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>748</td>\n",
       "      <td>5</td>\n",
       "      <td>We develop translation models using the phrase...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>750</td>\n",
       "      <td>3</td>\n",
       "      <td>The term frequency count is normalized with th...</td>\n",
       "      <td>TF-IDF is a standard statistical method that c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>751</td>\n",
       "      <td>4</td>\n",
       "      <td>We assessed the statistical significance of di...</td>\n",
       "      <td>We assess statistical significance of the diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>752</td>\n",
       "      <td>5</td>\n",
       "      <td>A framework for human error analysis and error...</td>\n",
       "      <td>A framework for human error analysis has been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>753</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, Chang et al. (2009) found that th...</td>\n",
       "      <td>Chang et al. (2009) stated that one reason is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>754</td>\n",
       "      <td>4</td>\n",
       "      <td>On the Chinese side, we used the morphological...</td>\n",
       "      <td>The MMA system (Kruengkrai et al., 2009) train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>755</td>\n",
       "      <td>5</td>\n",
       "      <td>conducted using the Moses phrase-based decoder...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>756</td>\n",
       "      <td>3</td>\n",
       "      <td>conducted using the Moses phrase-based decoder...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>757</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>conducted using the Moses phrase-based decoder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>759</td>\n",
       "      <td>3</td>\n",
       "      <td>Then the processed data was performed for toke...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>760</td>\n",
       "      <td>5</td>\n",
       "      <td>We applied bootstrap resampling (Koehn, 2004) ...</td>\n",
       "      <td>We measure significance of results using boots...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>761</td>\n",
       "      <td>5</td>\n",
       "      <td>This system uses the attentional encoder-decod...</td>\n",
       "      <td>We followed the encoder-decoder architecture w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>762</td>\n",
       "      <td>3</td>\n",
       "      <td>The language model is a 5-gram KenLM (Heafield...</td>\n",
       "      <td>We built a trigram language model with Kneser-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>763</td>\n",
       "      <td>5</td>\n",
       "      <td>Weighted Finite State Transducers (FSTs) used ...</td>\n",
       "      <td>The decoder is implemented with Weighted Finit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>764</td>\n",
       "      <td>5</td>\n",
       "      <td>Weighted Finite State Transducers (FSTs) used ...</td>\n",
       "      <td>The decoder is implemented with Weighted Finit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>765</td>\n",
       "      <td>3</td>\n",
       "      <td>Then the processed data was performed for toke...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>766</td>\n",
       "      <td>4</td>\n",
       "      <td>It is a modification of the model proposed by ...</td>\n",
       "      <td>Our first baseline is MI09, a distantly superv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>767</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Scikit-learn (Pedregosa et al., 2011 ),...</td>\n",
       "      <td>We used the Scikit-learn machine learning libr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>768</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Universal POS Tagset (UPOS) of Petr...</td>\n",
       "      <td>This in turn relies on the same underlying fea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>768</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Universal POS Tagset (UPOS) of Petr...</td>\n",
       "      <td>This in turn relies on the same underlying fea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>769</td>\n",
       "      <td>3</td>\n",
       "      <td>The CRF is trained using decisions from the fo...</td>\n",
       "      <td>MADAMIRA (Pasha et al., 2014) is a morphologic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>770</td>\n",
       "      <td>5</td>\n",
       "      <td>For language modeling, we trained a separate 5...</td>\n",
       "      <td>We built a modified Kneser- Ney smoothed 5-gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>771</td>\n",
       "      <td>5</td>\n",
       "      <td>with the training script of the Moses toolkit ...</td>\n",
       "      <td>We preprocessed the training corpora with scri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>772</td>\n",
       "      <td>5</td>\n",
       "      <td>The word alignment was trained using GIZA++ (O...</td>\n",
       "      <td>We performed word alignment using GIZA++ (Och ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>773</td>\n",
       "      <td>4</td>\n",
       "      <td>We use linear SVMs from LIBLINEAR and SVMs wit...</td>\n",
       "      <td>As our learner, we use LIBSVM with a linear ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>774</td>\n",
       "      <td>5</td>\n",
       "      <td>We specify the hierarchical aligner in terms o...</td>\n",
       "      <td>We specify our dynamic programming algorithm a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>775</td>\n",
       "      <td>4</td>\n",
       "      <td>We use Stanford parser (de Marneffe et al., 20...</td>\n",
       "      <td>We use the Stanford dependency parser (de Marn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>776</td>\n",
       "      <td>4</td>\n",
       "      <td>In this work, we use the Stanford neural depen...</td>\n",
       "      <td>In order to detect the object pronouns, we emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>778</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Stanford parser (de Marneffe et al., 20...</td>\n",
       "      <td>We also obtain the dependency parse of the sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>779</td>\n",
       "      <td>5</td>\n",
       "      <td>The learning algorithm used in our coreference...</td>\n",
       "      <td>The question classifier used in the experiment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>780</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Stanford parser (de Marneffe et al., 20...</td>\n",
       "      <td>We use the Stanford dependency parser (de Marn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>781</td>\n",
       "      <td>5</td>\n",
       "      <td>Distributional semantics (see Cohen and Widdow...</td>\n",
       "      <td>The former that is the most popular relies on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>782</td>\n",
       "      <td>4</td>\n",
       "      <td>Their work is part of the state-of-the-art Ara...</td>\n",
       "      <td>MADAMIRA (Pasha et al., 2014) is a morphologic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>783</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Stanford parser (de Marneffe et al., 20...</td>\n",
       "      <td>We also obtain the dependency parse of the sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>784</td>\n",
       "      <td>4</td>\n",
       "      <td>Phrasal follows the log-linear approach to phr...</td>\n",
       "      <td>The log-linear approach to phrase-based transl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>785</td>\n",
       "      <td>5</td>\n",
       "      <td>Training data are based on a concatenation of ...</td>\n",
       "      <td>Both CDS corpora are available from the CHILDE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>786</td>\n",
       "      <td>3</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>787</td>\n",
       "      <td>5</td>\n",
       "      <td>5-gram language models of Turkish and English ...</td>\n",
       "      <td>We built a 5-gram language model on the Englis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>788</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the relation classification dataset of...</td>\n",
       "      <td>We evaluated our model on a semantic relation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>789</td>\n",
       "      <td>4</td>\n",
       "      <td>We then run word alignment with GIZA++ (Och an...</td>\n",
       "      <td>We performed word alignment using GIZA++ (Och ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>790</td>\n",
       "      <td>3</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>For all experiments, we used the Moses SMT sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>791</td>\n",
       "      <td>5</td>\n",
       "      <td>The significance tests were performed using th...</td>\n",
       "      <td>The statistical significance tests using 95% c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>792</td>\n",
       "      <td>5</td>\n",
       "      <td>All experiments were carried out using the ope...</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>793</td>\n",
       "      <td>5</td>\n",
       "      <td>These sentences have then be fed into an effic...</td>\n",
       "      <td>The sentences are fed into the PET HPSG parser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>794</td>\n",
       "      <td>4</td>\n",
       "      <td>The language model is a 5-gram KenLM (Heafield...</td>\n",
       "      <td>The 5-gram target language model was trained u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>795</td>\n",
       "      <td>4</td>\n",
       "      <td>Cohen et al. (2012)  present a spectral algori...</td>\n",
       "      <td>First, we present an algorithm for estimating ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>796</td>\n",
       "      <td>5</td>\n",
       "      <td>Distributional hypothesis theory (Harris, 1954...</td>\n",
       "      <td>Similar row vectors in T indicate similar cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>797</td>\n",
       "      <td>5</td>\n",
       "      <td>Distributional hypothesis theory (Harris, 1954...</td>\n",
       "      <td>Similar row vectors in T indicate similar cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>799</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Stanford dependency parser (Marneff...</td>\n",
       "      <td>We first use a dependency parser (de Marneffe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>800</td>\n",
       "      <td>3</td>\n",
       "      <td>We also replicated the experiment of Holmqvist...</td>\n",
       "      <td>We also carried out a chunk-reordering PB-SMT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>1301</td>\n",
       "      <td>4</td>\n",
       "      <td>We develop translation models using the phrase...</td>\n",
       "      <td>conducted using the Moses phrase-based decoder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1302</td>\n",
       "      <td>5</td>\n",
       "      <td>POS tagging was performed with TreeTagger (Sch...</td>\n",
       "      <td>The test set was tagged with the French TreeTa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>1303</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the MaltParser (Nivre et al., 2007) fo...</td>\n",
       "      <td>For the parsing experimens I used MaltParser (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>1304</td>\n",
       "      <td>4</td>\n",
       "      <td>Classification uses the scikit-learn Python pa...</td>\n",
       "      <td>We used a GB implementation of the scikit-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1305</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Stanford dependency parser (Marneff...</td>\n",
       "      <td>These features were obtained using the Stanfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1306</td>\n",
       "      <td>5</td>\n",
       "      <td>We used Adam (Kingma and Ba, 2014) with a lear...</td>\n",
       "      <td>We used Adam as the optimizer (Kingma and Ba, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>1307</td>\n",
       "      <td>4</td>\n",
       "      <td>Word alignment is performed using GIZA++ (Och ...</td>\n",
       "      <td>This is done using IBM Model 1 (Brown et al., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1308</td>\n",
       "      <td>4</td>\n",
       "      <td>The test set was tagged with the French TreeTa...</td>\n",
       "      <td>For the French side, the TreeTagger (Schmid, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>1309</td>\n",
       "      <td>4</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "      <td>conducted using the Moses phrase-based decoder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>1310</td>\n",
       "      <td>5</td>\n",
       "      <td>POS tagging was performed with TreeTagger (Sch...</td>\n",
       "      <td>POS tagging is performed using the IMS Tree Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>1311</td>\n",
       "      <td>5</td>\n",
       "      <td>For building the word alignment models we use ...</td>\n",
       "      <td>For word alignments, we used Mgiza++ (Gao and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1313</td>\n",
       "      <td>3</td>\n",
       "      <td>The Polish data is taken from the EUROPARL cor...</td>\n",
       "      <td>All the data come from Europarl (Koehn, 2005 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1314</td>\n",
       "      <td>5</td>\n",
       "      <td>The German-to-English corpus is Europarl v7 (K...</td>\n",
       "      <td>The Polish data is taken from the EUROPARL cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>1315</td>\n",
       "      <td>5</td>\n",
       "      <td>Distributional models of meaning follow the di...</td>\n",
       "      <td>Distributional similarity relies on the distri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1316</td>\n",
       "      <td>4</td>\n",
       "      <td>conducted using the Moses phrase-based decoder...</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1317</td>\n",
       "      <td>3</td>\n",
       "      <td>Europarl 2 (Koehn, 2005 ): it is a corpus of p...</td>\n",
       "      <td>In our MT experiments, we translate French int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1318</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted statistical significance tests fo...</td>\n",
       "      <td>Statistical significance is tested on the BLEU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>1320</td>\n",
       "      <td>5</td>\n",
       "      <td>The English side was tokenized using the Moses...</td>\n",
       "      <td>The baseline will be created by the Moses SMT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>1321</td>\n",
       "      <td>5</td>\n",
       "      <td>We develop translation models using the phrase...</td>\n",
       "      <td>We use the Moses phrase-based translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>1322</td>\n",
       "      <td>5</td>\n",
       "      <td>The baseline will be created by the Moses SMT ...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>1323</td>\n",
       "      <td>4</td>\n",
       "      <td>The resulting matrix is weighted using pointwi...</td>\n",
       "      <td>A popular measure of this association is point...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1324</td>\n",
       "      <td>3</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "      <td>The questions are translated using a phrase-ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1325</td>\n",
       "      <td>4</td>\n",
       "      <td>The morpho-syntactic tagging has been made wit...</td>\n",
       "      <td>POS tagging is performed using the IMS Tree Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1326</td>\n",
       "      <td>4</td>\n",
       "      <td>For the determination of POS tags we use the S...</td>\n",
       "      <td>For both we use TreeTagger (Schmid, 1994) with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1327</td>\n",
       "      <td>5</td>\n",
       "      <td>The Polish data is taken from the EUROPARL cor...</td>\n",
       "      <td>The Europarl corpus (Koehn, 2005) is built fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1328</td>\n",
       "      <td>5</td>\n",
       "      <td>All annotations were done using the brat rapid...</td>\n",
       "      <td>The annotation was performed using the BRAT 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>1329</td>\n",
       "      <td>4</td>\n",
       "      <td>The Spanish-English (S2E) training corpus was ...</td>\n",
       "      <td>The Polish data is taken from the EUROPARL cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1330</td>\n",
       "      <td>4</td>\n",
       "      <td>We used TnT (Brants, 2000 ), trained on the Ne...</td>\n",
       "      <td>HCRC is tagged with TnT (Brants, 2000 ), train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1331</td>\n",
       "      <td>4</td>\n",
       "      <td>For all experiments, we used the Moses SMT sys...</td>\n",
       "      <td>For German-English we also have a system based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1332</td>\n",
       "      <td>5</td>\n",
       "      <td>Two baseNP data sets have been put forward by ...</td>\n",
       "      <td>An alternative representation for baseNPs has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1333</td>\n",
       "      <td>5</td>\n",
       "      <td>Both of our systems were based on the Moses de...</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1334</td>\n",
       "      <td>5</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "      <td>The baseline will be created by the Moses SMT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>1335</td>\n",
       "      <td>5</td>\n",
       "      <td>Corpus-based meaning representations rely on t...</td>\n",
       "      <td>Distributional models of meaning follow the di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>1336</td>\n",
       "      <td>4</td>\n",
       "      <td>Recently, Naim et al. (2014)  proposed an unsu...</td>\n",
       "      <td>Recently, Naim et al. (2014) proposed a fully ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>1337</td>\n",
       "      <td>5</td>\n",
       "      <td>A distributional similarity model is construct...</td>\n",
       "      <td>Distributional models of meaning follow the di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>1338</td>\n",
       "      <td>4</td>\n",
       "      <td>This paper describes the details of our system...</td>\n",
       "      <td>In the following we will describe the system w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1339</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the phrase-based model Moses (Koehn et...</td>\n",
       "      <td>It was built with the Moses toolkit (Koehn et ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>1340</td>\n",
       "      <td>5</td>\n",
       "      <td>Combinatory Categorial grammar (CCG) is a ling...</td>\n",
       "      <td>Combinatory Categorial Grammar (CCG) (Steedman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1341</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the Random Forests implementation of s...</td>\n",
       "      <td>We used the Linear SVM implementation (with de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>1342</td>\n",
       "      <td>4</td>\n",
       "      <td>The proposed model extends the LDA framework o...</td>\n",
       "      <td>The article of Blei et al. (2003) compares LDA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1343</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the training section of the dataset fr...</td>\n",
       "      <td>Gimpel et al. (2011)  provided a dataset of PO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1344</td>\n",
       "      <td>5</td>\n",
       "      <td>5-gram language models of Turkish and English ...</td>\n",
       "      <td>In order to evaluate the fluency of each syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>1345</td>\n",
       "      <td>3</td>\n",
       "      <td>The corpora are first tokenized and lowercased...</td>\n",
       "      <td>The corpus was then automatically tagged with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>1346</td>\n",
       "      <td>5</td>\n",
       "      <td>The statistical significance test is also carr...</td>\n",
       "      <td>The improvement is statistically significant a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1348</td>\n",
       "      <td>3</td>\n",
       "      <td>The realisation ranking component is an SVM ra...</td>\n",
       "      <td>The learner is implemented as a ranking compon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>1351</td>\n",
       "      <td>5</td>\n",
       "      <td>The statistical significance test is also carr...</td>\n",
       "      <td>A statistical significance test was performed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>1352</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the NLTK toolkit (Loper and Bird, 2002)...</td>\n",
       "      <td>We use the Punkt sentence splitter from NLTK (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>1357</td>\n",
       "      <td>2</td>\n",
       "      <td>We mark the source tokens to which each target...</td>\n",
       "      <td>We find this method provides an additional 1.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>1358</td>\n",
       "      <td>3</td>\n",
       "      <td>We rely on the hybrid aligned lexical semantic...</td>\n",
       "      <td>In particular, the contribution of this paper ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1359</td>\n",
       "      <td>3</td>\n",
       "      <td>To help improve the information extraction too...</td>\n",
       "      <td>The BioScope corpus is a manually annotated co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>1362</td>\n",
       "      <td>3</td>\n",
       "      <td>The module of coreference resolution included ...</td>\n",
       "      <td>We apply the Stanford coreference resolution s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>1363</td>\n",
       "      <td>5</td>\n",
       "      <td>We run our experiments on Europarl (Koehn, 200...</td>\n",
       "      <td>Europarl 2 (Koehn, 2005 ): it is a corpus of p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>1364</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the scikit-learn (Pedregosa et al., 20...</td>\n",
       "      <td>We used a GB implementation of the scikit-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>1365</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the scikit-learn (Pedregosa et al., 20...</td>\n",
       "      <td>We used the scikit-learn toolkit to train our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>1366</td>\n",
       "      <td>5</td>\n",
       "      <td>Automatic Multi-Document Summarization (MDS) a...</td>\n",
       "      <td>Automatic text summarization aims to automatic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>1367</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the scikit-learn (Pedregosa et al., 20...</td>\n",
       "      <td>We used the scikit-learn (Pedregosa et al., 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>1369</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>1370</td>\n",
       "      <td>4</td>\n",
       "      <td>We train classifiers for each of the above fea...</td>\n",
       "      <td>We train Random Forest classifiers (Breiman, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>1371</td>\n",
       "      <td>4</td>\n",
       "      <td>In addition, the corpus was lemmatised using t...</td>\n",
       "      <td>1 The corpus is lemmatised and tagged by part-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>1372</td>\n",
       "      <td>4</td>\n",
       "      <td>They are based on Distributional Hypothesis wh...</td>\n",
       "      <td>Distributional models of meaning follow the di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1373</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the same test set used in (Li et al. (...</td>\n",
       "      <td>But we randomly selected 90% of the training d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>1374</td>\n",
       "      <td>4</td>\n",
       "      <td>We develop translation models using the phrase...</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>1375</td>\n",
       "      <td>3</td>\n",
       "      <td>We used non-local features based on Finkel et ...</td>\n",
       "      <td>For example, Finkel et al. (2005) enabled the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>1376</td>\n",
       "      <td>5</td>\n",
       "      <td>For building the baseline SMT system, we used ...</td>\n",
       "      <td>For all experiments, we used the Moses SMT sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>1377</td>\n",
       "      <td>3</td>\n",
       "      <td>For translation, we use Moses (Koehn et al., 2...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>1378</td>\n",
       "      <td>4</td>\n",
       "      <td>To recognize explicit connectives, we construc...</td>\n",
       "      <td>In addition, we show that the latent represent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>1379</td>\n",
       "      <td>3</td>\n",
       "      <td>To construct language models and measure perpl...</td>\n",
       "      <td>Both language models use modified Kneser-Ney s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>1380</td>\n",
       "      <td>4</td>\n",
       "      <td>We use Gibbs sampling to estimate the distribu...</td>\n",
       "      <td>We use the Gibbs sampling based LDA (Griffiths...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>1381</td>\n",
       "      <td>5</td>\n",
       "      <td>Arabizi is not a letter-based transliteration ...</td>\n",
       "      <td>1 Arabic transliteration is presented in the B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>1382</td>\n",
       "      <td>4</td>\n",
       "      <td>To construct language models and measure perpl...</td>\n",
       "      <td>Both language models use modified Kneser-Ney s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>1384</td>\n",
       "      <td>4</td>\n",
       "      <td>We build upon our previous Markov Logic based ...</td>\n",
       "      <td>Scope-ignorant (Disambig.): Our previous MLN-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>1385</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the feedforward neural probabilistic la...</td>\n",
       "      <td>We follow the neural network architecture of V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>1386</td>\n",
       "      <td>5</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "      <td>We trained a number of French-English SMT syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>1387</td>\n",
       "      <td>5</td>\n",
       "      <td>For building the baseline SMT system, we used ...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>1388</td>\n",
       "      <td>4</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>SMT translations: a phrase-based SMT system bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>1389</td>\n",
       "      <td>3</td>\n",
       "      <td>Following resource collection and construction...</td>\n",
       "      <td>We trained a model using Moses toolkit (Koehn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>1390</td>\n",
       "      <td>4</td>\n",
       "      <td>One semiautomatic approach to evaluation is RO...</td>\n",
       "      <td>The method, ROUGE (Lin and Hovy, 2003 ), is ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>1391</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Web 1T 5-gram corpus (Brants and Fr...</td>\n",
       "      <td>We used the unigram counts from the Web 1T 5-g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>1392</td>\n",
       "      <td>3</td>\n",
       "      <td>The SemEval-2015 Aspect Based Sentiment Analys...</td>\n",
       "      <td>This paper describes the approach of the SemaN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>1393</td>\n",
       "      <td>4</td>\n",
       "      <td>The 2009 Bio NLP shared task (Kim et al., 2009...</td>\n",
       "      <td>The recent BioNLP 2009 shared task (BioNLP09ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>1394</td>\n",
       "      <td>4</td>\n",
       "      <td>The constituent context model (CCM) for induci...</td>\n",
       "      <td>The CCM is a generative model for the unsuperv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>1395</td>\n",
       "      <td>4</td>\n",
       "      <td>The CCM is a generative model for the unsuperv...</td>\n",
       "      <td>The idea of representing a constituent by its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>1397</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the implementation provided by Tai et a...</td>\n",
       "      <td>We use the dependency tree long short-term mem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>1398</td>\n",
       "      <td>3</td>\n",
       "      <td>Europarl (Koehn, 2005) is a multilingual paral...</td>\n",
       "      <td>We use the French- English parallel corpus (ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>1399</td>\n",
       "      <td>5</td>\n",
       "      <td>The Stanford dependency parser (De Marneffe et...</td>\n",
       "      <td>We used the Stanford Dependency Parser (de Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>1400</td>\n",
       "      <td>5</td>\n",
       "      <td>The Stanford dependency parser (De Marneffe et...</td>\n",
       "      <td>We used the Stanford Dependency Parser (de Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>1901</td>\n",
       "      <td>4</td>\n",
       "      <td>The major part of data comes from current and ...</td>\n",
       "      <td>The data used for the experiments described in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>1903</td>\n",
       "      <td>2</td>\n",
       "      <td>We compare the proposed model to our implement...</td>\n",
       "      <td>The model architecture, shown in figure 1 , is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>1904</td>\n",
       "      <td>4</td>\n",
       "      <td>It is a phrase-based system built using the Mo...</td>\n",
       "      <td>SMT translations: a phrase-based SMT system bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>1905</td>\n",
       "      <td>4</td>\n",
       "      <td>In this section, we first discuss the hybrid t...</td>\n",
       "      <td>In Lu et al. (2008) , the mixgram model (an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>1906</td>\n",
       "      <td>4</td>\n",
       "      <td>The Europarl corpus (Koehn, 2005) is built fro...</td>\n",
       "      <td>We use the French- English parallel corpus (ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>1907</td>\n",
       "      <td>4</td>\n",
       "      <td>The Europarl corpus (Koehn, 2005) is built fro...</td>\n",
       "      <td>We use the French- English parallel corpus (ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>1908</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Adam (Kingma and Ba, 2015) for optimisa...</td>\n",
       "      <td>Each model was trained during 50 epochs using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>1909</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the open-source Moses toolkit (Koehn et...</td>\n",
       "      <td>We use the Moses phrase-based translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>1910</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the mkcls tool in GIZA++ (Och and Ney,...</td>\n",
       "      <td>We also used Giza++ word alignment tool (Och a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>1911</td>\n",
       "      <td>3</td>\n",
       "      <td>We then use the phrase extraction utility in t...</td>\n",
       "      <td>We use the Moses phrase-based translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>1912</td>\n",
       "      <td>3</td>\n",
       "      <td>Phrase pairs are extracted from IBM4 alignment...</td>\n",
       "      <td>Phrase pairs were extracted from symmetrized w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>1914</td>\n",
       "      <td>5</td>\n",
       "      <td>Specifically, we build off the Bayesian block ...</td>\n",
       "      <td>Our work is motivated by the Bayesian HMM appr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>1915</td>\n",
       "      <td>4</td>\n",
       "      <td>Like (Cho &amp; Chai (2000)) , our analysis also p...</td>\n",
       "      <td>Given the LP constraints in (4), (7), and (8),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1917</td>\n",
       "      <td>5</td>\n",
       "      <td>We propose a relatively simple and nuanced uns...</td>\n",
       "      <td>As mentioned above, we extend the WMF model pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>1918</td>\n",
       "      <td>4</td>\n",
       "      <td>Our work is also related to (Bunescu and Moone...</td>\n",
       "      <td>The dependency path is the shortest path betwe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>1919</td>\n",
       "      <td>5</td>\n",
       "      <td>Motivated by previous work, we include a frequ...</td>\n",
       "      <td>Outside of the rhetorical features, the discou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>1920</td>\n",
       "      <td>5</td>\n",
       "      <td>The highest performance levels were achieved u...</td>\n",
       "      <td>Table 8 to Table 12 show the Macro-Average (F ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>1921</td>\n",
       "      <td>4</td>\n",
       "      <td>In this rest of this paper, we discuss related...</td>\n",
       "      <td>This paper describes the details of our system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>1922</td>\n",
       "      <td>5</td>\n",
       "      <td>In principle, classifiers trained on PDTB data...</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>1926</td>\n",
       "      <td>5</td>\n",
       "      <td>In the next section we briefly review modeling...</td>\n",
       "      <td>We briefly review the HMM based word alignment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>1927</td>\n",
       "      <td>4</td>\n",
       "      <td>This result is statistically significant at p ...</td>\n",
       "      <td>The improvement is statistically significant a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>1928</td>\n",
       "      <td>4</td>\n",
       "      <td>Starting with TextRank (Mihalcea and Tarau, 20...</td>\n",
       "      <td>In the unsupervised approach, graph-based rank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>1929</td>\n",
       "      <td>5</td>\n",
       "      <td>Because of our experience with the Weka packag...</td>\n",
       "      <td>Especially, we use the WEKA (Hall et al., 2009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>1930</td>\n",
       "      <td>4</td>\n",
       "      <td>Dropout (Srivastava et al., 2014) is implement...</td>\n",
       "      <td>Dropout (Srivastava et al., 2014) is a very ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1931</td>\n",
       "      <td>3</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>1932</td>\n",
       "      <td>5</td>\n",
       "      <td>As mentioned in Section 3, we obtained depende...</td>\n",
       "      <td>We make use of dependency parse information fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1933</td>\n",
       "      <td>5</td>\n",
       "      <td>We tokenize and truecase all of the corpora us...</td>\n",
       "      <td>We preprocessed the training corpora with scri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1935</td>\n",
       "      <td>5</td>\n",
       "      <td>To obtain these we use the Stanford dependency...</td>\n",
       "      <td>We use the Stanford dependency parser (de Marn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>1936</td>\n",
       "      <td>5</td>\n",
       "      <td>The NMT models are trained using Adam optimize...</td>\n",
       "      <td>All parameters are learned by Adam optimizer (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1937</td>\n",
       "      <td>4</td>\n",
       "      <td>The improved alignments gave a gain of Table 8...</td>\n",
       "      <td>One of the phrase-based systems moreover utili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>1938</td>\n",
       "      <td>5</td>\n",
       "      <td>The statistical significance tests using 95% c...</td>\n",
       "      <td>A statistical significance test was performed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>1939</td>\n",
       "      <td>5</td>\n",
       "      <td>To obtain these we use the Stanford dependency...</td>\n",
       "      <td>We use the Stanford dependency parser (de Marn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>1941</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the AdaGrad algorithm (Duchi et al., 20...</td>\n",
       "      <td>We use online learning to train model paramete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>1942</td>\n",
       "      <td>5</td>\n",
       "      <td>The other is from (Jeffrey Pennington et al. (...</td>\n",
       "      <td>We use the 100-dimensional GloVe word embeddin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>1943</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the AdaGrad optimizer (Duchi et al., 20...</td>\n",
       "      <td>We use online learning to train model paramete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>1944</td>\n",
       "      <td>4</td>\n",
       "      <td>We experiment with the phrase-based statistica...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>1945</td>\n",
       "      <td>3</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>1946</td>\n",
       "      <td>4</td>\n",
       "      <td>We follow the definition in Cohen et al. (2012...</td>\n",
       "      <td>We also use these perturbation schemes to crea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>1947</td>\n",
       "      <td>3</td>\n",
       "      <td>The language model is a 5-gram KenLM (Heafield...</td>\n",
       "      <td>We trained an English 5-gram language model us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>1949</td>\n",
       "      <td>5</td>\n",
       "      <td>As mentioned in Section 3, we obtained depende...</td>\n",
       "      <td>We make use of dependency parse information fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>1950</td>\n",
       "      <td>4</td>\n",
       "      <td>Also, we evaluate on the RTE part of the SICK ...</td>\n",
       "      <td>The second is the RTE part of the SICK dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>1951</td>\n",
       "      <td>3</td>\n",
       "      <td>Our model has a\" Siamese\" structure (Bromley e...</td>\n",
       "      <td>Most previous work use sentence modeling with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>1953</td>\n",
       "      <td>5</td>\n",
       "      <td>The dialogue act labelling of the corpus follo...</td>\n",
       "      <td>The classes used to train the DATE tagger are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>1954</td>\n",
       "      <td>4</td>\n",
       "      <td>We also obtain the dependency parse of the sen...</td>\n",
       "      <td>To examine the effect of normalization on depe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>1955</td>\n",
       "      <td>4</td>\n",
       "      <td>The resulting matrix is weighted using pointwi...</td>\n",
       "      <td>We construct word-word co-occurrence matrix X;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>1956</td>\n",
       "      <td>5</td>\n",
       "      <td>We extract structured facts using two methods:...</td>\n",
       "      <td>We extract facts from captions using Clausie (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>1957</td>\n",
       "      <td>4</td>\n",
       "      <td>Dropout (Srivastava et al., 2014) is implement...</td>\n",
       "      <td>Next, the output of the max-pooling layer is p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>1958</td>\n",
       "      <td>3</td>\n",
       "      <td>The population distribution was estimated by t...</td>\n",
       "      <td>The bootstrap sampling method provides a way f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>1959</td>\n",
       "      <td>5</td>\n",
       "      <td>The statistical significance test is also carr...</td>\n",
       "      <td>The significance testing is performed by paire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>1960</td>\n",
       "      <td>3</td>\n",
       "      <td>The bootstrap sampling method provides a way f...</td>\n",
       "      <td>The population distribution was estimated by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>1961</td>\n",
       "      <td>3</td>\n",
       "      <td>For medical we use the biomedical data from EM...</td>\n",
       "      <td>For French-English experiments, we used the EM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>1962</td>\n",
       "      <td>5</td>\n",
       "      <td>The MT experiments were carried out using the ...</td>\n",
       "      <td>The translation system is trained using the we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>1963</td>\n",
       "      <td>4</td>\n",
       "      <td>For the theory of Cho &amp; Chai (2000) to be comp...</td>\n",
       "      <td>Then we revise the two LP constraints of Cho &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>1964</td>\n",
       "      <td>5</td>\n",
       "      <td>3 With these trees fixed, the partial derivati...</td>\n",
       "      <td>Derivatives are computed efficiently via backp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>1965</td>\n",
       "      <td>5</td>\n",
       "      <td>Many researchers have considered generating pa...</td>\n",
       "      <td>Distributional models of meaning follow the di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>1966</td>\n",
       "      <td>5</td>\n",
       "      <td>We used Weka (Hall et al., 2009) for all our c...</td>\n",
       "      <td>We use the WEKA toolkit (Hall et al., 2009) fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>1967</td>\n",
       "      <td>3</td>\n",
       "      <td>Bilingual Corpora The corpus used in the follo...</td>\n",
       "      <td>The experiments are carried out on a subset of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>1968</td>\n",
       "      <td>4</td>\n",
       "      <td>First, we used the Moses toolkit (Koehn et al....</td>\n",
       "      <td>The Moses toolkit (Koehn et al., 2007) is used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>1969</td>\n",
       "      <td>3</td>\n",
       "      <td>Another parallel corpus is the JRC-Acquis Mult...</td>\n",
       "      <td>corpus (Steinberger et al., 2006 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>1970</td>\n",
       "      <td>3</td>\n",
       "      <td>The parallel corpus is wordaligned using GIZA+...</td>\n",
       "      <td>This is done using IBM Model 1 (Brown et al., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>1972</td>\n",
       "      <td>2</td>\n",
       "      <td>We thus cast MSC as a semantic sentence classi...</td>\n",
       "      <td>We compare the proposed model to our implement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>1973</td>\n",
       "      <td>5</td>\n",
       "      <td>All these features are inherited from Moses (K...</td>\n",
       "      <td>All the experiments are carried out in Moses t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>1974</td>\n",
       "      <td>3</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "      <td>For our experiments, we use 40,000 sentences f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>1975</td>\n",
       "      <td>3</td>\n",
       "      <td>The texts were first automatically segmented a...</td>\n",
       "      <td>The data was tagged using TnT (Brants, 2000 ),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>1976</td>\n",
       "      <td>5</td>\n",
       "      <td>Secondly, Holmqvist et al. (2012) reordered so...</td>\n",
       "      <td>Holmqvist et al. (2012) presented a method whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>1977</td>\n",
       "      <td>5</td>\n",
       "      <td>For language modeling, we use the English Giga...</td>\n",
       "      <td>Language Model: For all 3-SCFG systems we use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>1978</td>\n",
       "      <td>5</td>\n",
       "      <td>For our LDA implementations, we use MALLET (Mc...</td>\n",
       "      <td>For training bilingual topic models, we use Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1979</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the standard Stanford-style set of depe...</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1980</td>\n",
       "      <td>4</td>\n",
       "      <td>Next we evaluate how well the complexity measu...</td>\n",
       "      <td>We also compare annotation strategies in terms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1981</td>\n",
       "      <td>3</td>\n",
       "      <td>Syntax in EPEC is annotated following the depe...</td>\n",
       "      <td>For the German experiments, we used the NEGRA ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1983</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the scikit-learn (Pedregosa et al., 20...</td>\n",
       "      <td>We use the scikit implementation of SVM (Pedre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1984</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the scikit-learn toolkit to train our ...</td>\n",
       "      <td>For Indep-Logistic, we used scikit-learn (Pedr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1985</td>\n",
       "      <td>5</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "      <td>We trained a standard Moses baseline (Koehn et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1986</td>\n",
       "      <td>5</td>\n",
       "      <td>These features were obtained using the Stanfor...</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1987</td>\n",
       "      <td>3</td>\n",
       "      <td>We built a 5-gram language model on the Englis...</td>\n",
       "      <td>One is a 3-gram language model built using Ken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1988</td>\n",
       "      <td>3</td>\n",
       "      <td>In the other side, the French corpus is part-o...</td>\n",
       "      <td>1 The corpus is lemmatised and tagged by part-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1989</td>\n",
       "      <td>3</td>\n",
       "      <td>The COMLEX syntax dictionary (Grishman et al.,...</td>\n",
       "      <td>The COMLEX syntax dictionary (Grishman et al.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>1990</td>\n",
       "      <td>5</td>\n",
       "      <td>We lemmatise the head of each constituent with...</td>\n",
       "      <td>POS tagging was performed with the TreeTagger ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>1991</td>\n",
       "      <td>5</td>\n",
       "      <td>with the KenLM toolkit (Heafield, 2011 ).</td>\n",
       "      <td>For language modeling we used the KenLM toolki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>1992</td>\n",
       "      <td>5</td>\n",
       "      <td>We briefly review the HMM based word alignment...</td>\n",
       "      <td>Och is the HMM alignment model of (Och and Ney...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1993</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "      <td>These features were obtained using the Stanfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>1995</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "      <td>We use the standard Stanford-style set of depe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>1996</td>\n",
       "      <td>4</td>\n",
       "      <td>All the summaries are evaluated using ROUGE (L...</td>\n",
       "      <td>For the summarization task, we compare results...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>1997</td>\n",
       "      <td>4</td>\n",
       "      <td>The reported confidence intervals were estimat...</td>\n",
       "      <td>We calculated significance using paired bootst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>1999</td>\n",
       "      <td>4</td>\n",
       "      <td>We implemented CharWNN using the Theano librar...</td>\n",
       "      <td>Gradients computed using the automatic differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>2000</td>\n",
       "      <td>5</td>\n",
       "      <td>For the linear logistic regression implementat...</td>\n",
       "      <td>For Indep-Logistic, we used scikit-learn (Pedr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>2501</td>\n",
       "      <td>4</td>\n",
       "      <td>The tagger we use is TnT (Brants, 2000) , a hi...</td>\n",
       "      <td>The data was tagged using TnT (Brants, 2000 ),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>2502</td>\n",
       "      <td>3</td>\n",
       "      <td>We have used the implementation described in (...</td>\n",
       "      <td>1 Regarding the learning algorithm, we used ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>2503</td>\n",
       "      <td>3</td>\n",
       "      <td>We built a 5-gram language model on the Englis...</td>\n",
       "      <td>We build our PB-SMT systems in a standard way ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>2504</td>\n",
       "      <td>3</td>\n",
       "      <td>DELPH-IN Minimal Recursion Semantics (DM) As p...</td>\n",
       "      <td>As output, the grammar delivers detailed seman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>2505</td>\n",
       "      <td>5</td>\n",
       "      <td>Like the CoNLL-2006 shared task, the 2007 shar...</td>\n",
       "      <td>One of the first venues at which domain adapta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>2506</td>\n",
       "      <td>4</td>\n",
       "      <td>Like (Cho &amp; Chai 2000 ), our analysis also pro...</td>\n",
       "      <td>However, like Cho &amp; Chai (2000) , our analysis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>2507</td>\n",
       "      <td>4</td>\n",
       "      <td>The phrase table is extracted from a bilingual...</td>\n",
       "      <td>A core component of every PBSMT system is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>2508</td>\n",
       "      <td>4</td>\n",
       "      <td>Google Web 1T (Brants and Franz, 2006) has bee...</td>\n",
       "      <td>The Google Web 1T data (Brants and Franz, 2006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>2509</td>\n",
       "      <td>5</td>\n",
       "      <td>We first use a dependency parser (de Marneffe ...</td>\n",
       "      <td>A complete set of parser tags and the method u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>2510</td>\n",
       "      <td>5</td>\n",
       "      <td>We first use a dependency parser (de Marneffe ...</td>\n",
       "      <td>A complete set of parser tags and the method u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>2511</td>\n",
       "      <td>5</td>\n",
       "      <td>These analyses provide an alternative but theo...</td>\n",
       "      <td>Liang et al. (2006) observe that standard upda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>2512</td>\n",
       "      <td>5</td>\n",
       "      <td>We experiment with the phrase-based statistica...</td>\n",
       "      <td>We present a system that takes a general Moses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>2513</td>\n",
       "      <td>3</td>\n",
       "      <td>For our corpus, we randomly selected documents...</td>\n",
       "      <td>New York Times consists of 500 random sentence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>2514</td>\n",
       "      <td>4</td>\n",
       "      <td>They used the Web-based annotation tool brat (...</td>\n",
       "      <td>We consider the following types of implicit re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>2515</td>\n",
       "      <td>4</td>\n",
       "      <td>All corpora were taken from the CHILDES databa...</td>\n",
       "      <td>The eval-uation set was comprised of adult utt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>2516</td>\n",
       "      <td>5</td>\n",
       "      <td>The system generated tweets were evaluated usi...</td>\n",
       "      <td>The summaries from the above algorithm for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>2517</td>\n",
       "      <td>5</td>\n",
       "      <td>The TOEFL synonym selection task is to select ...</td>\n",
       "      <td>The TOEFL synonym dataset (Landauer and Dumais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>2518</td>\n",
       "      <td>4</td>\n",
       "      <td>We calculate our features using the KenLM tool...</td>\n",
       "      <td>1 We used the KenLM toolkit (Heafield, 2011) t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>2519</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the scikit implementation of Random For...</td>\n",
       "      <td>The regressor used is a Random Forest Regresso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>2521</td>\n",
       "      <td>3</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "      <td>The statistical dictionary for this task was e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>2522</td>\n",
       "      <td>5</td>\n",
       "      <td>The statistical significance tests using 95% c...</td>\n",
       "      <td>We used bootstrap resampling for testing stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>2523</td>\n",
       "      <td>5</td>\n",
       "      <td>We then run word alignment with GIZA++ (Och an...</td>\n",
       "      <td>We used the GIZA++ software (Och and Ney, 2003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>2524</td>\n",
       "      <td>4</td>\n",
       "      <td>We built a 5-gram language model on the Englis...</td>\n",
       "      <td>We calculate our features using the KenLM tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>2525</td>\n",
       "      <td>5</td>\n",
       "      <td>We used GIZA++ (Och and Ney, 2003) to align th...</td>\n",
       "      <td>We then run word alignment with GIZA++ (Och an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>2526</td>\n",
       "      <td>5</td>\n",
       "      <td>The phrase tables were generated by means of s...</td>\n",
       "      <td>The word alignment was obtained by running Giz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>2527</td>\n",
       "      <td>5</td>\n",
       "      <td>Zhu et al. (2013) applied Kalman filter model ...</td>\n",
       "      <td>In the scenario of human-computer interactive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>2528</td>\n",
       "      <td>4</td>\n",
       "      <td>( Koo et al. 2008)  have proposed to use word ...</td>\n",
       "      <td>In order to reduce the amount of annotated dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>2529</td>\n",
       "      <td>4</td>\n",
       "      <td>More recently, (Pasha et al., 2014) created MA...</td>\n",
       "      <td>Next, the files are processed with the morphol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>2530</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>For building our SMT systems, the open-source ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>2531</td>\n",
       "      <td>4</td>\n",
       "      <td>@BULLET Naive Bayes(NB): We use Binomial varia...</td>\n",
       "      <td>We use the Bernoulli Naive Bayes classifier in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>2532</td>\n",
       "      <td>5</td>\n",
       "      <td>We use AdaGrad (Duchi et al., 2011)  with the ...</td>\n",
       "      <td>We use online learning to train model paramete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>2533</td>\n",
       "      <td>5</td>\n",
       "      <td>The MT experiments were carried out using the ...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>2534</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the AdaGrad method (Duchi et al., 2011)...</td>\n",
       "      <td>We use online learning to train model paramete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>2535</td>\n",
       "      <td>4</td>\n",
       "      <td>Among the existing sense-tagged corpora, the S...</td>\n",
       "      <td>The SEMCOR corpus (Miller et al., 1994) is one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>2536</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Liblinear Support Vector Machine (S...</td>\n",
       "      <td>A linear-kernel Support Vector Machine (Chang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>2537</td>\n",
       "      <td>5</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "      <td>We tokenize and truecase all of the corpora us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>2538</td>\n",
       "      <td>5</td>\n",
       "      <td>We built a source-to-target PB-SMT model from ...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>2539</td>\n",
       "      <td>5</td>\n",
       "      <td>The dictionaries are automatically generated v...</td>\n",
       "      <td>We then made use of the GIZA++ software (Och a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2540</td>\n",
       "      <td>3</td>\n",
       "      <td>There are two main approaches to processing no...</td>\n",
       "      <td>There are two approaches proposed in the liter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>2541</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>We trained a model using Moses toolkit (Koehn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>2542</td>\n",
       "      <td>4</td>\n",
       "      <td>The task of identifying mentions to medical co...</td>\n",
       "      <td>We found the largest of such corpus to be the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>2543</td>\n",
       "      <td>5</td>\n",
       "      <td>We built a source-to-target PB-SMT model from ...</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>2545</td>\n",
       "      <td>5</td>\n",
       "      <td>We also use MADA+TOKAN (Habash et al., 2009) t...</td>\n",
       "      <td>We use the MADA package (Habash et al., 2009) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>2546</td>\n",
       "      <td>5</td>\n",
       "      <td>Both training and testing data consist of PubM...</td>\n",
       "      <td>The data provided for the shared task is prepa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>2547</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>We built a source-to-target PB-SMT model from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>2548</td>\n",
       "      <td>5</td>\n",
       "      <td>To extract our part-of-speech (POS) features, ...</td>\n",
       "      <td>We used the Brill tagger provided by NLTK for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>2549</td>\n",
       "      <td>5</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "      <td>We tokenize and truecase all of the corpora us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>2550</td>\n",
       "      <td>5</td>\n",
       "      <td>We trained a model using Moses toolkit (Koehn ...</td>\n",
       "      <td>Our baseline is a phrase-based MT system train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>2551</td>\n",
       "      <td>4</td>\n",
       "      <td>For seed and test paradigms we used verbal inf...</td>\n",
       "      <td>For English, we used the CELEX database (Baaye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>2552</td>\n",
       "      <td>5</td>\n",
       "      <td>Then we did word alignment using GIZA++ (Och a...</td>\n",
       "      <td>We then run word alignment with GIZA++ (Och an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>2553</td>\n",
       "      <td>4</td>\n",
       "      <td>( Pang et al. 2002) have reported the effectiv...</td>\n",
       "      <td>Pang et al. (2002)  use machine learning metho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>2555</td>\n",
       "      <td>5</td>\n",
       "      <td>We train the concept identification stage usin...</td>\n",
       "      <td>We use online learning to train model paramete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>2556</td>\n",
       "      <td>3</td>\n",
       "      <td>For the contextual check we use the Google Web...</td>\n",
       "      <td>The Web1T corpus (Brants and Franz, 2006) is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>2557</td>\n",
       "      <td>5</td>\n",
       "      <td>We built a source-to-target PB-SMT model from ...</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>2558</td>\n",
       "      <td>4</td>\n",
       "      <td>@BULLET Naive Bayes(NB): We use Binomial varia...</td>\n",
       "      <td>@BULLET Logistic Regression(LR): We use Logist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>2559</td>\n",
       "      <td>4</td>\n",
       "      <td>We built a source-to-target PB-SMT model from ...</td>\n",
       "      <td>We tokenize and frequent-case the data with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>2560</td>\n",
       "      <td>5</td>\n",
       "      <td>An unpruned, modified Kneser-Ney-smoothed 4-gr...</td>\n",
       "      <td>The language model is a standard 5-gram model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>2561</td>\n",
       "      <td>4</td>\n",
       "      <td>For native data, several teams make use of the...</td>\n",
       "      <td>We used the unigram counts from the Web 1T 5-g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>2562</td>\n",
       "      <td>4</td>\n",
       "      <td>We measure statistical significance using 95% ...</td>\n",
       "      <td>The column\" pair-CI\" shows 95% confidence inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>2563</td>\n",
       "      <td>4</td>\n",
       "      <td>1 The models are constructed using C4.5 decisi...</td>\n",
       "      <td>The Weka SMO implementation of SVM (Hall et al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>2564</td>\n",
       "      <td>5</td>\n",
       "      <td>It therefore follows the distributional hypoth...</td>\n",
       "      <td>Distributional similarity relies on the distri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>2565</td>\n",
       "      <td>4</td>\n",
       "      <td>Consider the two examples below, drawn from th...</td>\n",
       "      <td>Pitler and Nenkova (2008) used discourse relat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>2566</td>\n",
       "      <td>5</td>\n",
       "      <td>BLE is based on the distributional hypothesis ...</td>\n",
       "      <td>Distributional similarity relies on the distri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>2567</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the BLEU score as primary criterion whi...</td>\n",
       "      <td>The optimization is done using the Downhill Si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>2568</td>\n",
       "      <td>4</td>\n",
       "      <td>The relation prediction task of Science IE is ...</td>\n",
       "      <td>The SemEval-2010 Task 8 dataset is a widely us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>2569</td>\n",
       "      <td>3</td>\n",
       "      <td>Machine translation system settings: We used a...</td>\n",
       "      <td>We compare the model against the Moses phrase-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>2570</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the MRS analyses that are produced by t...</td>\n",
       "      <td>The experiments are carried out on a broad-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>2571</td>\n",
       "      <td>5</td>\n",
       "      <td>In each case, the improvement of EBMT TM + SMT...</td>\n",
       "      <td>In all cases, results are statistically signif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2572</td>\n",
       "      <td>4</td>\n",
       "      <td>To capture typical part-of bridging (see Examp...</td>\n",
       "      <td>We extract a list containing around 4,000 rela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>2574</td>\n",
       "      <td>5</td>\n",
       "      <td>The surfacesyntactic representation ? p was a ...</td>\n",
       "      <td>The number of features extracted from the PDT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>2575</td>\n",
       "      <td>5</td>\n",
       "      <td>In our work, like Hernault et al. (2010) , we ...</td>\n",
       "      <td>Similar to the work of Hernault et al. (2010) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>2576</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the data that were recorded and preproc...</td>\n",
       "      <td>1 Full details of the experimental protocol, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>2577</td>\n",
       "      <td>5</td>\n",
       "      <td>We trained non-projective dependency parsers f...</td>\n",
       "      <td>For non-projective parsing experiments, four l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>2578</td>\n",
       "      <td>4</td>\n",
       "      <td>Our LP constraints based on the new type marke...</td>\n",
       "      <td>For the theory of Cho &amp; Chai (2000) to be comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>2579</td>\n",
       "      <td>2</td>\n",
       "      <td>The WSJ grammar covers the UPenn Wall Street J...</td>\n",
       "      <td>The approach presented in this paper is a firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>2580</td>\n",
       "      <td>5</td>\n",
       "      <td>Statistically significant results, calculated ...</td>\n",
       "      <td>We calculated significance using paired bootst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2581</td>\n",
       "      <td>4</td>\n",
       "      <td>In testing, we used minimum Bayes risk decodin...</td>\n",
       "      <td>selects the translation with minimum Bayes ris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2582</td>\n",
       "      <td>3</td>\n",
       "      <td>selects the translation with minimum Bayes ris...</td>\n",
       "      <td>Additionally, we will compare two decision rul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>2583</td>\n",
       "      <td>4</td>\n",
       "      <td>We use a prototype-based selectional preferenc...</td>\n",
       "      <td>We build on a recent selectional preference mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>2584</td>\n",
       "      <td>4</td>\n",
       "      <td>We used 10-fold cross-validation, set the conf...</td>\n",
       "      <td>ROUGE-2 metric (Lin, 2004) is used for the eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>2585</td>\n",
       "      <td>3</td>\n",
       "      <td>The Brown Corpus tagged with WordNet senses (M...</td>\n",
       "      <td>The senses in WordNet are ordered according to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>2586</td>\n",
       "      <td>5</td>\n",
       "      <td>As an implementation, we use SVM light (Joachi...</td>\n",
       "      <td>As a supervised classifier for VSM and WIKI, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>2587</td>\n",
       "      <td>4</td>\n",
       "      <td>For translation tables , the Moses system (Koe...</td>\n",
       "      <td>We compare the model against the Moses phrase-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>2588</td>\n",
       "      <td>5</td>\n",
       "      <td>The particle filter of Canini et al. (2009) re...</td>\n",
       "      <td>The particle filter studied empirically by Can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>2589</td>\n",
       "      <td>4</td>\n",
       "      <td>We describe an approximation to the BLEU score...</td>\n",
       "      <td>We here describe a linear approximation to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>2590</td>\n",
       "      <td>5</td>\n",
       "      <td>We applied the Naive Bayes probabilistic super...</td>\n",
       "      <td>We conducted experiments using Multinomial Nai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>2593</td>\n",
       "      <td>4</td>\n",
       "      <td>We minimize the cross entropy loss using gradi...</td>\n",
       "      <td>The network is trained using SGD with shuffled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>2594</td>\n",
       "      <td>5</td>\n",
       "      <td>Statistical significance in BLEU score differe...</td>\n",
       "      <td>Statistical significance of the difference bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>2595</td>\n",
       "      <td>5</td>\n",
       "      <td>We measure statistical significance using 95% ...</td>\n",
       "      <td>For all results, we computed their confidence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2596</td>\n",
       "      <td>3</td>\n",
       "      <td>3 As verbs, we take all tags that map to V in ...</td>\n",
       "      <td>In the POS tag level, we basically used the un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>2597</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the MRS analyses that are produced by t...</td>\n",
       "      <td>Here, we use (1) the Link Grammar Parser 8 and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2598</td>\n",
       "      <td>5</td>\n",
       "      <td>We measure statistical significance using 95% ...</td>\n",
       "      <td>For statistical significance testing, we use a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>2599</td>\n",
       "      <td>5</td>\n",
       "      <td>On Semantic Role Labeling Gildea and Jurafsky ...</td>\n",
       "      <td>Gildea and Jurafsky (2002)  were the first to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2600</td>\n",
       "      <td>4</td>\n",
       "      <td>We apply the stochastic gradient descent algor...</td>\n",
       "      <td>We use a minibatch size of 100, and use AdaDel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>3101</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the Linear SVM implementation (with de...</td>\n",
       "      <td>We use the scikit implementation of SVM (Pedre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>3102</td>\n",
       "      <td>5</td>\n",
       "      <td>We test the statistical significance of differ...</td>\n",
       "      <td>Statistical significance tests are performed u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>3103</td>\n",
       "      <td>5</td>\n",
       "      <td>In particular, the parser implements the arc-s...</td>\n",
       "      <td>1 The parser implements the arc-standard algor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>3104</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Adadelta (Zeiler, 2012) to update the p...</td>\n",
       "      <td>In the training procedure, we use AdaDelta (Ze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>3105</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "      <td>The systems for the English ? Spanish translat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>3106</td>\n",
       "      <td>3</td>\n",
       "      <td>CCG is a lexicalized theory of grammar (Steedm...</td>\n",
       "      <td>The input of the Boxer system is a syntactic a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>3107</td>\n",
       "      <td>5</td>\n",
       "      <td>The results for TESLA-M and TESLA-F have previ...</td>\n",
       "      <td>TESLA (Translation Evaluation of Sentences wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>3108</td>\n",
       "      <td>3</td>\n",
       "      <td>The semantic representation is Minimal Recursi...</td>\n",
       "      <td>The ERG produces Minimal Recursion Semantics (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>3109</td>\n",
       "      <td>4</td>\n",
       "      <td>GermaNet (GN) is the German counterpart to WN ...</td>\n",
       "      <td>Future work could be to extend the German data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>3110</td>\n",
       "      <td>3</td>\n",
       "      <td>(Baroni et al 2002)  report that 47% of the vo...</td>\n",
       "      <td>Baroni et al. (2002)  analyzed the 28 million ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>3111</td>\n",
       "      <td>4</td>\n",
       "      <td>Both language models use modified Kneser-Ney s...</td>\n",
       "      <td>5-gram language models are trained over the ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>3112</td>\n",
       "      <td>5</td>\n",
       "      <td>We used MALLET (McCallum, 2002) for this exper...</td>\n",
       "      <td>10 We used the PLTM implementation in Mallet (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>3114</td>\n",
       "      <td>5</td>\n",
       "      <td>We calculated significance using paired bootst...</td>\n",
       "      <td>The statistical significance test is also carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>3115</td>\n",
       "      <td>5</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "      <td>Finally, we used Moses toolkit as phrase-based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>3116</td>\n",
       "      <td>5</td>\n",
       "      <td>For building the baseline SMT system, we used ...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>3117</td>\n",
       "      <td>3</td>\n",
       "      <td>This results in the semantic triple shot(man,b...</td>\n",
       "      <td>We also lemmatized all words using Stanford Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>3119</td>\n",
       "      <td>4</td>\n",
       "      <td>We used standard classifiers available in scik...</td>\n",
       "      <td>3 We used logistic regression (though linear S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>3120</td>\n",
       "      <td>3</td>\n",
       "      <td>Rhetorical Structure Theory (Mann and Thompson...</td>\n",
       "      <td>The closest area to our work consists of inves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>3121</td>\n",
       "      <td>4</td>\n",
       "      <td>Both language models use modified Kneser-Ney s...</td>\n",
       "      <td>5-gram language models are trained over the ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>3123</td>\n",
       "      <td>5</td>\n",
       "      <td>We train the concept identification stage usin...</td>\n",
       "      <td>1 using AdaGrad (Duchi et al., 2011 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>3124</td>\n",
       "      <td>5</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "      <td>All our translation systems are based on Moses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>3125</td>\n",
       "      <td>5</td>\n",
       "      <td>We use ROUGE score as our evaluation metric (L...</td>\n",
       "      <td>We expect this restriction is more consistent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>3126</td>\n",
       "      <td>5</td>\n",
       "      <td>For building the baseline SMT system, we used ...</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>3127</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Europarl parallel corpus (Koehn, 20...</td>\n",
       "      <td>We extract our paraphrase grammar from the Fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>3128</td>\n",
       "      <td>3</td>\n",
       "      <td>We use Ridge Regression (RR) with l2-norm regu...</td>\n",
       "      <td>@BULLET Logistic Regression(LR): We use Logist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>3129</td>\n",
       "      <td>4</td>\n",
       "      <td>All data used in our experiments are sentence-...</td>\n",
       "      <td>We tokenize English data and segment Chinese d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>3130</td>\n",
       "      <td>4</td>\n",
       "      <td>Since the phrase table contains lemmas, the Wi...</td>\n",
       "      <td>1 The corpus is lemmatised and tagged by part-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>3131</td>\n",
       "      <td>5</td>\n",
       "      <td>For this purpose we used the logistic regressi...</td>\n",
       "      <td>We use logistic regression with L2 regularizat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>3132</td>\n",
       "      <td>5</td>\n",
       "      <td>All our systems are contrasted with a standard...</td>\n",
       "      <td>Our machine translation systems this year are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>3133</td>\n",
       "      <td>4</td>\n",
       "      <td>We test our metrics in the setting of the WMT ...</td>\n",
       "      <td>The compared systems are evaluated on the Engl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>3134</td>\n",
       "      <td>4</td>\n",
       "      <td>These classifiers have been used in related wo...</td>\n",
       "      <td>We evaluated our method with movie review docu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>3135</td>\n",
       "      <td>4</td>\n",
       "      <td>One is from (Turian et al, 2010) , the dimensi...</td>\n",
       "      <td>For example, Turian et al. (2010)  showed that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>3136</td>\n",
       "      <td>5</td>\n",
       "      <td>Word alignment using GIZA++ toolkit (Och and N...</td>\n",
       "      <td>Word alignment is performed by GIZA++ (Och and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>3137</td>\n",
       "      <td>5</td>\n",
       "      <td>We found that using AdaGrad (Duchi et al., 201...</td>\n",
       "      <td>1 using AdaGrad (Duchi et al., 2011 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>3138</td>\n",
       "      <td>5</td>\n",
       "      <td>The Penn Discourse Treebank (PDTB, Prasad et a...</td>\n",
       "      <td>One of the most important resources for discou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>3139</td>\n",
       "      <td>5</td>\n",
       "      <td>The system was trained on the English and Dani...</td>\n",
       "      <td>The EuroParl data set consists of 707 sentence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>3140</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, Turian et al. (2010) compared Bro...</td>\n",
       "      <td>Turian et al. (2010) applied word embeddings t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>3141</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "      <td>In particular, we use Moses (Koehn et al., 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>3142</td>\n",
       "      <td>3</td>\n",
       "      <td>Past experiences on this system have shown tha...</td>\n",
       "      <td>Our submitted system for the second task is ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>3143</td>\n",
       "      <td>5</td>\n",
       "      <td>We trained a number of French-English SMT syst...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>3144</td>\n",
       "      <td>5</td>\n",
       "      <td>@BULLET Zhang2015: Zhang et al. (2015) propose...</td>\n",
       "      <td>Zhang et al. (2015) explore a shallow convolut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>3145</td>\n",
       "      <td>4</td>\n",
       "      <td>All data used in our experiments are sentence-...</td>\n",
       "      <td>We tokenize English data and segment Chinese d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>3146</td>\n",
       "      <td>5</td>\n",
       "      <td>The major part of data comes from current and ...</td>\n",
       "      <td>The source of bilingual data used in the exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>3147</td>\n",
       "      <td>5</td>\n",
       "      <td>We expect this restriction is more consistent ...</td>\n",
       "      <td>ROUGE (Lin, 2004) is the fully automatic metri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>3148</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>3149</td>\n",
       "      <td>5</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>3150</td>\n",
       "      <td>4</td>\n",
       "      <td>The decoder searches for the best translation ...</td>\n",
       "      <td>The log-linear approach to phrase-based transl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>3151</td>\n",
       "      <td>5</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>3152</td>\n",
       "      <td>5</td>\n",
       "      <td>The usages from the ukWaC are tokenised and le...</td>\n",
       "      <td>3 All English data are POS tagged and lemmatis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>3153</td>\n",
       "      <td>4</td>\n",
       "      <td>To quantify the redundancy of structures, we p...</td>\n",
       "      <td>The trigram target language model is trained f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>3154</td>\n",
       "      <td>5</td>\n",
       "      <td>Baseline word alignments were obtained by runn...</td>\n",
       "      <td>Translation models were trained over the bilin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>3156</td>\n",
       "      <td>3</td>\n",
       "      <td>The results displayed in Table 3 are obtained ...</td>\n",
       "      <td>The core model is a decision tree classifier t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>3157</td>\n",
       "      <td>5</td>\n",
       "      <td>We trained a number of French-English SMT syst...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>3158</td>\n",
       "      <td>3</td>\n",
       "      <td>The performance of our algorithm is compared w...</td>\n",
       "      <td>Note that word sense disambiguation is perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>3159</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the scikit implementation of SVM (Pedre...</td>\n",
       "      <td>We use a well-known scikitlearn (Pedregosa et ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>3160</td>\n",
       "      <td>4</td>\n",
       "      <td>For example, Turian et al. (2010) showed that ...</td>\n",
       "      <td>Turian et al. (2010) applied word embeddings t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>3161</td>\n",
       "      <td>3</td>\n",
       "      <td>EASE uses NLTK (Bird et al., 2009) for POS tag...</td>\n",
       "      <td>We used the Brill tagger provided by NLTK for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>3163</td>\n",
       "      <td>5</td>\n",
       "      <td>All our translation systems are based on Moses...</td>\n",
       "      <td>All our systems are contrasted with a standard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>3164</td>\n",
       "      <td>5</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>3165</td>\n",
       "      <td>5</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>3166</td>\n",
       "      <td>4</td>\n",
       "      <td>We thank Gbor Recski (HAS Research Institute f...</td>\n",
       "      <td>We employed the state-of-the-art sentiment ann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>3167</td>\n",
       "      <td>4</td>\n",
       "      <td>It has been shown in previous work on relation...</td>\n",
       "      <td>Our work is also related to (Bunescu and Moone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>3168</td>\n",
       "      <td>4</td>\n",
       "      <td>These algorithms were used to participate in t...</td>\n",
       "      <td>The system that obtained the best performance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>3169</td>\n",
       "      <td>5</td>\n",
       "      <td>Phrase-Based SMT system: a standard non factor...</td>\n",
       "      <td>SMT translations: a phrase-based SMT system bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>3171</td>\n",
       "      <td>4</td>\n",
       "      <td>For the theory of Cho &amp; Chai (2000) to be comp...</td>\n",
       "      <td>Given the LP constraints in (4), (7), and (8),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>3172</td>\n",
       "      <td>5</td>\n",
       "      <td>All linguistic annotations needed for features...</td>\n",
       "      <td>Part of speech tagging and named entity recogn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>3173</td>\n",
       "      <td>3</td>\n",
       "      <td>Algorithm 5 shows a Passive-Aggressive algorit...</td>\n",
       "      <td>To build a parser, we use a structured classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>3175</td>\n",
       "      <td>4</td>\n",
       "      <td>The task is part of the Semantic Evaluation 20...</td>\n",
       "      <td>Semantic Textual Similarity (STS) is the task ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>3176</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the same annotation guidelines as Zaid...</td>\n",
       "      <td>We also use A0 to compare against the\" masking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>3177</td>\n",
       "      <td>4</td>\n",
       "      <td>The Stanford parser 1 (Marneffe et al., 2006) ...</td>\n",
       "      <td>The same kind of process was applied to the Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>3178</td>\n",
       "      <td>5</td>\n",
       "      <td>1 After tokenization , we lemmatize and stem t...</td>\n",
       "      <td>For Reuters we segmented and tokenized the dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>3179</td>\n",
       "      <td>5</td>\n",
       "      <td>Machine translation system settings: We used a...</td>\n",
       "      <td>We use the Moses phrase-based translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>3180</td>\n",
       "      <td>4</td>\n",
       "      <td>The phrase table is extracted from a bilingual...</td>\n",
       "      <td>The corpus is first word-aligned using a word ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>3181</td>\n",
       "      <td>3</td>\n",
       "      <td>This architecture is very similar to the frame...</td>\n",
       "      <td>3 DKPro is a collection of software components...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>3182</td>\n",
       "      <td>5</td>\n",
       "      <td>Besides using SentiStrength, we use the lexico...</td>\n",
       "      <td>In order to construct the lexical prior knowle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>3183</td>\n",
       "      <td>4</td>\n",
       "      <td>Run1: We firstly use the Stanford CoreNLP tool...</td>\n",
       "      <td>We use the Stanford CoreNLP caseless tagger fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>3184</td>\n",
       "      <td>4</td>\n",
       "      <td>By setting (n inw and enw(n)=1 for all nodes, ...</td>\n",
       "      <td>The proof is similar to the proof for the tree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>3185</td>\n",
       "      <td>5</td>\n",
       "      <td>2 We tested the difference in performance for ...</td>\n",
       "      <td>We calculate statistical significance of perfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>3186</td>\n",
       "      <td>4</td>\n",
       "      <td>Run1: We firstly use the Stanford CoreNLP tool...</td>\n",
       "      <td>We use the Stanford CoreNLP caseless tagger fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>3187</td>\n",
       "      <td>5</td>\n",
       "      <td>All linguistic annotations needed for features...</td>\n",
       "      <td>Part of speech tagging and named entity recogn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>3188</td>\n",
       "      <td>4</td>\n",
       "      <td>In (Erkan and Radev, 2004 ), the concept of gr...</td>\n",
       "      <td>It was also the model used to rank sentences i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>3189</td>\n",
       "      <td>5</td>\n",
       "      <td>Significance was tested using a paired bootstr...</td>\n",
       "      <td>Statistical significance is tested on the BLEU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>3194</td>\n",
       "      <td>4</td>\n",
       "      <td>For seed and test paradigms we used verbal inf...</td>\n",
       "      <td>To validate this measure, we computed the cosi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>3195</td>\n",
       "      <td>5</td>\n",
       "      <td>We used KenLM (Heafield, 2011) to create 3-gra...</td>\n",
       "      <td>1 We used the KenLM toolkit (Heafield, 2011) t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>3196</td>\n",
       "      <td>5</td>\n",
       "      <td>We built a 5-gram language model on the Englis...</td>\n",
       "      <td>We used KenLM (Heafield, 2011) to create 3-gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>3197</td>\n",
       "      <td>4</td>\n",
       "      <td>An estimate of the likelihood of a verb taking...</td>\n",
       "      <td>In the News column , we show the statistics of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>3198</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Web 1T 5-gram corpus (Brants and Fr...</td>\n",
       "      <td>Since the GoogleAPI is not available any more,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>3199</td>\n",
       "      <td>5</td>\n",
       "      <td>Machine translation system settings: We used a...</td>\n",
       "      <td>The Moses SMT toolkit (Koehn et al., 2007) pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>3200</td>\n",
       "      <td>5</td>\n",
       "      <td>Machine translation system settings: We used a...</td>\n",
       "      <td>We trained a model using Moses toolkit (Koehn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>3701</td>\n",
       "      <td>4</td>\n",
       "      <td>A chunk is a minimal , non-recursive structure...</td>\n",
       "      <td>The chunk label tagset is a coarser version of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>3702</td>\n",
       "      <td>5</td>\n",
       "      <td>MC-30: A subset of RG-65 dataset with 30 word ...</td>\n",
       "      <td>We use a set of 30 word pairs from a study car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>3703</td>\n",
       "      <td>5</td>\n",
       "      <td>We obtained news-peg judgments using the Brat ...</td>\n",
       "      <td>All annotations were done using the brat rapid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>3704</td>\n",
       "      <td>4</td>\n",
       "      <td>We transformed the parse trees in OntoNotes in...</td>\n",
       "      <td>We use the Stanford CoreNLP caseless tagger fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>3705</td>\n",
       "      <td>4</td>\n",
       "      <td>Gradient clipping heuristic to prevent the\" ex...</td>\n",
       "      <td>For the exploding gradient problem, numerical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>3706</td>\n",
       "      <td>5</td>\n",
       "      <td>We obtained news-peg judgments using the Brat ...</td>\n",
       "      <td>The annotations were made using the BRAT rapid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>3707</td>\n",
       "      <td>5</td>\n",
       "      <td>We then describe in more detail a modern Chine...</td>\n",
       "      <td>For Chinese, we use the Penn Chinese Treebank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>3708</td>\n",
       "      <td>5</td>\n",
       "      <td>In order to assess statistical significance of...</td>\n",
       "      <td>For statistical significance testing, we use a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>3710</td>\n",
       "      <td>5</td>\n",
       "      <td>Statistical significance in BLEU score differe...</td>\n",
       "      <td>Significance was tested using a paired bootstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>3711</td>\n",
       "      <td>4</td>\n",
       "      <td>The article system builds on the elements of t...</td>\n",
       "      <td>The preposition classifier uses a combined sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>3712</td>\n",
       "      <td>5</td>\n",
       "      <td>For calculating the required frequencies, we u...</td>\n",
       "      <td>As a practical approximation, we use bigram co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>3713</td>\n",
       "      <td>3</td>\n",
       "      <td>Phrase pairs were extracted from symmetrized w...</td>\n",
       "      <td>Word alignments were created using GIZA++ (Och...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>3714</td>\n",
       "      <td>4</td>\n",
       "      <td>We ran all of our experiments in Weka (Hall et...</td>\n",
       "      <td>Weka (Hall et al., 2009) which contains the im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>3715</td>\n",
       "      <td>4</td>\n",
       "      <td>Both corpora were extracted from the open para...</td>\n",
       "      <td>The parallel data were taken from OPUS (Tiedem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>3716</td>\n",
       "      <td>4</td>\n",
       "      <td>We ran all of our experiments in Weka (Hall et...</td>\n",
       "      <td>We conducted experiments using Multinomial Nai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>3717</td>\n",
       "      <td>4</td>\n",
       "      <td>The Spanish-English (S2E) training corpus was ...</td>\n",
       "      <td>The experiments focus on translation from Germ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>3718</td>\n",
       "      <td>4</td>\n",
       "      <td>Type System extends the type system that is bu...</td>\n",
       "      <td>This architecture is very similar to the frame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>3719</td>\n",
       "      <td>4</td>\n",
       "      <td>For other languages we use the corpora made av...</td>\n",
       "      <td>We use the CoNLL-X (Buchholz and Marsi, 2006) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>3720</td>\n",
       "      <td>5</td>\n",
       "      <td>Table 5 compares our reordering model with a r...</td>\n",
       "      <td>We note that our model outperforms the model p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>3720</td>\n",
       "      <td>4</td>\n",
       "      <td>Table 5 compares our reordering model with a r...</td>\n",
       "      <td>We note that our model outperforms the model p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>3721</td>\n",
       "      <td>3</td>\n",
       "      <td>These results contradict those given in Zelenk...</td>\n",
       "      <td>Zelenko et al. (2003) have shown the contiguou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>3722</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, OntoNotes (Hovy et al., 2006 ), a...</td>\n",
       "      <td>To this end, a recent large-scale annotation e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>3723</td>\n",
       "      <td>4</td>\n",
       "      <td>As for EJ translation, we use the Stanford par...</td>\n",
       "      <td>We use Stanford parser (de Marneffe et al., 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>3724</td>\n",
       "      <td>4</td>\n",
       "      <td>For both English and German we used the part-o...</td>\n",
       "      <td>Only for German data did we used the TreeTagge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>3725</td>\n",
       "      <td>4</td>\n",
       "      <td>We train our model on a subset of the WaCkyped...</td>\n",
       "      <td>We built a knowledge base (V 2 R) 1 using the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>3726</td>\n",
       "      <td>3</td>\n",
       "      <td>The questions are translated using a phrase-ba...</td>\n",
       "      <td>The first two baselines are standard systems u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>3727</td>\n",
       "      <td>3</td>\n",
       "      <td>The questions are translated using a phrase-ba...</td>\n",
       "      <td>The decoder is built on top of an open-source ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>3728</td>\n",
       "      <td>5</td>\n",
       "      <td>We measure statistical significance using 95% ...</td>\n",
       "      <td>Significance was tested using a paired bootstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>3729</td>\n",
       "      <td>4</td>\n",
       "      <td>As for EJ translation, we use the Stanford par...</td>\n",
       "      <td>We use Stanford parser (de Marneffe et al., 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>3730</td>\n",
       "      <td>3</td>\n",
       "      <td>Words were downcased and lemmatized using the ...</td>\n",
       "      <td>For Reuters we segmented and tokenized the dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>3731</td>\n",
       "      <td>3</td>\n",
       "      <td>English sentences are parsed into dependency s...</td>\n",
       "      <td>The grammatical relations are all the collapse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>3732</td>\n",
       "      <td>5</td>\n",
       "      <td>Both of our systems were based on the Moses de...</td>\n",
       "      <td>The decoder is built on top of an open-source ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>3733</td>\n",
       "      <td>4</td>\n",
       "      <td>We also used ANEW (Bradley and Lang, 1999) for...</td>\n",
       "      <td>Finally the ANEW lexicon (Bradley and Lang, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>3734</td>\n",
       "      <td>3</td>\n",
       "      <td>translation at the DiscoMT 2015 workshop (Hard...</td>\n",
       "      <td>The NLP Group of the Idiap Research Institute ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>3735</td>\n",
       "      <td>3</td>\n",
       "      <td>Establishing and maintaining common ground is ...</td>\n",
       "      <td>An example of such a pragmatic factor is commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>3736</td>\n",
       "      <td>4</td>\n",
       "      <td>SentiWordNet score (senti) We used the Senti- ...</td>\n",
       "      <td>We then have assigned a sentiment score using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>3737</td>\n",
       "      <td>3</td>\n",
       "      <td>Europarl 2 (Koehn, 2005 ): it is a corpus of p...</td>\n",
       "      <td>We used a subset of the data provided for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>3738</td>\n",
       "      <td>4</td>\n",
       "      <td>We also carried out a chunk-reordering PB-SMT ...</td>\n",
       "      <td>Holmqvist et al. (2012) presented a method whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>3739</td>\n",
       "      <td>5</td>\n",
       "      <td>WSI is generally considered as an unsupervised...</td>\n",
       "      <td>Distributional models of meaning follow the di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>3740</td>\n",
       "      <td>3</td>\n",
       "      <td>The data used for the experiments described in...</td>\n",
       "      <td> Europarl 2 (Koehn, 2005 ): it is a corpus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>3743</td>\n",
       "      <td>4</td>\n",
       "      <td>For the language model, we used the KenLM tool...</td>\n",
       "      <td>Language Model: For all 3-SCFG systems we use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>3744</td>\n",
       "      <td>5</td>\n",
       "      <td>First used by Blitzer et al. (2007) , the MDS ...</td>\n",
       "      <td>To evaluate DA for sentiment classification, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>3748</td>\n",
       "      <td>5</td>\n",
       "      <td>The most famous example would probably be the ...</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>3749</td>\n",
       "      <td>4</td>\n",
       "      <td>For language modeling, we trained a separate 5...</td>\n",
       "      <td>In order to evaluate the fluency of each syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>3750</td>\n",
       "      <td>5</td>\n",
       "      <td>They used the Web-based annotation tool brat (...</td>\n",
       "      <td>The annotations were made using the BRAT rapid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>3751</td>\n",
       "      <td>5</td>\n",
       "      <td>The classification was conducted, using differ...</td>\n",
       "      <td>The SVM models were trained using the Scikit-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>3752</td>\n",
       "      <td>4</td>\n",
       "      <td>The Europarl corpus (Koehn, 2005) is built fro...</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>3753</td>\n",
       "      <td>5</td>\n",
       "      <td>For language modeling, we trained a separate 5...</td>\n",
       "      <td>The language model is a 5-gram KenLM (Heafield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>3754</td>\n",
       "      <td>5</td>\n",
       "      <td>The Spanish-English (S2E) training corpus was ...</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>3756</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Stanford CoreNLP caseless tagger fo...</td>\n",
       "      <td>We also lemmatized all words using Stanford Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>3757</td>\n",
       "      <td>5</td>\n",
       "      <td>All of our models are trained using Nematus (S...</td>\n",
       "      <td>We trained our basic neural machine translatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>3758</td>\n",
       "      <td>5</td>\n",
       "      <td>We use GIZA++ (Och and Ney, 2003) with its def...</td>\n",
       "      <td>We used the GIZA++ software (Och and Ney, 2003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>3759</td>\n",
       "      <td>4</td>\n",
       "      <td>We are working with standard tools as DISSECT ...</td>\n",
       "      <td>The matrix is weighted with PPMI as implemente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>3760</td>\n",
       "      <td>5</td>\n",
       "      <td>They are based on Distributional Hypothesis wh...</td>\n",
       "      <td>Corpus-based VSMs follow the standard\" distrib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>3761</td>\n",
       "      <td>4</td>\n",
       "      <td>Its segmentation model is a class-based hidden...</td>\n",
       "      <td>For Chinese, a segmentation model (Zhang et al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>3762</td>\n",
       "      <td>5</td>\n",
       "      <td>The edit distance kernel was trained with LIBS...</td>\n",
       "      <td>As our learner, we use LIBSVM with a linear ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>3763</td>\n",
       "      <td>5</td>\n",
       "      <td>The corpora are first tokenized and lowercased...</td>\n",
       "      <td>The corpus was converted from XML to raw text,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>3764</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, OntoNotes (Hovy et al., 2006 ), a...</td>\n",
       "      <td>For example, the OntoNotes (Hovy et al., 2006)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>3766</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the open-source Moses toolkit (Koehn et...</td>\n",
       "      <td>The Moses toolkit (Koehn et al., 2007) is used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>3767</td>\n",
       "      <td>3</td>\n",
       "      <td>In our experiments, we use the LIBLINEAR packa...</td>\n",
       "      <td>Specifically, we use the LIBLINEAR SVM package...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>3768</td>\n",
       "      <td>5</td>\n",
       "      <td>The Spanish-English (S2E) training corpus was ...</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>3769</td>\n",
       "      <td>5</td>\n",
       "      <td>The ICSI meeting corpus (Janin et al., 2003) i...</td>\n",
       "      <td>The ICSI Meeting Corpus: The ICSI Meeting Corp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>3770</td>\n",
       "      <td>5</td>\n",
       "      <td>We use ROUGE (Lin, 2004) for evaluating the co...</td>\n",
       "      <td>We use ROUGE metric (Lin, 2004) to evaluate ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>3771</td>\n",
       "      <td>2</td>\n",
       "      <td>The measure selected is the normalised Pearson...</td>\n",
       "      <td>The task is part of the Semantic Evaluation 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>3772</td>\n",
       "      <td>5</td>\n",
       "      <td>The main corpora we use are Europarl (Koehn, 2...</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>655</th>\n",
       "      <td>3773</td>\n",
       "      <td>4</td>\n",
       "      <td>We exploit this monolingual data for training ...</td>\n",
       "      <td>We also used automatically back-translated in-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>3774</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Stanford CoreNLP caseless tagger fo...</td>\n",
       "      <td>We also lemmatized all words using Stanford Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>3775</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a corpus-based metric relying on the d...</td>\n",
       "      <td>Corpus-based meaning representations rely on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>3776</td>\n",
       "      <td>3</td>\n",
       "      <td>Similarly, (Turian et al, 2010) evaluated thre...</td>\n",
       "      <td>Turian et al. (2010)  evaluate different techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>3777</td>\n",
       "      <td>5</td>\n",
       "      <td>3.3.1 Reference System We compare a number of ...</td>\n",
       "      <td>We then use the phrase extraction utility in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>3778</td>\n",
       "      <td>5</td>\n",
       "      <td>For training SVM classifiers we used LIBSVM pa...</td>\n",
       "      <td>We used LIBSVM to implement our own SVM for re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>3779</td>\n",
       "      <td>3</td>\n",
       "      <td>The word alignment was trained using GIZA++ (O...</td>\n",
       "      <td>The word alignment was obtained by running Giz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>3781</td>\n",
       "      <td>5</td>\n",
       "      <td>The corpus is first word-aligned using a word ...</td>\n",
       "      <td>The word alignment was obtained by running Giz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>3782</td>\n",
       "      <td>4</td>\n",
       "      <td>We perform bootstrap resampling with bounds es...</td>\n",
       "      <td>We used bootstrap resampling for testing stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>3784</td>\n",
       "      <td>5</td>\n",
       "      <td>We perform bootstrap resampling with bounds es...</td>\n",
       "      <td>We used bootstrap resampling for testing stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>3785</td>\n",
       "      <td>4</td>\n",
       "      <td>Markov Logic Networks (MLN) (Richardson and Do...</td>\n",
       "      <td>Markov Logic Networks (MLN) (Richardson and Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>3786</td>\n",
       "      <td>5</td>\n",
       "      <td>with the training script of the Moses toolkit ...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>3787</td>\n",
       "      <td>3</td>\n",
       "      <td>We train for 15 epochs using mini-batch stocha...</td>\n",
       "      <td>We initialize parameters uniformly, using the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>3788</td>\n",
       "      <td>3</td>\n",
       "      <td>This confirms the finding of Liu et al. (2012)...</td>\n",
       "      <td>More recently, Liu et al. (2012) have proposed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>3789</td>\n",
       "      <td>5</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "      <td>We evaluate our method by means of the Europar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>3790</td>\n",
       "      <td>4</td>\n",
       "      <td>We exploit this monolingual data for training ...</td>\n",
       "      <td>We also used automatically back-translated in-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>3791</td>\n",
       "      <td>4</td>\n",
       "      <td>Our decoder is a stack decoder similar to Koeh...</td>\n",
       "      <td>Phrase extraction was performed following Koeh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>3792</td>\n",
       "      <td>3</td>\n",
       "      <td>For the English- Spanish and French-English sy...</td>\n",
       "      <td>In our low-resource condition, we trained an S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673</th>\n",
       "      <td>3794</td>\n",
       "      <td>5</td>\n",
       "      <td>The word alignment was trained using GIZA++ (O...</td>\n",
       "      <td>Word alignment was done with GIZA++ (Och and N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>3795</td>\n",
       "      <td>4</td>\n",
       "      <td>Rhetorical Structure Theory (RST) (Mann and Th...</td>\n",
       "      <td>The closest area to our work consists of inves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>3796</td>\n",
       "      <td>4</td>\n",
       "      <td>The dataset used for the experiments reported ...</td>\n",
       "      <td>The other groups have been used already (for e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>3797</td>\n",
       "      <td>5</td>\n",
       "      <td>A good data source for this is the Europarl Co...</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>3798</td>\n",
       "      <td>5</td>\n",
       "      <td>The most famous example would probably be the ...</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>3800</td>\n",
       "      <td>5</td>\n",
       "      <td>Previously (Socher et al, 2011)  used a recurs...</td>\n",
       "      <td>For the MSRP task, Socher et al. (2011) used a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>4302</td>\n",
       "      <td>4</td>\n",
       "      <td>The first source is the CoNLL 2003 shared task...</td>\n",
       "      <td>AIDA/YAGO is derived from the CoNLL-2003 share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>4303</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>For this purpose, we use the Moses toolkit to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>4305</td>\n",
       "      <td>3</td>\n",
       "      <td>We therefore propose an alternative method bas...</td>\n",
       "      <td>We measure translation quality via the BLEU sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>4306</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "      <td>We used the Linear SVM implementation (with de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>4307</td>\n",
       "      <td>4</td>\n",
       "      <td>Finally, the CoNLL-2003 shared task (Tjong Kim...</td>\n",
       "      <td>AIDA/YAGO is derived from the CoNLL-2003 share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>4308</td>\n",
       "      <td>5</td>\n",
       "      <td>Further, we sentence-split, tokenized, and lem...</td>\n",
       "      <td>We also lemmatized all words using Stanford Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>4309</td>\n",
       "      <td>5</td>\n",
       "      <td>The corpora are first tokenized and lowercased...</td>\n",
       "      <td>All novels were lemmatized and POS-tagged usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>4310</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Adadelta (Zeiler, 2012) to update the p...</td>\n",
       "      <td>We also use Adadelta (Zeiler, 2012) to optimiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>4311</td>\n",
       "      <td>3</td>\n",
       "      <td>The first source is the CoNLL 2003 shared task...</td>\n",
       "      <td>AIDA/YAGO is derived from the CoNLL-2003 share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>4313</td>\n",
       "      <td>5</td>\n",
       "      <td>We used a 2009 snapshot of Wikipedia, 2 which ...</td>\n",
       "      <td>All novels were lemmatized and POS-tagged usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>4314</td>\n",
       "      <td>3</td>\n",
       "      <td>Recently, Hovy et al. (2013) utilized word emb...</td>\n",
       "      <td>Hovy et al. (2013) applied tree kernels to met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>4316</td>\n",
       "      <td>4</td>\n",
       "      <td>Finally, the CoNLL-2003 shared task (Tjong Kim...</td>\n",
       "      <td>AIDA/YAGO is derived from the CoNLL-2003 share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>4317</td>\n",
       "      <td>5</td>\n",
       "      <td>We run our experiments on Europarl (Koehn, 200...</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>4318</td>\n",
       "      <td>5</td>\n",
       "      <td>We applied bootstrap resampling (Koehn, 2004) ...</td>\n",
       "      <td>We used bootstrap resampling for testing stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>4319</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>The Moses toolkit (Koehn et al., 2007) is used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>4320</td>\n",
       "      <td>5</td>\n",
       "      <td>The statistical significance test is also carr...</td>\n",
       "      <td>We used bootstrap resampling for testing stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>4321</td>\n",
       "      <td>4</td>\n",
       "      <td>This model is motivated by Vector Space Model ...</td>\n",
       "      <td>One of the best-known methods of representing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>4322</td>\n",
       "      <td>4</td>\n",
       "      <td>The data was segmented into baseNP parts and n...</td>\n",
       "      <td>We have used the baseNP data presented in (Ram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>4324</td>\n",
       "      <td>3</td>\n",
       "      <td>The NJU-Parser is based on the state-of-the ar...</td>\n",
       "      <td>The second algorithm, denoted GloTr, is the Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>4325</td>\n",
       "      <td>3</td>\n",
       "      <td>One is a 3-gram language model built using Ken...</td>\n",
       "      <td>Training and querying of a modified Kneser-Ney...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>4326</td>\n",
       "      <td>5</td>\n",
       "      <td>The word alignment was obtained by running Giz...</td>\n",
       "      <td>Baseline word alignments were obtained by runn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>4327</td>\n",
       "      <td>5</td>\n",
       "      <td>Translations for English words in the lexical ...</td>\n",
       "      <td>In order to improve the robustness of the word...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>4331</td>\n",
       "      <td>5</td>\n",
       "      <td>All the Language Models (LM) used in our exper...</td>\n",
       "      <td>The language models are estimated using the Ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>4332</td>\n",
       "      <td>4</td>\n",
       "      <td>In all experiments, we use the SVM classifier ...</td>\n",
       "      <td>For all experiments, we use a decision-tree cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>4333</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Stanford parser to generate a DG fo...</td>\n",
       "      <td>We used the lexicalized dependency parser in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>4334</td>\n",
       "      <td>4</td>\n",
       "      <td>Preprocessing: We tokenized the English side o...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>4336</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "      <td>We use Moses 7 (Koehn et al., 2007) to impleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>4337</td>\n",
       "      <td>3</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>4338</td>\n",
       "      <td>5</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>4339</td>\n",
       "      <td>5</td>\n",
       "      <td>The first competitive learning based system is...</td>\n",
       "      <td>To solve coreference, we used a variation of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>4340</td>\n",
       "      <td>5</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>4341</td>\n",
       "      <td>3</td>\n",
       "      <td>We used predicted collapsed Stanford dependenc...</td>\n",
       "      <td>We extract syntactic dependencies using Stanfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>4342</td>\n",
       "      <td>4</td>\n",
       "      <td>We select as a general-purpose corpus Europarl...</td>\n",
       "      <td>We run our experiments on Europarl (Koehn, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>4343</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the lexicalized dependency parser in t...</td>\n",
       "      <td>In our experiments , we used the Stanford pars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>4344</td>\n",
       "      <td>3</td>\n",
       "      <td>The result of the operation is equivalent to w...</td>\n",
       "      <td>The OpenFst library is used to perform all of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>4345</td>\n",
       "      <td>3</td>\n",
       "      <td>The distributional hypothesis of meaning (Harr...</td>\n",
       "      <td>This is a corpus-based metric relying on the d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>4346</td>\n",
       "      <td>5</td>\n",
       "      <td>Preprocessing: We tokenized the English side o...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>4347</td>\n",
       "      <td>3</td>\n",
       "      <td>One is from (Turian et al, 2010) , the dimensi...</td>\n",
       "      <td>For BROWN, the features are the prefix feature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>4348</td>\n",
       "      <td>5</td>\n",
       "      <td>The thresholds were thoroughly selected depend...</td>\n",
       "      <td>For a pair of words, WordNet provides a series...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>4349</td>\n",
       "      <td>5</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>Our baseline is a phrase-based MT system train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>4350</td>\n",
       "      <td>5</td>\n",
       "      <td>The statistical significance test is also carr...</td>\n",
       "      <td>We measure significance of results using boots...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>4351</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "      <td>3.3.1 Reference System We compare a number of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>4352</td>\n",
       "      <td>5</td>\n",
       "      <td>We used a 2009 snapshot of Wikipedia, 2 which ...</td>\n",
       "      <td>3 All English data are POS tagged and lemmatis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>4353</td>\n",
       "      <td>4</td>\n",
       "      <td>The text was pre-processed using wp2txt 6 to r...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>4354</td>\n",
       "      <td>5</td>\n",
       "      <td>We run our experiments on Europarl (Koehn, 200...</td>\n",
       "      <td>Europarl (Koehn, 2005) is a multilingual paral...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>4355</td>\n",
       "      <td>4</td>\n",
       "      <td>We use Scikit-learn (Pedregosa et al., 2011 ),...</td>\n",
       "      <td>Logistic regression, implemented in Python wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>4356</td>\n",
       "      <td>5</td>\n",
       "      <td>The way they were added is similar to incorpor...</td>\n",
       "      <td>In this paper, we use the subjectivity corpus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>4358</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the lexicalized dependency parser in t...</td>\n",
       "      <td>In our experiments , we used the Stanford pars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>4359</td>\n",
       "      <td>4</td>\n",
       "      <td>We modified the implementation of the SWELL Ja...</td>\n",
       "      <td>We also compare our word embeddings with the E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>4360</td>\n",
       "      <td>4</td>\n",
       "      <td>The Stanford dependency parser (De Marneffe et...</td>\n",
       "      <td>We used the lexicalized dependency parser in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>4361</td>\n",
       "      <td>4</td>\n",
       "      <td>The first work on this topic was done back in ...</td>\n",
       "      <td>The Dutch version was tagged automatically usi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>4362</td>\n",
       "      <td>3</td>\n",
       "      <td>The OpenFst library is used to perform all of ...</td>\n",
       "      <td>The decoder is implemented with Weighted Finit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>4363</td>\n",
       "      <td>4</td>\n",
       "      <td>For our experiments, we used translated movie ...</td>\n",
       "      <td>For Chinese-English experiments, we used the O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>4364</td>\n",
       "      <td>3</td>\n",
       "      <td>The Moses15 result is obtained by applying the...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>4365</td>\n",
       "      <td>3</td>\n",
       "      <td>The OpenFst library is used to perform all of ...</td>\n",
       "      <td>The decoder is implemented with Weighted Finit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>4366</td>\n",
       "      <td>3</td>\n",
       "      <td>We select as a general-purpose corpus Europarl...</td>\n",
       "      <td>We run our experiments on Europarl (Koehn, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>4367</td>\n",
       "      <td>4</td>\n",
       "      <td>We also compare our word embeddings with the E...</td>\n",
       "      <td>Dhillon et al. (2015) used CCA to derive word ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>4368</td>\n",
       "      <td>4</td>\n",
       "      <td>The relationship between language and sentimen...</td>\n",
       "      <td>The rapid growth of user-generated content, mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>4371</td>\n",
       "      <td>3</td>\n",
       "      <td>Support vector machines (SVM) are one of the b...</td>\n",
       "      <td>These classifiers are based on a discriminativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>4372</td>\n",
       "      <td>4</td>\n",
       "      <td>In Barankov and Tamchyna (2014), we experiment...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>4373</td>\n",
       "      <td>4</td>\n",
       "      <td>To learn the parameters of the model we minimi...</td>\n",
       "      <td>For optimization, we employed the Adam 6 http:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>4374</td>\n",
       "      <td>5</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "      <td>was used for word alignment and phrase transla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>4375</td>\n",
       "      <td>5</td>\n",
       "      <td>The text was pre-processed using wp2txt 6 to r...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>4376</td>\n",
       "      <td>3</td>\n",
       "      <td>The task reported on here is to produce PropBa...</td>\n",
       "      <td>A standard for predicate argument annotation i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>4377</td>\n",
       "      <td>4</td>\n",
       "      <td>For both systems, the used training data is fr...</td>\n",
       "      <td>The source of bilingual data used in the exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>4378</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Stanford parser to generate a DG fo...</td>\n",
       "      <td>We used the lexicalized dependency parser in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>4379</td>\n",
       "      <td>4</td>\n",
       "      <td>We used predicted collapsed Stanford dependenc...</td>\n",
       "      <td>We extract syntactic dependencies using Stanfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>4380</td>\n",
       "      <td>4</td>\n",
       "      <td>We tokenize English data and segment Chinese d...</td>\n",
       "      <td>2 Further, we sentence-split, tokenized, and l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>4381</td>\n",
       "      <td>3</td>\n",
       "      <td>The evaluation emphasis in multi-document summ...</td>\n",
       "      <td>ROUGE (Lin and Hovy, 2003) has been adopted as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>4382</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phrase b...</td>\n",
       "      <td>All experiments were carried out using the ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>4383</td>\n",
       "      <td>5</td>\n",
       "      <td>The similarity function is here the Smoothed P...</td>\n",
       "      <td>We directly apply the Smoothed Partial Tree-Ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>4384</td>\n",
       "      <td>5</td>\n",
       "      <td>We tokenize English data and segment Chinese d...</td>\n",
       "      <td>2 Further, we sentence-split, tokenized, and l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>4385</td>\n",
       "      <td>3</td>\n",
       "      <td>The Stanford dependency parser (De Marneffe et...</td>\n",
       "      <td>We used the lexicalized dependency parser in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>4386</td>\n",
       "      <td>3</td>\n",
       "      <td>To demonstrate the effect of the proposed meth...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>4387</td>\n",
       "      <td>5</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "      <td>The Moses toolkit (Koehn et al., 2007) is used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>4388</td>\n",
       "      <td>3</td>\n",
       "      <td>3.3.1 Reference System We compare a number of ...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>4389</td>\n",
       "      <td>5</td>\n",
       "      <td>Preprocessing: We tokenized the English side o...</td>\n",
       "      <td>We tokenize and frequent-case the data with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>4391</td>\n",
       "      <td>3</td>\n",
       "      <td>Finally, it is also noticeable that the percen...</td>\n",
       "      <td>Baroni et al. (2002)  report that 47% of the v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>4392</td>\n",
       "      <td>3</td>\n",
       "      <td>As a learning algorithm we adopt a ranking SVM...</td>\n",
       "      <td>To train a mention-pair classifier, we use the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>4393</td>\n",
       "      <td>2</td>\n",
       "      <td>Brain images are quite noisy, so we used the m...</td>\n",
       "      <td>Following the evaluation paradigm of Mitchell ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>4394</td>\n",
       "      <td>3</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "      <td>This system is the Moses decoder (Koehn et al....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>4395</td>\n",
       "      <td>4</td>\n",
       "      <td>Results on English-French, English-Romanian, a...</td>\n",
       "      <td>Our alignment model is based on a simple varia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>4396</td>\n",
       "      <td>4</td>\n",
       "      <td>The work presented in Berger et al. (1996) tha...</td>\n",
       "      <td>The original reordering constraint in Berger e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>4398</td>\n",
       "      <td>4</td>\n",
       "      <td>Our second method is based on the recurrent ne...</td>\n",
       "      <td>Mikolov et al. (2013a) proposed a faster skip-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>4399</td>\n",
       "      <td>4</td>\n",
       "      <td>The SUSANNE Corpus is a modified and condensed...</td>\n",
       "      <td>The corpus consists of a subset of the Brown C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>4400</td>\n",
       "      <td>5</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "      <td>All these features are inherited from Moses (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>4901</td>\n",
       "      <td>5</td>\n",
       "      <td>English annotations were all produced using th...</td>\n",
       "      <td>We also lemmatized all words using Stanford Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>4903</td>\n",
       "      <td>5</td>\n",
       "      <td>The parameters are estimated by Gibbs sampling...</td>\n",
       "      <td>10 We used the PLTM implementation in Mallet (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>4904</td>\n",
       "      <td>4</td>\n",
       "      <td>The meta-classifier is a linear SVM (Fan et al...</td>\n",
       "      <td>LIBLINEAR (Fan et al., 2008 ), a library for l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>4905</td>\n",
       "      <td>3</td>\n",
       "      <td>We use Adadelta (Zeiler, 2012) to update the p...</td>\n",
       "      <td>We use a mini-batch stochastic gradient descen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>4907</td>\n",
       "      <td>5</td>\n",
       "      <td>We train a Support Vector Machine (SVM) (Corte...</td>\n",
       "      <td>Specifically, we use Support Vector Machine (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>4908</td>\n",
       "      <td>5</td>\n",
       "      <td>Parameter tuning is carried out using Z- MERT ...</td>\n",
       "      <td>Feature weights, based on BLEU, are then tuned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>771</th>\n",
       "      <td>4909</td>\n",
       "      <td>4</td>\n",
       "      <td>Brin identifies the use of patterns in the dis...</td>\n",
       "      <td>Brin proposed the bootstrapping method for rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>4910</td>\n",
       "      <td>5</td>\n",
       "      <td>We adopt the setting of Socher et al. (2012) .</td>\n",
       "      <td>Neural networks are first used in this task in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>4912</td>\n",
       "      <td>5</td>\n",
       "      <td>The publicly available tool GIZA++ was used to...</td>\n",
       "      <td>We used the GIZA++ software (Och and Ney, 2003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>4913</td>\n",
       "      <td>5</td>\n",
       "      <td>CRFSuite implementation (Okazaki, 2007 ).</td>\n",
       "      <td>We trained a CRF tagger using CRFSuite 1 (Okaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>4914</td>\n",
       "      <td>3</td>\n",
       "      <td>The HMM classifier used in our experiments fol...</td>\n",
       "      <td>The HMM classifier used in the experiments in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>4915</td>\n",
       "      <td>5</td>\n",
       "      <td>We computed 4-gram LMs with modified Kneser-Ne...</td>\n",
       "      <td>The language model is a 5-gram with interpolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>4916</td>\n",
       "      <td>5</td>\n",
       "      <td>Socher et al. (2011) come closest to our targe...</td>\n",
       "      <td>We plan to adapt ideas from Socher et al. (201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>4917</td>\n",
       "      <td>3</td>\n",
       "      <td>We use Ridge Regression (RR) with l2-norm regu...</td>\n",
       "      <td>We use the Support Vector Machines implementat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>4918</td>\n",
       "      <td>3</td>\n",
       "      <td>As for the former (hereafter it is referred to...</td>\n",
       "      <td>For argument, a dependency version of the prun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>4920</td>\n",
       "      <td>5</td>\n",
       "      <td>15 The significance tests were performed using...</td>\n",
       "      <td>We used bootstrap resampling for testing stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>4921</td>\n",
       "      <td>5</td>\n",
       "      <td>In this work, we focus on learning with Suppor...</td>\n",
       "      <td>Specifically, we use Support Vector Machine (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>4922</td>\n",
       "      <td>4</td>\n",
       "      <td>We used TreeTagger (Schmid, 1994) to obtain a ...</td>\n",
       "      <td>We used the Stuttgart TreeTagger (Schmid, 1994...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>4923</td>\n",
       "      <td>5</td>\n",
       "      <td>To account for this constraint,  include infor...</td>\n",
       "      <td>Earlier models made use of latent semantic ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>784</th>\n",
       "      <td>4924</td>\n",
       "      <td>3</td>\n",
       "      <td>The system was trained on the English and Dani...</td>\n",
       "      <td>In principle, classifiers trained on PDTB data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>4925</td>\n",
       "      <td>3</td>\n",
       "      <td>We use linear SVMs from LIBLINEAR and SVMs wit...</td>\n",
       "      <td>The edit distance kernel was trained with LIBS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>4926</td>\n",
       "      <td>3</td>\n",
       "      <td>In current phrase-based statistical machine tr...</td>\n",
       "      <td>It is a standard phrase-based machine translat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>4928</td>\n",
       "      <td>5</td>\n",
       "      <td>We train a Support Vector Machine (SVM) (Corte...</td>\n",
       "      <td>Specifically, we use Support Vector Machine (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>4929</td>\n",
       "      <td>5</td>\n",
       "      <td>The word alignment was obtained by running Giz...</td>\n",
       "      <td>The word alignment is created by GIZA++ (Och a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>4930</td>\n",
       "      <td>3</td>\n",
       "      <td>For training SVM classifiers we used LIBSVM pa...</td>\n",
       "      <td>The SVM implementation used was LIBSVM (Chang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>4931</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the standard alignment tool Giza++ (Och...</td>\n",
       "      <td>We used GIZA++ (Och and Ney, 2003) to align th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>4932</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the standard alignment tool Giza++ (Och...</td>\n",
       "      <td>We used the GIZA++ software (Och and Ney, 2003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>4933</td>\n",
       "      <td>3</td>\n",
       "      <td>In the Penn Discourse TreeBank 2.0 (Prasad et ...</td>\n",
       "      <td>In contrast, the set of discourse markers in o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>4934</td>\n",
       "      <td>4</td>\n",
       "      <td>Weka (Hall et al., 2009) was used to apply lea...</td>\n",
       "      <td>The SVM implementation of Weka (Hall et al., 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>4937</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>This was done with a specific tool provided wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>4938</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the standard alignment tool Giza++ (Och...</td>\n",
       "      <td>We used the GIZA++ software (Och and Ney, 2003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>4939</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the GIZA++ software (Och and Ney, 2003...</td>\n",
       "      <td>We performed word alignment using GIZA++ (Och ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>4940</td>\n",
       "      <td>5</td>\n",
       "      <td>The language model used is the 5-gram corpus f...</td>\n",
       "      <td>We used Google Books ngrams (Michel et al., 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>4941</td>\n",
       "      <td>5</td>\n",
       "      <td>English annotations were all produced using th...</td>\n",
       "      <td>We also lemmatized all words using Stanford Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>4942</td>\n",
       "      <td>3</td>\n",
       "      <td>For example, (Turian et al, 2010) compared Bro...</td>\n",
       "      <td>We evaluate C&amp;W word embeddings with 25, 50 an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>4943</td>\n",
       "      <td>3</td>\n",
       "      <td>Distributional representations encode an expre...</td>\n",
       "      <td>This is often called distributional semantics ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>4944</td>\n",
       "      <td>4</td>\n",
       "      <td>(Li et al, 2013)  use crowdsourcing to build p...</td>\n",
       "      <td>Our neural network is similar to that of Li et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>4945</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the standard alignment tool Giza++ (Och...</td>\n",
       "      <td>We used GIZA++ (Och and Ney, 2003) to align th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>4946</td>\n",
       "      <td>4</td>\n",
       "      <td>The formalism that is used to represent the se...</td>\n",
       "      <td>Meaning representation and composition in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>4947</td>\n",
       "      <td>5</td>\n",
       "      <td>For French, Hungarian, Polish and Swedish we u...</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>4950</td>\n",
       "      <td>4</td>\n",
       "      <td>The English text was tokenized using the word ...</td>\n",
       "      <td>The stopwords are taken from the stop-word lis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>4951</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the MOSES decoder (Koehn et al., 2007) ...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>4952</td>\n",
       "      <td>4</td>\n",
       "      <td>Specifically, the sentence compression dataset...</td>\n",
       "      <td>One idea is to apply it to the language model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>4953</td>\n",
       "      <td>4</td>\n",
       "      <td>use the Stanford Parser (de Marneffe et al., 2...</td>\n",
       "      <td>We extract syntactic dependencies using Stanfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>4954</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>4955</td>\n",
       "      <td>4</td>\n",
       "      <td>In practice, the decoder has to employ beam se...</td>\n",
       "      <td>To make the exponential algorithm practical, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>4956</td>\n",
       "      <td>5</td>\n",
       "      <td>The source of bilingual data used in the exper...</td>\n",
       "      <td>The EuroParl data set consists of 707 sentence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>4957</td>\n",
       "      <td>4</td>\n",
       "      <td>The tectogrammatical annotation layer is based...</td>\n",
       "      <td>Our approach is based on the Czech linguistic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>4958</td>\n",
       "      <td>4</td>\n",
       "      <td>In this paper, we use the subjectivity corpus ...</td>\n",
       "      <td>These classifiers have been used in related wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>4959</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>We build a baseline error correction system, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>4960</td>\n",
       "      <td>5</td>\n",
       "      <td>All our systems are contrasted with a standard...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>4961</td>\n",
       "      <td>4</td>\n",
       "      <td>A common approach to computing similarity is t...</td>\n",
       "      <td>This sense similarity measure is inspired by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>4962</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>4963</td>\n",
       "      <td>3</td>\n",
       "      <td>The Stanford dependency parser (De Marneffe et...</td>\n",
       "      <td>In our experiments , we used the Stanford pars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>4964</td>\n",
       "      <td>3</td>\n",
       "      <td>We tokenize and frequent-case the data with th...</td>\n",
       "      <td>We implement MERT and MIRA 1 , and directly us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>4965</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>We tokenize and frequent-case the data with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>4966</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>Our baseline is a phrase-based MT system train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>4967</td>\n",
       "      <td>4</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>4968</td>\n",
       "      <td>4</td>\n",
       "      <td>We tokenize and frequent-case the data with th...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>4969</td>\n",
       "      <td>5</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>4970</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>4971</td>\n",
       "      <td>4</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>4972</td>\n",
       "      <td>5</td>\n",
       "      <td>In contrast, (McClosky et al, 2006) focus on l...</td>\n",
       "      <td>McClosky et al. (2006)  use self-training in c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>4973</td>\n",
       "      <td>3</td>\n",
       "      <td>We use logistic regression with L2 regularizat...</td>\n",
       "      <td>@BULLET Logistic Regression(LR): We use Logist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>4974</td>\n",
       "      <td>5</td>\n",
       "      <td>Since the phrase table contains lemmas, the Wi...</td>\n",
       "      <td>3 All English data are POS tagged and lemmatis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>4975</td>\n",
       "      <td>4</td>\n",
       "      <td>We use logistic regression with L2 regularizat...</td>\n",
       "      <td>We use the Bernoulli Naive Bayes classifier in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>4976</td>\n",
       "      <td>4</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "      <td>Both systems are based on the Moses SMT toolki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>4977</td>\n",
       "      <td>4</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "      <td>We tokenize and frequent-case the data with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>4978</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>4980</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>4981</td>\n",
       "      <td>3</td>\n",
       "      <td>Europarl (Koehn, 2005) is a multilingual paral...</td>\n",
       "      <td>We extract our paraphrase grammar from the Fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>4982</td>\n",
       "      <td>5</td>\n",
       "      <td>For all syntactic parsers, we used the\" basic\"...</td>\n",
       "      <td>In our experiments , we used the Stanford pars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>4983</td>\n",
       "      <td>3</td>\n",
       "      <td>The Stanford dependency parser (De Marneffe et...</td>\n",
       "      <td>In our experiments , we used the Stanford pars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>4984</td>\n",
       "      <td>5</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "      <td>All our systems are contrasted with a standard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>4985</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the cross-entropy loss function and min...</td>\n",
       "      <td>We use AdaGrad (Duchi et al., 2011)  with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>4986</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the cross-entropy loss function and min...</td>\n",
       "      <td>We train the concept identification stage usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>4987</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the AdaGrad method (Duchi et al., 2011)...</td>\n",
       "      <td>We use the cross-entropy loss function and min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>4988</td>\n",
       "      <td>5</td>\n",
       "      <td>Socher et al. (2011)  explored using recursive...</td>\n",
       "      <td>Socher et al. (2011)  use recursive auto-encod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>4989</td>\n",
       "      <td>5</td>\n",
       "      <td>2 Translation Model Moses is a state-of-the-ar...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>4989</td>\n",
       "      <td>5</td>\n",
       "      <td>2 Translation Model Moses is a state-of-the-ar...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>4990</td>\n",
       "      <td>5</td>\n",
       "      <td>2 Translation Model Moses is a state-of-the-ar...</td>\n",
       "      <td>Our baseline is a phrase-based MT system train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>4991</td>\n",
       "      <td>4</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>4992</td>\n",
       "      <td>5</td>\n",
       "      <td>All our systems are contrasted with a standard...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>4993</td>\n",
       "      <td>5</td>\n",
       "      <td>For all syntactic parsers, we used the\" basic\"...</td>\n",
       "      <td>In our experiments , we used the Stanford pars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>4994</td>\n",
       "      <td>4</td>\n",
       "      <td>can be evaluated by maximising the pseudo-like...</td>\n",
       "      <td>The parameters can be efficiently estimated fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>4995</td>\n",
       "      <td>5</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>4996</td>\n",
       "      <td>4</td>\n",
       "      <td>2 Translation Model Moses is a state-of-the-ar...</td>\n",
       "      <td>All our systems are contrasted with a standard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>4997</td>\n",
       "      <td>3</td>\n",
       "      <td>In this approach we used the NERsuite software...</td>\n",
       "      <td>NERsuite is a NER system that is built on top ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>4998</td>\n",
       "      <td>4</td>\n",
       "      <td>2 Translation Model Moses is a state-of-the-ar...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>4999</td>\n",
       "      <td>4</td>\n",
       "      <td>These efforts focused exclusively on the meron...</td>\n",
       "      <td>Experts can manually specify the attributes of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>5000</td>\n",
       "      <td>4</td>\n",
       "      <td>Special forms of relatedness are represented i...</td>\n",
       "      <td>Experts can manually specify the attributes of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>5501</td>\n",
       "      <td>5</td>\n",
       "      <td>We train our model on a subset of the WaCkyped...</td>\n",
       "      <td>We used the publicly available Wacky corpus (B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>5502</td>\n",
       "      <td>4</td>\n",
       "      <td>Since the first shared task on Recognising Tex...</td>\n",
       "      <td>Although there had been research on reasoning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>5503</td>\n",
       "      <td>5</td>\n",
       "      <td>We used k-best batch MIRA (Cherry and Foster, ...</td>\n",
       "      <td>We tune the systems using kbest batch MIRA (Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>5504</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the Mallet toolkit (McCallum, 2002) fo...</td>\n",
       "      <td>We use the MALLET package (McCallum, 2002) for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>5505</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the implementation provided by CRFsuite...</td>\n",
       "      <td>We use CRFsuite (Okazaki, 2007) as an implemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>5506</td>\n",
       "      <td>5</td>\n",
       "      <td>glish source with target French by using GIZA+...</td>\n",
       "      <td>Word alignments were created using GIZA++ (Och...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>5507</td>\n",
       "      <td>5</td>\n",
       "      <td>We used ROUGE-N (Lin, 2004) for evaluation of ...</td>\n",
       "      <td>We expect this restriction is more consistent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>5508</td>\n",
       "      <td>4</td>\n",
       "      <td>1 with 2 -regularization using AdaGrad (Duchi ...</td>\n",
       "      <td>We adopt online learning, updating parameters ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>5509</td>\n",
       "      <td>4</td>\n",
       "      <td>We used Weka (Hall et al., 2009) for all our c...</td>\n",
       "      <td>Weka (Hall et al., 2009) which contains the im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>5510</td>\n",
       "      <td>4</td>\n",
       "      <td>POS tagging was performed with TreeTagger (Sch...</td>\n",
       "      <td>TreeTagger is a statistical, decision tree-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>5511</td>\n",
       "      <td>5</td>\n",
       "      <td>Word alignments on the parallel corpus are per...</td>\n",
       "      <td>The parallel corpus is wordaligned using GIZA+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>5512</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "      <td>Classification uses the scikit-learn Python pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>5514</td>\n",
       "      <td>3</td>\n",
       "      <td>The Lexical sample data was parsed using the C...</td>\n",
       "      <td>It builds on the C&amp;C CCG parser (Clark and Cur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>5515</td>\n",
       "      <td>5</td>\n",
       "      <td>For POS-tagging, we used the Stanford POS-tagg...</td>\n",
       "      <td>Next, we replace all nouns with their POS tag;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>5516</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "      <td>All the data come from Europarl (Koehn, 2005 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>5517</td>\n",
       "      <td>3</td>\n",
       "      <td>This data is part of the NUCLE corpus (Dahlmei...</td>\n",
       "      <td>The training data released by the task organiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>5518</td>\n",
       "      <td>3</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "      <td>conducted using the Moses phrase-based decoder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>5519</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the word alignment produced by Giza (O...</td>\n",
       "      <td>Och is the HMM alignment model of (Och and Ney...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>5520</td>\n",
       "      <td>5</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "      <td>All the data come from Europarl (Koehn, 2005 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>5521</td>\n",
       "      <td>4</td>\n",
       "      <td>Word alignment is performed using GIZA++ (Och ...</td>\n",
       "      <td>glish source with target French) by using GIZA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>5522</td>\n",
       "      <td>5</td>\n",
       "      <td>Word alignment was done with GIZA++ (Och and N...</td>\n",
       "      <td>Word alignment is performed with Giza++ (Och a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>5524</td>\n",
       "      <td>4</td>\n",
       "      <td>The evaluation results were provided by the or...</td>\n",
       "      <td>The models were evaluated using M2 scorer (Dah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>5525</td>\n",
       "      <td>3</td>\n",
       "      <td>In future work we can therefore incorporate un...</td>\n",
       "      <td>Han et al. (2012) introduced a dictionary base...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>5526</td>\n",
       "      <td>4</td>\n",
       "      <td>glish source with target French) by using GIZA...</td>\n",
       "      <td>Word alignment is done using GIZA++ (Och and N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>5527</td>\n",
       "      <td>3</td>\n",
       "      <td>We conducted statistical significance tests fo...</td>\n",
       "      <td>We used bootstrap resampling for testing stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>5528</td>\n",
       "      <td>5</td>\n",
       "      <td>The word alignment was obtained by running Giz...</td>\n",
       "      <td>Word alignment is performed with Giza++ (Och a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>5529</td>\n",
       "      <td>5</td>\n",
       "      <td>Word alignment was done with GIZA++ (Och and N...</td>\n",
       "      <td>Word alignment is performed using GIZA++ (Och ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>5531</td>\n",
       "      <td>4</td>\n",
       "      <td>For the theory of Cho &amp; Chai (2000) to be comp...</td>\n",
       "      <td>In section 3, we present a solution to the all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>5532</td>\n",
       "      <td>5</td>\n",
       "      <td>In order to compare our method to a well under...</td>\n",
       "      <td>In our experiments we investigated both weak a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>5534</td>\n",
       "      <td>4</td>\n",
       "      <td>We examine the quality of translations to Engl...</td>\n",
       "      <td>To calculate the number of changes, we used a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>5535</td>\n",
       "      <td>4</td>\n",
       "      <td>Integer Linear Programming (ILP) has recently ...</td>\n",
       "      <td>Second, to avoid the error propagation problem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>5536</td>\n",
       "      <td>5</td>\n",
       "      <td>For that purpose, we use the word analogy task...</td>\n",
       "      <td>Mikolov et al. (2013a) proposed a faster skip-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>5537</td>\n",
       "      <td>3</td>\n",
       "      <td>The corpus is first word-aligned using a word ...</td>\n",
       "      <td>A core component of every PBSMT system is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>5538</td>\n",
       "      <td>3</td>\n",
       "      <td>For ranking, we use the SVM rank ranker (Joach...</td>\n",
       "      <td>We use the SVM rank implementation (Joachims, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>5539</td>\n",
       "      <td>3</td>\n",
       "      <td>Parameters are updated using AdaGrad (Duchi et...</td>\n",
       "      <td>All models were trained using Adagrad (Duchi e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>5540</td>\n",
       "      <td>3</td>\n",
       "      <td>The tagger we use is TnT (Brants, 2000) , a hi...</td>\n",
       "      <td>HCRC is tagged with TnT (Brants, 2000 ), train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>5541</td>\n",
       "      <td>3</td>\n",
       "      <td>Sentiment score of the last post of the observ...</td>\n",
       "      <td>We trained the classifiers using the LIBLINEAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>5544</td>\n",
       "      <td>4</td>\n",
       "      <td>For German English we also have a system based...</td>\n",
       "      <td>For reference, we also show the MT performance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>5546</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Universal POS Tagset (UPOS) of Petr...</td>\n",
       "      <td>For example, Petrov et al. (2012)  build super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>5547</td>\n",
       "      <td>3</td>\n",
       "      <td>We also considered an ensemble of our approach...</td>\n",
       "      <td>We chose a threshold such that our approach pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>5548</td>\n",
       "      <td>3</td>\n",
       "      <td>One way to solve this problem is to use a kern...</td>\n",
       "      <td>The proof is similar to the proof for the tree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>5549</td>\n",
       "      <td>3</td>\n",
       "      <td>We use Adam (Kingma and Ba, 2015) for optimisa...</td>\n",
       "      <td>We train with the Adam optimizer (Kingma and B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>5550</td>\n",
       "      <td>3</td>\n",
       "      <td>We build our PB-SMT systems in a standard way ...</td>\n",
       "      <td>We use KenLM 3 (Heafield, 2011) for computing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>5551</td>\n",
       "      <td>4</td>\n",
       "      <td>The starting point for our model is the skipgr...</td>\n",
       "      <td>Our model is an extension of the contextual ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>5553</td>\n",
       "      <td>4</td>\n",
       "      <td>We introduce a new anaphoricity detection mode...</td>\n",
       "      <td>We generated embeddings by training a characte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>5554</td>\n",
       "      <td>5</td>\n",
       "      <td>The realisation ranking component is an SVM ra...</td>\n",
       "      <td>The advantage of an SVM rank model is that it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>5556</td>\n",
       "      <td>3</td>\n",
       "      <td>WSI is generally considered as an unsupervised...</td>\n",
       "      <td>According to the distributional hypothesis of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>5557</td>\n",
       "      <td>4</td>\n",
       "      <td>We run our experiments on Europarl (Koehn, 200...</td>\n",
       "      <td>Our experiments were carried out on the Europa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>5558</td>\n",
       "      <td>3</td>\n",
       "      <td>For the language model, we used the KenLM tool...</td>\n",
       "      <td>The language model is a 5-gram KenLM (Heafield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>5559</td>\n",
       "      <td>3</td>\n",
       "      <td>The second collection is constituted by the GE...</td>\n",
       "      <td>GENIA (Kim et al., 2003) is a collection of 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>5560</td>\n",
       "      <td>4</td>\n",
       "      <td>We introduce a new anaphoricity detection mode...</td>\n",
       "      <td>We generated embeddings by training a characte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>5563</td>\n",
       "      <td>5</td>\n",
       "      <td>The sentence aligned parallel data is first wo...</td>\n",
       "      <td>Baseline word alignments were obtained by runn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>5564</td>\n",
       "      <td>3</td>\n",
       "      <td>All the Language Models (LM) used in our exper...</td>\n",
       "      <td>For the language model, we used all monolingua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>5565</td>\n",
       "      <td>4</td>\n",
       "      <td>They are based on Distributional Hypothesis wh...</td>\n",
       "      <td>Many researchers have considered generating pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>910</th>\n",
       "      <td>5566</td>\n",
       "      <td>4</td>\n",
       "      <td>Finally, we consider the Europarl corpus v7 (K...</td>\n",
       "      <td>Our experiments were carried out on the Europa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>5568</td>\n",
       "      <td>3</td>\n",
       "      <td>For the contextual check we use the Google Web...</td>\n",
       "      <td>org/wiki/Fog_Index 8 For word frequency we use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>5569</td>\n",
       "      <td>3</td>\n",
       "      <td>By imposing constraints on the possible word r...</td>\n",
       "      <td>The first is a reimplementation of the stack-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>5571</td>\n",
       "      <td>5</td>\n",
       "      <td>Surdeanu et al. (2012) propose a two-layer mul...</td>\n",
       "      <td>Surdeanu et al. (2012) proposed a novel approa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>5572</td>\n",
       "      <td>3</td>\n",
       "      <td>We have theoretically suggested that based on ...</td>\n",
       "      <td>Then we revise the two LP constraints of Cho &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>915</th>\n",
       "      <td>5574</td>\n",
       "      <td>3</td>\n",
       "      <td>For the contextual check we use the Google Web...</td>\n",
       "      <td>We used the unigram counts from the Web 1T 5-g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>5575</td>\n",
       "      <td>5</td>\n",
       "      <td>ROUGE (Lin, 2004) is a set of evaluation metri...</td>\n",
       "      <td>We expect this restriction is more consistent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917</th>\n",
       "      <td>5576</td>\n",
       "      <td>5</td>\n",
       "      <td>Here we review the parameters of the standard ...</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>5577</td>\n",
       "      <td>4</td>\n",
       "      <td>The Spanish-English (S2E) training corpus was ...</td>\n",
       "      <td>The systems for the English ? Spanish translat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>5578</td>\n",
       "      <td>3</td>\n",
       "      <td>We convert the trees in both treebanks from co...</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>5579</td>\n",
       "      <td>3</td>\n",
       "      <td>We convert the trees in both treebanks from co...</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>5580</td>\n",
       "      <td>5</td>\n",
       "      <td>We use KenLM 3 (Heafield, 2011) for computing ...</td>\n",
       "      <td>We use a 5-gram LM trained on the Gigaword cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>5581</td>\n",
       "      <td>5</td>\n",
       "      <td>The most famous example would probably be the ...</td>\n",
       "      <td>The system was trained on the English and Dani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>5582</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "      <td>We convert the trees in both treebanks from co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>5583</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "      <td>Here we review the parameters of the standard ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>5584</td>\n",
       "      <td>4</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "      <td>The Moses decoder (Koehn et al., 2007) was use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>5585</td>\n",
       "      <td>5</td>\n",
       "      <td>The morpho syntactically annotated corpus we u...</td>\n",
       "      <td>This corresponds to a new version of the Frenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>5586</td>\n",
       "      <td>3</td>\n",
       "      <td>There has been a large amount of work on senti...</td>\n",
       "      <td>For a detailed survey of the field of sentimen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>5587</td>\n",
       "      <td>5</td>\n",
       "      <td>This task setup is further described in the ta...</td>\n",
       "      <td>A complete description of the training and tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>5588</td>\n",
       "      <td>5</td>\n",
       "      <td>The baseline will be created by the Moses SMT ...</td>\n",
       "      <td>18 Our baseline is the SMT toolkit Moses (Koeh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>5591</td>\n",
       "      <td>3</td>\n",
       "      <td>Discourse structure in summarization Rhetorica...</td>\n",
       "      <td>Causal relations are among discourse relations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>5592</td>\n",
       "      <td>3</td>\n",
       "      <td>It builds on the C&amp;C CCG parser (Clark and Cur...</td>\n",
       "      <td>It receives CCG derivations from the C&amp;C parse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>5593</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Boxer (Bos et al., 2004) to parse natur...</td>\n",
       "      <td>In transforming natural language text to logic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>5594</td>\n",
       "      <td>5</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "      <td>We use the Moses phrase-based translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>5595</td>\n",
       "      <td>4</td>\n",
       "      <td>We perform bootstrap resampling with bounds es...</td>\n",
       "      <td>We also report 95% confidence intervals (CI) m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>5596</td>\n",
       "      <td>4</td>\n",
       "      <td>The major part of data comes from current and ...</td>\n",
       "      <td>The Europarl corpus (Koehn, 2005) is built fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>5598</td>\n",
       "      <td>4</td>\n",
       "      <td>For Italian, we use the word2vec to train word...</td>\n",
       "      <td>7 http://opennlp.apache.org 5.2.3.), we use th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>5599</td>\n",
       "      <td>3</td>\n",
       "      <td>The GATE plugin-based architecture (Cunningham...</td>\n",
       "      <td>GATE (Cunningham et al., 2002a) is an architec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>5600</td>\n",
       "      <td>3</td>\n",
       "      <td>We trained a 5-gram language model on the Xinh...</td>\n",
       "      <td>Both language models use modified Kneser-Ney s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>6101</td>\n",
       "      <td>5</td>\n",
       "      <td>We used GIZA++ (Och and Ney, 2003) along with ...</td>\n",
       "      <td>We automatically word-aligned the German part ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>6102</td>\n",
       "      <td>3</td>\n",
       "      <td>Our previous MLN-based approach for joint disa...</td>\n",
       "      <td>Joint disambiguation and clustering of mention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>6103</td>\n",
       "      <td>4</td>\n",
       "      <td>Pang et al. (2002) compare the performance of ...</td>\n",
       "      <td>Pang et al. (2002)  use machine learning metho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>6104</td>\n",
       "      <td>4</td>\n",
       "      <td>Jans et al. (2012) focused solely on the narra...</td>\n",
       "      <td>We refer to the system of Jans et al. (2012) a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>6105</td>\n",
       "      <td>4</td>\n",
       "      <td>We applied the Naive Bayes probabilistic super...</td>\n",
       "      <td>Naive Bayes and Decision Tree models were buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>6106</td>\n",
       "      <td>4</td>\n",
       "      <td>We therefore seek to allow quick incremental u...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>6107</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "      <td>We compare the model against the Moses phrase-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>6108</td>\n",
       "      <td>5</td>\n",
       "      <td>It has a much longer average sentence length t...</td>\n",
       "      <td>We then describe in more detail a modern Chine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>6109</td>\n",
       "      <td>4</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "      <td>The first two baselines are standard systems u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>6110</td>\n",
       "      <td>4</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "      <td>The first two baselines are standard systems u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>6111</td>\n",
       "      <td>4</td>\n",
       "      <td>The dropout rate was set to 0.5, and the model...</td>\n",
       "      <td>We use the cross-entropy loss function and min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>6112</td>\n",
       "      <td>5</td>\n",
       "      <td>The dropout rate was set to 0.5, and the model...</td>\n",
       "      <td>We use AdaGrad (Duchi et al., 2011)  with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>6113</td>\n",
       "      <td>5</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "      <td>We compare the model against the Moses phrase-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>6114</td>\n",
       "      <td>3</td>\n",
       "      <td>They had shown that the Penn Discourse TreeBan...</td>\n",
       "      <td>The Penn Discourse TreeBank (PDTB; Prasad et a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>6115</td>\n",
       "      <td>5</td>\n",
       "      <td>They had shown that the Penn Discourse TreeBan...</td>\n",
       "      <td>Pitler and Nenkova (2008) used discourse relat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>6117</td>\n",
       "      <td>5</td>\n",
       "      <td>McDonald et al. (2005) present a technique for...</td>\n",
       "      <td>The details of parsing model were presented in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>6118</td>\n",
       "      <td>3</td>\n",
       "      <td>We transformed the parse trees in OntoNotes in...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>6119</td>\n",
       "      <td>5</td>\n",
       "      <td>2 Translation Model Moses is a state-of-the-ar...</td>\n",
       "      <td>The decoder is built on top of an open-source ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>6120</td>\n",
       "      <td>4</td>\n",
       "      <td>For the OntoNotes data sets, Same speaker (Lee...</td>\n",
       "      <td>The mention detection of the Stanford corefere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>6121</td>\n",
       "      <td>4</td>\n",
       "      <td>It was one of the best parsers in the CoNLL Sh...</td>\n",
       "      <td>CTB6 is used as the Chinese data set in the Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>6122</td>\n",
       "      <td>4</td>\n",
       "      <td>We transformed the parse trees in OntoNotes in...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>6123</td>\n",
       "      <td>5</td>\n",
       "      <td>Results are reported on the test data using F1...</td>\n",
       "      <td>We report F1 performance scored using the offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>6125</td>\n",
       "      <td>5</td>\n",
       "      <td>The perplexity achieved by the 6-gram NN LM in...</td>\n",
       "      <td>The results of this experiment appear in Table...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>6126</td>\n",
       "      <td>3</td>\n",
       "      <td>We address the QE problem as a regression task...</td>\n",
       "      <td>We use LibSVM (Chang and Lin, 2011) as the SVM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>6127</td>\n",
       "      <td>5</td>\n",
       "      <td>Finally, it is also noticeable that the percen...</td>\n",
       "      <td>Baroni et al. (2002) also pointed out that the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>6128</td>\n",
       "      <td>4</td>\n",
       "      <td>Facts such as these are difficult to account f...</td>\n",
       "      <td>The most influential of the semantic approache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>6130</td>\n",
       "      <td>4</td>\n",
       "      <td>All these reordering models are tested using M...</td>\n",
       "      <td>We refer to that model as Moses en-es-100k , b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>6131</td>\n",
       "      <td>5</td>\n",
       "      <td>@BULLET RAE-Subj: Socher et al. (2011) propose...</td>\n",
       "      <td>We follow the formulation of vector compositio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>6132</td>\n",
       "      <td>5</td>\n",
       "      <td>We train a ridge regression model (Scikit-lear...</td>\n",
       "      <td>Our system is a linear model estimated using r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>6133</td>\n",
       "      <td>4</td>\n",
       "      <td>We follow the protocols in Collobert et al. (2...</td>\n",
       "      <td>We use SRL Collobert et al. (2011) to determin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>6134</td>\n",
       "      <td>4</td>\n",
       "      <td>This is equivalent to an SVM with the compound...</td>\n",
       "      <td>The sparsity of lexical features can also be t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>6135</td>\n",
       "      <td>4</td>\n",
       "      <td>All experiments are conducted using the Moses ...</td>\n",
       "      <td>For the comparisons of translation quality, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>6136</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Moses toolkit (Koehn et al., 2007) ...</td>\n",
       "      <td>We refer to that model as Moses en-es-100k , b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>6137</td>\n",
       "      <td>4</td>\n",
       "      <td>All experiments were on English part of speech...</td>\n",
       "      <td>These techniques were evaluated in experiments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>6138</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the word alignments to construct a phra...</td>\n",
       "      <td>We use the intersection of direct and reverse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>6139</td>\n",
       "      <td>4</td>\n",
       "      <td>We estimated a hierarchical MT model for the t...</td>\n",
       "      <td>We also compare with the standard phrase-based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>6140</td>\n",
       "      <td>4</td>\n",
       "      <td>A small amount of labeled data is used to map ...</td>\n",
       "      <td>Thus, inducing a number of clusters similar to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>6141</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the word alignments to construct a phra...</td>\n",
       "      <td>We use the intersection of direct and reverse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>6143</td>\n",
       "      <td>3</td>\n",
       "      <td>1 For a reference\" standard\" text we used the ...</td>\n",
       "      <td>The evaluation corpus is a subset of an ungram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>6144</td>\n",
       "      <td>4</td>\n",
       "      <td>We worked with the Europarl corpus (Koehn, 200...</td>\n",
       "      <td>Translations for English words in the lexical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>6145</td>\n",
       "      <td>5</td>\n",
       "      <td>The organization from SemEval-2013 Task 2: Sen...</td>\n",
       "      <td>We participated in both subtask A and B of Sem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>6146</td>\n",
       "      <td>5</td>\n",
       "      <td>Pending a planned full evaluation using the Mo...</td>\n",
       "      <td>To test our method, we conducted two lowresour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>6147</td>\n",
       "      <td>5</td>\n",
       "      <td>The entity transition features are then used t...</td>\n",
       "      <td>We use the support vector machine (SVM) rank a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>6150</td>\n",
       "      <td>4</td>\n",
       "      <td>Our second method is based on the recurrent ne...</td>\n",
       "      <td>In all of the above tasks, we compare the neur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>6153</td>\n",
       "      <td>4</td>\n",
       "      <td>We apply the Hidden Markov Model (HMM) (Viterb...</td>\n",
       "      <td>We use Viterbi algorithm (Viterbi, 1967) to de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>6155</td>\n",
       "      <td>5</td>\n",
       "      <td>We report BLEU (Papineni et al., 2001) of tran...</td>\n",
       "      <td>We measure translation quality via the BLEU sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>6156</td>\n",
       "      <td>5</td>\n",
       "      <td>It is a standard phrase-based machine translat...</td>\n",
       "      <td>All our systems are contrasted with a standard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>6158</td>\n",
       "      <td>5</td>\n",
       "      <td>Finally, we used Moses toolkit as phrase-based...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>6160</td>\n",
       "      <td>4</td>\n",
       "      <td>Concept similarity is computed using the edge-...</td>\n",
       "      <td>The degree of similarity between two similar w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>6161</td>\n",
       "      <td>5</td>\n",
       "      <td>Finally, we used Moses toolkit as phrase-based...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>6163</td>\n",
       "      <td>5</td>\n",
       "      <td>It is a standard phrase-based machine translat...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>6164</td>\n",
       "      <td>4</td>\n",
       "      <td>2 Translation Model Moses is a state-of-the-ar...</td>\n",
       "      <td>Finally, we used Moses toolkit as phrase-based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>6165</td>\n",
       "      <td>5</td>\n",
       "      <td>Otherwise, it is measured by WordNet similarit...</td>\n",
       "      <td>The degree of similarity between two similar w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>6166</td>\n",
       "      <td>4</td>\n",
       "      <td>We use 5-gram language models with Kneser-Ney ...</td>\n",
       "      <td>The language models are estimated using the Ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>6167</td>\n",
       "      <td>4</td>\n",
       "      <td>We adopt online learning, updating parameters ...</td>\n",
       "      <td>We use the cross-entropy loss function and min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>6169</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "      <td>We used SVM implementations from scikit-learn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>6170</td>\n",
       "      <td>4</td>\n",
       "      <td>We adopt online learning, updating parameters ...</td>\n",
       "      <td>We train the concept identification stage usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>6172</td>\n",
       "      <td>5</td>\n",
       "      <td>Finally, we used Moses toolkit as phrase-based...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>6173</td>\n",
       "      <td>3</td>\n",
       "      <td>Europarl (Koehn, 2005) is a multilingual paral...</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>6174</td>\n",
       "      <td>4</td>\n",
       "      <td>We leave the third-order models (Koo and Colli...</td>\n",
       "      <td>\" Koo10\" stands for the Model 1 in (Koo and Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>6175</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "      <td>Specifically, we used the standard Gradient Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>6176</td>\n",
       "      <td>5</td>\n",
       "      <td>For English and French a model was trained usi...</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>6177</td>\n",
       "      <td>3</td>\n",
       "      <td>Empirically we show that our model beats the s...</td>\n",
       "      <td>Very recently Rush et al. (2015) proposed a ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>6178</td>\n",
       "      <td>4</td>\n",
       "      <td>We use logistic regression with L2 regularizat...</td>\n",
       "      <td>We use the scikit implementation of Random For...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>6179</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the New York Times Annotated Corpus (Sa...</td>\n",
       "      <td>Of the remaining twelve, eight came from a lar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>6180</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the NMF and tf-idf implementations prov...</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>6181</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>6182</td>\n",
       "      <td>3</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "      <td>We also lemmatized all words using Stanford Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>6183</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>6184</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>6185</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>@BULLET Decoder: Moses (Koehn et al., 2007) wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>6186</td>\n",
       "      <td>4</td>\n",
       "      <td>Feature selection was performed using chi-squa...</td>\n",
       "      <td>We train and evaluate the classifiers in a 10-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>We use the Support Vector Machines implementat...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>Recently, there has been a successful attempt ...</td>\n",
       "      <td>Recently, there has been a successful attempt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>The first one is the WS-353 dataset (Finkelste...</td>\n",
       "      <td>The first one is the WS-353 dataset (Finkelste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>Abstract Meaning Representation (AMR) (Banares...</td>\n",
       "      <td>Abstract Meaning Representation (AMR) (Banares...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>We perform bootstrap resampling with bounds es...</td>\n",
       "      <td>We perform bootstrap resampling with bounds es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>It is used to support semantic analyses in HPS...</td>\n",
       "      <td>It is used to support semantic analyses in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>1201</td>\n",
       "      <td>4</td>\n",
       "      <td>We use Stanford parser (de Marneffe et al., 20...</td>\n",
       "      <td>We first use a dependency parser (de Marneffe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>1202</td>\n",
       "      <td>3</td>\n",
       "      <td>Rooth et al. (1999) propose an Expectation-Max...</td>\n",
       "      <td>Alternatively, Rooth et al. (1999)  propose an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>1203</td>\n",
       "      <td>3</td>\n",
       "      <td>The Levenshtein distance (Levenshtein, 1966) b...</td>\n",
       "      <td>The Levenshtein distance gives an indication o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>1204</td>\n",
       "      <td>1</td>\n",
       "      <td>We use the Moses toolkit (Koehn et al., 2007) ...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>We perform bootstrap resampling with bounds es...</td>\n",
       "      <td>We perform bootstrap resampling with bounds es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>It is used to support semantic analyses in HPS...</td>\n",
       "      <td>It is used to support semantic analyses in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>38</td>\n",
       "      <td>3</td>\n",
       "      <td>It is used to support semantic analyses in the...</td>\n",
       "      <td>It is used to support semantic analyses in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>It is used to support semantic analyses in HPS...</td>\n",
       "      <td>It is used to support semantic analyses in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>It is used to support semantic analyses in the...</td>\n",
       "      <td>It is used to support semantic analyses in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>We build upon our previous Markov Logic based ...</td>\n",
       "      <td>We build upon our previous approach for joint ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>Details about SVM and KFD can be found in (Tay...</td>\n",
       "      <td>Details about SVM and KRR can be found in (Tay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>We learn the parameters using a quasi-Newton p...</td>\n",
       "      <td>We learn the parameters  using a quasi-Newton ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>47</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the SCFG decoder cdec (Dyer et al., 201...</td>\n",
       "      <td>For direct translation, we use the SCFG decode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>This is known as the Distributional Hypothesis...</td>\n",
       "      <td>This is known as the Distributional Hypothesis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>All our models , as well as the parser describ...</td>\n",
       "      <td>The models , as well as the parser described i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>For strings, many such kernel functions exist ...</td>\n",
       "      <td>For strings, a lot of such kernel functions ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>The estimation of the semantically Smoothed Pa...</td>\n",
       "      <td>The estimation of the semantically Smoothed Pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>We train with the Adam optimizer (Kingma and B...</td>\n",
       "      <td>We train with the Adam optimizer (Kingma and B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>56</td>\n",
       "      <td>3</td>\n",
       "      <td>In our experimental study, we use the freely a...</td>\n",
       "      <td>In our experimental study, we use the freely a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>More recently, (Carpineto and Romano, 2010) sh...</td>\n",
       "      <td>OPTIMSRC: (Carpineto and Romano, 2010) showed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>They are based on the distributional hypothesi...</td>\n",
       "      <td>These methods rely on the distributional hypot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>Word alignment is performed using GIZA++ (Och ...</td>\n",
       "      <td>Word alignment is done using GIZA++ (Och and N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>61</td>\n",
       "      <td>4</td>\n",
       "      <td>Word alignment is performed using GIZA++ (Och ...</td>\n",
       "      <td>Word alignment is done using GIZA++ (Och and N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>We used Mallet software (McCallum, 2002) for C...</td>\n",
       "      <td>We used Mallet software (McCallum, 2002) for S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "      <td>The thesaurus consists of a hierarchy of 2,710...</td>\n",
       "      <td>The ontology, GoiTaikei, consists of a hierarc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>The detailed discussion is provided in the lon...</td>\n",
       "      <td>A detailed discussion on the results is provid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1043</th>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>The first one is the WS- 353 3 dataset (Finkel...</td>\n",
       "      <td>The first one is the WS-353 dataset (Finkelste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>68</td>\n",
       "      <td>3</td>\n",
       "      <td>A framework for human error analysis and error...</td>\n",
       "      <td>A framework for human error analysis has been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>MaxEnt classifier is a good example of this gr...</td>\n",
       "      <td>MaxEnt classifier is an example of this group ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>Among these media, blog is one of the communic...</td>\n",
       "      <td>Blog is one of the crucial, communicative and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>For preprocessing, we used MADA (Morphological...</td>\n",
       "      <td>For this purpose, we use MADA (Morphological A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>74</td>\n",
       "      <td>4</td>\n",
       "      <td>We trained a 5-gram language model on the Xinh...</td>\n",
       "      <td>Our 5-gram language model was trained on the X...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "      <td>To determine semantic type and subtype, we tra...</td>\n",
       "      <td>To determine semantic types and subtypes, we t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1050</th>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>We use the scikit implementation of Random For...</td>\n",
       "      <td>We use the scikit implementation of SVM (Pedre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>Each term in the input text will be represente...</td>\n",
       "      <td>Each term in the input text is represented by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>79</td>\n",
       "      <td>5</td>\n",
       "      <td>An algorithm, the Kuhn-Munkres method (Kuhn, 1...</td>\n",
       "      <td>An algorithm, the Kuhn-Munkres method (Kuhn, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>We use Collapsed Gibbs Sampling (Griffiths and...</td>\n",
       "      <td>We use collapsed Gibbs sampling (Griffiths and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>82</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Stanford dependency parser (Marneff...</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>83</td>\n",
       "      <td>5</td>\n",
       "      <td>Filter weights are initialized using Glorot-Be...</td>\n",
       "      <td>Weights are initialized using Glorot-Bengio st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>Automatic sentence alignment of the training d...</td>\n",
       "      <td>Automatic sentence alignment of the training d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "      <td>We use the AdaGrad optimizer (Duchi et al., 20...</td>\n",
       "      <td>We use AdaGrad (Duchi et al., 2011)  with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>87</td>\n",
       "      <td>5</td>\n",
       "      <td>Then we did word alignment using GIZA++ (Och a...</td>\n",
       "      <td>We performed word alignment using GIZA++ (Och ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>88</td>\n",
       "      <td>4</td>\n",
       "      <td>For example, DIRT (Lin and Pantel, 2001) aims ...</td>\n",
       "      <td>For example, DIRT (Lin and Pantel, 2001) aims ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>89</td>\n",
       "      <td>3</td>\n",
       "      <td>The annotation was performed manually using th...</td>\n",
       "      <td>The annotation was performed using the BRAT 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>91</td>\n",
       "      <td>4</td>\n",
       "      <td>System proposed by (Li et al., 2006 ), uses a ...</td>\n",
       "      <td>A similar semantic similarity measure, propose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>92</td>\n",
       "      <td>3</td>\n",
       "      <td>This corpus contains around 11,000 NPs annotat...</td>\n",
       "      <td>It consists of 50 texts taken from the WSJ por...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>93</td>\n",
       "      <td>4</td>\n",
       "      <td>All modules take as input the corpus documents...</td>\n",
       "      <td>All components take as input the corpus docume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>95</td>\n",
       "      <td>4</td>\n",
       "      <td>From the pioneering work of (Rapp, 1995 ), con...</td>\n",
       "      <td>From the pioneering work of (Rapp, 1995 ), BLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>96</td>\n",
       "      <td>5</td>\n",
       "      <td>Europarl (Koehn, 2005) is a multilingual paral...</td>\n",
       "      <td>We run our experiments on Europarl (Koehn, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>TESLA (Translation Evaluation of Sentences wit...</td>\n",
       "      <td>TESLA-F was called TESLA in Liu et al. (2010) .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>For building the baseline SMT system, we used ...</td>\n",
       "      <td>For building our SMT systems, the open-source ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>102</td>\n",
       "      <td>4</td>\n",
       "      <td>Rhetorical Structure Theory (RST) (Mann and Th...</td>\n",
       "      <td>Rhetorical Structure Theory (RST) (Mann and Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>In addition, the fix-discount method in (Foste...</td>\n",
       "      <td>In addition, the fix-discount method (Foster e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>104</td>\n",
       "      <td>5</td>\n",
       "      <td>Memory-based language processing (Daelemans an...</td>\n",
       "      <td>Memory-based language processing (Daelemans an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>We calculate statistical significance of perfo...</td>\n",
       "      <td>We tested the significance of differences usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>106</td>\n",
       "      <td>4</td>\n",
       "      <td>For instance, machine translation (MT) systems...</td>\n",
       "      <td>In addition, machine translation (MT) systems ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>109</td>\n",
       "      <td>4</td>\n",
       "      <td>1 with 2 -regularization using AdaGrad (Duchi ...</td>\n",
       "      <td>1 -regularization using AdaGrad (Duchi et al.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>111</td>\n",
       "      <td>3</td>\n",
       "      <td>Why does the lr model outperform Berkeley 13 T...</td>\n",
       "      <td>The MUC score (Vilain et al., 1995) counts the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>112</td>\n",
       "      <td>3</td>\n",
       "      <td>In the context of this paper we will be focusi...</td>\n",
       "      <td>We will focus on the syntactic tree kernel des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>113</td>\n",
       "      <td>5</td>\n",
       "      <td>Europarl (Koehn, 2005) is a multilingual paral...</td>\n",
       "      <td>The Europarl corpus (Koehn, 2005) is built fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>114</td>\n",
       "      <td>4</td>\n",
       "      <td>We have used Foma, a free software tool to spe...</td>\n",
       "      <td>The module was implemented using Foma, a free ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>116</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the same test set used in Li et al. (2...</td>\n",
       "      <td>We used the same test data as in Li et al. (20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>117</td>\n",
       "      <td>3</td>\n",
       "      <td>The Penn Discourse Treebank (PDTB, Prasad et a...</td>\n",
       "      <td>Penn Discourse Treebank The Penn Discourse Tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "      <td>We used a GB implementation of the scikit-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>119</td>\n",
       "      <td>3</td>\n",
       "      <td>Since the commonly used word similarity datase...</td>\n",
       "      <td>The second is the MEN dataset (Bruni et al., 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>The training data of the shared task is the NU...</td>\n",
       "      <td>The training data released by the task organiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>All system implementation was done using Pytho...</td>\n",
       "      <td>All of the machine learning was done using sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>123</td>\n",
       "      <td>3</td>\n",
       "      <td>It has been shown that a diverse set of predic...</td>\n",
       "      <td>It has been long identified in NLP that a dive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>124</td>\n",
       "      <td>4</td>\n",
       "      <td>In the 2013 system, we had used SentiStrength ...</td>\n",
       "      <td>In our system, we used the sentiment lexicon p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>125</td>\n",
       "      <td>3</td>\n",
       "      <td>Finally, we also compare the quality of the ca...</td>\n",
       "      <td>We also compared the quality of the candidate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>126</td>\n",
       "      <td>4</td>\n",
       "      <td>These methods are based on the distributional ...</td>\n",
       "      <td>Corpus-based VSMs follow the standard distribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>127</td>\n",
       "      <td>5</td>\n",
       "      <td>All annotations were done using the BRAT rapid...</td>\n",
       "      <td>The annotations were made using the BRAT rapid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>Compared to WordNet (Fellbaum, 1998 ), there a...</td>\n",
       "      <td>Compared to WordNet (Fellbaum, 1998 ), there a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>131</td>\n",
       "      <td>5</td>\n",
       "      <td>For training, we use Adam (Kingma and Ba, 2015...</td>\n",
       "      <td>We use Adam (Kingma and Ba, 2015) for optimiza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>132</td>\n",
       "      <td>3</td>\n",
       "      <td>For our classifier, we use SVMs, specifically ...</td>\n",
       "      <td>Specifically, we use the LIBLINEAR SVM package...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>133</td>\n",
       "      <td>4</td>\n",
       "      <td>The first two experiments concern the predicti...</td>\n",
       "      <td>The first two experiments involve predicting t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "      <td>We used only the non-ensembled left-to-right r...</td>\n",
       "      <td>We used only the non-ensembled left-to-right r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>137</td>\n",
       "      <td>2</td>\n",
       "      <td>We used only the non-ensembled left-to-right r...</td>\n",
       "      <td>We used only the non-ensembled left-to-right r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>138</td>\n",
       "      <td>4</td>\n",
       "      <td>The MSD morphological coding system was develo...</td>\n",
       "      <td>The MSD morphological coding system is a posit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>139</td>\n",
       "      <td>5</td>\n",
       "      <td>The phrase tables were generated by means of s...</td>\n",
       "      <td>The phrase table was generated employing symme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>140</td>\n",
       "      <td>5</td>\n",
       "      <td>Word alignment is performed using GIZA++ (Och ...</td>\n",
       "      <td>Word alignment is performed with Giza++ (Och a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>141</td>\n",
       "      <td>4</td>\n",
       "      <td>We experimented with several levels of cluster...</td>\n",
       "      <td>Following Koo et al. (2008) , we also experime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>143</td>\n",
       "      <td>4</td>\n",
       "      <td>(Raghavan et al. (2007)) measure the benefit f...</td>\n",
       "      <td>Raghavan et al. (2007)  evaluate benefit from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>144</td>\n",
       "      <td>4</td>\n",
       "      <td>We built a modified Kneser-Ney smoothed 5-gram...</td>\n",
       "      <td>Training and querying of a modified Kneser-Ney...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>146</td>\n",
       "      <td>4</td>\n",
       "      <td>A formal PAC-style analysis can be found in (A...</td>\n",
       "      <td>The formal derivation can be found in (Ando an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>147</td>\n",
       "      <td>4</td>\n",
       "      <td>The first model we introduce is based on the r...</td>\n",
       "      <td>The model as described thus far is identical t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>148</td>\n",
       "      <td>4</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>149</td>\n",
       "      <td>3</td>\n",
       "      <td>The classifier experiments were carried out us...</td>\n",
       "      <td>The classifier evaluations were carried out us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>The training data of the shared task is the NU...</td>\n",
       "      <td>The training data for the task is from the NUC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>151</td>\n",
       "      <td>5</td>\n",
       "      <td>SALDO (Borin et al., 2013) is the largest free...</td>\n",
       "      <td>SALDO (Borin et al., 2013) is the most compreh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>152</td>\n",
       "      <td>5</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1108</th>\n",
       "      <td>153</td>\n",
       "      <td>5</td>\n",
       "      <td>However, those string-to-tree systems run slow...</td>\n",
       "      <td>However such string-to-tree systems run slowly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>154</td>\n",
       "      <td>4</td>\n",
       "      <td>The 5-gram target language model was trained u...</td>\n",
       "      <td>We trained an English 5-gram language model us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>155</td>\n",
       "      <td>5</td>\n",
       "      <td>For all experiments, we used the Moses SMT sys...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>156</td>\n",
       "      <td>4</td>\n",
       "      <td>We evaluate our method on the following data s...</td>\n",
       "      <td>@BULLET OntoNotes-Test: Test set of the OntoNo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Moses phrase-based translation syst...</td>\n",
       "      <td>We use the state-of-the-art phrase-based machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>158</td>\n",
       "      <td>4</td>\n",
       "      <td>But we randomly selected 90% of the training d...</td>\n",
       "      <td>Since the training data used in Li et al. (200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>The BLEU score measures the precision of n-gra...</td>\n",
       "      <td>BLEU score: This score measures the precision ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>160</td>\n",
       "      <td>3</td>\n",
       "      <td>The training data of the shared task is the NU...</td>\n",
       "      <td>The training data provided for the task is a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>161</td>\n",
       "      <td>3</td>\n",
       "      <td>Test data was drawn from the Open American Nat...</td>\n",
       "      <td>We selected the dataset of Jurgens and Klapaft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>162</td>\n",
       "      <td>5</td>\n",
       "      <td>We measure statistical significance using 95% ...</td>\n",
       "      <td>The statistical significance tests using 95% c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>163</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the English portion of the ACE 2005 rel...</td>\n",
       "      <td>We evaluate our relation extraction system on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>164</td>\n",
       "      <td>5</td>\n",
       "      <td>This data was collected for the 2014 SemEval c...</td>\n",
       "      <td>Sentences Involving Compositional Knowledge (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>165</td>\n",
       "      <td>5</td>\n",
       "      <td>The parsing model used for intra-sentential pa...</td>\n",
       "      <td>Our novel parsing model is the Dynamic Conditi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>166</td>\n",
       "      <td>4</td>\n",
       "      <td>Latent Dirichlet Allocation (LDA) is a generat...</td>\n",
       "      <td>Combination of latent topics ent Dirichlet All...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>167</td>\n",
       "      <td>5</td>\n",
       "      <td>Distributional hypothesis theory (Harris, 1954...</td>\n",
       "      <td>It therefore follows the distributional hypoth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>168</td>\n",
       "      <td>5</td>\n",
       "      <td>Distributional hypothesis theory (Harris, 1954...</td>\n",
       "      <td>It therefore follows the distributional hypoth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>169</td>\n",
       "      <td>5</td>\n",
       "      <td>In 2009, Yefang Wang (Wang et al., 2009) used ...</td>\n",
       "      <td>In 2009, Yefang Wang (Wang et al., 2009) used ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>170</td>\n",
       "      <td>3</td>\n",
       "      <td>We built a 5-gram language model on the Englis...</td>\n",
       "      <td>We built a modified Kneser-Ney smoothed 5-gram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>171</td>\n",
       "      <td>3</td>\n",
       "      <td>The remaining three models are all Naive Bayes...</td>\n",
       "      <td>The other models are trained on native English...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>172</td>\n",
       "      <td>4</td>\n",
       "      <td>We apply bootstrapping (Kozareva et al., 2008)...</td>\n",
       "      <td>We then apply bootstrapping (Kozareva et al., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>173</td>\n",
       "      <td>5</td>\n",
       "      <td>These methods are based on the distributional ...</td>\n",
       "      <td>It therefore follows the distributional hypoth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>174</td>\n",
       "      <td>5</td>\n",
       "      <td>These results verify the benefit of using LTAG...</td>\n",
       "      <td>Our hypothesis is that the LTAG based features...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>175</td>\n",
       "      <td>5</td>\n",
       "      <td>RG-65: (Rubenstein and Goodenough, 1965) has 6...</td>\n",
       "      <td>RG-65: (Rubenstein and Goodenough, 1965) is se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>176</td>\n",
       "      <td>5</td>\n",
       "      <td>The significance tests were performed using th...</td>\n",
       "      <td>Statistical significance tests are performed u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>177</td>\n",
       "      <td>5</td>\n",
       "      <td>The default Phrasal search algorithm is cube p...</td>\n",
       "      <td>The search is typically carried out using the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>178</td>\n",
       "      <td>5</td>\n",
       "      <td>In-domain data is mainly used to solve the pro...</td>\n",
       "      <td>In-domain data only solves the problem of data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>179</td>\n",
       "      <td>4</td>\n",
       "      <td>All experiments were carried out using the ope...</td>\n",
       "      <td>All the experiments are carried out in Moses t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>180</td>\n",
       "      <td>4</td>\n",
       "      <td>First, we apply heuristics to determine number...</td>\n",
       "      <td>We then apply heuristics to determine number a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>181</td>\n",
       "      <td>4</td>\n",
       "      <td>Rank SVM (Joachims, 2002) is a method based on...</td>\n",
       "      <td>For this task we use RankSVM (Joachims, 2002) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>182</td>\n",
       "      <td>3</td>\n",
       "      <td>A more detailed description of the task can be...</td>\n",
       "      <td>A precise description of the corpus and metric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>184</td>\n",
       "      <td>5</td>\n",
       "      <td>A Tree Kernel function is a convolution kernel...</td>\n",
       "      <td>Tree Kernel (TK) functions are convolution ker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>185</td>\n",
       "      <td>4</td>\n",
       "      <td>We build upon our previous approach for joint ...</td>\n",
       "      <td>For disambiguation and clustering we build upo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>186</td>\n",
       "      <td>4</td>\n",
       "      <td>ROUGE-2 metric (Lin, 2004) is used for the eva...</td>\n",
       "      <td>We used the ROUGE-1 evaluation metric (Lin, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>187</td>\n",
       "      <td>5</td>\n",
       "      <td>We used Mallet software (McCallum, 2002) for C...</td>\n",
       "      <td>We used Mallet toolkit (McCallum, 2002) for CR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>188</td>\n",
       "      <td>3</td>\n",
       "      <td>We exploit a transition-based framework with g...</td>\n",
       "      <td>Our joint parsing model exploits a transition-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>189</td>\n",
       "      <td>4</td>\n",
       "      <td>The annotation was performed using the BRAT 2 ...</td>\n",
       "      <td>The annotations were made using the BRAT rapid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>190</td>\n",
       "      <td>5</td>\n",
       "      <td>For building the word alignment models we use ...</td>\n",
       "      <td>To build the word alignment models we used the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>191</td>\n",
       "      <td>5</td>\n",
       "      <td>The reliability of the annotation was evaluate...</td>\n",
       "      <td>We evaluated annotation reliability by using t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>192</td>\n",
       "      <td>5</td>\n",
       "      <td>It therefore follows the distributional hypoth...</td>\n",
       "      <td>Distributional models of meaning follow the di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>193</td>\n",
       "      <td>4</td>\n",
       "      <td>This dataset is composed of 35 triplets of sen...</td>\n",
       "      <td>This dataset is composed of 32 sentence quadru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>194</td>\n",
       "      <td>5</td>\n",
       "      <td>The significance tests were performed using th...</td>\n",
       "      <td>A statistical significance test was performed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>195</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the standard alignment tool Giza++ (Och...</td>\n",
       "      <td>We use the Giza++ tool (Och and Ney, 2003) to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>196</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the standard alignment tool Giza++ (Och...</td>\n",
       "      <td>We use the Giza++ tool (Och and Ney, 2003) to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>198</td>\n",
       "      <td>3</td>\n",
       "      <td>The kernels are combined using Gaussian proces...</td>\n",
       "      <td>Gaussian process regression (GPR) (Rasmussen a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>199</td>\n",
       "      <td>5</td>\n",
       "      <td>To overcome this, Agirre et al. (2009) used Ma...</td>\n",
       "      <td>In another work, a corpus of roughly 1.6 Teraw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>We measure statistical significance using 95% ...</td>\n",
       "      <td>We calculated significance using paired bootst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>701</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Stanford dependency parser (Chen an...</td>\n",
       "      <td>In this work, we use the Stanford neural depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>702</td>\n",
       "      <td>3</td>\n",
       "      <td>Next, a tweet was tokenized and fed into MADAM...</td>\n",
       "      <td>MADAMIRA (Pasha et al., 2014) is a morphologic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>703</td>\n",
       "      <td>4</td>\n",
       "      <td>(Yarowsky ,(1995)) has proposed a bootstrappin...</td>\n",
       "      <td>Yarowsky (1995) proposed such a method for wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>704</td>\n",
       "      <td>5</td>\n",
       "      <td>We also list the previous state-of-the-art per...</td>\n",
       "      <td>We also list the results from SMT model (Durra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>705</td>\n",
       "      <td>4</td>\n",
       "      <td>We used Adam (Kingma and Ba, 2014) with a lear...</td>\n",
       "      <td>We use the Adam (Kingma and Ba, 2014) algorith...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>706</td>\n",
       "      <td>2</td>\n",
       "      <td>Distributional semantics is based on the idea ...</td>\n",
       "      <td>Word co-occurence statistics\" You shall know a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>707</td>\n",
       "      <td>3</td>\n",
       "      <td>In order to estimate the basic lexical similar...</td>\n",
       "      <td>The co-occurrence Word Space is acquired throu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>709</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "      <td>We used the scikit-learn toolkit to train our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>710</td>\n",
       "      <td>3</td>\n",
       "      <td>The translation model was trained by GIZA++ (O...</td>\n",
       "      <td>The probability p(E) is computed using a simpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>711</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a generalization of the operator Id in...</td>\n",
       "      <td>This is similar to the operator Intro in (Kapl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>712</td>\n",
       "      <td>4</td>\n",
       "      <td>We used standard classifiers available in scik...</td>\n",
       "      <td>We used a GB implementation of the scikit-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>714</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "      <td>For the linear logistic regression implementat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>715</td>\n",
       "      <td>5</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "      <td>Finally, we used Moses toolkit as phrase-based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1167</th>\n",
       "      <td>716</td>\n",
       "      <td>3</td>\n",
       "      <td>4 Word alignments are created by aligning the ...</td>\n",
       "      <td>Baseline word alignments were obtained by runn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>717</td>\n",
       "      <td>4</td>\n",
       "      <td>The perplexity achieved by the 6- gram NN LM i...</td>\n",
       "      <td>The language model is a 5-gram with interpolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>718</td>\n",
       "      <td>5</td>\n",
       "      <td>We used standard classifiers available in scik...</td>\n",
       "      <td>We used the scikit-learn toolkit to train our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1170</th>\n",
       "      <td>719</td>\n",
       "      <td>5</td>\n",
       "      <td>Statistical machine translation is typically p...</td>\n",
       "      <td>It is a standard phrase-based machine translat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>720</td>\n",
       "      <td>3</td>\n",
       "      <td>WordNet (Miller et al., 1990) is an on-line hi...</td>\n",
       "      <td>The WordNet on-line lexical database (Miller e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>721</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the scikit implementation of Random For...</td>\n",
       "      <td>We used a GB implementation of the scikit-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>724</td>\n",
       "      <td>4</td>\n",
       "      <td>We lemmatise the head of each constituent with...</td>\n",
       "      <td>We used the Stuttgart TreeTagger (Schmid, 1994...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>725</td>\n",
       "      <td>4</td>\n",
       "      <td>Our text processing uses the Natural Language ...</td>\n",
       "      <td>Part-of-speech tagging was accomplished using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>726</td>\n",
       "      <td>5</td>\n",
       "      <td>They used the Web-based annotation tool brat (...</td>\n",
       "      <td>The annotation was performed manually using th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176</th>\n",
       "      <td>729</td>\n",
       "      <td>3</td>\n",
       "      <td>Our system participated in SemEval-2013 Task 2...</td>\n",
       "      <td>We participated in both subtask A and B of Sem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>730</td>\n",
       "      <td>5</td>\n",
       "      <td>The English text was tokenized using the word ...</td>\n",
       "      <td>We tokenise the text using the default tokenis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178</th>\n",
       "      <td>731</td>\n",
       "      <td>4</td>\n",
       "      <td>The webpages were parsed using the Stanford Co...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1179</th>\n",
       "      <td>732</td>\n",
       "      <td>3</td>\n",
       "      <td>Statistical machine translation is typically p...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>733</td>\n",
       "      <td>3</td>\n",
       "      <td>The English side was tokenized using the Moses...</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>734</td>\n",
       "      <td>5</td>\n",
       "      <td>We trained an English 5-gram language model us...</td>\n",
       "      <td>We built a trigram language model with Kneser-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1182</th>\n",
       "      <td>735</td>\n",
       "      <td>5</td>\n",
       "      <td>All of the text data from Reddit was tokenized...</td>\n",
       "      <td>We tokenise the text using the default tokenis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>736</td>\n",
       "      <td>4</td>\n",
       "      <td>Our machine translation systems are trained us...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>737</td>\n",
       "      <td>5</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185</th>\n",
       "      <td>738</td>\n",
       "      <td>5</td>\n",
       "      <td>We build upon our previous approach for joint ...</td>\n",
       "      <td>Scope-ignorant (Disambig.): Our previous MLN-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1186</th>\n",
       "      <td>739</td>\n",
       "      <td>5</td>\n",
       "      <td>We used Adam (Kingma and Ba, 2014) with a lear...</td>\n",
       "      <td>We use ADAM (Kingma and Ba, 2014) with a learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>740</td>\n",
       "      <td>5</td>\n",
       "      <td>For all experiments, we used the Moses SMT sys...</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1188</th>\n",
       "      <td>741</td>\n",
       "      <td>4</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>742</td>\n",
       "      <td>5</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>743</td>\n",
       "      <td>5</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "      <td>First, we used the Moses toolkit (Koehn et al....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>744</td>\n",
       "      <td>5</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>745</td>\n",
       "      <td>5</td>\n",
       "      <td>We develop translation models using the phrase...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>746</td>\n",
       "      <td>4</td>\n",
       "      <td>We used TnT (Brants, 2000 ), trained on the Ne...</td>\n",
       "      <td>We employed the TnT tagger (Brants, 2000) whic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>747</td>\n",
       "      <td>3</td>\n",
       "      <td>The webpages were parsed using the Stanford Co...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>748</td>\n",
       "      <td>5</td>\n",
       "      <td>We develop translation models using the phrase...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>750</td>\n",
       "      <td>3</td>\n",
       "      <td>The term frequency count is normalized with th...</td>\n",
       "      <td>TF-IDF is a standard statistical method that c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>751</td>\n",
       "      <td>4</td>\n",
       "      <td>We assessed the statistical significance of di...</td>\n",
       "      <td>We assess statistical significance of the diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>752</td>\n",
       "      <td>5</td>\n",
       "      <td>A framework for human error analysis and error...</td>\n",
       "      <td>A framework for human error analysis has been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>753</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, Chang et al. (2009) found that th...</td>\n",
       "      <td>Chang et al. (2009) stated that one reason is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>754</td>\n",
       "      <td>4</td>\n",
       "      <td>On the Chinese side, we used the morphological...</td>\n",
       "      <td>The MMA system (Kruengkrai et al., 2009) train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>755</td>\n",
       "      <td>5</td>\n",
       "      <td>conducted using the Moses phrase-based decoder...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>756</td>\n",
       "      <td>3</td>\n",
       "      <td>conducted using the Moses phrase-based decoder...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>757</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>conducted using the Moses phrase-based decoder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>759</td>\n",
       "      <td>3</td>\n",
       "      <td>Then the processed data was performed for toke...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1205</th>\n",
       "      <td>760</td>\n",
       "      <td>5</td>\n",
       "      <td>We applied bootstrap resampling (Koehn, 2004) ...</td>\n",
       "      <td>We measure significance of results using boots...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>761</td>\n",
       "      <td>5</td>\n",
       "      <td>This system uses the attentional encoder-decod...</td>\n",
       "      <td>We followed the encoder-decoder architecture w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>762</td>\n",
       "      <td>3</td>\n",
       "      <td>The language model is a 5-gram KenLM (Heafield...</td>\n",
       "      <td>We built a trigram language model with Kneser-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>763</td>\n",
       "      <td>5</td>\n",
       "      <td>Weighted Finite State Transducers (FSTs) used ...</td>\n",
       "      <td>The decoder is implemented with Weighted Finit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>764</td>\n",
       "      <td>5</td>\n",
       "      <td>Weighted Finite State Transducers (FSTs) used ...</td>\n",
       "      <td>The decoder is implemented with Weighted Finit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>765</td>\n",
       "      <td>3</td>\n",
       "      <td>Then the processed data was performed for toke...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>766</td>\n",
       "      <td>4</td>\n",
       "      <td>It is a modification of the model proposed by ...</td>\n",
       "      <td>Our first baseline is MI09, a distantly superv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>767</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Scikit-learn (Pedregosa et al., 2011 ),...</td>\n",
       "      <td>We used the Scikit-learn machine learning libr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>768</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Universal POS Tagset (UPOS) of Petr...</td>\n",
       "      <td>This in turn relies on the same underlying fea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1214</th>\n",
       "      <td>768</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Universal POS Tagset (UPOS) of Petr...</td>\n",
       "      <td>This in turn relies on the same underlying fea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>769</td>\n",
       "      <td>3</td>\n",
       "      <td>The CRF is trained using decisions from the fo...</td>\n",
       "      <td>MADAMIRA (Pasha et al., 2014) is a morphologic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>770</td>\n",
       "      <td>5</td>\n",
       "      <td>For language modeling, we trained a separate 5...</td>\n",
       "      <td>We built a modified Kneser- Ney smoothed 5-gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>771</td>\n",
       "      <td>5</td>\n",
       "      <td>with the training script of the Moses toolkit ...</td>\n",
       "      <td>We preprocessed the training corpora with scri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>772</td>\n",
       "      <td>5</td>\n",
       "      <td>The word alignment was trained using GIZA++ (O...</td>\n",
       "      <td>We performed word alignment using GIZA++ (Och ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>773</td>\n",
       "      <td>4</td>\n",
       "      <td>We use linear SVMs from LIBLINEAR and SVMs wit...</td>\n",
       "      <td>As our learner, we use LIBSVM with a linear ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>774</td>\n",
       "      <td>5</td>\n",
       "      <td>We specify the hierarchical aligner in terms o...</td>\n",
       "      <td>We specify our dynamic programming algorithm a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>775</td>\n",
       "      <td>4</td>\n",
       "      <td>We use Stanford parser (de Marneffe et al., 20...</td>\n",
       "      <td>We use the Stanford dependency parser (de Marn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>776</td>\n",
       "      <td>4</td>\n",
       "      <td>In this work, we use the Stanford neural depen...</td>\n",
       "      <td>In order to detect the object pronouns, we emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>778</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Stanford parser (de Marneffe et al., 20...</td>\n",
       "      <td>We also obtain the dependency parse of the sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>779</td>\n",
       "      <td>5</td>\n",
       "      <td>The learning algorithm used in our coreference...</td>\n",
       "      <td>The question classifier used in the experiment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>780</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Stanford parser (de Marneffe et al., 20...</td>\n",
       "      <td>We use the Stanford dependency parser (de Marn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>781</td>\n",
       "      <td>5</td>\n",
       "      <td>Distributional semantics (see Cohen and Widdow...</td>\n",
       "      <td>The former that is the most popular relies on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>782</td>\n",
       "      <td>4</td>\n",
       "      <td>Their work is part of the state-of-the-art Ara...</td>\n",
       "      <td>MADAMIRA (Pasha et al., 2014) is a morphologic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228</th>\n",
       "      <td>783</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Stanford parser (de Marneffe et al., 20...</td>\n",
       "      <td>We also obtain the dependency parse of the sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>784</td>\n",
       "      <td>4</td>\n",
       "      <td>Phrasal follows the log-linear approach to phr...</td>\n",
       "      <td>The log-linear approach to phrase-based transl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>785</td>\n",
       "      <td>5</td>\n",
       "      <td>Training data are based on a concatenation of ...</td>\n",
       "      <td>Both CDS corpora are available from the CHILDE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>786</td>\n",
       "      <td>3</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>787</td>\n",
       "      <td>5</td>\n",
       "      <td>5-gram language models of Turkish and English ...</td>\n",
       "      <td>We built a 5-gram language model on the Englis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>788</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the relation classification dataset of...</td>\n",
       "      <td>We evaluated our model on a semantic relation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1234</th>\n",
       "      <td>789</td>\n",
       "      <td>4</td>\n",
       "      <td>We then run word alignment with GIZA++ (Och an...</td>\n",
       "      <td>We performed word alignment using GIZA++ (Och ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235</th>\n",
       "      <td>790</td>\n",
       "      <td>3</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>For all experiments, we used the Moses SMT sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1236</th>\n",
       "      <td>791</td>\n",
       "      <td>5</td>\n",
       "      <td>The significance tests were performed using th...</td>\n",
       "      <td>The statistical significance tests using 95% c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>792</td>\n",
       "      <td>5</td>\n",
       "      <td>All experiments were carried out using the ope...</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>793</td>\n",
       "      <td>5</td>\n",
       "      <td>These sentences have then be fed into an effic...</td>\n",
       "      <td>The sentences are fed into the PET HPSG parser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>794</td>\n",
       "      <td>4</td>\n",
       "      <td>The language model is a 5-gram KenLM (Heafield...</td>\n",
       "      <td>The 5-gram target language model was trained u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>795</td>\n",
       "      <td>4</td>\n",
       "      <td>Cohen et al. (2012)  present a spectral algori...</td>\n",
       "      <td>First, we present an algorithm for estimating ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>796</td>\n",
       "      <td>5</td>\n",
       "      <td>Distributional hypothesis theory (Harris, 1954...</td>\n",
       "      <td>Similar row vectors in T indicate similar cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>797</td>\n",
       "      <td>5</td>\n",
       "      <td>Distributional hypothesis theory (Harris, 1954...</td>\n",
       "      <td>Similar row vectors in T indicate similar cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>799</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Stanford dependency parser (Marneff...</td>\n",
       "      <td>We first use a dependency parser (de Marneffe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>800</td>\n",
       "      <td>3</td>\n",
       "      <td>We also replicated the experiment of Holmqvist...</td>\n",
       "      <td>We also carried out a chunk-reordering PB-SMT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>1301</td>\n",
       "      <td>4</td>\n",
       "      <td>We develop translation models using the phrase...</td>\n",
       "      <td>conducted using the Moses phrase-based decoder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>1302</td>\n",
       "      <td>5</td>\n",
       "      <td>POS tagging was performed with TreeTagger (Sch...</td>\n",
       "      <td>The test set was tagged with the French TreeTa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>1303</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the MaltParser (Nivre et al., 2007) fo...</td>\n",
       "      <td>For the parsing experimens I used MaltParser (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>1304</td>\n",
       "      <td>4</td>\n",
       "      <td>Classification uses the scikit-learn Python pa...</td>\n",
       "      <td>We used a GB implementation of the scikit-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>1305</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Stanford dependency parser (Marneff...</td>\n",
       "      <td>These features were obtained using the Stanfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>1306</td>\n",
       "      <td>5</td>\n",
       "      <td>We used Adam (Kingma and Ba, 2014) with a lear...</td>\n",
       "      <td>We used Adam as the optimizer (Kingma and Ba, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>1307</td>\n",
       "      <td>4</td>\n",
       "      <td>Word alignment is performed using GIZA++ (Och ...</td>\n",
       "      <td>This is done using IBM Model 1 (Brown et al., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>1308</td>\n",
       "      <td>4</td>\n",
       "      <td>The test set was tagged with the French TreeTa...</td>\n",
       "      <td>For the French side, the TreeTagger (Schmid, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>1309</td>\n",
       "      <td>4</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "      <td>conducted using the Moses phrase-based decoder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>1310</td>\n",
       "      <td>5</td>\n",
       "      <td>POS tagging was performed with TreeTagger (Sch...</td>\n",
       "      <td>POS tagging is performed using the IMS Tree Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>1311</td>\n",
       "      <td>5</td>\n",
       "      <td>For building the word alignment models we use ...</td>\n",
       "      <td>For word alignments, we used Mgiza++ (Gao and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>1313</td>\n",
       "      <td>3</td>\n",
       "      <td>The Polish data is taken from the EUROPARL cor...</td>\n",
       "      <td>All the data come from Europarl (Koehn, 2005 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>1314</td>\n",
       "      <td>5</td>\n",
       "      <td>The German-to-English corpus is Europarl v7 (K...</td>\n",
       "      <td>The Polish data is taken from the EUROPARL cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>1315</td>\n",
       "      <td>5</td>\n",
       "      <td>Distributional models of meaning follow the di...</td>\n",
       "      <td>Distributional similarity relies on the distri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>1316</td>\n",
       "      <td>4</td>\n",
       "      <td>conducted using the Moses phrase-based decoder...</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>1317</td>\n",
       "      <td>3</td>\n",
       "      <td>Europarl 2 (Koehn, 2005 ): it is a corpus of p...</td>\n",
       "      <td>In our MT experiments, we translate French int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>1318</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted statistical significance tests fo...</td>\n",
       "      <td>Statistical significance is tested on the BLEU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>1320</td>\n",
       "      <td>5</td>\n",
       "      <td>The English side was tokenized using the Moses...</td>\n",
       "      <td>The baseline will be created by the Moses SMT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>1321</td>\n",
       "      <td>5</td>\n",
       "      <td>We develop translation models using the phrase...</td>\n",
       "      <td>We use the Moses phrase-based translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>1322</td>\n",
       "      <td>5</td>\n",
       "      <td>The baseline will be created by the Moses SMT ...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>1323</td>\n",
       "      <td>4</td>\n",
       "      <td>The resulting matrix is weighted using pointwi...</td>\n",
       "      <td>A popular measure of this association is point...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>1324</td>\n",
       "      <td>3</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "      <td>The questions are translated using a phrase-ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>1325</td>\n",
       "      <td>4</td>\n",
       "      <td>The morpho-syntactic tagging has been made wit...</td>\n",
       "      <td>POS tagging is performed using the IMS Tree Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>1326</td>\n",
       "      <td>4</td>\n",
       "      <td>For the determination of POS tags we use the S...</td>\n",
       "      <td>For both we use TreeTagger (Schmid, 1994) with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1269</th>\n",
       "      <td>1327</td>\n",
       "      <td>5</td>\n",
       "      <td>The Polish data is taken from the EUROPARL cor...</td>\n",
       "      <td>The Europarl corpus (Koehn, 2005) is built fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>1328</td>\n",
       "      <td>5</td>\n",
       "      <td>All annotations were done using the brat rapid...</td>\n",
       "      <td>The annotation was performed using the BRAT 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271</th>\n",
       "      <td>1329</td>\n",
       "      <td>4</td>\n",
       "      <td>The Spanish-English (S2E) training corpus was ...</td>\n",
       "      <td>The Polish data is taken from the EUROPARL cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1272</th>\n",
       "      <td>1330</td>\n",
       "      <td>4</td>\n",
       "      <td>We used TnT (Brants, 2000 ), trained on the Ne...</td>\n",
       "      <td>HCRC is tagged with TnT (Brants, 2000 ), train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>1331</td>\n",
       "      <td>4</td>\n",
       "      <td>For all experiments, we used the Moses SMT sys...</td>\n",
       "      <td>For German-English we also have a system based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>1332</td>\n",
       "      <td>5</td>\n",
       "      <td>Two baseNP data sets have been put forward by ...</td>\n",
       "      <td>An alternative representation for baseNPs has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>1333</td>\n",
       "      <td>5</td>\n",
       "      <td>Both of our systems were based on the Moses de...</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1276</th>\n",
       "      <td>1334</td>\n",
       "      <td>5</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "      <td>The baseline will be created by the Moses SMT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>1335</td>\n",
       "      <td>5</td>\n",
       "      <td>Corpus-based meaning representations rely on t...</td>\n",
       "      <td>Distributional models of meaning follow the di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>1336</td>\n",
       "      <td>4</td>\n",
       "      <td>Recently, Naim et al. (2014)  proposed an unsu...</td>\n",
       "      <td>Recently, Naim et al. (2014) proposed a fully ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1279</th>\n",
       "      <td>1337</td>\n",
       "      <td>5</td>\n",
       "      <td>A distributional similarity model is construct...</td>\n",
       "      <td>Distributional models of meaning follow the di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>1338</td>\n",
       "      <td>4</td>\n",
       "      <td>This paper describes the details of our system...</td>\n",
       "      <td>In the following we will describe the system w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>1339</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the phrase-based model Moses (Koehn et...</td>\n",
       "      <td>It was built with the Moses toolkit (Koehn et ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>1340</td>\n",
       "      <td>5</td>\n",
       "      <td>Combinatory Categorial grammar (CCG) is a ling...</td>\n",
       "      <td>Combinatory Categorial Grammar (CCG) (Steedman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>1341</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the Random Forests implementation of s...</td>\n",
       "      <td>We used the Linear SVM implementation (with de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>1342</td>\n",
       "      <td>4</td>\n",
       "      <td>The proposed model extends the LDA framework o...</td>\n",
       "      <td>The article of Blei et al. (2003) compares LDA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>1343</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the training section of the dataset fr...</td>\n",
       "      <td>Gimpel et al. (2011)  provided a dataset of PO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>1344</td>\n",
       "      <td>5</td>\n",
       "      <td>5-gram language models of Turkish and English ...</td>\n",
       "      <td>In order to evaluate the fluency of each syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>1345</td>\n",
       "      <td>3</td>\n",
       "      <td>The corpora are first tokenized and lowercased...</td>\n",
       "      <td>The corpus was then automatically tagged with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>1346</td>\n",
       "      <td>5</td>\n",
       "      <td>The statistical significance test is also carr...</td>\n",
       "      <td>The improvement is statistically significant a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>1348</td>\n",
       "      <td>3</td>\n",
       "      <td>The realisation ranking component is an SVM ra...</td>\n",
       "      <td>The learner is implemented as a ranking compon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>1351</td>\n",
       "      <td>5</td>\n",
       "      <td>The statistical significance test is also carr...</td>\n",
       "      <td>A statistical significance test was performed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>1352</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the NLTK toolkit (Loper and Bird, 2002)...</td>\n",
       "      <td>We use the Punkt sentence splitter from NLTK (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1292</th>\n",
       "      <td>1357</td>\n",
       "      <td>2</td>\n",
       "      <td>We mark the source tokens to which each target...</td>\n",
       "      <td>We find this method provides an additional 1.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>1358</td>\n",
       "      <td>3</td>\n",
       "      <td>We rely on the hybrid aligned lexical semantic...</td>\n",
       "      <td>In particular, the contribution of this paper ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>1359</td>\n",
       "      <td>3</td>\n",
       "      <td>To help improve the information extraction too...</td>\n",
       "      <td>The BioScope corpus is a manually annotated co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>1362</td>\n",
       "      <td>3</td>\n",
       "      <td>The module of coreference resolution included ...</td>\n",
       "      <td>We apply the Stanford coreference resolution s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>1363</td>\n",
       "      <td>5</td>\n",
       "      <td>We run our experiments on Europarl (Koehn, 200...</td>\n",
       "      <td>Europarl 2 (Koehn, 2005 ): it is a corpus of p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>1364</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the scikit-learn (Pedregosa et al., 20...</td>\n",
       "      <td>We used a GB implementation of the scikit-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>1365</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the scikit-learn (Pedregosa et al., 20...</td>\n",
       "      <td>We used the scikit-learn toolkit to train our ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>1366</td>\n",
       "      <td>5</td>\n",
       "      <td>Automatic Multi-Document Summarization (MDS) a...</td>\n",
       "      <td>Automatic text summarization aims to automatic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>1367</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>1368</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the scikit-learn (Pedregosa et al., 20...</td>\n",
       "      <td>We used the scikit-learn (Pedregosa et al., 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>1369</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "      <td>For the phrase-based SMT system, we adopted th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>1370</td>\n",
       "      <td>4</td>\n",
       "      <td>We train classifiers for each of the above fea...</td>\n",
       "      <td>We train Random Forest classifiers (Breiman, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>1371</td>\n",
       "      <td>4</td>\n",
       "      <td>In addition, the corpus was lemmatised using t...</td>\n",
       "      <td>1 The corpus is lemmatised and tagged by part-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>1372</td>\n",
       "      <td>4</td>\n",
       "      <td>They are based on Distributional Hypothesis wh...</td>\n",
       "      <td>Distributional models of meaning follow the di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>1373</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the same test set used in (Li et al. (...</td>\n",
       "      <td>But we randomly selected 90% of the training d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>1374</td>\n",
       "      <td>4</td>\n",
       "      <td>We develop translation models using the phrase...</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>1375</td>\n",
       "      <td>3</td>\n",
       "      <td>We used non-local features based on Finkel et ...</td>\n",
       "      <td>For example, Finkel et al. (2005) enabled the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>1376</td>\n",
       "      <td>5</td>\n",
       "      <td>For building the baseline SMT system, we used ...</td>\n",
       "      <td>For all experiments, we used the Moses SMT sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>1377</td>\n",
       "      <td>3</td>\n",
       "      <td>For translation, we use Moses (Koehn et al., 2...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>1378</td>\n",
       "      <td>4</td>\n",
       "      <td>To recognize explicit connectives, we construc...</td>\n",
       "      <td>In addition, we show that the latent represent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>1379</td>\n",
       "      <td>3</td>\n",
       "      <td>To construct language models and measure perpl...</td>\n",
       "      <td>Both language models use modified Kneser-Ney s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>1380</td>\n",
       "      <td>4</td>\n",
       "      <td>We use Gibbs sampling to estimate the distribu...</td>\n",
       "      <td>We use the Gibbs sampling based LDA (Griffiths...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>1381</td>\n",
       "      <td>5</td>\n",
       "      <td>Arabizi is not a letter-based transliteration ...</td>\n",
       "      <td>1 Arabic transliteration is presented in the B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>1382</td>\n",
       "      <td>4</td>\n",
       "      <td>To construct language models and measure perpl...</td>\n",
       "      <td>Both language models use modified Kneser-Ney s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>1384</td>\n",
       "      <td>4</td>\n",
       "      <td>We build upon our previous Markov Logic based ...</td>\n",
       "      <td>Scope-ignorant (Disambig.): Our previous MLN-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>1385</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the feedforward neural probabilistic la...</td>\n",
       "      <td>We follow the neural network architecture of V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>1386</td>\n",
       "      <td>5</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "      <td>We trained a number of French-English SMT syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>1387</td>\n",
       "      <td>5</td>\n",
       "      <td>For building the baseline SMT system, we used ...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>1388</td>\n",
       "      <td>4</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>SMT translations: a phrase-based SMT system bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321</th>\n",
       "      <td>1389</td>\n",
       "      <td>3</td>\n",
       "      <td>Following resource collection and construction...</td>\n",
       "      <td>We trained a model using Moses toolkit (Koehn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1322</th>\n",
       "      <td>1390</td>\n",
       "      <td>4</td>\n",
       "      <td>One semiautomatic approach to evaluation is RO...</td>\n",
       "      <td>The method, ROUGE (Lin and Hovy, 2003 ), is ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>1391</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Web 1T 5-gram corpus (Brants and Fr...</td>\n",
       "      <td>We used the unigram counts from the Web 1T 5-g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>1392</td>\n",
       "      <td>3</td>\n",
       "      <td>The SemEval-2015 Aspect Based Sentiment Analys...</td>\n",
       "      <td>This paper describes the approach of the SemaN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>1393</td>\n",
       "      <td>4</td>\n",
       "      <td>The 2009 Bio NLP shared task (Kim et al., 2009...</td>\n",
       "      <td>The recent BioNLP 2009 shared task (BioNLP09ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>1394</td>\n",
       "      <td>4</td>\n",
       "      <td>The constituent context model (CCM) for induci...</td>\n",
       "      <td>The CCM is a generative model for the unsuperv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>1395</td>\n",
       "      <td>4</td>\n",
       "      <td>The CCM is a generative model for the unsuperv...</td>\n",
       "      <td>The idea of representing a constituent by its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>1397</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the implementation provided by Tai et a...</td>\n",
       "      <td>We use the dependency tree long short-term mem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>1398</td>\n",
       "      <td>3</td>\n",
       "      <td>Europarl (Koehn, 2005) is a multilingual paral...</td>\n",
       "      <td>We use the French- English parallel corpus (ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>1399</td>\n",
       "      <td>5</td>\n",
       "      <td>The Stanford dependency parser (De Marneffe et...</td>\n",
       "      <td>We used the Stanford Dependency Parser (de Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>1400</td>\n",
       "      <td>5</td>\n",
       "      <td>The Stanford dependency parser (De Marneffe et...</td>\n",
       "      <td>We used the Stanford Dependency Parser (de Mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>1901</td>\n",
       "      <td>4</td>\n",
       "      <td>The major part of data comes from current and ...</td>\n",
       "      <td>The data used for the experiments described in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>1903</td>\n",
       "      <td>2</td>\n",
       "      <td>We compare the proposed model to our implement...</td>\n",
       "      <td>The model architecture, shown in figure 1 , is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>1904</td>\n",
       "      <td>4</td>\n",
       "      <td>It is a phrase-based system built using the Mo...</td>\n",
       "      <td>SMT translations: a phrase-based SMT system bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>1905</td>\n",
       "      <td>4</td>\n",
       "      <td>In this section, we first discuss the hybrid t...</td>\n",
       "      <td>In Lu et al. (2008) , the mixgram model (an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>1906</td>\n",
       "      <td>4</td>\n",
       "      <td>The Europarl corpus (Koehn, 2005) is built fro...</td>\n",
       "      <td>We use the French- English parallel corpus (ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>1907</td>\n",
       "      <td>4</td>\n",
       "      <td>The Europarl corpus (Koehn, 2005) is built fro...</td>\n",
       "      <td>We use the French- English parallel corpus (ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>1908</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Adam (Kingma and Ba, 2015) for optimisa...</td>\n",
       "      <td>Each model was trained during 50 epochs using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339</th>\n",
       "      <td>1909</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the open-source Moses toolkit (Koehn et...</td>\n",
       "      <td>We use the Moses phrase-based translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>1910</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the mkcls tool in GIZA++ (Och and Ney,...</td>\n",
       "      <td>We also used Giza++ word alignment tool (Och a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>1911</td>\n",
       "      <td>3</td>\n",
       "      <td>We then use the phrase extraction utility in t...</td>\n",
       "      <td>We use the Moses phrase-based translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>1912</td>\n",
       "      <td>3</td>\n",
       "      <td>Phrase pairs are extracted from IBM4 alignment...</td>\n",
       "      <td>Phrase pairs were extracted from symmetrized w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>1914</td>\n",
       "      <td>5</td>\n",
       "      <td>Specifically, we build off the Bayesian block ...</td>\n",
       "      <td>Our work is motivated by the Bayesian HMM appr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>1915</td>\n",
       "      <td>4</td>\n",
       "      <td>Like (Cho &amp; Chai (2000)) , our analysis also p...</td>\n",
       "      <td>Given the LP constraints in (4), (7), and (8),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>1917</td>\n",
       "      <td>5</td>\n",
       "      <td>We propose a relatively simple and nuanced uns...</td>\n",
       "      <td>As mentioned above, we extend the WMF model pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>1918</td>\n",
       "      <td>4</td>\n",
       "      <td>Our work is also related to (Bunescu and Moone...</td>\n",
       "      <td>The dependency path is the shortest path betwe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>1919</td>\n",
       "      <td>5</td>\n",
       "      <td>Motivated by previous work, we include a frequ...</td>\n",
       "      <td>Outside of the rhetorical features, the discou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>1920</td>\n",
       "      <td>5</td>\n",
       "      <td>The highest performance levels were achieved u...</td>\n",
       "      <td>Table 8 to Table 12 show the Macro-Average (F ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>1921</td>\n",
       "      <td>4</td>\n",
       "      <td>In this rest of this paper, we discuss related...</td>\n",
       "      <td>This paper describes the details of our system...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>1922</td>\n",
       "      <td>5</td>\n",
       "      <td>In principle, classifiers trained on PDTB data...</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>1926</td>\n",
       "      <td>5</td>\n",
       "      <td>In the next section we briefly review modeling...</td>\n",
       "      <td>We briefly review the HMM based word alignment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>1927</td>\n",
       "      <td>4</td>\n",
       "      <td>This result is statistically significant at p ...</td>\n",
       "      <td>The improvement is statistically significant a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>1928</td>\n",
       "      <td>4</td>\n",
       "      <td>Starting with TextRank (Mihalcea and Tarau, 20...</td>\n",
       "      <td>In the unsupervised approach, graph-based rank...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>1929</td>\n",
       "      <td>5</td>\n",
       "      <td>Because of our experience with the Weka packag...</td>\n",
       "      <td>Especially, we use the WEKA (Hall et al., 2009...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>1930</td>\n",
       "      <td>4</td>\n",
       "      <td>Dropout (Srivastava et al., 2014) is implement...</td>\n",
       "      <td>Dropout (Srivastava et al., 2014) is a very ef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>1931</td>\n",
       "      <td>3</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>For our SMT experiments, we use the Moses tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>1932</td>\n",
       "      <td>5</td>\n",
       "      <td>As mentioned in Section 3, we obtained depende...</td>\n",
       "      <td>We make use of dependency parse information fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>1933</td>\n",
       "      <td>5</td>\n",
       "      <td>We tokenize and truecase all of the corpora us...</td>\n",
       "      <td>We preprocessed the training corpora with scri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>1935</td>\n",
       "      <td>5</td>\n",
       "      <td>To obtain these we use the Stanford dependency...</td>\n",
       "      <td>We use the Stanford dependency parser (de Marn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>1936</td>\n",
       "      <td>5</td>\n",
       "      <td>The NMT models are trained using Adam optimize...</td>\n",
       "      <td>All parameters are learned by Adam optimizer (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>1937</td>\n",
       "      <td>4</td>\n",
       "      <td>The improved alignments gave a gain of Table 8...</td>\n",
       "      <td>One of the phrase-based systems moreover utili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>1938</td>\n",
       "      <td>5</td>\n",
       "      <td>The statistical significance tests using 95% c...</td>\n",
       "      <td>A statistical significance test was performed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>1939</td>\n",
       "      <td>5</td>\n",
       "      <td>To obtain these we use the Stanford dependency...</td>\n",
       "      <td>We use the Stanford dependency parser (de Marn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>1941</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the AdaGrad algorithm (Duchi et al., 20...</td>\n",
       "      <td>We use online learning to train model paramete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>1942</td>\n",
       "      <td>5</td>\n",
       "      <td>The other is from (Jeffrey Pennington et al. (...</td>\n",
       "      <td>We use the 100-dimensional GloVe word embeddin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>1943</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the AdaGrad optimizer (Duchi et al., 20...</td>\n",
       "      <td>We use online learning to train model paramete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>1944</td>\n",
       "      <td>4</td>\n",
       "      <td>We experiment with the phrase-based statistica...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>1945</td>\n",
       "      <td>3</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>The SMT systems were built using the Moses too...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>1946</td>\n",
       "      <td>4</td>\n",
       "      <td>We follow the definition in Cohen et al. (2012...</td>\n",
       "      <td>We also use these perturbation schemes to crea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>1947</td>\n",
       "      <td>3</td>\n",
       "      <td>The language model is a 5-gram KenLM (Heafield...</td>\n",
       "      <td>We trained an English 5-gram language model us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>1949</td>\n",
       "      <td>5</td>\n",
       "      <td>As mentioned in Section 3, we obtained depende...</td>\n",
       "      <td>We make use of dependency parse information fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>1950</td>\n",
       "      <td>4</td>\n",
       "      <td>Also, we evaluate on the RTE part of the SICK ...</td>\n",
       "      <td>The second is the RTE part of the SICK dataset...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>1951</td>\n",
       "      <td>3</td>\n",
       "      <td>Our model has a\" Siamese\" structure (Bromley e...</td>\n",
       "      <td>Most previous work use sentence modeling with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>1953</td>\n",
       "      <td>5</td>\n",
       "      <td>The dialogue act labelling of the corpus follo...</td>\n",
       "      <td>The classes used to train the DATE tagger are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>1954</td>\n",
       "      <td>4</td>\n",
       "      <td>We also obtain the dependency parse of the sen...</td>\n",
       "      <td>To examine the effect of normalization on depe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>1955</td>\n",
       "      <td>4</td>\n",
       "      <td>The resulting matrix is weighted using pointwi...</td>\n",
       "      <td>We construct word-word co-occurrence matrix X;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>1956</td>\n",
       "      <td>5</td>\n",
       "      <td>We extract structured facts using two methods:...</td>\n",
       "      <td>We extract facts from captions using Clausie (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>1957</td>\n",
       "      <td>4</td>\n",
       "      <td>Dropout (Srivastava et al., 2014) is implement...</td>\n",
       "      <td>Next, the output of the max-pooling layer is p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>1958</td>\n",
       "      <td>3</td>\n",
       "      <td>The population distribution was estimated by t...</td>\n",
       "      <td>The bootstrap sampling method provides a way f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>1959</td>\n",
       "      <td>5</td>\n",
       "      <td>The statistical significance test is also carr...</td>\n",
       "      <td>The significance testing is performed by paire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>1960</td>\n",
       "      <td>3</td>\n",
       "      <td>The bootstrap sampling method provides a way f...</td>\n",
       "      <td>The population distribution was estimated by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>1961</td>\n",
       "      <td>3</td>\n",
       "      <td>For medical we use the biomedical data from EM...</td>\n",
       "      <td>For French-English experiments, we used the EM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>1962</td>\n",
       "      <td>5</td>\n",
       "      <td>The MT experiments were carried out using the ...</td>\n",
       "      <td>The translation system is trained using the we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>1963</td>\n",
       "      <td>4</td>\n",
       "      <td>For the theory of Cho &amp; Chai (2000) to be comp...</td>\n",
       "      <td>Then we revise the two LP constraints of Cho &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>1964</td>\n",
       "      <td>5</td>\n",
       "      <td>3 With these trees fixed, the partial derivati...</td>\n",
       "      <td>Derivatives are computed efficiently via backp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>1965</td>\n",
       "      <td>5</td>\n",
       "      <td>Many researchers have considered generating pa...</td>\n",
       "      <td>Distributional models of meaning follow the di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>1966</td>\n",
       "      <td>5</td>\n",
       "      <td>We used Weka (Hall et al., 2009) for all our c...</td>\n",
       "      <td>We use the WEKA toolkit (Hall et al., 2009) fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>1967</td>\n",
       "      <td>3</td>\n",
       "      <td>Bilingual Corpora The corpus used in the follo...</td>\n",
       "      <td>The experiments are carried out on a subset of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>1968</td>\n",
       "      <td>4</td>\n",
       "      <td>First, we used the Moses toolkit (Koehn et al....</td>\n",
       "      <td>The Moses toolkit (Koehn et al., 2007) is used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>1969</td>\n",
       "      <td>3</td>\n",
       "      <td>Another parallel corpus is the JRC-Acquis Mult...</td>\n",
       "      <td>corpus (Steinberger et al., 2006 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>1970</td>\n",
       "      <td>3</td>\n",
       "      <td>The parallel corpus is wordaligned using GIZA+...</td>\n",
       "      <td>This is done using IBM Model 1 (Brown et al., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>1972</td>\n",
       "      <td>2</td>\n",
       "      <td>We thus cast MSC as a semantic sentence classi...</td>\n",
       "      <td>We compare the proposed model to our implement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>1973</td>\n",
       "      <td>5</td>\n",
       "      <td>All these features are inherited from Moses (K...</td>\n",
       "      <td>All the experiments are carried out in Moses t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>1974</td>\n",
       "      <td>3</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "      <td>For our experiments, we use 40,000 sentences f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>1975</td>\n",
       "      <td>3</td>\n",
       "      <td>The texts were first automatically segmented a...</td>\n",
       "      <td>The data was tagged using TnT (Brants, 2000 ),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>1976</td>\n",
       "      <td>5</td>\n",
       "      <td>Secondly, Holmqvist et al. (2012) reordered so...</td>\n",
       "      <td>Holmqvist et al. (2012) presented a method whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>1977</td>\n",
       "      <td>5</td>\n",
       "      <td>For language modeling, we use the English Giga...</td>\n",
       "      <td>Language Model: For all 3-SCFG systems we use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>1978</td>\n",
       "      <td>5</td>\n",
       "      <td>For our LDA implementations, we use MALLET (Mc...</td>\n",
       "      <td>For training bilingual topic models, we use Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>1979</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the standard Stanford-style set of depe...</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>1980</td>\n",
       "      <td>4</td>\n",
       "      <td>Next we evaluate how well the complexity measu...</td>\n",
       "      <td>We also compare annotation strategies in terms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>1981</td>\n",
       "      <td>3</td>\n",
       "      <td>Syntax in EPEC is annotated following the depe...</td>\n",
       "      <td>For the German experiments, we used the NEGRA ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>1983</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the scikit-learn (Pedregosa et al., 20...</td>\n",
       "      <td>We use the scikit implementation of SVM (Pedre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>1984</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the scikit-learn toolkit to train our ...</td>\n",
       "      <td>For Indep-Logistic, we used scikit-learn (Pedr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>1985</td>\n",
       "      <td>5</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "      <td>We trained a standard Moses baseline (Koehn et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>1986</td>\n",
       "      <td>5</td>\n",
       "      <td>These features were obtained using the Stanfor...</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>1987</td>\n",
       "      <td>3</td>\n",
       "      <td>We built a 5-gram language model on the Englis...</td>\n",
       "      <td>One is a 3-gram language model built using Ken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>1988</td>\n",
       "      <td>3</td>\n",
       "      <td>In the other side, the French corpus is part-o...</td>\n",
       "      <td>1 The corpus is lemmatised and tagged by part-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>1989</td>\n",
       "      <td>3</td>\n",
       "      <td>The COMLEX syntax dictionary (Grishman et al.,...</td>\n",
       "      <td>The COMLEX syntax dictionary (Grishman et al.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>1990</td>\n",
       "      <td>5</td>\n",
       "      <td>We lemmatise the head of each constituent with...</td>\n",
       "      <td>POS tagging was performed with the TreeTagger ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>1991</td>\n",
       "      <td>5</td>\n",
       "      <td>with the KenLM toolkit (Heafield, 2011 ).</td>\n",
       "      <td>For language modeling we used the KenLM toolki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>1992</td>\n",
       "      <td>5</td>\n",
       "      <td>We briefly review the HMM based word alignment...</td>\n",
       "      <td>Och is the HMM alignment model of (Och and Ney...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>1993</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "      <td>These features were obtained using the Stanfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>1995</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "      <td>We use the standard Stanford-style set of depe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>1996</td>\n",
       "      <td>4</td>\n",
       "      <td>All the summaries are evaluated using ROUGE (L...</td>\n",
       "      <td>For the summarization task, we compare results...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>1997</td>\n",
       "      <td>4</td>\n",
       "      <td>The reported confidence intervals were estimat...</td>\n",
       "      <td>We calculated significance using paired bootst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>1999</td>\n",
       "      <td>4</td>\n",
       "      <td>We implemented CharWNN using the Theano librar...</td>\n",
       "      <td>Gradients computed using the automatic differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>2000</td>\n",
       "      <td>5</td>\n",
       "      <td>For the linear logistic regression implementat...</td>\n",
       "      <td>For Indep-Logistic, we used scikit-learn (Pedr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>2501</td>\n",
       "      <td>4</td>\n",
       "      <td>The tagger we use is TnT (Brants, 2000) , a hi...</td>\n",
       "      <td>The data was tagged using TnT (Brants, 2000 ),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>2502</td>\n",
       "      <td>3</td>\n",
       "      <td>We have used the implementation described in (...</td>\n",
       "      <td>1 Regarding the learning algorithm, we used ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>2503</td>\n",
       "      <td>3</td>\n",
       "      <td>We built a 5-gram language model on the Englis...</td>\n",
       "      <td>We build our PB-SMT systems in a standard way ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>2504</td>\n",
       "      <td>3</td>\n",
       "      <td>DELPH-IN Minimal Recursion Semantics (DM) As p...</td>\n",
       "      <td>As output, the grammar delivers detailed seman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>2505</td>\n",
       "      <td>5</td>\n",
       "      <td>Like the CoNLL-2006 shared task, the 2007 shar...</td>\n",
       "      <td>One of the first venues at which domain adapta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>2506</td>\n",
       "      <td>4</td>\n",
       "      <td>Like (Cho &amp; Chai 2000 ), our analysis also pro...</td>\n",
       "      <td>However, like Cho &amp; Chai (2000) , our analysis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>2507</td>\n",
       "      <td>4</td>\n",
       "      <td>The phrase table is extracted from a bilingual...</td>\n",
       "      <td>A core component of every PBSMT system is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>2508</td>\n",
       "      <td>4</td>\n",
       "      <td>Google Web 1T (Brants and Franz, 2006) has bee...</td>\n",
       "      <td>The Google Web 1T data (Brants and Franz, 2006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>2509</td>\n",
       "      <td>5</td>\n",
       "      <td>We first use a dependency parser (de Marneffe ...</td>\n",
       "      <td>A complete set of parser tags and the method u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>2510</td>\n",
       "      <td>5</td>\n",
       "      <td>We first use a dependency parser (de Marneffe ...</td>\n",
       "      <td>A complete set of parser tags and the method u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>2511</td>\n",
       "      <td>5</td>\n",
       "      <td>These analyses provide an alternative but theo...</td>\n",
       "      <td>Liang et al. (2006) observe that standard upda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>2512</td>\n",
       "      <td>5</td>\n",
       "      <td>We experiment with the phrase-based statistica...</td>\n",
       "      <td>We present a system that takes a general Moses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>2513</td>\n",
       "      <td>3</td>\n",
       "      <td>For our corpus, we randomly selected documents...</td>\n",
       "      <td>New York Times consists of 500 random sentence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>2514</td>\n",
       "      <td>4</td>\n",
       "      <td>They used the Web-based annotation tool brat (...</td>\n",
       "      <td>We consider the following types of implicit re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>2515</td>\n",
       "      <td>4</td>\n",
       "      <td>All corpora were taken from the CHILDES databa...</td>\n",
       "      <td>The eval-uation set was comprised of adult utt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>2516</td>\n",
       "      <td>5</td>\n",
       "      <td>The system generated tweets were evaluated usi...</td>\n",
       "      <td>The summaries from the above algorithm for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>2517</td>\n",
       "      <td>5</td>\n",
       "      <td>The TOEFL synonym selection task is to select ...</td>\n",
       "      <td>The TOEFL synonym dataset (Landauer and Dumais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>2518</td>\n",
       "      <td>4</td>\n",
       "      <td>We calculate our features using the KenLM tool...</td>\n",
       "      <td>1 We used the KenLM toolkit (Heafield, 2011) t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>2519</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the scikit implementation of Random For...</td>\n",
       "      <td>The regressor used is a Random Forest Regresso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2521</td>\n",
       "      <td>3</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "      <td>The statistical dictionary for this task was e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2522</td>\n",
       "      <td>5</td>\n",
       "      <td>The statistical significance tests using 95% c...</td>\n",
       "      <td>We used bootstrap resampling for testing stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>2523</td>\n",
       "      <td>5</td>\n",
       "      <td>We then run word alignment with GIZA++ (Och an...</td>\n",
       "      <td>We used the GIZA++ software (Och and Ney, 2003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>2524</td>\n",
       "      <td>4</td>\n",
       "      <td>We built a 5-gram language model on the Englis...</td>\n",
       "      <td>We calculate our features using the KenLM tool...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1441</th>\n",
       "      <td>2525</td>\n",
       "      <td>5</td>\n",
       "      <td>We used GIZA++ (Och and Ney, 2003) to align th...</td>\n",
       "      <td>We then run word alignment with GIZA++ (Och an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>2526</td>\n",
       "      <td>5</td>\n",
       "      <td>The phrase tables were generated by means of s...</td>\n",
       "      <td>The word alignment was obtained by running Giz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>2527</td>\n",
       "      <td>5</td>\n",
       "      <td>Zhu et al. (2013) applied Kalman filter model ...</td>\n",
       "      <td>In the scenario of human-computer interactive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>2528</td>\n",
       "      <td>4</td>\n",
       "      <td>( Koo et al. 2008)  have proposed to use word ...</td>\n",
       "      <td>In order to reduce the amount of annotated dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>2529</td>\n",
       "      <td>4</td>\n",
       "      <td>More recently, (Pasha et al., 2014) created MA...</td>\n",
       "      <td>Next, the files are processed with the morphol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>2530</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>For building our SMT systems, the open-source ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>2531</td>\n",
       "      <td>4</td>\n",
       "      <td>@BULLET Naive Bayes(NB): We use Binomial varia...</td>\n",
       "      <td>We use the Bernoulli Naive Bayes classifier in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>2532</td>\n",
       "      <td>5</td>\n",
       "      <td>We use AdaGrad (Duchi et al., 2011)  with the ...</td>\n",
       "      <td>We use online learning to train model paramete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>2533</td>\n",
       "      <td>5</td>\n",
       "      <td>The MT experiments were carried out using the ...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>2534</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the AdaGrad method (Duchi et al., 2011)...</td>\n",
       "      <td>We use online learning to train model paramete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>2535</td>\n",
       "      <td>4</td>\n",
       "      <td>Among the existing sense-tagged corpora, the S...</td>\n",
       "      <td>The SEMCOR corpus (Miller et al., 1994) is one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>2536</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Liblinear Support Vector Machine (S...</td>\n",
       "      <td>A linear-kernel Support Vector Machine (Chang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>2537</td>\n",
       "      <td>5</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "      <td>We tokenize and truecase all of the corpora us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2538</td>\n",
       "      <td>5</td>\n",
       "      <td>We built a source-to-target PB-SMT model from ...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2539</td>\n",
       "      <td>5</td>\n",
       "      <td>The dictionaries are automatically generated v...</td>\n",
       "      <td>We then made use of the GIZA++ software (Och a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2540</td>\n",
       "      <td>3</td>\n",
       "      <td>There are two main approaches to processing no...</td>\n",
       "      <td>There are two approaches proposed in the liter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2541</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>We trained a model using Moses toolkit (Koehn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2542</td>\n",
       "      <td>4</td>\n",
       "      <td>The task of identifying mentions to medical co...</td>\n",
       "      <td>We found the largest of such corpus to be the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>2543</td>\n",
       "      <td>5</td>\n",
       "      <td>We built a source-to-target PB-SMT model from ...</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>2545</td>\n",
       "      <td>5</td>\n",
       "      <td>We also use MADA+TOKAN (Habash et al., 2009) t...</td>\n",
       "      <td>We use the MADA package (Habash et al., 2009) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>2546</td>\n",
       "      <td>5</td>\n",
       "      <td>Both training and testing data consist of PubM...</td>\n",
       "      <td>The data provided for the shared task is prepa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>2547</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>We built a source-to-target PB-SMT model from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>2548</td>\n",
       "      <td>5</td>\n",
       "      <td>To extract our part-of-speech (POS) features, ...</td>\n",
       "      <td>We used the Brill tagger provided by NLTK for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>2549</td>\n",
       "      <td>5</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "      <td>We tokenize and truecase all of the corpora us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>2550</td>\n",
       "      <td>5</td>\n",
       "      <td>We trained a model using Moses toolkit (Koehn ...</td>\n",
       "      <td>Our baseline is a phrase-based MT system train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>2551</td>\n",
       "      <td>4</td>\n",
       "      <td>For seed and test paradigms we used verbal inf...</td>\n",
       "      <td>For English, we used the CELEX database (Baaye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>2552</td>\n",
       "      <td>5</td>\n",
       "      <td>Then we did word alignment using GIZA++ (Och a...</td>\n",
       "      <td>We then run word alignment with GIZA++ (Och an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>2553</td>\n",
       "      <td>4</td>\n",
       "      <td>( Pang et al. 2002) have reported the effectiv...</td>\n",
       "      <td>Pang et al. (2002)  use machine learning metho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>2555</td>\n",
       "      <td>5</td>\n",
       "      <td>We train the concept identification stage usin...</td>\n",
       "      <td>We use online learning to train model paramete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>2556</td>\n",
       "      <td>3</td>\n",
       "      <td>For the contextual check we use the Google Web...</td>\n",
       "      <td>The Web1T corpus (Brants and Franz, 2006) is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>2557</td>\n",
       "      <td>5</td>\n",
       "      <td>We built a source-to-target PB-SMT model from ...</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>2558</td>\n",
       "      <td>4</td>\n",
       "      <td>@BULLET Naive Bayes(NB): We use Binomial varia...</td>\n",
       "      <td>@BULLET Logistic Regression(LR): We use Logist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1473</th>\n",
       "      <td>2559</td>\n",
       "      <td>4</td>\n",
       "      <td>We built a source-to-target PB-SMT model from ...</td>\n",
       "      <td>We tokenize and frequent-case the data with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>2560</td>\n",
       "      <td>5</td>\n",
       "      <td>An unpruned, modified Kneser-Ney-smoothed 4-gr...</td>\n",
       "      <td>The language model is a standard 5-gram model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>2561</td>\n",
       "      <td>4</td>\n",
       "      <td>For native data, several teams make use of the...</td>\n",
       "      <td>We used the unigram counts from the Web 1T 5-g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>2562</td>\n",
       "      <td>4</td>\n",
       "      <td>We measure statistical significance using 95% ...</td>\n",
       "      <td>The column\" pair-CI\" shows 95% confidence inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1477</th>\n",
       "      <td>2563</td>\n",
       "      <td>4</td>\n",
       "      <td>1 The models are constructed using C4.5 decisi...</td>\n",
       "      <td>The Weka SMO implementation of SVM (Hall et al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>2564</td>\n",
       "      <td>5</td>\n",
       "      <td>It therefore follows the distributional hypoth...</td>\n",
       "      <td>Distributional similarity relies on the distri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>2565</td>\n",
       "      <td>4</td>\n",
       "      <td>Consider the two examples below, drawn from th...</td>\n",
       "      <td>Pitler and Nenkova (2008) used discourse relat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>2566</td>\n",
       "      <td>5</td>\n",
       "      <td>BLE is based on the distributional hypothesis ...</td>\n",
       "      <td>Distributional similarity relies on the distri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>2567</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the BLEU score as primary criterion whi...</td>\n",
       "      <td>The optimization is done using the Downhill Si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>2568</td>\n",
       "      <td>4</td>\n",
       "      <td>The relation prediction task of Science IE is ...</td>\n",
       "      <td>The SemEval-2010 Task 8 dataset is a widely us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>2569</td>\n",
       "      <td>3</td>\n",
       "      <td>Machine translation system settings: We used a...</td>\n",
       "      <td>We compare the model against the Moses phrase-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>2570</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the MRS analyses that are produced by t...</td>\n",
       "      <td>The experiments are carried out on a broad-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>2571</td>\n",
       "      <td>5</td>\n",
       "      <td>In each case, the improvement of EBMT TM + SMT...</td>\n",
       "      <td>In all cases, results are statistically signif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>2572</td>\n",
       "      <td>4</td>\n",
       "      <td>To capture typical part-of bridging (see Examp...</td>\n",
       "      <td>We extract a list containing around 4,000 rela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>2574</td>\n",
       "      <td>5</td>\n",
       "      <td>The surfacesyntactic representation ? p was a ...</td>\n",
       "      <td>The number of features extracted from the PDT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>2575</td>\n",
       "      <td>5</td>\n",
       "      <td>In our work, like Hernault et al. (2010) , we ...</td>\n",
       "      <td>Similar to the work of Hernault et al. (2010) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>2576</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the data that were recorded and preproc...</td>\n",
       "      <td>1 Full details of the experimental protocol, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>2577</td>\n",
       "      <td>5</td>\n",
       "      <td>We trained non-projective dependency parsers f...</td>\n",
       "      <td>For non-projective parsing experiments, four l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>2578</td>\n",
       "      <td>4</td>\n",
       "      <td>Our LP constraints based on the new type marke...</td>\n",
       "      <td>For the theory of Cho &amp; Chai (2000) to be comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>2579</td>\n",
       "      <td>2</td>\n",
       "      <td>The WSJ grammar covers the UPenn Wall Street J...</td>\n",
       "      <td>The approach presented in this paper is a firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>2580</td>\n",
       "      <td>5</td>\n",
       "      <td>Statistically significant results, calculated ...</td>\n",
       "      <td>We calculated significance using paired bootst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>2581</td>\n",
       "      <td>4</td>\n",
       "      <td>In testing, we used minimum Bayes risk decodin...</td>\n",
       "      <td>selects the translation with minimum Bayes ris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>2582</td>\n",
       "      <td>3</td>\n",
       "      <td>selects the translation with minimum Bayes ris...</td>\n",
       "      <td>Additionally, we will compare two decision rul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>2583</td>\n",
       "      <td>4</td>\n",
       "      <td>We use a prototype-based selectional preferenc...</td>\n",
       "      <td>We build on a recent selectional preference mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>2584</td>\n",
       "      <td>4</td>\n",
       "      <td>We used 10-fold cross-validation, set the conf...</td>\n",
       "      <td>ROUGE-2 metric (Lin, 2004) is used for the eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>2585</td>\n",
       "      <td>3</td>\n",
       "      <td>The Brown Corpus tagged with WordNet senses (M...</td>\n",
       "      <td>The senses in WordNet are ordered according to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>2586</td>\n",
       "      <td>5</td>\n",
       "      <td>As an implementation, we use SVM light (Joachi...</td>\n",
       "      <td>As a supervised classifier for VSM and WIKI, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>2587</td>\n",
       "      <td>4</td>\n",
       "      <td>For translation tables , the Moses system (Koe...</td>\n",
       "      <td>We compare the model against the Moses phrase-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>2588</td>\n",
       "      <td>5</td>\n",
       "      <td>The particle filter of Canini et al. (2009) re...</td>\n",
       "      <td>The particle filter studied empirically by Can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>2589</td>\n",
       "      <td>4</td>\n",
       "      <td>We describe an approximation to the BLEU score...</td>\n",
       "      <td>We here describe a linear approximation to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>2590</td>\n",
       "      <td>5</td>\n",
       "      <td>We applied the Naive Bayes probabilistic super...</td>\n",
       "      <td>We conducted experiments using Multinomial Nai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>2593</td>\n",
       "      <td>4</td>\n",
       "      <td>We minimize the cross entropy loss using gradi...</td>\n",
       "      <td>The network is trained using SGD with shuffled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>2594</td>\n",
       "      <td>5</td>\n",
       "      <td>Statistical significance in BLEU score differe...</td>\n",
       "      <td>Statistical significance of the difference bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>2595</td>\n",
       "      <td>5</td>\n",
       "      <td>We measure statistical significance using 95% ...</td>\n",
       "      <td>For all results, we computed their confidence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>2596</td>\n",
       "      <td>3</td>\n",
       "      <td>3 As verbs, we take all tags that map to V in ...</td>\n",
       "      <td>In the POS tag level, we basically used the un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>2597</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the MRS analyses that are produced by t...</td>\n",
       "      <td>Here, we use (1) the Link Grammar Parser 8 and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>2598</td>\n",
       "      <td>5</td>\n",
       "      <td>We measure statistical significance using 95% ...</td>\n",
       "      <td>For statistical significance testing, we use a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>2599</td>\n",
       "      <td>5</td>\n",
       "      <td>On Semantic Role Labeling Gildea and Jurafsky ...</td>\n",
       "      <td>Gildea and Jurafsky (2002)  were the first to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>2600</td>\n",
       "      <td>4</td>\n",
       "      <td>We apply the stochastic gradient descent algor...</td>\n",
       "      <td>We use a minibatch size of 100, and use AdaDel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>3101</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the Linear SVM implementation (with de...</td>\n",
       "      <td>We use the scikit implementation of SVM (Pedre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>3102</td>\n",
       "      <td>5</td>\n",
       "      <td>We test the statistical significance of differ...</td>\n",
       "      <td>Statistical significance tests are performed u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514</th>\n",
       "      <td>3103</td>\n",
       "      <td>5</td>\n",
       "      <td>In particular, the parser implements the arc-s...</td>\n",
       "      <td>1 The parser implements the arc-standard algor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>3104</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Adadelta (Zeiler, 2012) to update the p...</td>\n",
       "      <td>In the training procedure, we use AdaDelta (Ze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>3105</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "      <td>The systems for the English ? Spanish translat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>3106</td>\n",
       "      <td>3</td>\n",
       "      <td>CCG is a lexicalized theory of grammar (Steedm...</td>\n",
       "      <td>The input of the Boxer system is a syntactic a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>3107</td>\n",
       "      <td>5</td>\n",
       "      <td>The results for TESLA-M and TESLA-F have previ...</td>\n",
       "      <td>TESLA (Translation Evaluation of Sentences wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>3108</td>\n",
       "      <td>3</td>\n",
       "      <td>The semantic representation is Minimal Recursi...</td>\n",
       "      <td>The ERG produces Minimal Recursion Semantics (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>3109</td>\n",
       "      <td>4</td>\n",
       "      <td>GermaNet (GN) is the German counterpart to WN ...</td>\n",
       "      <td>Future work could be to extend the German data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>3110</td>\n",
       "      <td>3</td>\n",
       "      <td>(Baroni et al 2002)  report that 47% of the vo...</td>\n",
       "      <td>Baroni et al. (2002)  analyzed the 28 million ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>3111</td>\n",
       "      <td>4</td>\n",
       "      <td>Both language models use modified Kneser-Ney s...</td>\n",
       "      <td>5-gram language models are trained over the ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>3112</td>\n",
       "      <td>5</td>\n",
       "      <td>We used MALLET (McCallum, 2002) for this exper...</td>\n",
       "      <td>10 We used the PLTM implementation in Mallet (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>3114</td>\n",
       "      <td>5</td>\n",
       "      <td>We calculated significance using paired bootst...</td>\n",
       "      <td>The statistical significance test is also carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>3115</td>\n",
       "      <td>5</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "      <td>Finally, we used Moses toolkit as phrase-based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>3116</td>\n",
       "      <td>5</td>\n",
       "      <td>For building the baseline SMT system, we used ...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>3117</td>\n",
       "      <td>3</td>\n",
       "      <td>This results in the semantic triple shot(man,b...</td>\n",
       "      <td>We also lemmatized all words using Stanford Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>3119</td>\n",
       "      <td>4</td>\n",
       "      <td>We used standard classifiers available in scik...</td>\n",
       "      <td>3 We used logistic regression (though linear S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1529</th>\n",
       "      <td>3120</td>\n",
       "      <td>3</td>\n",
       "      <td>Rhetorical Structure Theory (Mann and Thompson...</td>\n",
       "      <td>The closest area to our work consists of inves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1530</th>\n",
       "      <td>3121</td>\n",
       "      <td>4</td>\n",
       "      <td>Both language models use modified Kneser-Ney s...</td>\n",
       "      <td>5-gram language models are trained over the ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1531</th>\n",
       "      <td>3123</td>\n",
       "      <td>5</td>\n",
       "      <td>We train the concept identification stage usin...</td>\n",
       "      <td>1 using AdaGrad (Duchi et al., 2011 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1532</th>\n",
       "      <td>3124</td>\n",
       "      <td>5</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "      <td>All our translation systems are based on Moses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>3125</td>\n",
       "      <td>5</td>\n",
       "      <td>We use ROUGE score as our evaluation metric (L...</td>\n",
       "      <td>We expect this restriction is more consistent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>3126</td>\n",
       "      <td>5</td>\n",
       "      <td>For building the baseline SMT system, we used ...</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1535</th>\n",
       "      <td>3127</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Europarl parallel corpus (Koehn, 20...</td>\n",
       "      <td>We extract our paraphrase grammar from the Fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>3128</td>\n",
       "      <td>3</td>\n",
       "      <td>We use Ridge Regression (RR) with l2-norm regu...</td>\n",
       "      <td>@BULLET Logistic Regression(LR): We use Logist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>3129</td>\n",
       "      <td>4</td>\n",
       "      <td>All data used in our experiments are sentence-...</td>\n",
       "      <td>We tokenize English data and segment Chinese d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>3130</td>\n",
       "      <td>4</td>\n",
       "      <td>Since the phrase table contains lemmas, the Wi...</td>\n",
       "      <td>1 The corpus is lemmatised and tagged by part-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>3131</td>\n",
       "      <td>5</td>\n",
       "      <td>For this purpose we used the logistic regressi...</td>\n",
       "      <td>We use logistic regression with L2 regularizat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>3132</td>\n",
       "      <td>5</td>\n",
       "      <td>All our systems are contrasted with a standard...</td>\n",
       "      <td>Our machine translation systems this year are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>3133</td>\n",
       "      <td>4</td>\n",
       "      <td>We test our metrics in the setting of the WMT ...</td>\n",
       "      <td>The compared systems are evaluated on the Engl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>3134</td>\n",
       "      <td>4</td>\n",
       "      <td>These classifiers have been used in related wo...</td>\n",
       "      <td>We evaluated our method with movie review docu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>3135</td>\n",
       "      <td>4</td>\n",
       "      <td>One is from (Turian et al, 2010) , the dimensi...</td>\n",
       "      <td>For example, Turian et al. (2010)  showed that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>3136</td>\n",
       "      <td>5</td>\n",
       "      <td>Word alignment using GIZA++ toolkit (Och and N...</td>\n",
       "      <td>Word alignment is performed by GIZA++ (Och and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>3137</td>\n",
       "      <td>5</td>\n",
       "      <td>We found that using AdaGrad (Duchi et al., 201...</td>\n",
       "      <td>1 using AdaGrad (Duchi et al., 2011 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>3138</td>\n",
       "      <td>5</td>\n",
       "      <td>The Penn Discourse Treebank (PDTB, Prasad et a...</td>\n",
       "      <td>One of the most important resources for discou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>3139</td>\n",
       "      <td>5</td>\n",
       "      <td>The system was trained on the English and Dani...</td>\n",
       "      <td>The EuroParl data set consists of 707 sentence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>3140</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, Turian et al. (2010) compared Bro...</td>\n",
       "      <td>Turian et al. (2010) applied word embeddings t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>3141</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "      <td>In particular, we use Moses (Koehn et al., 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>3142</td>\n",
       "      <td>3</td>\n",
       "      <td>Past experiences on this system have shown tha...</td>\n",
       "      <td>Our submitted system for the second task is ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>3143</td>\n",
       "      <td>5</td>\n",
       "      <td>We trained a number of French-English SMT syst...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>3144</td>\n",
       "      <td>5</td>\n",
       "      <td>@BULLET Zhang2015: Zhang et al. (2015) propose...</td>\n",
       "      <td>Zhang et al. (2015) explore a shallow convolut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>3145</td>\n",
       "      <td>4</td>\n",
       "      <td>All data used in our experiments are sentence-...</td>\n",
       "      <td>We tokenize English data and segment Chinese d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>3146</td>\n",
       "      <td>5</td>\n",
       "      <td>The major part of data comes from current and ...</td>\n",
       "      <td>The source of bilingual data used in the exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>3147</td>\n",
       "      <td>5</td>\n",
       "      <td>We expect this restriction is more consistent ...</td>\n",
       "      <td>ROUGE (Lin, 2004) is the fully automatic metri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>3148</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>3149</td>\n",
       "      <td>5</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>3150</td>\n",
       "      <td>4</td>\n",
       "      <td>The decoder searches for the best translation ...</td>\n",
       "      <td>The log-linear approach to phrase-based transl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>3151</td>\n",
       "      <td>5</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>3152</td>\n",
       "      <td>5</td>\n",
       "      <td>The usages from the ukWaC are tokenised and le...</td>\n",
       "      <td>3 All English data are POS tagged and lemmatis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>3153</td>\n",
       "      <td>4</td>\n",
       "      <td>To quantify the redundancy of structures, we p...</td>\n",
       "      <td>The trigram target language model is trained f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>3154</td>\n",
       "      <td>5</td>\n",
       "      <td>Baseline word alignments were obtained by runn...</td>\n",
       "      <td>Translation models were trained over the bilin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>3156</td>\n",
       "      <td>3</td>\n",
       "      <td>The results displayed in Table 3 are obtained ...</td>\n",
       "      <td>The core model is a decision tree classifier t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>3157</td>\n",
       "      <td>5</td>\n",
       "      <td>We trained a number of French-English SMT syst...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>3158</td>\n",
       "      <td>3</td>\n",
       "      <td>The performance of our algorithm is compared w...</td>\n",
       "      <td>Note that word sense disambiguation is perform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>3159</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the scikit implementation of SVM (Pedre...</td>\n",
       "      <td>We use a well-known scikitlearn (Pedregosa et ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>3160</td>\n",
       "      <td>4</td>\n",
       "      <td>For example, Turian et al. (2010) showed that ...</td>\n",
       "      <td>Turian et al. (2010) applied word embeddings t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>3161</td>\n",
       "      <td>3</td>\n",
       "      <td>EASE uses NLTK (Bird et al., 2009) for POS tag...</td>\n",
       "      <td>We used the Brill tagger provided by NLTK for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1569</th>\n",
       "      <td>3163</td>\n",
       "      <td>5</td>\n",
       "      <td>All our translation systems are based on Moses...</td>\n",
       "      <td>All our systems are contrasted with a standard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1570</th>\n",
       "      <td>3164</td>\n",
       "      <td>5</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>3165</td>\n",
       "      <td>5</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>3166</td>\n",
       "      <td>4</td>\n",
       "      <td>We thank Gbor Recski (HAS Research Institute f...</td>\n",
       "      <td>We employed the state-of-the-art sentiment ann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>3167</td>\n",
       "      <td>4</td>\n",
       "      <td>It has been shown in previous work on relation...</td>\n",
       "      <td>Our work is also related to (Bunescu and Moone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>3168</td>\n",
       "      <td>4</td>\n",
       "      <td>These algorithms were used to participate in t...</td>\n",
       "      <td>The system that obtained the best performance ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>3169</td>\n",
       "      <td>5</td>\n",
       "      <td>Phrase-Based SMT system: a standard non factor...</td>\n",
       "      <td>SMT translations: a phrase-based SMT system bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>3171</td>\n",
       "      <td>4</td>\n",
       "      <td>For the theory of Cho &amp; Chai (2000) to be comp...</td>\n",
       "      <td>Given the LP constraints in (4), (7), and (8),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>3172</td>\n",
       "      <td>5</td>\n",
       "      <td>All linguistic annotations needed for features...</td>\n",
       "      <td>Part of speech tagging and named entity recogn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>3173</td>\n",
       "      <td>3</td>\n",
       "      <td>Algorithm 5 shows a Passive-Aggressive algorit...</td>\n",
       "      <td>To build a parser, we use a structured classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>3175</td>\n",
       "      <td>4</td>\n",
       "      <td>The task is part of the Semantic Evaluation 20...</td>\n",
       "      <td>Semantic Textual Similarity (STS) is the task ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>3176</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the same annotation guidelines as Zaid...</td>\n",
       "      <td>We also use A0 to compare against the\" masking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>3177</td>\n",
       "      <td>4</td>\n",
       "      <td>The Stanford parser 1 (Marneffe et al., 2006) ...</td>\n",
       "      <td>The same kind of process was applied to the Pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>3178</td>\n",
       "      <td>5</td>\n",
       "      <td>1 After tokenization , we lemmatize and stem t...</td>\n",
       "      <td>For Reuters we segmented and tokenized the dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>3179</td>\n",
       "      <td>5</td>\n",
       "      <td>Machine translation system settings: We used a...</td>\n",
       "      <td>We use the Moses phrase-based translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>3180</td>\n",
       "      <td>4</td>\n",
       "      <td>The phrase table is extracted from a bilingual...</td>\n",
       "      <td>The corpus is first word-aligned using a word ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>3181</td>\n",
       "      <td>3</td>\n",
       "      <td>This architecture is very similar to the frame...</td>\n",
       "      <td>3 DKPro is a collection of software components...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1586</th>\n",
       "      <td>3182</td>\n",
       "      <td>5</td>\n",
       "      <td>Besides using SentiStrength, we use the lexico...</td>\n",
       "      <td>In order to construct the lexical prior knowle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>3183</td>\n",
       "      <td>4</td>\n",
       "      <td>Run1: We firstly use the Stanford CoreNLP tool...</td>\n",
       "      <td>We use the Stanford CoreNLP caseless tagger fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>3184</td>\n",
       "      <td>4</td>\n",
       "      <td>By setting (n inw and enw(n)=1 for all nodes, ...</td>\n",
       "      <td>The proof is similar to the proof for the tree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>3185</td>\n",
       "      <td>5</td>\n",
       "      <td>2 We tested the difference in performance for ...</td>\n",
       "      <td>We calculate statistical significance of perfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>3186</td>\n",
       "      <td>4</td>\n",
       "      <td>Run1: We firstly use the Stanford CoreNLP tool...</td>\n",
       "      <td>We use the Stanford CoreNLP caseless tagger fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>3187</td>\n",
       "      <td>5</td>\n",
       "      <td>All linguistic annotations needed for features...</td>\n",
       "      <td>Part of speech tagging and named entity recogn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1592</th>\n",
       "      <td>3188</td>\n",
       "      <td>4</td>\n",
       "      <td>In (Erkan and Radev, 2004 ), the concept of gr...</td>\n",
       "      <td>It was also the model used to rank sentences i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>3189</td>\n",
       "      <td>5</td>\n",
       "      <td>Significance was tested using a paired bootstr...</td>\n",
       "      <td>Statistical significance is tested on the BLEU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>3194</td>\n",
       "      <td>4</td>\n",
       "      <td>For seed and test paradigms we used verbal inf...</td>\n",
       "      <td>To validate this measure, we computed the cosi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>3195</td>\n",
       "      <td>5</td>\n",
       "      <td>We used KenLM (Heafield, 2011) to create 3-gra...</td>\n",
       "      <td>1 We used the KenLM toolkit (Heafield, 2011) t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>3196</td>\n",
       "      <td>5</td>\n",
       "      <td>We built a 5-gram language model on the Englis...</td>\n",
       "      <td>We used KenLM (Heafield, 2011) to create 3-gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>3197</td>\n",
       "      <td>4</td>\n",
       "      <td>An estimate of the likelihood of a verb taking...</td>\n",
       "      <td>In the News column , we show the statistics of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>3198</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Web 1T 5-gram corpus (Brants and Fr...</td>\n",
       "      <td>Since the GoogleAPI is not available any more,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>3199</td>\n",
       "      <td>5</td>\n",
       "      <td>Machine translation system settings: We used a...</td>\n",
       "      <td>The Moses SMT toolkit (Koehn et al., 2007) pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>3200</td>\n",
       "      <td>5</td>\n",
       "      <td>Machine translation system settings: We used a...</td>\n",
       "      <td>We trained a model using Moses toolkit (Koehn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>3701</td>\n",
       "      <td>4</td>\n",
       "      <td>A chunk is a minimal , non-recursive structure...</td>\n",
       "      <td>The chunk label tagset is a coarser version of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>3702</td>\n",
       "      <td>5</td>\n",
       "      <td>MC-30: A subset of RG-65 dataset with 30 word ...</td>\n",
       "      <td>We use a set of 30 word pairs from a study car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>3703</td>\n",
       "      <td>5</td>\n",
       "      <td>We obtained news-peg judgments using the Brat ...</td>\n",
       "      <td>All annotations were done using the brat rapid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>3704</td>\n",
       "      <td>4</td>\n",
       "      <td>We transformed the parse trees in OntoNotes in...</td>\n",
       "      <td>We use the Stanford CoreNLP caseless tagger fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>3705</td>\n",
       "      <td>4</td>\n",
       "      <td>Gradient clipping heuristic to prevent the\" ex...</td>\n",
       "      <td>For the exploding gradient problem, numerical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>3706</td>\n",
       "      <td>5</td>\n",
       "      <td>We obtained news-peg judgments using the Brat ...</td>\n",
       "      <td>The annotations were made using the BRAT rapid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1607</th>\n",
       "      <td>3707</td>\n",
       "      <td>5</td>\n",
       "      <td>We then describe in more detail a modern Chine...</td>\n",
       "      <td>For Chinese, we use the Penn Chinese Treebank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>3708</td>\n",
       "      <td>5</td>\n",
       "      <td>In order to assess statistical significance of...</td>\n",
       "      <td>For statistical significance testing, we use a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>3710</td>\n",
       "      <td>5</td>\n",
       "      <td>Statistical significance in BLEU score differe...</td>\n",
       "      <td>Significance was tested using a paired bootstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610</th>\n",
       "      <td>3711</td>\n",
       "      <td>4</td>\n",
       "      <td>The article system builds on the elements of t...</td>\n",
       "      <td>The preposition classifier uses a combined sys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>3712</td>\n",
       "      <td>5</td>\n",
       "      <td>For calculating the required frequencies, we u...</td>\n",
       "      <td>As a practical approximation, we use bigram co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>3713</td>\n",
       "      <td>3</td>\n",
       "      <td>Phrase pairs were extracted from symmetrized w...</td>\n",
       "      <td>Word alignments were created using GIZA++ (Och...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>3714</td>\n",
       "      <td>4</td>\n",
       "      <td>We ran all of our experiments in Weka (Hall et...</td>\n",
       "      <td>Weka (Hall et al., 2009) which contains the im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>3715</td>\n",
       "      <td>4</td>\n",
       "      <td>Both corpora were extracted from the open para...</td>\n",
       "      <td>The parallel data were taken from OPUS (Tiedem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>3716</td>\n",
       "      <td>4</td>\n",
       "      <td>We ran all of our experiments in Weka (Hall et...</td>\n",
       "      <td>We conducted experiments using Multinomial Nai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>3717</td>\n",
       "      <td>4</td>\n",
       "      <td>The Spanish-English (S2E) training corpus was ...</td>\n",
       "      <td>The experiments focus on translation from Germ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>3718</td>\n",
       "      <td>4</td>\n",
       "      <td>Type System extends the type system that is bu...</td>\n",
       "      <td>This architecture is very similar to the frame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>3719</td>\n",
       "      <td>4</td>\n",
       "      <td>For other languages we use the corpora made av...</td>\n",
       "      <td>We use the CoNLL-X (Buchholz and Marsi, 2006) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>3720</td>\n",
       "      <td>5</td>\n",
       "      <td>Table 5 compares our reordering model with a r...</td>\n",
       "      <td>We note that our model outperforms the model p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>3720</td>\n",
       "      <td>4</td>\n",
       "      <td>Table 5 compares our reordering model with a r...</td>\n",
       "      <td>We note that our model outperforms the model p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>3721</td>\n",
       "      <td>3</td>\n",
       "      <td>These results contradict those given in Zelenk...</td>\n",
       "      <td>Zelenko et al. (2003) have shown the contiguou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>3722</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, OntoNotes (Hovy et al., 2006 ), a...</td>\n",
       "      <td>To this end, a recent large-scale annotation e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>3723</td>\n",
       "      <td>4</td>\n",
       "      <td>As for EJ translation, we use the Stanford par...</td>\n",
       "      <td>We use Stanford parser (de Marneffe et al., 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>3724</td>\n",
       "      <td>4</td>\n",
       "      <td>For both English and German we used the part-o...</td>\n",
       "      <td>Only for German data did we used the TreeTagge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>3725</td>\n",
       "      <td>4</td>\n",
       "      <td>We train our model on a subset of the WaCkyped...</td>\n",
       "      <td>We built a knowledge base (V 2 R) 1 using the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>3726</td>\n",
       "      <td>3</td>\n",
       "      <td>The questions are translated using a phrase-ba...</td>\n",
       "      <td>The first two baselines are standard systems u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1627</th>\n",
       "      <td>3727</td>\n",
       "      <td>3</td>\n",
       "      <td>The questions are translated using a phrase-ba...</td>\n",
       "      <td>The decoder is built on top of an open-source ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>3728</td>\n",
       "      <td>5</td>\n",
       "      <td>We measure statistical significance using 95% ...</td>\n",
       "      <td>Significance was tested using a paired bootstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>3729</td>\n",
       "      <td>4</td>\n",
       "      <td>As for EJ translation, we use the Stanford par...</td>\n",
       "      <td>We use Stanford parser (de Marneffe et al., 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1630</th>\n",
       "      <td>3730</td>\n",
       "      <td>3</td>\n",
       "      <td>Words were downcased and lemmatized using the ...</td>\n",
       "      <td>For Reuters we segmented and tokenized the dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>3731</td>\n",
       "      <td>3</td>\n",
       "      <td>English sentences are parsed into dependency s...</td>\n",
       "      <td>The grammatical relations are all the collapse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>3732</td>\n",
       "      <td>5</td>\n",
       "      <td>Both of our systems were based on the Moses de...</td>\n",
       "      <td>The decoder is built on top of an open-source ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>3733</td>\n",
       "      <td>4</td>\n",
       "      <td>We also used ANEW (Bradley and Lang, 1999) for...</td>\n",
       "      <td>Finally the ANEW lexicon (Bradley and Lang, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1634</th>\n",
       "      <td>3734</td>\n",
       "      <td>3</td>\n",
       "      <td>translation at the DiscoMT 2015 workshop (Hard...</td>\n",
       "      <td>The NLP Group of the Idiap Research Institute ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1635</th>\n",
       "      <td>3735</td>\n",
       "      <td>3</td>\n",
       "      <td>Establishing and maintaining common ground is ...</td>\n",
       "      <td>An example of such a pragmatic factor is commo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>3736</td>\n",
       "      <td>4</td>\n",
       "      <td>SentiWordNet score (senti) We used the Senti- ...</td>\n",
       "      <td>We then have assigned a sentiment score using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>3737</td>\n",
       "      <td>3</td>\n",
       "      <td>Europarl 2 (Koehn, 2005 ): it is a corpus of p...</td>\n",
       "      <td>We used a subset of the data provided for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>3738</td>\n",
       "      <td>4</td>\n",
       "      <td>We also carried out a chunk-reordering PB-SMT ...</td>\n",
       "      <td>Holmqvist et al. (2012) presented a method whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>3739</td>\n",
       "      <td>5</td>\n",
       "      <td>WSI is generally considered as an unsupervised...</td>\n",
       "      <td>Distributional models of meaning follow the di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>3740</td>\n",
       "      <td>3</td>\n",
       "      <td>The data used for the experiments described in...</td>\n",
       "      <td> Europarl 2 (Koehn, 2005 ): it is a corpus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>3743</td>\n",
       "      <td>4</td>\n",
       "      <td>For the language model, we used the KenLM tool...</td>\n",
       "      <td>Language Model: For all 3-SCFG systems we use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>3744</td>\n",
       "      <td>5</td>\n",
       "      <td>First used by Blitzer et al. (2007) , the MDS ...</td>\n",
       "      <td>To evaluate DA for sentiment classification, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>3748</td>\n",
       "      <td>5</td>\n",
       "      <td>The most famous example would probably be the ...</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>3749</td>\n",
       "      <td>4</td>\n",
       "      <td>For language modeling, we trained a separate 5...</td>\n",
       "      <td>In order to evaluate the fluency of each syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>3750</td>\n",
       "      <td>5</td>\n",
       "      <td>They used the Web-based annotation tool brat (...</td>\n",
       "      <td>The annotations were made using the BRAT rapid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>3751</td>\n",
       "      <td>5</td>\n",
       "      <td>The classification was conducted, using differ...</td>\n",
       "      <td>The SVM models were trained using the Scikit-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>3752</td>\n",
       "      <td>4</td>\n",
       "      <td>The Europarl corpus (Koehn, 2005) is built fro...</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>3753</td>\n",
       "      <td>5</td>\n",
       "      <td>For language modeling, we trained a separate 5...</td>\n",
       "      <td>The language model is a 5-gram KenLM (Heafield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>3754</td>\n",
       "      <td>5</td>\n",
       "      <td>The Spanish-English (S2E) training corpus was ...</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>3756</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Stanford CoreNLP caseless tagger fo...</td>\n",
       "      <td>We also lemmatized all words using Stanford Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>3757</td>\n",
       "      <td>5</td>\n",
       "      <td>All of our models are trained using Nematus (S...</td>\n",
       "      <td>We trained our basic neural machine translatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>3758</td>\n",
       "      <td>5</td>\n",
       "      <td>We use GIZA++ (Och and Ney, 2003) with its def...</td>\n",
       "      <td>We used the GIZA++ software (Och and Ney, 2003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>3759</td>\n",
       "      <td>4</td>\n",
       "      <td>We are working with standard tools as DISSECT ...</td>\n",
       "      <td>The matrix is weighted with PPMI as implemente...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>3760</td>\n",
       "      <td>5</td>\n",
       "      <td>They are based on Distributional Hypothesis wh...</td>\n",
       "      <td>Corpus-based VSMs follow the standard\" distrib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>3761</td>\n",
       "      <td>4</td>\n",
       "      <td>Its segmentation model is a class-based hidden...</td>\n",
       "      <td>For Chinese, a segmentation model (Zhang et al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>3762</td>\n",
       "      <td>5</td>\n",
       "      <td>The edit distance kernel was trained with LIBS...</td>\n",
       "      <td>As our learner, we use LIBSVM with a linear ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>3763</td>\n",
       "      <td>5</td>\n",
       "      <td>The corpora are first tokenized and lowercased...</td>\n",
       "      <td>The corpus was converted from XML to raw text,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1658</th>\n",
       "      <td>3764</td>\n",
       "      <td>5</td>\n",
       "      <td>For example, OntoNotes (Hovy et al., 2006 ), a...</td>\n",
       "      <td>For example, the OntoNotes (Hovy et al., 2006)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659</th>\n",
       "      <td>3766</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the open-source Moses toolkit (Koehn et...</td>\n",
       "      <td>The Moses toolkit (Koehn et al., 2007) is used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>3767</td>\n",
       "      <td>3</td>\n",
       "      <td>In our experiments, we use the LIBLINEAR packa...</td>\n",
       "      <td>Specifically, we use the LIBLINEAR SVM package...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>3768</td>\n",
       "      <td>5</td>\n",
       "      <td>The Spanish-English (S2E) training corpus was ...</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>3769</td>\n",
       "      <td>5</td>\n",
       "      <td>The ICSI meeting corpus (Janin et al., 2003) i...</td>\n",
       "      <td>The ICSI Meeting Corpus: The ICSI Meeting Corp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>3770</td>\n",
       "      <td>5</td>\n",
       "      <td>We use ROUGE (Lin, 2004) for evaluating the co...</td>\n",
       "      <td>We use ROUGE metric (Lin, 2004) to evaluate ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>3771</td>\n",
       "      <td>2</td>\n",
       "      <td>The measure selected is the normalised Pearson...</td>\n",
       "      <td>The task is part of the Semantic Evaluation 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>3772</td>\n",
       "      <td>5</td>\n",
       "      <td>The main corpora we use are Europarl (Koehn, 2...</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>3773</td>\n",
       "      <td>4</td>\n",
       "      <td>We exploit this monolingual data for training ...</td>\n",
       "      <td>We also used automatically back-translated in-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667</th>\n",
       "      <td>3774</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Stanford CoreNLP caseless tagger fo...</td>\n",
       "      <td>We also lemmatized all words using Stanford Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>3775</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a corpus-based metric relying on the d...</td>\n",
       "      <td>Corpus-based meaning representations rely on t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>3776</td>\n",
       "      <td>3</td>\n",
       "      <td>Similarly, (Turian et al, 2010) evaluated thre...</td>\n",
       "      <td>Turian et al. (2010)  evaluate different techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>3777</td>\n",
       "      <td>5</td>\n",
       "      <td>3.3.1 Reference System We compare a number of ...</td>\n",
       "      <td>We then use the phrase extraction utility in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>3778</td>\n",
       "      <td>5</td>\n",
       "      <td>For training SVM classifiers we used LIBSVM pa...</td>\n",
       "      <td>We used LIBSVM to implement our own SVM for re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1672</th>\n",
       "      <td>3779</td>\n",
       "      <td>3</td>\n",
       "      <td>The word alignment was trained using GIZA++ (O...</td>\n",
       "      <td>The word alignment was obtained by running Giz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>3781</td>\n",
       "      <td>5</td>\n",
       "      <td>The corpus is first word-aligned using a word ...</td>\n",
       "      <td>The word alignment was obtained by running Giz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>3782</td>\n",
       "      <td>4</td>\n",
       "      <td>We perform bootstrap resampling with bounds es...</td>\n",
       "      <td>We used bootstrap resampling for testing stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>3784</td>\n",
       "      <td>5</td>\n",
       "      <td>We perform bootstrap resampling with bounds es...</td>\n",
       "      <td>We used bootstrap resampling for testing stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>3785</td>\n",
       "      <td>4</td>\n",
       "      <td>Markov Logic Networks (MLN) (Richardson and Do...</td>\n",
       "      <td>Markov Logic Networks (MLN) (Richardson and Do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>3786</td>\n",
       "      <td>5</td>\n",
       "      <td>with the training script of the Moses toolkit ...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>3787</td>\n",
       "      <td>3</td>\n",
       "      <td>We train for 15 epochs using mini-batch stocha...</td>\n",
       "      <td>We initialize parameters uniformly, using the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>3788</td>\n",
       "      <td>3</td>\n",
       "      <td>This confirms the finding of Liu et al. (2012)...</td>\n",
       "      <td>More recently, Liu et al. (2012) have proposed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>3789</td>\n",
       "      <td>5</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "      <td>We evaluate our method by means of the Europar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>3790</td>\n",
       "      <td>4</td>\n",
       "      <td>We exploit this monolingual data for training ...</td>\n",
       "      <td>We also used automatically back-translated in-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>3791</td>\n",
       "      <td>4</td>\n",
       "      <td>Our decoder is a stack decoder similar to Koeh...</td>\n",
       "      <td>Phrase extraction was performed following Koeh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>3792</td>\n",
       "      <td>3</td>\n",
       "      <td>For the English- Spanish and French-English sy...</td>\n",
       "      <td>In our low-resource condition, we trained an S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>3794</td>\n",
       "      <td>5</td>\n",
       "      <td>The word alignment was trained using GIZA++ (O...</td>\n",
       "      <td>Word alignment was done with GIZA++ (Och and N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>3795</td>\n",
       "      <td>4</td>\n",
       "      <td>Rhetorical Structure Theory (RST) (Mann and Th...</td>\n",
       "      <td>The closest area to our work consists of inves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1686</th>\n",
       "      <td>3796</td>\n",
       "      <td>4</td>\n",
       "      <td>The dataset used for the experiments reported ...</td>\n",
       "      <td>The other groups have been used already (for e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687</th>\n",
       "      <td>3797</td>\n",
       "      <td>5</td>\n",
       "      <td>A good data source for this is the Europarl Co...</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>3798</td>\n",
       "      <td>5</td>\n",
       "      <td>The most famous example would probably be the ...</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>3800</td>\n",
       "      <td>5</td>\n",
       "      <td>Previously (Socher et al, 2011)  used a recurs...</td>\n",
       "      <td>For the MSRP task, Socher et al. (2011) used a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>4302</td>\n",
       "      <td>4</td>\n",
       "      <td>The first source is the CoNLL 2003 shared task...</td>\n",
       "      <td>AIDA/YAGO is derived from the CoNLL-2003 share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>4303</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>For this purpose, we use the Moses toolkit to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>4305</td>\n",
       "      <td>3</td>\n",
       "      <td>We therefore propose an alternative method bas...</td>\n",
       "      <td>We measure translation quality via the BLEU sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>4306</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "      <td>We used the Linear SVM implementation (with de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>4307</td>\n",
       "      <td>4</td>\n",
       "      <td>Finally, the CoNLL-2003 shared task (Tjong Kim...</td>\n",
       "      <td>AIDA/YAGO is derived from the CoNLL-2003 share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>4308</td>\n",
       "      <td>5</td>\n",
       "      <td>Further, we sentence-split, tokenized, and lem...</td>\n",
       "      <td>We also lemmatized all words using Stanford Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>4309</td>\n",
       "      <td>5</td>\n",
       "      <td>The corpora are first tokenized and lowercased...</td>\n",
       "      <td>All novels were lemmatized and POS-tagged usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>4310</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Adadelta (Zeiler, 2012) to update the p...</td>\n",
       "      <td>We also use Adadelta (Zeiler, 2012) to optimiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>4311</td>\n",
       "      <td>3</td>\n",
       "      <td>The first source is the CoNLL 2003 shared task...</td>\n",
       "      <td>AIDA/YAGO is derived from the CoNLL-2003 share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>4313</td>\n",
       "      <td>5</td>\n",
       "      <td>We used a 2009 snapshot of Wikipedia, 2 which ...</td>\n",
       "      <td>All novels were lemmatized and POS-tagged usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>4314</td>\n",
       "      <td>3</td>\n",
       "      <td>Recently, Hovy et al. (2013) utilized word emb...</td>\n",
       "      <td>Hovy et al. (2013) applied tree kernels to met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701</th>\n",
       "      <td>4316</td>\n",
       "      <td>4</td>\n",
       "      <td>Finally, the CoNLL-2003 shared task (Tjong Kim...</td>\n",
       "      <td>AIDA/YAGO is derived from the CoNLL-2003 share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>4317</td>\n",
       "      <td>5</td>\n",
       "      <td>We run our experiments on Europarl (Koehn, 200...</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>4318</td>\n",
       "      <td>5</td>\n",
       "      <td>We applied bootstrap resampling (Koehn, 2004) ...</td>\n",
       "      <td>We used bootstrap resampling for testing stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>4319</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>The Moses toolkit (Koehn et al., 2007) is used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>4320</td>\n",
       "      <td>5</td>\n",
       "      <td>The statistical significance test is also carr...</td>\n",
       "      <td>We used bootstrap resampling for testing stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>4321</td>\n",
       "      <td>4</td>\n",
       "      <td>This model is motivated by Vector Space Model ...</td>\n",
       "      <td>One of the best-known methods of representing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>4322</td>\n",
       "      <td>4</td>\n",
       "      <td>The data was segmented into baseNP parts and n...</td>\n",
       "      <td>We have used the baseNP data presented in (Ram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>4324</td>\n",
       "      <td>3</td>\n",
       "      <td>The NJU-Parser is based on the state-of-the ar...</td>\n",
       "      <td>The second algorithm, denoted GloTr, is the Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>4325</td>\n",
       "      <td>3</td>\n",
       "      <td>One is a 3-gram language model built using Ken...</td>\n",
       "      <td>Training and querying of a modified Kneser-Ney...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>4326</td>\n",
       "      <td>5</td>\n",
       "      <td>The word alignment was obtained by running Giz...</td>\n",
       "      <td>Baseline word alignments were obtained by runn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>4327</td>\n",
       "      <td>5</td>\n",
       "      <td>Translations for English words in the lexical ...</td>\n",
       "      <td>In order to improve the robustness of the word...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>4331</td>\n",
       "      <td>5</td>\n",
       "      <td>All the Language Models (LM) used in our exper...</td>\n",
       "      <td>The language models are estimated using the Ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>4332</td>\n",
       "      <td>4</td>\n",
       "      <td>In all experiments, we use the SVM classifier ...</td>\n",
       "      <td>For all experiments, we use a decision-tree cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>4333</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Stanford parser to generate a DG fo...</td>\n",
       "      <td>We used the lexicalized dependency parser in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>4334</td>\n",
       "      <td>4</td>\n",
       "      <td>Preprocessing: We tokenized the English side o...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716</th>\n",
       "      <td>4336</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "      <td>We use Moses 7 (Koehn et al., 2007) to impleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>4337</td>\n",
       "      <td>3</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1718</th>\n",
       "      <td>4338</td>\n",
       "      <td>5</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>4339</td>\n",
       "      <td>5</td>\n",
       "      <td>The first competitive learning based system is...</td>\n",
       "      <td>To solve coreference, we used a variation of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>4340</td>\n",
       "      <td>5</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>4341</td>\n",
       "      <td>3</td>\n",
       "      <td>We used predicted collapsed Stanford dependenc...</td>\n",
       "      <td>We extract syntactic dependencies using Stanfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>4342</td>\n",
       "      <td>4</td>\n",
       "      <td>We select as a general-purpose corpus Europarl...</td>\n",
       "      <td>We run our experiments on Europarl (Koehn, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>4343</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the lexicalized dependency parser in t...</td>\n",
       "      <td>In our experiments , we used the Stanford pars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>4344</td>\n",
       "      <td>3</td>\n",
       "      <td>The result of the operation is equivalent to w...</td>\n",
       "      <td>The OpenFst library is used to perform all of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>4345</td>\n",
       "      <td>3</td>\n",
       "      <td>The distributional hypothesis of meaning (Harr...</td>\n",
       "      <td>This is a corpus-based metric relying on the d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>4346</td>\n",
       "      <td>5</td>\n",
       "      <td>Preprocessing: We tokenized the English side o...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1727</th>\n",
       "      <td>4347</td>\n",
       "      <td>3</td>\n",
       "      <td>One is from (Turian et al, 2010) , the dimensi...</td>\n",
       "      <td>For BROWN, the features are the prefix feature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>4348</td>\n",
       "      <td>5</td>\n",
       "      <td>The thresholds were thoroughly selected depend...</td>\n",
       "      <td>For a pair of words, WordNet provides a series...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>4349</td>\n",
       "      <td>5</td>\n",
       "      <td>In-domain SMT: we used the parallel corpus (Se...</td>\n",
       "      <td>Our baseline is a phrase-based MT system train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>4350</td>\n",
       "      <td>5</td>\n",
       "      <td>The statistical significance test is also carr...</td>\n",
       "      <td>We measure significance of results using boots...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>4351</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "      <td>3.3.1 Reference System We compare a number of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>4352</td>\n",
       "      <td>5</td>\n",
       "      <td>We used a 2009 snapshot of Wikipedia, 2 which ...</td>\n",
       "      <td>3 All English data are POS tagged and lemmatis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>4353</td>\n",
       "      <td>4</td>\n",
       "      <td>The text was pre-processed using wp2txt 6 to r...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>4354</td>\n",
       "      <td>5</td>\n",
       "      <td>We run our experiments on Europarl (Koehn, 200...</td>\n",
       "      <td>Europarl (Koehn, 2005) is a multilingual paral...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>4355</td>\n",
       "      <td>4</td>\n",
       "      <td>We use Scikit-learn (Pedregosa et al., 2011 ),...</td>\n",
       "      <td>Logistic regression, implemented in Python wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1736</th>\n",
       "      <td>4356</td>\n",
       "      <td>5</td>\n",
       "      <td>The way they were added is similar to incorpor...</td>\n",
       "      <td>In this paper, we use the subjectivity corpus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>4358</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the lexicalized dependency parser in t...</td>\n",
       "      <td>In our experiments , we used the Stanford pars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>4359</td>\n",
       "      <td>4</td>\n",
       "      <td>We modified the implementation of the SWELL Ja...</td>\n",
       "      <td>We also compare our word embeddings with the E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>4360</td>\n",
       "      <td>4</td>\n",
       "      <td>The Stanford dependency parser (De Marneffe et...</td>\n",
       "      <td>We used the lexicalized dependency parser in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>4361</td>\n",
       "      <td>4</td>\n",
       "      <td>The first work on this topic was done back in ...</td>\n",
       "      <td>The Dutch version was tagged automatically usi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>4362</td>\n",
       "      <td>3</td>\n",
       "      <td>The OpenFst library is used to perform all of ...</td>\n",
       "      <td>The decoder is implemented with Weighted Finit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>4363</td>\n",
       "      <td>4</td>\n",
       "      <td>For our experiments, we used translated movie ...</td>\n",
       "      <td>For Chinese-English experiments, we used the O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>4364</td>\n",
       "      <td>3</td>\n",
       "      <td>The Moses15 result is obtained by applying the...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744</th>\n",
       "      <td>4365</td>\n",
       "      <td>3</td>\n",
       "      <td>The OpenFst library is used to perform all of ...</td>\n",
       "      <td>The decoder is implemented with Weighted Finit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>4366</td>\n",
       "      <td>3</td>\n",
       "      <td>We select as a general-purpose corpus Europarl...</td>\n",
       "      <td>We run our experiments on Europarl (Koehn, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>4367</td>\n",
       "      <td>4</td>\n",
       "      <td>We also compare our word embeddings with the E...</td>\n",
       "      <td>Dhillon et al. (2015) used CCA to derive word ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>4368</td>\n",
       "      <td>4</td>\n",
       "      <td>The relationship between language and sentimen...</td>\n",
       "      <td>The rapid growth of user-generated content, mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>4371</td>\n",
       "      <td>3</td>\n",
       "      <td>Support vector machines (SVM) are one of the b...</td>\n",
       "      <td>These classifiers are based on a discriminativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>4372</td>\n",
       "      <td>4</td>\n",
       "      <td>In Barankov and Tamchyna (2014), we experiment...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>4373</td>\n",
       "      <td>4</td>\n",
       "      <td>To learn the parameters of the model we minimi...</td>\n",
       "      <td>For optimization, we employed the Adam 6 http:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>4374</td>\n",
       "      <td>5</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "      <td>was used for word alignment and phrase transla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>4375</td>\n",
       "      <td>5</td>\n",
       "      <td>The text was pre-processed using wp2txt 6 to r...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>4376</td>\n",
       "      <td>3</td>\n",
       "      <td>The task reported on here is to produce PropBa...</td>\n",
       "      <td>A standard for predicate argument annotation i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>4377</td>\n",
       "      <td>4</td>\n",
       "      <td>For both systems, the used training data is fr...</td>\n",
       "      <td>The source of bilingual data used in the exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>4378</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Stanford parser to generate a DG fo...</td>\n",
       "      <td>We used the lexicalized dependency parser in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>4379</td>\n",
       "      <td>4</td>\n",
       "      <td>We used predicted collapsed Stanford dependenc...</td>\n",
       "      <td>We extract syntactic dependencies using Stanfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>4380</td>\n",
       "      <td>4</td>\n",
       "      <td>We tokenize English data and segment Chinese d...</td>\n",
       "      <td>2 Further, we sentence-split, tokenized, and l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>4381</td>\n",
       "      <td>3</td>\n",
       "      <td>The evaluation emphasis in multi-document summ...</td>\n",
       "      <td>ROUGE (Lin and Hovy, 2003) has been adopted as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>4382</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phrase b...</td>\n",
       "      <td>All experiments were carried out using the ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>4383</td>\n",
       "      <td>5</td>\n",
       "      <td>The similarity function is here the Smoothed P...</td>\n",
       "      <td>We directly apply the Smoothed Partial Tree-Ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>4384</td>\n",
       "      <td>5</td>\n",
       "      <td>We tokenize English data and segment Chinese d...</td>\n",
       "      <td>2 Further, we sentence-split, tokenized, and l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>4385</td>\n",
       "      <td>3</td>\n",
       "      <td>The Stanford dependency parser (De Marneffe et...</td>\n",
       "      <td>We used the lexicalized dependency parser in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>4386</td>\n",
       "      <td>3</td>\n",
       "      <td>To demonstrate the effect of the proposed meth...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>4387</td>\n",
       "      <td>5</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "      <td>The Moses toolkit (Koehn et al., 2007) is used...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>4388</td>\n",
       "      <td>3</td>\n",
       "      <td>3.3.1 Reference System We compare a number of ...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>4389</td>\n",
       "      <td>5</td>\n",
       "      <td>Preprocessing: We tokenized the English side o...</td>\n",
       "      <td>We tokenize and frequent-case the data with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>4391</td>\n",
       "      <td>3</td>\n",
       "      <td>Finally, it is also noticeable that the percen...</td>\n",
       "      <td>Baroni et al. (2002)  report that 47% of the v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>4392</td>\n",
       "      <td>3</td>\n",
       "      <td>As a learning algorithm we adopt a ranking SVM...</td>\n",
       "      <td>To train a mention-pair classifier, we use the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>4393</td>\n",
       "      <td>2</td>\n",
       "      <td>Brain images are quite noisy, so we used the m...</td>\n",
       "      <td>Following the evaluation paradigm of Mitchell ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>4394</td>\n",
       "      <td>3</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "      <td>This system is the Moses decoder (Koehn et al....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>4395</td>\n",
       "      <td>4</td>\n",
       "      <td>Results on English-French, English-Romanian, a...</td>\n",
       "      <td>Our alignment model is based on a simple varia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>4396</td>\n",
       "      <td>4</td>\n",
       "      <td>The work presented in Berger et al. (1996) tha...</td>\n",
       "      <td>The original reordering constraint in Berger e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>4398</td>\n",
       "      <td>4</td>\n",
       "      <td>Our second method is based on the recurrent ne...</td>\n",
       "      <td>Mikolov et al. (2013a) proposed a faster skip-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>4399</td>\n",
       "      <td>4</td>\n",
       "      <td>The SUSANNE Corpus is a modified and condensed...</td>\n",
       "      <td>The corpus consists of a subset of the Brown C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>4400</td>\n",
       "      <td>5</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "      <td>All these features are inherited from Moses (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>4901</td>\n",
       "      <td>5</td>\n",
       "      <td>English annotations were all produced using th...</td>\n",
       "      <td>We also lemmatized all words using Stanford Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>4903</td>\n",
       "      <td>5</td>\n",
       "      <td>The parameters are estimated by Gibbs sampling...</td>\n",
       "      <td>10 We used the PLTM implementation in Mallet (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>4904</td>\n",
       "      <td>4</td>\n",
       "      <td>The meta-classifier is a linear SVM (Fan et al...</td>\n",
       "      <td>LIBLINEAR (Fan et al., 2008 ), a library for l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>4905</td>\n",
       "      <td>3</td>\n",
       "      <td>We use Adadelta (Zeiler, 2012) to update the p...</td>\n",
       "      <td>We use a mini-batch stochastic gradient descen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>4907</td>\n",
       "      <td>5</td>\n",
       "      <td>We train a Support Vector Machine (SVM) (Corte...</td>\n",
       "      <td>Specifically, we use Support Vector Machine (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>4908</td>\n",
       "      <td>5</td>\n",
       "      <td>Parameter tuning is carried out using Z- MERT ...</td>\n",
       "      <td>Feature weights, based on BLEU, are then tuned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>4909</td>\n",
       "      <td>4</td>\n",
       "      <td>Brin identifies the use of patterns in the dis...</td>\n",
       "      <td>Brin proposed the bootstrapping method for rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>4910</td>\n",
       "      <td>5</td>\n",
       "      <td>We adopt the setting of Socher et al. (2012) .</td>\n",
       "      <td>Neural networks are first used in this task in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>4912</td>\n",
       "      <td>5</td>\n",
       "      <td>The publicly available tool GIZA++ was used to...</td>\n",
       "      <td>We used the GIZA++ software (Och and Ney, 2003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>4913</td>\n",
       "      <td>5</td>\n",
       "      <td>CRFSuite implementation (Okazaki, 2007 ).</td>\n",
       "      <td>We trained a CRF tagger using CRFSuite 1 (Okaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>4914</td>\n",
       "      <td>3</td>\n",
       "      <td>The HMM classifier used in our experiments fol...</td>\n",
       "      <td>The HMM classifier used in the experiments in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>4915</td>\n",
       "      <td>5</td>\n",
       "      <td>We computed 4-gram LMs with modified Kneser-Ne...</td>\n",
       "      <td>The language model is a 5-gram with interpolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>4916</td>\n",
       "      <td>5</td>\n",
       "      <td>Socher et al. (2011) come closest to our targe...</td>\n",
       "      <td>We plan to adapt ideas from Socher et al. (201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>4917</td>\n",
       "      <td>3</td>\n",
       "      <td>We use Ridge Regression (RR) with l2-norm regu...</td>\n",
       "      <td>We use the Support Vector Machines implementat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>4918</td>\n",
       "      <td>3</td>\n",
       "      <td>As for the former (hereafter it is referred to...</td>\n",
       "      <td>For argument, a dependency version of the prun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>4920</td>\n",
       "      <td>5</td>\n",
       "      <td>15 The significance tests were performed using...</td>\n",
       "      <td>We used bootstrap resampling for testing stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>4921</td>\n",
       "      <td>5</td>\n",
       "      <td>In this work, we focus on learning with Suppor...</td>\n",
       "      <td>Specifically, we use Support Vector Machine (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>4922</td>\n",
       "      <td>4</td>\n",
       "      <td>We used TreeTagger (Schmid, 1994) to obtain a ...</td>\n",
       "      <td>We used the Stuttgart TreeTagger (Schmid, 1994...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>4923</td>\n",
       "      <td>5</td>\n",
       "      <td>To account for this constraint,  include infor...</td>\n",
       "      <td>Earlier models made use of latent semantic ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>4924</td>\n",
       "      <td>3</td>\n",
       "      <td>The system was trained on the English and Dani...</td>\n",
       "      <td>In principle, classifiers trained on PDTB data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>4925</td>\n",
       "      <td>3</td>\n",
       "      <td>We use linear SVMs from LIBLINEAR and SVMs wit...</td>\n",
       "      <td>The edit distance kernel was trained with LIBS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>4926</td>\n",
       "      <td>3</td>\n",
       "      <td>In current phrase-based statistical machine tr...</td>\n",
       "      <td>It is a standard phrase-based machine translat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>4928</td>\n",
       "      <td>5</td>\n",
       "      <td>We train a Support Vector Machine (SVM) (Corte...</td>\n",
       "      <td>Specifically, we use Support Vector Machine (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>4929</td>\n",
       "      <td>5</td>\n",
       "      <td>The word alignment was obtained by running Giz...</td>\n",
       "      <td>The word alignment is created by GIZA++ (Och a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>4930</td>\n",
       "      <td>3</td>\n",
       "      <td>For training SVM classifiers we used LIBSVM pa...</td>\n",
       "      <td>The SVM implementation used was LIBSVM (Chang ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>4931</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the standard alignment tool Giza++ (Och...</td>\n",
       "      <td>We used GIZA++ (Och and Ney, 2003) to align th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>4932</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the standard alignment tool Giza++ (Och...</td>\n",
       "      <td>We used the GIZA++ software (Och and Ney, 2003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>4933</td>\n",
       "      <td>3</td>\n",
       "      <td>In the Penn Discourse TreeBank 2.0 (Prasad et ...</td>\n",
       "      <td>In contrast, the set of discourse markers in o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>4934</td>\n",
       "      <td>4</td>\n",
       "      <td>Weka (Hall et al., 2009) was used to apply lea...</td>\n",
       "      <td>The SVM implementation of Weka (Hall et al., 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>4937</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>This was done with a specific tool provided wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>4938</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the standard alignment tool Giza++ (Och...</td>\n",
       "      <td>We used the GIZA++ software (Och and Ney, 2003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1807</th>\n",
       "      <td>4939</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the GIZA++ software (Och and Ney, 2003...</td>\n",
       "      <td>We performed word alignment using GIZA++ (Och ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>4940</td>\n",
       "      <td>5</td>\n",
       "      <td>The language model used is the 5-gram corpus f...</td>\n",
       "      <td>We used Google Books ngrams (Michel et al., 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1809</th>\n",
       "      <td>4941</td>\n",
       "      <td>5</td>\n",
       "      <td>English annotations were all produced using th...</td>\n",
       "      <td>We also lemmatized all words using Stanford Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>4942</td>\n",
       "      <td>3</td>\n",
       "      <td>For example, (Turian et al, 2010) compared Bro...</td>\n",
       "      <td>We evaluate C&amp;W word embeddings with 25, 50 an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>4943</td>\n",
       "      <td>3</td>\n",
       "      <td>Distributional representations encode an expre...</td>\n",
       "      <td>This is often called distributional semantics ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1812</th>\n",
       "      <td>4944</td>\n",
       "      <td>4</td>\n",
       "      <td>(Li et al, 2013)  use crowdsourcing to build p...</td>\n",
       "      <td>Our neural network is similar to that of Li et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1813</th>\n",
       "      <td>4945</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the standard alignment tool Giza++ (Och...</td>\n",
       "      <td>We used GIZA++ (Och and Ney, 2003) to align th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1814</th>\n",
       "      <td>4946</td>\n",
       "      <td>4</td>\n",
       "      <td>The formalism that is used to represent the se...</td>\n",
       "      <td>Meaning representation and composition in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>4947</td>\n",
       "      <td>5</td>\n",
       "      <td>For French, Hungarian, Polish and Swedish we u...</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>4950</td>\n",
       "      <td>4</td>\n",
       "      <td>The English text was tokenized using the word ...</td>\n",
       "      <td>The stopwords are taken from the stop-word lis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>4951</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the MOSES decoder (Koehn et al., 2007) ...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>4952</td>\n",
       "      <td>4</td>\n",
       "      <td>Specifically, the sentence compression dataset...</td>\n",
       "      <td>One idea is to apply it to the language model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>4953</td>\n",
       "      <td>4</td>\n",
       "      <td>use the Stanford Parser (de Marneffe et al., 2...</td>\n",
       "      <td>We extract syntactic dependencies using Stanfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>4954</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>4955</td>\n",
       "      <td>4</td>\n",
       "      <td>In practice, the decoder has to employ beam se...</td>\n",
       "      <td>To make the exponential algorithm practical, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>4956</td>\n",
       "      <td>5</td>\n",
       "      <td>The source of bilingual data used in the exper...</td>\n",
       "      <td>The EuroParl data set consists of 707 sentence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>4957</td>\n",
       "      <td>4</td>\n",
       "      <td>The tectogrammatical annotation layer is based...</td>\n",
       "      <td>Our approach is based on the Czech linguistic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>4958</td>\n",
       "      <td>4</td>\n",
       "      <td>In this paper, we use the subjectivity corpus ...</td>\n",
       "      <td>These classifiers have been used in related wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>4959</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>We build a baseline error correction system, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>4960</td>\n",
       "      <td>5</td>\n",
       "      <td>All our systems are contrasted with a standard...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>4961</td>\n",
       "      <td>4</td>\n",
       "      <td>A common approach to computing similarity is t...</td>\n",
       "      <td>This sense similarity measure is inspired by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828</th>\n",
       "      <td>4962</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>4963</td>\n",
       "      <td>3</td>\n",
       "      <td>The Stanford dependency parser (De Marneffe et...</td>\n",
       "      <td>In our experiments , we used the Stanford pars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>4964</td>\n",
       "      <td>3</td>\n",
       "      <td>We tokenize and frequent-case the data with th...</td>\n",
       "      <td>We implement MERT and MIRA 1 , and directly us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>4965</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>We tokenize and frequent-case the data with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>4966</td>\n",
       "      <td>4</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>Our baseline is a phrase-based MT system train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>4967</td>\n",
       "      <td>4</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834</th>\n",
       "      <td>4968</td>\n",
       "      <td>4</td>\n",
       "      <td>We tokenize and frequent-case the data with th...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>4969</td>\n",
       "      <td>5</td>\n",
       "      <td>We conducted baseline experiments for phraseba...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>4970</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>4971</td>\n",
       "      <td>4</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838</th>\n",
       "      <td>4972</td>\n",
       "      <td>5</td>\n",
       "      <td>In contrast, (McClosky et al, 2006) focus on l...</td>\n",
       "      <td>McClosky et al. (2006)  use self-training in c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>4973</td>\n",
       "      <td>3</td>\n",
       "      <td>We use logistic regression with L2 regularizat...</td>\n",
       "      <td>@BULLET Logistic Regression(LR): We use Logist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1840</th>\n",
       "      <td>4974</td>\n",
       "      <td>5</td>\n",
       "      <td>Since the phrase table contains lemmas, the Wi...</td>\n",
       "      <td>3 All English data are POS tagged and lemmatis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841</th>\n",
       "      <td>4975</td>\n",
       "      <td>4</td>\n",
       "      <td>We use logistic regression with L2 regularizat...</td>\n",
       "      <td>We use the Bernoulli Naive Bayes classifier in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>4976</td>\n",
       "      <td>4</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "      <td>Both systems are based on the Moses SMT toolki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>4977</td>\n",
       "      <td>4</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "      <td>We tokenize and frequent-case the data with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>4978</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>4980</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>4981</td>\n",
       "      <td>3</td>\n",
       "      <td>Europarl (Koehn, 2005) is a multilingual paral...</td>\n",
       "      <td>We extract our paraphrase grammar from the Fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>4982</td>\n",
       "      <td>5</td>\n",
       "      <td>For all syntactic parsers, we used the\" basic\"...</td>\n",
       "      <td>In our experiments , we used the Stanford pars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>4983</td>\n",
       "      <td>3</td>\n",
       "      <td>The Stanford dependency parser (De Marneffe et...</td>\n",
       "      <td>In our experiments , we used the Stanford pars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>4984</td>\n",
       "      <td>5</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "      <td>All our systems are contrasted with a standard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>4985</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the cross-entropy loss function and min...</td>\n",
       "      <td>We use AdaGrad (Duchi et al., 2011)  with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>4986</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the cross-entropy loss function and min...</td>\n",
       "      <td>We train the concept identification stage usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852</th>\n",
       "      <td>4987</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the AdaGrad method (Duchi et al., 2011)...</td>\n",
       "      <td>We use the cross-entropy loss function and min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>4988</td>\n",
       "      <td>5</td>\n",
       "      <td>Socher et al. (2011)  explored using recursive...</td>\n",
       "      <td>Socher et al. (2011)  use recursive auto-encod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>4989</td>\n",
       "      <td>5</td>\n",
       "      <td>2 Translation Model Moses is a state-of-the-ar...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>4989</td>\n",
       "      <td>5</td>\n",
       "      <td>2 Translation Model Moses is a state-of-the-ar...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856</th>\n",
       "      <td>4990</td>\n",
       "      <td>5</td>\n",
       "      <td>2 Translation Model Moses is a state-of-the-ar...</td>\n",
       "      <td>Our baseline is a phrase-based MT system train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>4991</td>\n",
       "      <td>4</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>4992</td>\n",
       "      <td>5</td>\n",
       "      <td>All our systems are contrasted with a standard...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1859</th>\n",
       "      <td>4993</td>\n",
       "      <td>5</td>\n",
       "      <td>For all syntactic parsers, we used the\" basic\"...</td>\n",
       "      <td>In our experiments , we used the Stanford pars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>4994</td>\n",
       "      <td>4</td>\n",
       "      <td>can be evaluated by maximising the pseudo-like...</td>\n",
       "      <td>The parameters can be efficiently estimated fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1861</th>\n",
       "      <td>4995</td>\n",
       "      <td>5</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1862</th>\n",
       "      <td>4996</td>\n",
       "      <td>4</td>\n",
       "      <td>2 Translation Model Moses is a state-of-the-ar...</td>\n",
       "      <td>All our systems are contrasted with a standard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1863</th>\n",
       "      <td>4997</td>\n",
       "      <td>3</td>\n",
       "      <td>In this approach we used the NERsuite software...</td>\n",
       "      <td>NERsuite is a NER system that is built on top ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>4998</td>\n",
       "      <td>4</td>\n",
       "      <td>2 Translation Model Moses is a state-of-the-ar...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865</th>\n",
       "      <td>4999</td>\n",
       "      <td>4</td>\n",
       "      <td>These efforts focused exclusively on the meron...</td>\n",
       "      <td>Experts can manually specify the attributes of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1866</th>\n",
       "      <td>5000</td>\n",
       "      <td>4</td>\n",
       "      <td>Special forms of relatedness are represented i...</td>\n",
       "      <td>Experts can manually specify the attributes of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867</th>\n",
       "      <td>5501</td>\n",
       "      <td>5</td>\n",
       "      <td>We train our model on a subset of the WaCkyped...</td>\n",
       "      <td>We used the publicly available Wacky corpus (B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1868</th>\n",
       "      <td>5502</td>\n",
       "      <td>4</td>\n",
       "      <td>Since the first shared task on Recognising Tex...</td>\n",
       "      <td>Although there had been research on reasoning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1869</th>\n",
       "      <td>5503</td>\n",
       "      <td>5</td>\n",
       "      <td>We used k-best batch MIRA (Cherry and Foster, ...</td>\n",
       "      <td>We tune the systems using kbest batch MIRA (Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>5504</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the Mallet toolkit (McCallum, 2002) fo...</td>\n",
       "      <td>We use the MALLET package (McCallum, 2002) for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871</th>\n",
       "      <td>5505</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the implementation provided by CRFsuite...</td>\n",
       "      <td>We use CRFsuite (Okazaki, 2007) as an implemen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1872</th>\n",
       "      <td>5506</td>\n",
       "      <td>5</td>\n",
       "      <td>glish source with target French by using GIZA+...</td>\n",
       "      <td>Word alignments were created using GIZA++ (Och...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1873</th>\n",
       "      <td>5507</td>\n",
       "      <td>5</td>\n",
       "      <td>We used ROUGE-N (Lin, 2004) for evaluation of ...</td>\n",
       "      <td>We expect this restriction is more consistent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1874</th>\n",
       "      <td>5508</td>\n",
       "      <td>4</td>\n",
       "      <td>1 with 2 -regularization using AdaGrad (Duchi ...</td>\n",
       "      <td>We adopt online learning, updating parameters ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875</th>\n",
       "      <td>5509</td>\n",
       "      <td>4</td>\n",
       "      <td>We used Weka (Hall et al., 2009) for all our c...</td>\n",
       "      <td>Weka (Hall et al., 2009) which contains the im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1876</th>\n",
       "      <td>5510</td>\n",
       "      <td>4</td>\n",
       "      <td>POS tagging was performed with TreeTagger (Sch...</td>\n",
       "      <td>TreeTagger is a statistical, decision tree-bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1877</th>\n",
       "      <td>5511</td>\n",
       "      <td>5</td>\n",
       "      <td>Word alignments on the parallel corpus are per...</td>\n",
       "      <td>The parallel corpus is wordaligned using GIZA+...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878</th>\n",
       "      <td>5512</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "      <td>Classification uses the scikit-learn Python pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>5514</td>\n",
       "      <td>3</td>\n",
       "      <td>The Lexical sample data was parsed using the C...</td>\n",
       "      <td>It builds on the C&amp;C CCG parser (Clark and Cur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>5515</td>\n",
       "      <td>5</td>\n",
       "      <td>For POS-tagging, we used the Stanford POS-tagg...</td>\n",
       "      <td>Next, we replace all nouns with their POS tag;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>5516</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "      <td>All the data come from Europarl (Koehn, 2005 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>5517</td>\n",
       "      <td>3</td>\n",
       "      <td>This data is part of the NUCLE corpus (Dahlmei...</td>\n",
       "      <td>The training data released by the task organiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>5518</td>\n",
       "      <td>3</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "      <td>conducted using the Moses phrase-based decoder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>5519</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the word alignment produced by Giza (O...</td>\n",
       "      <td>Och is the HMM alignment model of (Och and Ney...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>5520</td>\n",
       "      <td>5</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "      <td>All the data come from Europarl (Koehn, 2005 ).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>5521</td>\n",
       "      <td>4</td>\n",
       "      <td>Word alignment is performed using GIZA++ (Och ...</td>\n",
       "      <td>glish source with target French) by using GIZA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>5522</td>\n",
       "      <td>5</td>\n",
       "      <td>Word alignment was done with GIZA++ (Och and N...</td>\n",
       "      <td>Word alignment is performed with Giza++ (Och a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>5524</td>\n",
       "      <td>4</td>\n",
       "      <td>The evaluation results were provided by the or...</td>\n",
       "      <td>The models were evaluated using M2 scorer (Dah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>5525</td>\n",
       "      <td>3</td>\n",
       "      <td>In future work we can therefore incorporate un...</td>\n",
       "      <td>Han et al. (2012) introduced a dictionary base...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>5526</td>\n",
       "      <td>4</td>\n",
       "      <td>glish source with target French) by using GIZA...</td>\n",
       "      <td>Word alignment is done using GIZA++ (Och and N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>5527</td>\n",
       "      <td>3</td>\n",
       "      <td>We conducted statistical significance tests fo...</td>\n",
       "      <td>We used bootstrap resampling for testing stati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1892</th>\n",
       "      <td>5528</td>\n",
       "      <td>5</td>\n",
       "      <td>The word alignment was obtained by running Giz...</td>\n",
       "      <td>Word alignment is performed with Giza++ (Och a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>5529</td>\n",
       "      <td>5</td>\n",
       "      <td>Word alignment was done with GIZA++ (Och and N...</td>\n",
       "      <td>Word alignment is performed using GIZA++ (Och ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>5531</td>\n",
       "      <td>4</td>\n",
       "      <td>For the theory of Cho &amp; Chai (2000) to be comp...</td>\n",
       "      <td>In section 3, we present a solution to the all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>5532</td>\n",
       "      <td>5</td>\n",
       "      <td>In order to compare our method to a well under...</td>\n",
       "      <td>In our experiments we investigated both weak a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896</th>\n",
       "      <td>5534</td>\n",
       "      <td>4</td>\n",
       "      <td>We examine the quality of translations to Engl...</td>\n",
       "      <td>To calculate the number of changes, we used a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>5535</td>\n",
       "      <td>4</td>\n",
       "      <td>Integer Linear Programming (ILP) has recently ...</td>\n",
       "      <td>Second, to avoid the error propagation problem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>5536</td>\n",
       "      <td>5</td>\n",
       "      <td>For that purpose, we use the word analogy task...</td>\n",
       "      <td>Mikolov et al. (2013a) proposed a faster skip-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>5537</td>\n",
       "      <td>3</td>\n",
       "      <td>The corpus is first word-aligned using a word ...</td>\n",
       "      <td>A core component of every PBSMT system is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>5538</td>\n",
       "      <td>3</td>\n",
       "      <td>For ranking, we use the SVM rank ranker (Joach...</td>\n",
       "      <td>We use the SVM rank implementation (Joachims, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1901</th>\n",
       "      <td>5539</td>\n",
       "      <td>3</td>\n",
       "      <td>Parameters are updated using AdaGrad (Duchi et...</td>\n",
       "      <td>All models were trained using Adagrad (Duchi e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>5540</td>\n",
       "      <td>3</td>\n",
       "      <td>The tagger we use is TnT (Brants, 2000) , a hi...</td>\n",
       "      <td>HCRC is tagged with TnT (Brants, 2000 ), train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>5541</td>\n",
       "      <td>3</td>\n",
       "      <td>Sentiment score of the last post of the observ...</td>\n",
       "      <td>We trained the classifiers using the LIBLINEAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>5544</td>\n",
       "      <td>4</td>\n",
       "      <td>For German English we also have a system based...</td>\n",
       "      <td>For reference, we also show the MT performance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1905</th>\n",
       "      <td>5546</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Universal POS Tagset (UPOS) of Petr...</td>\n",
       "      <td>For example, Petrov et al. (2012)  build super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>5547</td>\n",
       "      <td>3</td>\n",
       "      <td>We also considered an ensemble of our approach...</td>\n",
       "      <td>We chose a threshold such that our approach pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907</th>\n",
       "      <td>5548</td>\n",
       "      <td>3</td>\n",
       "      <td>One way to solve this problem is to use a kern...</td>\n",
       "      <td>The proof is similar to the proof for the tree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908</th>\n",
       "      <td>5549</td>\n",
       "      <td>3</td>\n",
       "      <td>We use Adam (Kingma and Ba, 2015) for optimisa...</td>\n",
       "      <td>We train with the Adam optimizer (Kingma and B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>5550</td>\n",
       "      <td>3</td>\n",
       "      <td>We build our PB-SMT systems in a standard way ...</td>\n",
       "      <td>We use KenLM 3 (Heafield, 2011) for computing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>5551</td>\n",
       "      <td>4</td>\n",
       "      <td>The starting point for our model is the skipgr...</td>\n",
       "      <td>Our model is an extension of the contextual ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>5553</td>\n",
       "      <td>4</td>\n",
       "      <td>We introduce a new anaphoricity detection mode...</td>\n",
       "      <td>We generated embeddings by training a characte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912</th>\n",
       "      <td>5554</td>\n",
       "      <td>5</td>\n",
       "      <td>The realisation ranking component is an SVM ra...</td>\n",
       "      <td>The advantage of an SVM rank model is that it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>5556</td>\n",
       "      <td>3</td>\n",
       "      <td>WSI is generally considered as an unsupervised...</td>\n",
       "      <td>According to the distributional hypothesis of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1914</th>\n",
       "      <td>5557</td>\n",
       "      <td>4</td>\n",
       "      <td>We run our experiments on Europarl (Koehn, 200...</td>\n",
       "      <td>Our experiments were carried out on the Europa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>5558</td>\n",
       "      <td>3</td>\n",
       "      <td>For the language model, we used the KenLM tool...</td>\n",
       "      <td>The language model is a 5-gram KenLM (Heafield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>5559</td>\n",
       "      <td>3</td>\n",
       "      <td>The second collection is constituted by the GE...</td>\n",
       "      <td>GENIA (Kim et al., 2003) is a collection of 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>5560</td>\n",
       "      <td>4</td>\n",
       "      <td>We introduce a new anaphoricity detection mode...</td>\n",
       "      <td>We generated embeddings by training a characte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>5563</td>\n",
       "      <td>5</td>\n",
       "      <td>The sentence aligned parallel data is first wo...</td>\n",
       "      <td>Baseline word alignments were obtained by runn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>5564</td>\n",
       "      <td>3</td>\n",
       "      <td>All the Language Models (LM) used in our exper...</td>\n",
       "      <td>For the language model, we used all monolingua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>5565</td>\n",
       "      <td>4</td>\n",
       "      <td>They are based on Distributional Hypothesis wh...</td>\n",
       "      <td>Many researchers have considered generating pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>5566</td>\n",
       "      <td>4</td>\n",
       "      <td>Finally, we consider the Europarl corpus v7 (K...</td>\n",
       "      <td>Our experiments were carried out on the Europa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>5568</td>\n",
       "      <td>3</td>\n",
       "      <td>For the contextual check we use the Google Web...</td>\n",
       "      <td>org/wiki/Fog_Index 8 For word frequency we use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>5569</td>\n",
       "      <td>3</td>\n",
       "      <td>By imposing constraints on the possible word r...</td>\n",
       "      <td>The first is a reimplementation of the stack-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>5571</td>\n",
       "      <td>5</td>\n",
       "      <td>Surdeanu et al. (2012) propose a two-layer mul...</td>\n",
       "      <td>Surdeanu et al. (2012) proposed a novel approa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>5572</td>\n",
       "      <td>3</td>\n",
       "      <td>We have theoretically suggested that based on ...</td>\n",
       "      <td>Then we revise the two LP constraints of Cho &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>5574</td>\n",
       "      <td>3</td>\n",
       "      <td>For the contextual check we use the Google Web...</td>\n",
       "      <td>We used the unigram counts from the Web 1T 5-g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>5575</td>\n",
       "      <td>5</td>\n",
       "      <td>ROUGE (Lin, 2004) is a set of evaluation metri...</td>\n",
       "      <td>We expect this restriction is more consistent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>5576</td>\n",
       "      <td>5</td>\n",
       "      <td>Here we review the parameters of the standard ...</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>5577</td>\n",
       "      <td>4</td>\n",
       "      <td>The Spanish-English (S2E) training corpus was ...</td>\n",
       "      <td>The systems for the English ? Spanish translat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>5578</td>\n",
       "      <td>3</td>\n",
       "      <td>We convert the trees in both treebanks from co...</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>5579</td>\n",
       "      <td>3</td>\n",
       "      <td>We convert the trees in both treebanks from co...</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>5580</td>\n",
       "      <td>5</td>\n",
       "      <td>We use KenLM 3 (Heafield, 2011) for computing ...</td>\n",
       "      <td>We use a 5-gram LM trained on the Gigaword cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>5581</td>\n",
       "      <td>5</td>\n",
       "      <td>The most famous example would probably be the ...</td>\n",
       "      <td>The system was trained on the English and Dani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>5582</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Stanford parser with Stanford depen...</td>\n",
       "      <td>We convert the trees in both treebanks from co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>5583</td>\n",
       "      <td>4</td>\n",
       "      <td>Additionally, we train phrase-based machine tr...</td>\n",
       "      <td>Here we review the parameters of the standard ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>5584</td>\n",
       "      <td>4</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "      <td>The Moses decoder (Koehn et al., 2007) was use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>5585</td>\n",
       "      <td>5</td>\n",
       "      <td>The morpho syntactically annotated corpus we u...</td>\n",
       "      <td>This corresponds to a new version of the Frenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>5586</td>\n",
       "      <td>3</td>\n",
       "      <td>There has been a large amount of work on senti...</td>\n",
       "      <td>For a detailed survey of the field of sentimen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>5587</td>\n",
       "      <td>5</td>\n",
       "      <td>This task setup is further described in the ta...</td>\n",
       "      <td>A complete description of the training and tes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>5588</td>\n",
       "      <td>5</td>\n",
       "      <td>The baseline will be created by the Moses SMT ...</td>\n",
       "      <td>18 Our baseline is the SMT toolkit Moses (Koeh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>5591</td>\n",
       "      <td>3</td>\n",
       "      <td>Discourse structure in summarization Rhetorica...</td>\n",
       "      <td>Causal relations are among discourse relations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>5592</td>\n",
       "      <td>3</td>\n",
       "      <td>It builds on the C&amp;C CCG parser (Clark and Cur...</td>\n",
       "      <td>It receives CCG derivations from the C&amp;C parse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>5593</td>\n",
       "      <td>5</td>\n",
       "      <td>We use Boxer (Bos et al., 2004) to parse natur...</td>\n",
       "      <td>In transforming natural language text to logic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>5594</td>\n",
       "      <td>5</td>\n",
       "      <td>The training set is used to train the phrase-b...</td>\n",
       "      <td>We use the Moses phrase-based translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>5595</td>\n",
       "      <td>4</td>\n",
       "      <td>We perform bootstrap resampling with bounds es...</td>\n",
       "      <td>We also report 95% confidence intervals (CI) m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>5596</td>\n",
       "      <td>4</td>\n",
       "      <td>The major part of data comes from current and ...</td>\n",
       "      <td>The Europarl corpus (Koehn, 2005) is built fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>5598</td>\n",
       "      <td>4</td>\n",
       "      <td>For Italian, we use the word2vec to train word...</td>\n",
       "      <td>7 http://opennlp.apache.org 5.2.3.), we use th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>5599</td>\n",
       "      <td>3</td>\n",
       "      <td>The GATE plugin-based architecture (Cunningham...</td>\n",
       "      <td>GATE (Cunningham et al., 2002a) is an architec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>5600</td>\n",
       "      <td>3</td>\n",
       "      <td>We trained a 5-gram language model on the Xinh...</td>\n",
       "      <td>Both language models use modified Kneser-Ney s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>6101</td>\n",
       "      <td>5</td>\n",
       "      <td>We used GIZA++ (Och and Ney, 2003) along with ...</td>\n",
       "      <td>We automatically word-aligned the German part ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>6102</td>\n",
       "      <td>3</td>\n",
       "      <td>Our previous MLN-based approach for joint disa...</td>\n",
       "      <td>Joint disambiguation and clustering of mention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>6103</td>\n",
       "      <td>4</td>\n",
       "      <td>Pang et al. (2002) compare the performance of ...</td>\n",
       "      <td>Pang et al. (2002)  use machine learning metho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>6104</td>\n",
       "      <td>4</td>\n",
       "      <td>Jans et al. (2012) focused solely on the narra...</td>\n",
       "      <td>We refer to the system of Jans et al. (2012) a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>6105</td>\n",
       "      <td>4</td>\n",
       "      <td>We applied the Naive Bayes probabilistic super...</td>\n",
       "      <td>Naive Bayes and Decision Tree models were buil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>6106</td>\n",
       "      <td>4</td>\n",
       "      <td>We therefore seek to allow quick incremental u...</td>\n",
       "      <td>We build a state of the art phrase-based SMT s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>6107</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Moses software package 5 to train a...</td>\n",
       "      <td>We compare the model against the Moses phrase-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>6108</td>\n",
       "      <td>5</td>\n",
       "      <td>It has a much longer average sentence length t...</td>\n",
       "      <td>We then describe in more detail a modern Chine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>6109</td>\n",
       "      <td>4</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "      <td>The first two baselines are standard systems u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>6110</td>\n",
       "      <td>4</td>\n",
       "      <td>The corpora are tokenised and truecased using ...</td>\n",
       "      <td>The first two baselines are standard systems u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>6111</td>\n",
       "      <td>4</td>\n",
       "      <td>The dropout rate was set to 0.5, and the model...</td>\n",
       "      <td>We use the cross-entropy loss function and min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>6112</td>\n",
       "      <td>5</td>\n",
       "      <td>The dropout rate was set to 0.5, and the model...</td>\n",
       "      <td>We use AdaGrad (Duchi et al., 2011)  with the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>6113</td>\n",
       "      <td>5</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "      <td>We compare the model against the Moses phrase-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>6114</td>\n",
       "      <td>3</td>\n",
       "      <td>They had shown that the Penn Discourse TreeBan...</td>\n",
       "      <td>The Penn Discourse TreeBank (PDTB; Prasad et a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>6115</td>\n",
       "      <td>5</td>\n",
       "      <td>They had shown that the Penn Discourse TreeBan...</td>\n",
       "      <td>Pitler and Nenkova (2008) used discourse relat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>6117</td>\n",
       "      <td>5</td>\n",
       "      <td>McDonald et al. (2005) present a technique for...</td>\n",
       "      <td>The details of parsing model were presented in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>6118</td>\n",
       "      <td>3</td>\n",
       "      <td>We transformed the parse trees in OntoNotes in...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>6119</td>\n",
       "      <td>5</td>\n",
       "      <td>2 Translation Model Moses is a state-of-the-ar...</td>\n",
       "      <td>The decoder is built on top of an open-source ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>6120</td>\n",
       "      <td>4</td>\n",
       "      <td>For the OntoNotes data sets, Same speaker (Lee...</td>\n",
       "      <td>The mention detection of the Stanford corefere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>6121</td>\n",
       "      <td>4</td>\n",
       "      <td>It was one of the best parsers in the CoNLL Sh...</td>\n",
       "      <td>CTB6 is used as the Chinese data set in the Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>6122</td>\n",
       "      <td>4</td>\n",
       "      <td>We transformed the parse trees in OntoNotes in...</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>6123</td>\n",
       "      <td>5</td>\n",
       "      <td>Results are reported on the test data using F1...</td>\n",
       "      <td>We report F1 performance scored using the offi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>6125</td>\n",
       "      <td>5</td>\n",
       "      <td>The perplexity achieved by the 6-gram NN LM in...</td>\n",
       "      <td>The results of this experiment appear in Table...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>6126</td>\n",
       "      <td>3</td>\n",
       "      <td>We address the QE problem as a regression task...</td>\n",
       "      <td>We use LibSVM (Chang and Lin, 2011) as the SVM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>6127</td>\n",
       "      <td>5</td>\n",
       "      <td>Finally, it is also noticeable that the percen...</td>\n",
       "      <td>Baroni et al. (2002) also pointed out that the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>6128</td>\n",
       "      <td>4</td>\n",
       "      <td>Facts such as these are difficult to account f...</td>\n",
       "      <td>The most influential of the semantic approache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>6130</td>\n",
       "      <td>4</td>\n",
       "      <td>All these reordering models are tested using M...</td>\n",
       "      <td>We refer to that model as Moses en-es-100k , b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>6131</td>\n",
       "      <td>5</td>\n",
       "      <td>@BULLET RAE-Subj: Socher et al. (2011) propose...</td>\n",
       "      <td>We follow the formulation of vector compositio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>6132</td>\n",
       "      <td>5</td>\n",
       "      <td>We train a ridge regression model (Scikit-lear...</td>\n",
       "      <td>Our system is a linear model estimated using r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>6133</td>\n",
       "      <td>4</td>\n",
       "      <td>We follow the protocols in Collobert et al. (2...</td>\n",
       "      <td>We use SRL Collobert et al. (2011) to determin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>6134</td>\n",
       "      <td>4</td>\n",
       "      <td>This is equivalent to an SVM with the compound...</td>\n",
       "      <td>The sparsity of lexical features can also be t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>6135</td>\n",
       "      <td>4</td>\n",
       "      <td>All experiments are conducted using the Moses ...</td>\n",
       "      <td>For the comparisons of translation quality, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>6136</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the Moses toolkit (Koehn et al., 2007) ...</td>\n",
       "      <td>We refer to that model as Moses en-es-100k , b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>6137</td>\n",
       "      <td>4</td>\n",
       "      <td>All experiments were on English part of speech...</td>\n",
       "      <td>These techniques were evaluated in experiments...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>6138</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the word alignments to construct a phra...</td>\n",
       "      <td>We use the intersection of direct and reverse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>6139</td>\n",
       "      <td>4</td>\n",
       "      <td>We estimated a hierarchical MT model for the t...</td>\n",
       "      <td>We also compare with the standard phrase-based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>6140</td>\n",
       "      <td>4</td>\n",
       "      <td>A small amount of labeled data is used to map ...</td>\n",
       "      <td>Thus, inducing a number of clusters similar to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>6141</td>\n",
       "      <td>5</td>\n",
       "      <td>We use the word alignments to construct a phra...</td>\n",
       "      <td>We use the intersection of direct and reverse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>6143</td>\n",
       "      <td>3</td>\n",
       "      <td>1 For a reference\" standard\" text we used the ...</td>\n",
       "      <td>The evaluation corpus is a subset of an ungram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>6144</td>\n",
       "      <td>4</td>\n",
       "      <td>We worked with the Europarl corpus (Koehn, 200...</td>\n",
       "      <td>Translations for English words in the lexical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>6145</td>\n",
       "      <td>5</td>\n",
       "      <td>The organization from SemEval-2013 Task 2: Sen...</td>\n",
       "      <td>We participated in both subtask A and B of Sem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>6146</td>\n",
       "      <td>5</td>\n",
       "      <td>Pending a planned full evaluation using the Mo...</td>\n",
       "      <td>To test our method, we conducted two lowresour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>6147</td>\n",
       "      <td>5</td>\n",
       "      <td>The entity transition features are then used t...</td>\n",
       "      <td>We use the support vector machine (SVM) rank a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>6150</td>\n",
       "      <td>4</td>\n",
       "      <td>Our second method is based on the recurrent ne...</td>\n",
       "      <td>In all of the above tasks, we compare the neur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>6153</td>\n",
       "      <td>4</td>\n",
       "      <td>We apply the Hidden Markov Model (HMM) (Viterb...</td>\n",
       "      <td>We use Viterbi algorithm (Viterbi, 1967) to de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>6155</td>\n",
       "      <td>5</td>\n",
       "      <td>We report BLEU (Papineni et al., 2001) of tran...</td>\n",
       "      <td>We measure translation quality via the BLEU sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>6156</td>\n",
       "      <td>5</td>\n",
       "      <td>It is a standard phrase-based machine translat...</td>\n",
       "      <td>All our systems are contrasted with a standard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>6158</td>\n",
       "      <td>5</td>\n",
       "      <td>Finally, we used Moses toolkit as phrase-based...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>6160</td>\n",
       "      <td>4</td>\n",
       "      <td>Concept similarity is computed using the edge-...</td>\n",
       "      <td>The degree of similarity between two similar w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>6161</td>\n",
       "      <td>5</td>\n",
       "      <td>Finally, we used Moses toolkit as phrase-based...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>6163</td>\n",
       "      <td>5</td>\n",
       "      <td>It is a standard phrase-based machine translat...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>6164</td>\n",
       "      <td>4</td>\n",
       "      <td>2 Translation Model Moses is a state-of-the-ar...</td>\n",
       "      <td>Finally, we used Moses toolkit as phrase-based...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>6165</td>\n",
       "      <td>5</td>\n",
       "      <td>Otherwise, it is measured by WordNet similarit...</td>\n",
       "      <td>The degree of similarity between two similar w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>6166</td>\n",
       "      <td>4</td>\n",
       "      <td>We use 5-gram language models with Kneser-Ney ...</td>\n",
       "      <td>The language models are estimated using the Ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>6167</td>\n",
       "      <td>4</td>\n",
       "      <td>We adopt online learning, updating parameters ...</td>\n",
       "      <td>We use the cross-entropy loss function and min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>6169</td>\n",
       "      <td>3</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "      <td>We used SVM implementations from scikit-learn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>6170</td>\n",
       "      <td>4</td>\n",
       "      <td>We adopt online learning, updating parameters ...</td>\n",
       "      <td>We train the concept identification stage usin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>6172</td>\n",
       "      <td>5</td>\n",
       "      <td>Finally, we used Moses toolkit as phrase-based...</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>6173</td>\n",
       "      <td>3</td>\n",
       "      <td>Europarl (Koehn, 2005) is a multilingual paral...</td>\n",
       "      <td>We used the English side of the Europarl corpu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>6174</td>\n",
       "      <td>4</td>\n",
       "      <td>We leave the third-order models (Koo and Colli...</td>\n",
       "      <td>\" Koo10\" stands for the Model 1 in (Koo and Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>6175</td>\n",
       "      <td>4</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "      <td>Specifically, we used the standard Gradient Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>6176</td>\n",
       "      <td>5</td>\n",
       "      <td>For English and French a model was trained usi...</td>\n",
       "      <td>For this purpose we use the Europarl corpus (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>6177</td>\n",
       "      <td>3</td>\n",
       "      <td>Empirically we show that our model beats the s...</td>\n",
       "      <td>Very recently Rush et al. (2015) proposed a ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>6178</td>\n",
       "      <td>4</td>\n",
       "      <td>We use logistic regression with L2 regularizat...</td>\n",
       "      <td>We use the scikit implementation of Random For...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>6179</td>\n",
       "      <td>3</td>\n",
       "      <td>We use the New York Times Annotated Corpus (Sa...</td>\n",
       "      <td>Of the remaining twelve, eight came from a lar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>6180</td>\n",
       "      <td>4</td>\n",
       "      <td>We use the NMF and tf-idf implementations prov...</td>\n",
       "      <td>We used the implementation of the scikit-learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>6181</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>We built phrase-based machine translation syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>6182</td>\n",
       "      <td>3</td>\n",
       "      <td>In addition, the data was tokenized, lemmatize...</td>\n",
       "      <td>We also lemmatized all words using Stanford Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>6183</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>The baseline systems are built with the openso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>6184</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>For training the translation model and for dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>6185</td>\n",
       "      <td>5</td>\n",
       "      <td>We used the Moses toolkit (Koehn et al., 2007)...</td>\n",
       "      <td>@BULLET Decoder: Moses (Koehn et al., 2007) wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>6186</td>\n",
       "      <td>4</td>\n",
       "      <td>Feature selection was performed using chi-squa...</td>\n",
       "      <td>We train and evaluate the classifiers in a 10-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>4753</td>\n",
       "      <td>0</td>\n",
       "      <td>Barzilay and Lapata (2008)  propose an entity ...</td>\n",
       "      <td>Cheung and Penn (2010) extend the approach of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>2789</td>\n",
       "      <td>0</td>\n",
       "      <td>The differences in BLEU points are 0.14 and 0....</td>\n",
       "      <td>Statistically significant results, calculated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>2746</td>\n",
       "      <td>0</td>\n",
       "      <td>This type of non-local feature was not used by...</td>\n",
       "      <td>For example, Finkel et al. (2005) enabled the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>1534</td>\n",
       "      <td>0</td>\n",
       "      <td>This type of non-local feature was not used by...</td>\n",
       "      <td>The resulting performance of the proposed algo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>Distributional semantics is based on the idea ...</td>\n",
       "      <td>Word co-occurence statistics\" You shall know a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id_case class                                          sentence1  \\\n",
       "0         32     5  Recently, there has been a successful attempt ...   \n",
       "1         34     5  The first one is the WS-353 dataset (Finkelste...   \n",
       "2         35     5  Abstract Meaning Representation (AMR) (Banares...   \n",
       "3         36     5  We perform bootstrap resampling with bounds es...   \n",
       "4         37     5  It is used to support semantic analyses in HPS...   \n",
       "5       1201     4  We use Stanford parser (de Marneffe et al., 20...   \n",
       "6       1202     3  Rooth et al. (1999) propose an Expectation-Max...   \n",
       "7       1203     3  The Levenshtein distance (Levenshtein, 1966) b...   \n",
       "8       1204     1  We use the Moses toolkit (Koehn et al., 2007) ...   \n",
       "9         36     5  We perform bootstrap resampling with bounds es...   \n",
       "10        37     5  It is used to support semantic analyses in HPS...   \n",
       "11        38     3  It is used to support semantic analyses in the...   \n",
       "12        39     4  It is used to support semantic analyses in HPS...   \n",
       "13        40     4  It is used to support semantic analyses in the...   \n",
       "14        43     3  We build upon our previous Markov Logic based ...   \n",
       "15        44     2  Details about SVM and KFD can be found in (Tay...   \n",
       "16        46     3  We learn the parameters using a quasi-Newton p...   \n",
       "17        47     5  We use the SCFG decoder cdec (Dyer et al., 201...   \n",
       "18        48     4  This is known as the Distributional Hypothesis...   \n",
       "19        49     4  All our models , as well as the parser describ...   \n",
       "20        51     4  For strings, many such kernel functions exist ...   \n",
       "21        53     2  The estimation of the semantically Smoothed Pa...   \n",
       "22        55     2  We train with the Adam optimizer (Kingma and B...   \n",
       "23        56     3  In our experimental study, we use the freely a...   \n",
       "24        58     2  More recently, (Carpineto and Romano, 2010) sh...   \n",
       "25        59     4  They are based on the distributional hypothesi...   \n",
       "26        60     4  Word alignment is performed using GIZA++ (Och ...   \n",
       "27        61     4  Word alignment is performed using GIZA++ (Och ...   \n",
       "28        62     3  We used Mallet software (McCallum, 2002) for C...   \n",
       "29        63     3  The thesaurus consists of a hierarchy of 2,710...   \n",
       "30        64     4  The detailed discussion is provided in the lon...   \n",
       "31        66     4  The first one is the WS- 353 3 dataset (Finkel...   \n",
       "32        68     3  A framework for human error analysis and error...   \n",
       "33        69     4  MaxEnt classifier is a good example of this gr...   \n",
       "34        70     4  Among these media, blog is one of the communic...   \n",
       "35        71     3  For the gold preprocessing and all 5k settings...   \n",
       "36        73     2  For preprocessing, we used MADA (Morphological...   \n",
       "37        74     4  We trained a 5-gram language model on the Xinh...   \n",
       "38        75     4  To determine semantic type and subtype, we tra...   \n",
       "39        76     2  We use the scikit implementation of Random For...   \n",
       "40        78     3  Each term in the input text will be represente...   \n",
       "41        79     5  An algorithm, the Kuhn-Munkres method (Kuhn, 1...   \n",
       "42        81     3  We use Collapsed Gibbs Sampling (Griffiths and...   \n",
       "43        82     5  We use the Stanford dependency parser (Marneff...   \n",
       "44        83     5  Filter weights are initialized using Glorot-Be...   \n",
       "45        85     4  Automatic sentence alignment of the training d...   \n",
       "46        86     2  We use the AdaGrad optimizer (Duchi et al., 20...   \n",
       "47        87     5  Then we did word alignment using GIZA++ (Och a...   \n",
       "48        88     4  For example, DIRT (Lin and Pantel, 2001) aims ...   \n",
       "49        89     3  The annotation was performed manually using th...   \n",
       "50        91     4  System proposed by (Li et al., 2006 ), uses a ...   \n",
       "51        92     3  This corpus contains around 11,000 NPs annotat...   \n",
       "52        93     4  All modules take as input the corpus documents...   \n",
       "53        95     4  From the pioneering work of (Rapp, 1995 ), con...   \n",
       "54        96     5  Europarl (Koehn, 2005) is a multilingual paral...   \n",
       "55        99     3  TESLA (Translation Evaluation of Sentences wit...   \n",
       "56       100     4  For building the baseline SMT system, we used ...   \n",
       "57       102     4  Rhetorical Structure Theory (RST) (Mann and Th...   \n",
       "58       103     4  In addition, the fix-discount method in (Foste...   \n",
       "59       104     5  Memory-based language processing (Daelemans an...   \n",
       "60       105     4  We calculate statistical significance of perfo...   \n",
       "61       106     4  For instance, machine translation (MT) systems...   \n",
       "62       109     4  1 with 2 -regularization using AdaGrad (Duchi ...   \n",
       "63       111     3  Why does the lr model outperform Berkeley 13 T...   \n",
       "64       112     3  In the context of this paper we will be focusi...   \n",
       "65       113     5  Europarl (Koehn, 2005) is a multilingual paral...   \n",
       "66       114     4  We have used Foma, a free software tool to spe...   \n",
       "67       116     3  We used the same test set used in Li et al. (2...   \n",
       "68       117     3  The Penn Discourse Treebank (PDTB, Prasad et a...   \n",
       "69       118     4  We used the implementation of the scikit-learn...   \n",
       "70       119     3  Since the commonly used word similarity datase...   \n",
       "71       120     5  The training data of the shared task is the NU...   \n",
       "72       122     3  All system implementation was done using Pytho...   \n",
       "73       123     3  It has been shown that a diverse set of predic...   \n",
       "74       124     4  In the 2013 system, we had used SentiStrength ...   \n",
       "75       125     3  Finally, we also compare the quality of the ca...   \n",
       "76       126     4  These methods are based on the distributional ...   \n",
       "77       127     5  All annotations were done using the BRAT rapid...   \n",
       "78       128     4  Compared to WordNet (Fellbaum, 1998 ), there a...   \n",
       "79       131     5  For training, we use Adam (Kingma and Ba, 2015...   \n",
       "80       132     3  For our classifier, we use SVMs, specifically ...   \n",
       "81       133     4  The first two experiments concern the predicti...   \n",
       "82       136     2  We used only the non-ensembled left-to-right r...   \n",
       "83       137     2  We used only the non-ensembled left-to-right r...   \n",
       "84       138     4  The MSD morphological coding system was develo...   \n",
       "85       139     5  The phrase tables were generated by means of s...   \n",
       "86       140     5  Word alignment is performed using GIZA++ (Och ...   \n",
       "87       141     4  We experimented with several levels of cluster...   \n",
       "88       143     4  (Raghavan et al. (2007)) measure the benefit f...   \n",
       "89       144     4  We built a modified Kneser-Ney smoothed 5-gram...   \n",
       "90       146     4  A formal PAC-style analysis can be found in (A...   \n",
       "91       147     4  The first model we introduce is based on the r...   \n",
       "92       148     4  The SMT systems were built using the Moses too...   \n",
       "93       149     3  The classifier experiments were carried out us...   \n",
       "94       150     3  The training data of the shared task is the NU...   \n",
       "95       151     5  SALDO (Borin et al., 2013) is the largest free...   \n",
       "96       152     5  For the phrase-based SMT system, we adopted th...   \n",
       "97       153     5  However, those string-to-tree systems run slow...   \n",
       "98       154     4  The 5-gram target language model was trained u...   \n",
       "99       155     5  For all experiments, we used the Moses SMT sys...   \n",
       "100      156     4  We evaluate our method on the following data s...   \n",
       "101      157     3  We use the Moses phrase-based translation syst...   \n",
       "102      158     4  But we randomly selected 90% of the training d...   \n",
       "103      159     3  The BLEU score measures the precision of n-gra...   \n",
       "104      160     3  The training data of the shared task is the NU...   \n",
       "105      161     3  Test data was drawn from the Open American Nat...   \n",
       "106      162     5  We measure statistical significance using 95% ...   \n",
       "107      163     4  We use the English portion of the ACE 2005 rel...   \n",
       "108      164     5  This data was collected for the 2014 SemEval c...   \n",
       "109      165     5  The parsing model used for intra-sentential pa...   \n",
       "110      166     4  Latent Dirichlet Allocation (LDA) is a generat...   \n",
       "111      167     5  Distributional hypothesis theory (Harris, 1954...   \n",
       "112      168     5  Distributional hypothesis theory (Harris, 1954...   \n",
       "113      169     5  In 2009, Yefang Wang (Wang et al., 2009) used ...   \n",
       "114      170     3  We built a 5-gram language model on the Englis...   \n",
       "115      171     3  The remaining three models are all Naive Bayes...   \n",
       "116      172     4  We apply bootstrapping (Kozareva et al., 2008)...   \n",
       "117      173     5  These methods are based on the distributional ...   \n",
       "118      174     5  These results verify the benefit of using LTAG...   \n",
       "119      175     5  RG-65: (Rubenstein and Goodenough, 1965) has 6...   \n",
       "120      176     5  The significance tests were performed using th...   \n",
       "121      177     5  The default Phrasal search algorithm is cube p...   \n",
       "122      178     5  In-domain data is mainly used to solve the pro...   \n",
       "123      179     4  All experiments were carried out using the ope...   \n",
       "124      180     4  First, we apply heuristics to determine number...   \n",
       "125      181     4  Rank SVM (Joachims, 2002) is a method based on...   \n",
       "126      182     3  A more detailed description of the task can be...   \n",
       "127      184     5  A Tree Kernel function is a convolution kernel...   \n",
       "128      185     4  We build upon our previous approach for joint ...   \n",
       "129      186     4  ROUGE-2 metric (Lin, 2004) is used for the eva...   \n",
       "130      187     5  We used Mallet software (McCallum, 2002) for C...   \n",
       "131      188     3  We exploit a transition-based framework with g...   \n",
       "132      189     4  The annotation was performed using the BRAT 2 ...   \n",
       "133      190     5  For building the word alignment models we use ...   \n",
       "134      191     5  The reliability of the annotation was evaluate...   \n",
       "135      192     5  It therefore follows the distributional hypoth...   \n",
       "136      193     4  This dataset is composed of 35 triplets of sen...   \n",
       "137      194     5  The significance tests were performed using th...   \n",
       "138      195     5  We use the standard alignment tool Giza++ (Och...   \n",
       "139      196     5  We use the standard alignment tool Giza++ (Och...   \n",
       "140      198     3  The kernels are combined using Gaussian proces...   \n",
       "141      199     5  To overcome this, Agirre et al. (2009) used Ma...   \n",
       "142      200     5  We measure statistical significance using 95% ...   \n",
       "143      701     4  We use the Stanford dependency parser (Chen an...   \n",
       "144      702     3  Next, a tweet was tokenized and fed into MADAM...   \n",
       "145      703     4  (Yarowsky ,(1995)) has proposed a bootstrappin...   \n",
       "146      704     5  We also list the previous state-of-the-art per...   \n",
       "147      705     4  We used Adam (Kingma and Ba, 2014) with a lear...   \n",
       "148      706     2  Distributional semantics is based on the idea ...   \n",
       "149      707     3  In order to estimate the basic lexical similar...   \n",
       "150      709     5  We used the implementation of the scikit-learn...   \n",
       "151      710     3  The translation model was trained by GIZA++ (O...   \n",
       "152      711     5  This is a generalization of the operator Id in...   \n",
       "153      712     4  We used standard classifiers available in scik...   \n",
       "154      714     5  We used the implementation of the scikit-learn...   \n",
       "155      715     5  For the phrase-based SMT system, we adopted th...   \n",
       "156      716     3  4 Word alignments are created by aligning the ...   \n",
       "157      717     4  The perplexity achieved by the 6- gram NN LM i...   \n",
       "158      718     5  We used standard classifiers available in scik...   \n",
       "159      719     5  Statistical machine translation is typically p...   \n",
       "160      720     3  WordNet (Miller et al., 1990) is an on-line hi...   \n",
       "161      721     4  We use the scikit implementation of Random For...   \n",
       "162      724     4  We lemmatise the head of each constituent with...   \n",
       "163      725     4  Our text processing uses the Natural Language ...   \n",
       "164      726     5  They used the Web-based annotation tool brat (...   \n",
       "165      729     3  Our system participated in SemEval-2013 Task 2...   \n",
       "166      730     5  The English text was tokenized using the word ...   \n",
       "167      731     4  The webpages were parsed using the Stanford Co...   \n",
       "168      732     3  Statistical machine translation is typically p...   \n",
       "169      733     3  The English side was tokenized using the Moses...   \n",
       "170      734     5  We trained an English 5-gram language model us...   \n",
       "171      735     5  All of the text data from Reddit was tokenized...   \n",
       "172      736     4  Our machine translation systems are trained us...   \n",
       "173      737     5  For the phrase-based SMT system, we adopted th...   \n",
       "174      738     5  We build upon our previous approach for joint ...   \n",
       "175      739     5  We used Adam (Kingma and Ba, 2014) with a lear...   \n",
       "176      740     5  For all experiments, we used the Moses SMT sys...   \n",
       "177      741     4  The SMT systems were built using the Moses too...   \n",
       "178      742     5  For training the translation model and for dec...   \n",
       "179      743     5  For training the translation model and for dec...   \n",
       "180      744     5  For the phrase-based SMT system, we adopted th...   \n",
       "181      745     5  We develop translation models using the phrase...   \n",
       "182      746     4  We used TnT (Brants, 2000 ), trained on the Ne...   \n",
       "183      747     3  The webpages were parsed using the Stanford Co...   \n",
       "184      748     5  We develop translation models using the phrase...   \n",
       "185      750     3  The term frequency count is normalized with th...   \n",
       "186      751     4  We assessed the statistical significance of di...   \n",
       "187      752     5  A framework for human error analysis and error...   \n",
       "188      753     5  For example, Chang et al. (2009) found that th...   \n",
       "189      754     4  On the Chinese side, we used the morphological...   \n",
       "190      755     5  conducted using the Moses phrase-based decoder...   \n",
       "191      756     3  conducted using the Moses phrase-based decoder...   \n",
       "192      757     4  We conducted baseline experiments for phraseba...   \n",
       "193      759     3  Then the processed data was performed for toke...   \n",
       "194      760     5  We applied bootstrap resampling (Koehn, 2004) ...   \n",
       "195      761     5  This system uses the attentional encoder-decod...   \n",
       "196      762     3  The language model is a 5-gram KenLM (Heafield...   \n",
       "197      763     5  Weighted Finite State Transducers (FSTs) used ...   \n",
       "198      764     5  Weighted Finite State Transducers (FSTs) used ...   \n",
       "199      765     3  Then the processed data was performed for toke...   \n",
       "200      766     4  It is a modification of the model proposed by ...   \n",
       "201      767     5  We use Scikit-learn (Pedregosa et al., 2011 ),...   \n",
       "202      768     3  We use the Universal POS Tagset (UPOS) of Petr...   \n",
       "203      768     3  We use the Universal POS Tagset (UPOS) of Petr...   \n",
       "204      769     3  The CRF is trained using decisions from the fo...   \n",
       "205      770     5  For language modeling, we trained a separate 5...   \n",
       "206      771     5  with the training script of the Moses toolkit ...   \n",
       "207      772     5  The word alignment was trained using GIZA++ (O...   \n",
       "208      773     4  We use linear SVMs from LIBLINEAR and SVMs wit...   \n",
       "209      774     5  We specify the hierarchical aligner in terms o...   \n",
       "210      775     4  We use Stanford parser (de Marneffe et al., 20...   \n",
       "211      776     4  In this work, we use the Stanford neural depen...   \n",
       "212      778     5  We use Stanford parser (de Marneffe et al., 20...   \n",
       "213      779     5  The learning algorithm used in our coreference...   \n",
       "214      780     5  We use Stanford parser (de Marneffe et al., 20...   \n",
       "215      781     5  Distributional semantics (see Cohen and Widdow...   \n",
       "216      782     4  Their work is part of the state-of-the-art Ara...   \n",
       "217      783     5  We use Stanford parser (de Marneffe et al., 20...   \n",
       "218      784     4  Phrasal follows the log-linear approach to phr...   \n",
       "219      785     5  Training data are based on a concatenation of ...   \n",
       "220      786     3  In-domain SMT: we used the parallel corpus (Se...   \n",
       "221      787     5  5-gram language models of Turkish and English ...   \n",
       "222      788     5  We used the relation classification dataset of...   \n",
       "223      789     4  We then run word alignment with GIZA++ (Och an...   \n",
       "224      790     3  In-domain SMT: we used the parallel corpus (Se...   \n",
       "225      791     5  The significance tests were performed using th...   \n",
       "226      792     5  All experiments were carried out using the ope...   \n",
       "227      793     5  These sentences have then be fed into an effic...   \n",
       "228      794     4  The language model is a 5-gram KenLM (Heafield...   \n",
       "229      795     4  Cohen et al. (2012)  present a spectral algori...   \n",
       "230      796     5  Distributional hypothesis theory (Harris, 1954...   \n",
       "231      797     5  Distributional hypothesis theory (Harris, 1954...   \n",
       "232      799     3  We use the Stanford dependency parser (Marneff...   \n",
       "233      800     3  We also replicated the experiment of Holmqvist...   \n",
       "234     1301     4  We develop translation models using the phrase...   \n",
       "235     1302     5  POS tagging was performed with TreeTagger (Sch...   \n",
       "236     1303     5  We used the MaltParser (Nivre et al., 2007) fo...   \n",
       "237     1304     4  Classification uses the scikit-learn Python pa...   \n",
       "238     1305     5  We use the Stanford dependency parser (Marneff...   \n",
       "239     1306     5  We used Adam (Kingma and Ba, 2014) with a lear...   \n",
       "240     1307     4  Word alignment is performed using GIZA++ (Och ...   \n",
       "241     1308     4  The test set was tagged with the French TreeTa...   \n",
       "242     1309     4  For the phrase-based SMT system, we adopted th...   \n",
       "243     1310     5  POS tagging was performed with TreeTagger (Sch...   \n",
       "244     1311     5  For building the word alignment models we use ...   \n",
       "245     1313     3  The Polish data is taken from the EUROPARL cor...   \n",
       "246     1314     5  The German-to-English corpus is Europarl v7 (K...   \n",
       "247     1315     5  Distributional models of meaning follow the di...   \n",
       "248     1316     4  conducted using the Moses phrase-based decoder...   \n",
       "249     1317     3  Europarl 2 (Koehn, 2005 ): it is a corpus of p...   \n",
       "250     1318     4  We conducted statistical significance tests fo...   \n",
       "251     1320     5  The English side was tokenized using the Moses...   \n",
       "252     1321     5  We develop translation models using the phrase...   \n",
       "253     1322     5  The baseline will be created by the Moses SMT ...   \n",
       "254     1323     4  The resulting matrix is weighted using pointwi...   \n",
       "255     1324     3  The SMT systems were built using the Moses too...   \n",
       "256     1325     4  The morpho-syntactic tagging has been made wit...   \n",
       "257     1326     4  For the determination of POS tags we use the S...   \n",
       "258     1327     5  The Polish data is taken from the EUROPARL cor...   \n",
       "259     1328     5  All annotations were done using the brat rapid...   \n",
       "260     1329     4  The Spanish-English (S2E) training corpus was ...   \n",
       "261     1330     4  We used TnT (Brants, 2000 ), trained on the Ne...   \n",
       "262     1331     4  For all experiments, we used the Moses SMT sys...   \n",
       "263     1332     5  Two baseNP data sets have been put forward by ...   \n",
       "264     1333     5  Both of our systems were based on the Moses de...   \n",
       "265     1334     5  For the phrase-based SMT system, we adopted th...   \n",
       "266     1335     5  Corpus-based meaning representations rely on t...   \n",
       "267     1336     4  Recently, Naim et al. (2014)  proposed an unsu...   \n",
       "268     1337     5  A distributional similarity model is construct...   \n",
       "269     1338     4  This paper describes the details of our system...   \n",
       "270     1339     3  We used the phrase-based model Moses (Koehn et...   \n",
       "271     1340     5  Combinatory Categorial grammar (CCG) is a ling...   \n",
       "272     1341     4  We used the Random Forests implementation of s...   \n",
       "273     1342     4  The proposed model extends the LDA framework o...   \n",
       "274     1343     4  We used the training section of the dataset fr...   \n",
       "275     1344     5  5-gram language models of Turkish and English ...   \n",
       "276     1345     3  The corpora are first tokenized and lowercased...   \n",
       "277     1346     5  The statistical significance test is also carr...   \n",
       "278     1348     3  The realisation ranking component is an SVM ra...   \n",
       "279     1351     5  The statistical significance test is also carr...   \n",
       "280     1352     3  We use the NLTK toolkit (Loper and Bird, 2002)...   \n",
       "281     1357     2  We mark the source tokens to which each target...   \n",
       "282     1358     3  We rely on the hybrid aligned lexical semantic...   \n",
       "283     1359     3  To help improve the information extraction too...   \n",
       "284     1362     3  The module of coreference resolution included ...   \n",
       "285     1363     5  We run our experiments on Europarl (Koehn, 200...   \n",
       "286     1364     3  We used the scikit-learn (Pedregosa et al., 20...   \n",
       "287     1365     3  We used the scikit-learn (Pedregosa et al., 20...   \n",
       "288     1366     5  Automatic Multi-Document Summarization (MDS) a...   \n",
       "289     1367     4  Additionally, we train phrase-based machine tr...   \n",
       "290     1368     4  We used the scikit-learn (Pedregosa et al., 20...   \n",
       "291     1369     4  Additionally, we train phrase-based machine tr...   \n",
       "292     1370     4  We train classifiers for each of the above fea...   \n",
       "293     1371     4  In addition, the corpus was lemmatised using t...   \n",
       "294     1372     4  They are based on Distributional Hypothesis wh...   \n",
       "295     1373     3  We used the same test set used in (Li et al. (...   \n",
       "296     1374     4  We develop translation models using the phrase...   \n",
       "297     1375     3  We used non-local features based on Finkel et ...   \n",
       "298     1376     5  For building the baseline SMT system, we used ...   \n",
       "299     1377     3  For translation, we use Moses (Koehn et al., 2...   \n",
       "300     1378     4  To recognize explicit connectives, we construc...   \n",
       "301     1379     3  To construct language models and measure perpl...   \n",
       "302     1380     4  We use Gibbs sampling to estimate the distribu...   \n",
       "303     1381     5  Arabizi is not a letter-based transliteration ...   \n",
       "304     1382     4  To construct language models and measure perpl...   \n",
       "305     1384     4  We build upon our previous Markov Logic based ...   \n",
       "306     1385     4  We use the feedforward neural probabilistic la...   \n",
       "307     1386     5  The SMT systems were built using the Moses too...   \n",
       "308     1387     5  For building the baseline SMT system, we used ...   \n",
       "309     1388     4  In-domain SMT: we used the parallel corpus (Se...   \n",
       "310     1389     3  Following resource collection and construction...   \n",
       "311     1390     4  One semiautomatic approach to evaluation is RO...   \n",
       "312     1391     4  We use the Web 1T 5-gram corpus (Brants and Fr...   \n",
       "313     1392     3  The SemEval-2015 Aspect Based Sentiment Analys...   \n",
       "314     1393     4  The 2009 Bio NLP shared task (Kim et al., 2009...   \n",
       "315     1394     4  The constituent context model (CCM) for induci...   \n",
       "316     1395     4  The CCM is a generative model for the unsuperv...   \n",
       "317     1397     3  We use the implementation provided by Tai et a...   \n",
       "318     1398     3  Europarl (Koehn, 2005) is a multilingual paral...   \n",
       "319     1399     5  The Stanford dependency parser (De Marneffe et...   \n",
       "320     1400     5  The Stanford dependency parser (De Marneffe et...   \n",
       "321     1901     4  The major part of data comes from current and ...   \n",
       "322     1903     2  We compare the proposed model to our implement...   \n",
       "323     1904     4  It is a phrase-based system built using the Mo...   \n",
       "324     1905     4  In this section, we first discuss the hybrid t...   \n",
       "325     1906     4  The Europarl corpus (Koehn, 2005) is built fro...   \n",
       "326     1907     4  The Europarl corpus (Koehn, 2005) is built fro...   \n",
       "327     1908     5  We use Adam (Kingma and Ba, 2015) for optimisa...   \n",
       "328     1909     4  We use the open-source Moses toolkit (Koehn et...   \n",
       "329     1910     4  We used the mkcls tool in GIZA++ (Och and Ney,...   \n",
       "330     1911     3  We then use the phrase extraction utility in t...   \n",
       "331     1912     3  Phrase pairs are extracted from IBM4 alignment...   \n",
       "332     1914     5  Specifically, we build off the Bayesian block ...   \n",
       "333     1915     4  Like (Cho & Chai (2000)) , our analysis also p...   \n",
       "334     1917     5  We propose a relatively simple and nuanced uns...   \n",
       "335     1918     4  Our work is also related to (Bunescu and Moone...   \n",
       "336     1919     5  Motivated by previous work, we include a frequ...   \n",
       "337     1920     5  The highest performance levels were achieved u...   \n",
       "338     1921     4  In this rest of this paper, we discuss related...   \n",
       "339     1922     5  In principle, classifiers trained on PDTB data...   \n",
       "340     1926     5  In the next section we briefly review modeling...   \n",
       "341     1927     4  This result is statistically significant at p ...   \n",
       "342     1928     4  Starting with TextRank (Mihalcea and Tarau, 20...   \n",
       "343     1929     5  Because of our experience with the Weka packag...   \n",
       "344     1930     4  Dropout (Srivastava et al., 2014) is implement...   \n",
       "345     1931     3  In-domain SMT: we used the parallel corpus (Se...   \n",
       "346     1932     5  As mentioned in Section 3, we obtained depende...   \n",
       "347     1933     5  We tokenize and truecase all of the corpora us...   \n",
       "348     1935     5  To obtain these we use the Stanford dependency...   \n",
       "349     1936     5  The NMT models are trained using Adam optimize...   \n",
       "350     1937     4  The improved alignments gave a gain of Table 8...   \n",
       "351     1938     5  The statistical significance tests using 95% c...   \n",
       "352     1939     5  To obtain these we use the Stanford dependency...   \n",
       "353     1941     4  We use the AdaGrad algorithm (Duchi et al., 20...   \n",
       "354     1942     5  The other is from (Jeffrey Pennington et al. (...   \n",
       "355     1943     4  We use the AdaGrad optimizer (Duchi et al., 20...   \n",
       "356     1944     4  We experiment with the phrase-based statistica...   \n",
       "357     1945     3  In-domain SMT: we used the parallel corpus (Se...   \n",
       "358     1946     4  We follow the definition in Cohen et al. (2012...   \n",
       "359     1947     3  The language model is a 5-gram KenLM (Heafield...   \n",
       "360     1949     5  As mentioned in Section 3, we obtained depende...   \n",
       "361     1950     4  Also, we evaluate on the RTE part of the SICK ...   \n",
       "362     1951     3  Our model has a\" Siamese\" structure (Bromley e...   \n",
       "363     1953     5  The dialogue act labelling of the corpus follo...   \n",
       "364     1954     4  We also obtain the dependency parse of the sen...   \n",
       "365     1955     4  The resulting matrix is weighted using pointwi...   \n",
       "366     1956     5  We extract structured facts using two methods:...   \n",
       "367     1957     4  Dropout (Srivastava et al., 2014) is implement...   \n",
       "368     1958     3  The population distribution was estimated by t...   \n",
       "369     1959     5  The statistical significance test is also carr...   \n",
       "370     1960     3  The bootstrap sampling method provides a way f...   \n",
       "371     1961     3  For medical we use the biomedical data from EM...   \n",
       "372     1962     5  The MT experiments were carried out using the ...   \n",
       "373     1963     4  For the theory of Cho & Chai (2000) to be comp...   \n",
       "374     1964     5  3 With these trees fixed, the partial derivati...   \n",
       "375     1965     5  Many researchers have considered generating pa...   \n",
       "376     1966     5  We used Weka (Hall et al., 2009) for all our c...   \n",
       "377     1967     3  Bilingual Corpora The corpus used in the follo...   \n",
       "378     1968     4  First, we used the Moses toolkit (Koehn et al....   \n",
       "379     1969     3  Another parallel corpus is the JRC-Acquis Mult...   \n",
       "380     1970     3  The parallel corpus is wordaligned using GIZA+...   \n",
       "381     1972     2  We thus cast MSC as a semantic sentence classi...   \n",
       "382     1973     5  All these features are inherited from Moses (K...   \n",
       "383     1974     3  For this purpose we use the Europarl corpus (K...   \n",
       "384     1975     3  The texts were first automatically segmented a...   \n",
       "385     1976     5  Secondly, Holmqvist et al. (2012) reordered so...   \n",
       "386     1977     5  For language modeling, we use the English Giga...   \n",
       "387     1978     5  For our LDA implementations, we use MALLET (Mc...   \n",
       "388     1979     5  We use the standard Stanford-style set of depe...   \n",
       "389     1980     4  Next we evaluate how well the complexity measu...   \n",
       "390     1981     3  Syntax in EPEC is annotated following the depe...   \n",
       "391     1983     5  We used the scikit-learn (Pedregosa et al., 20...   \n",
       "392     1984     5  We used the scikit-learn toolkit to train our ...   \n",
       "393     1985     5  The training set is used to train the phrase-b...   \n",
       "394     1986     5  These features were obtained using the Stanfor...   \n",
       "395     1987     3  We built a 5-gram language model on the Englis...   \n",
       "396     1988     3  In the other side, the French corpus is part-o...   \n",
       "397     1989     3  The COMLEX syntax dictionary (Grishman et al.,...   \n",
       "398     1990     5  We lemmatise the head of each constituent with...   \n",
       "399     1991     5          with the KenLM toolkit (Heafield, 2011 ).   \n",
       "400     1992     5  We briefly review the HMM based word alignment...   \n",
       "401     1993     4  We use the Stanford parser with Stanford depen...   \n",
       "402     1995     5  We use the Stanford parser with Stanford depen...   \n",
       "403     1996     4  All the summaries are evaluated using ROUGE (L...   \n",
       "404     1997     4  The reported confidence intervals were estimat...   \n",
       "405     1999     4  We implemented CharWNN using the Theano librar...   \n",
       "406     2000     5  For the linear logistic regression implementat...   \n",
       "407     2501     4  The tagger we use is TnT (Brants, 2000) , a hi...   \n",
       "408     2502     3  We have used the implementation described in (...   \n",
       "409     2503     3  We built a 5-gram language model on the Englis...   \n",
       "410     2504     3  DELPH-IN Minimal Recursion Semantics (DM) As p...   \n",
       "411     2505     5  Like the CoNLL-2006 shared task, the 2007 shar...   \n",
       "412     2506     4  Like (Cho & Chai 2000 ), our analysis also pro...   \n",
       "413     2507     4  The phrase table is extracted from a bilingual...   \n",
       "414     2508     4  Google Web 1T (Brants and Franz, 2006) has bee...   \n",
       "415     2509     5  We first use a dependency parser (de Marneffe ...   \n",
       "416     2510     5  We first use a dependency parser (de Marneffe ...   \n",
       "417     2511     5  These analyses provide an alternative but theo...   \n",
       "418     2512     5  We experiment with the phrase-based statistica...   \n",
       "419     2513     3  For our corpus, we randomly selected documents...   \n",
       "420     2514     4  They used the Web-based annotation tool brat (...   \n",
       "421     2515     4  All corpora were taken from the CHILDES databa...   \n",
       "422     2516     5  The system generated tweets were evaluated usi...   \n",
       "423     2517     5  The TOEFL synonym selection task is to select ...   \n",
       "424     2518     4  We calculate our features using the KenLM tool...   \n",
       "425     2519     5  We use the scikit implementation of Random For...   \n",
       "426     2521     3  For this purpose we use the Europarl corpus (K...   \n",
       "427     2522     5  The statistical significance tests using 95% c...   \n",
       "428     2523     5  We then run word alignment with GIZA++ (Och an...   \n",
       "429     2524     4  We built a 5-gram language model on the Englis...   \n",
       "430     2525     5  We used GIZA++ (Och and Ney, 2003) to align th...   \n",
       "431     2526     5  The phrase tables were generated by means of s...   \n",
       "432     2527     5  Zhu et al. (2013) applied Kalman filter model ...   \n",
       "433     2528     4  ( Koo et al. 2008)  have proposed to use word ...   \n",
       "434     2529     4  More recently, (Pasha et al., 2014) created MA...   \n",
       "435     2530     5  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "436     2531     4  @BULLET Naive Bayes(NB): We use Binomial varia...   \n",
       "437     2532     5  We use AdaGrad (Duchi et al., 2011)  with the ...   \n",
       "438     2533     5  The MT experiments were carried out using the ...   \n",
       "439     2534     5  We use the AdaGrad method (Duchi et al., 2011)...   \n",
       "440     2535     4  Among the existing sense-tagged corpora, the S...   \n",
       "441     2536     5  We use the Liblinear Support Vector Machine (S...   \n",
       "442     2537     5  We build a state of the art phrase-based SMT s...   \n",
       "443     2538     5  We built a source-to-target PB-SMT model from ...   \n",
       "444     2539     5  The dictionaries are automatically generated v...   \n",
       "445     2540     3  There are two main approaches to processing no...   \n",
       "446     2541     4  We conducted baseline experiments for phraseba...   \n",
       "447     2542     4  The task of identifying mentions to medical co...   \n",
       "448     2543     5  We built a source-to-target PB-SMT model from ...   \n",
       "449     2545     5  We also use MADA+TOKAN (Habash et al., 2009) t...   \n",
       "450     2546     5  Both training and testing data consist of PubM...   \n",
       "451     2547     4  We conducted baseline experiments for phraseba...   \n",
       "452     2548     5  To extract our part-of-speech (POS) features, ...   \n",
       "453     2549     5  The corpora are tokenised and truecased using ...   \n",
       "454     2550     5  We trained a model using Moses toolkit (Koehn ...   \n",
       "455     2551     4  For seed and test paradigms we used verbal inf...   \n",
       "456     2552     5  Then we did word alignment using GIZA++ (Och a...   \n",
       "457     2553     4  ( Pang et al. 2002) have reported the effectiv...   \n",
       "458     2555     5  We train the concept identification stage usin...   \n",
       "459     2556     3  For the contextual check we use the Google Web...   \n",
       "460     2557     5  We built a source-to-target PB-SMT model from ...   \n",
       "461     2558     4  @BULLET Naive Bayes(NB): We use Binomial varia...   \n",
       "462     2559     4  We built a source-to-target PB-SMT model from ...   \n",
       "463     2560     5  An unpruned, modified Kneser-Ney-smoothed 4-gr...   \n",
       "464     2561     4  For native data, several teams make use of the...   \n",
       "465     2562     4  We measure statistical significance using 95% ...   \n",
       "466     2563     4  1 The models are constructed using C4.5 decisi...   \n",
       "467     2564     5  It therefore follows the distributional hypoth...   \n",
       "468     2565     4  Consider the two examples below, drawn from th...   \n",
       "469     2566     5  BLE is based on the distributional hypothesis ...   \n",
       "470     2567     5  We use the BLEU score as primary criterion whi...   \n",
       "471     2568     4  The relation prediction task of Science IE is ...   \n",
       "472     2569     3  Machine translation system settings: We used a...   \n",
       "473     2570     5  We use the MRS analyses that are produced by t...   \n",
       "474     2571     5  In each case, the improvement of EBMT TM + SMT...   \n",
       "475     2572     4  To capture typical part-of bridging (see Examp...   \n",
       "476     2574     5  The surfacesyntactic representation ? p was a ...   \n",
       "477     2575     5  In our work, like Hernault et al. (2010) , we ...   \n",
       "478     2576     5  We use the data that were recorded and preproc...   \n",
       "479     2577     5  We trained non-projective dependency parsers f...   \n",
       "480     2578     4  Our LP constraints based on the new type marke...   \n",
       "481     2579     2  The WSJ grammar covers the UPenn Wall Street J...   \n",
       "482     2580     5  Statistically significant results, calculated ...   \n",
       "483     2581     4  In testing, we used minimum Bayes risk decodin...   \n",
       "484     2582     3  selects the translation with minimum Bayes ris...   \n",
       "485     2583     4  We use a prototype-based selectional preferenc...   \n",
       "486     2584     4  We used 10-fold cross-validation, set the conf...   \n",
       "487     2585     3  The Brown Corpus tagged with WordNet senses (M...   \n",
       "488     2586     5  As an implementation, we use SVM light (Joachi...   \n",
       "489     2587     4  For translation tables , the Moses system (Koe...   \n",
       "490     2588     5  The particle filter of Canini et al. (2009) re...   \n",
       "491     2589     4  We describe an approximation to the BLEU score...   \n",
       "492     2590     5  We applied the Naive Bayes probabilistic super...   \n",
       "493     2593     4  We minimize the cross entropy loss using gradi...   \n",
       "494     2594     5  Statistical significance in BLEU score differe...   \n",
       "495     2595     5  We measure statistical significance using 95% ...   \n",
       "496     2596     3  3 As verbs, we take all tags that map to V in ...   \n",
       "497     2597     5  We use the MRS analyses that are produced by t...   \n",
       "498     2598     5  We measure statistical significance using 95% ...   \n",
       "499     2599     5  On Semantic Role Labeling Gildea and Jurafsky ...   \n",
       "500     2600     4  We apply the stochastic gradient descent algor...   \n",
       "501     3101     4  We used the Linear SVM implementation (with de...   \n",
       "502     3102     5  We test the statistical significance of differ...   \n",
       "503     3103     5  In particular, the parser implements the arc-s...   \n",
       "504     3104     5  We use Adadelta (Zeiler, 2012) to update the p...   \n",
       "505     3105     4  We used the English side of the Europarl corpu...   \n",
       "506     3106     3  CCG is a lexicalized theory of grammar (Steedm...   \n",
       "507     3107     5  The results for TESLA-M and TESLA-F have previ...   \n",
       "508     3108     3  The semantic representation is Minimal Recursi...   \n",
       "509     3109     4  GermaNet (GN) is the German counterpart to WN ...   \n",
       "510     3110     3  (Baroni et al 2002)  report that 47% of the vo...   \n",
       "511     3111     4  Both language models use modified Kneser-Ney s...   \n",
       "512     3112     5  We used MALLET (McCallum, 2002) for this exper...   \n",
       "513     3114     5  We calculated significance using paired bootst...   \n",
       "514     3115     5  Additionally, we train phrase-based machine tr...   \n",
       "515     3116     5  For building the baseline SMT system, we used ...   \n",
       "516     3117     3  This results in the semantic triple shot(man,b...   \n",
       "517     3119     4  We used standard classifiers available in scik...   \n",
       "518     3120     3  Rhetorical Structure Theory (Mann and Thompson...   \n",
       "519     3121     4  Both language models use modified Kneser-Ney s...   \n",
       "520     3123     5  We train the concept identification stage usin...   \n",
       "521     3124     5  For training the translation model and for dec...   \n",
       "522     3125     5  We use ROUGE score as our evaluation metric (L...   \n",
       "523     3126     5  For building the baseline SMT system, we used ...   \n",
       "524     3127     4  We use the Europarl parallel corpus (Koehn, 20...   \n",
       "525     3128     3  We use Ridge Regression (RR) with l2-norm regu...   \n",
       "526     3129     4  All data used in our experiments are sentence-...   \n",
       "527     3130     4  Since the phrase table contains lemmas, the Wi...   \n",
       "528     3131     5  For this purpose we used the logistic regressi...   \n",
       "529     3132     5  All our systems are contrasted with a standard...   \n",
       "530     3133     4  We test our metrics in the setting of the WMT ...   \n",
       "531     3134     4  These classifiers have been used in related wo...   \n",
       "532     3135     4  One is from (Turian et al, 2010) , the dimensi...   \n",
       "533     3136     5  Word alignment using GIZA++ toolkit (Och and N...   \n",
       "534     3137     5  We found that using AdaGrad (Duchi et al., 201...   \n",
       "535     3138     5  The Penn Discourse Treebank (PDTB, Prasad et a...   \n",
       "536     3139     5  The system was trained on the English and Dani...   \n",
       "537     3140     5  For example, Turian et al. (2010) compared Bro...   \n",
       "538     3141     5  We use the Moses software package 5 to train a...   \n",
       "539     3142     3  Past experiences on this system have shown tha...   \n",
       "540     3143     5  We trained a number of French-English SMT syst...   \n",
       "541     3144     5  @BULLET Zhang2015: Zhang et al. (2015) propose...   \n",
       "542     3145     4  All data used in our experiments are sentence-...   \n",
       "543     3146     5  The major part of data comes from current and ...   \n",
       "544     3147     5  We expect this restriction is more consistent ...   \n",
       "545     3148     4  Additionally, we train phrase-based machine tr...   \n",
       "546     3149     5  Additionally, we train phrase-based machine tr...   \n",
       "547     3150     4  The decoder searches for the best translation ...   \n",
       "548     3151     5  The training set is used to train the phrase-b...   \n",
       "549     3152     5  The usages from the ukWaC are tokenised and le...   \n",
       "550     3153     4  To quantify the redundancy of structures, we p...   \n",
       "551     3154     5  Baseline word alignments were obtained by runn...   \n",
       "552     3156     3  The results displayed in Table 3 are obtained ...   \n",
       "553     3157     5  We trained a number of French-English SMT syst...   \n",
       "554     3158     3  The performance of our algorithm is compared w...   \n",
       "555     3159     3  We use the scikit implementation of SVM (Pedre...   \n",
       "556     3160     4  For example, Turian et al. (2010) showed that ...   \n",
       "557     3161     3  EASE uses NLTK (Bird et al., 2009) for POS tag...   \n",
       "558     3163     5  All our translation systems are based on Moses...   \n",
       "559     3164     5  The training set is used to train the phrase-b...   \n",
       "560     3165     5  Additionally, we train phrase-based machine tr...   \n",
       "561     3166     4  We thank Gbor Recski (HAS Research Institute f...   \n",
       "562     3167     4  It has been shown in previous work on relation...   \n",
       "563     3168     4  These algorithms were used to participate in t...   \n",
       "564     3169     5  Phrase-Based SMT system: a standard non factor...   \n",
       "565     3171     4  For the theory of Cho & Chai (2000) to be comp...   \n",
       "566     3172     5  All linguistic annotations needed for features...   \n",
       "567     3173     3  Algorithm 5 shows a Passive-Aggressive algorit...   \n",
       "568     3175     4  The task is part of the Semantic Evaluation 20...   \n",
       "569     3176     4  We used the same annotation guidelines as Zaid...   \n",
       "570     3177     4  The Stanford parser 1 (Marneffe et al., 2006) ...   \n",
       "571     3178     5  1 After tokenization , we lemmatize and stem t...   \n",
       "572     3179     5  Machine translation system settings: We used a...   \n",
       "573     3180     4  The phrase table is extracted from a bilingual...   \n",
       "574     3181     3  This architecture is very similar to the frame...   \n",
       "575     3182     5  Besides using SentiStrength, we use the lexico...   \n",
       "576     3183     4  Run1: We firstly use the Stanford CoreNLP tool...   \n",
       "577     3184     4  By setting (n inw and enw(n)=1 for all nodes, ...   \n",
       "578     3185     5  2 We tested the difference in performance for ...   \n",
       "579     3186     4  Run1: We firstly use the Stanford CoreNLP tool...   \n",
       "580     3187     5  All linguistic annotations needed for features...   \n",
       "581     3188     4  In (Erkan and Radev, 2004 ), the concept of gr...   \n",
       "582     3189     5  Significance was tested using a paired bootstr...   \n",
       "583     3194     4  For seed and test paradigms we used verbal inf...   \n",
       "584     3195     5  We used KenLM (Heafield, 2011) to create 3-gra...   \n",
       "585     3196     5  We built a 5-gram language model on the Englis...   \n",
       "586     3197     4  An estimate of the likelihood of a verb taking...   \n",
       "587     3198     4  We use the Web 1T 5-gram corpus (Brants and Fr...   \n",
       "588     3199     5  Machine translation system settings: We used a...   \n",
       "589     3200     5  Machine translation system settings: We used a...   \n",
       "590     3701     4  A chunk is a minimal , non-recursive structure...   \n",
       "591     3702     5  MC-30: A subset of RG-65 dataset with 30 word ...   \n",
       "592     3703     5  We obtained news-peg judgments using the Brat ...   \n",
       "593     3704     4  We transformed the parse trees in OntoNotes in...   \n",
       "594     3705     4  Gradient clipping heuristic to prevent the\" ex...   \n",
       "595     3706     5  We obtained news-peg judgments using the Brat ...   \n",
       "596     3707     5  We then describe in more detail a modern Chine...   \n",
       "597     3708     5  In order to assess statistical significance of...   \n",
       "598     3710     5  Statistical significance in BLEU score differe...   \n",
       "599     3711     4  The article system builds on the elements of t...   \n",
       "600     3712     5  For calculating the required frequencies, we u...   \n",
       "601     3713     3  Phrase pairs were extracted from symmetrized w...   \n",
       "602     3714     4  We ran all of our experiments in Weka (Hall et...   \n",
       "603     3715     4  Both corpora were extracted from the open para...   \n",
       "604     3716     4  We ran all of our experiments in Weka (Hall et...   \n",
       "605     3717     4  The Spanish-English (S2E) training corpus was ...   \n",
       "606     3718     4  Type System extends the type system that is bu...   \n",
       "607     3719     4  For other languages we use the corpora made av...   \n",
       "608     3720     5  Table 5 compares our reordering model with a r...   \n",
       "609     3720     4  Table 5 compares our reordering model with a r...   \n",
       "610     3721     3  These results contradict those given in Zelenk...   \n",
       "611     3722     5  For example, OntoNotes (Hovy et al., 2006 ), a...   \n",
       "612     3723     4  As for EJ translation, we use the Stanford par...   \n",
       "613     3724     4  For both English and German we used the part-o...   \n",
       "614     3725     4  We train our model on a subset of the WaCkyped...   \n",
       "615     3726     3  The questions are translated using a phrase-ba...   \n",
       "616     3727     3  The questions are translated using a phrase-ba...   \n",
       "617     3728     5  We measure statistical significance using 95% ...   \n",
       "618     3729     4  As for EJ translation, we use the Stanford par...   \n",
       "619     3730     3  Words were downcased and lemmatized using the ...   \n",
       "620     3731     3  English sentences are parsed into dependency s...   \n",
       "621     3732     5  Both of our systems were based on the Moses de...   \n",
       "622     3733     4  We also used ANEW (Bradley and Lang, 1999) for...   \n",
       "623     3734     3  translation at the DiscoMT 2015 workshop (Hard...   \n",
       "624     3735     3  Establishing and maintaining common ground is ...   \n",
       "625     3736     4  SentiWordNet score (senti) We used the Senti- ...   \n",
       "626     3737     3  Europarl 2 (Koehn, 2005 ): it is a corpus of p...   \n",
       "627     3738     4  We also carried out a chunk-reordering PB-SMT ...   \n",
       "628     3739     5  WSI is generally considered as an unsupervised...   \n",
       "629     3740     3  The data used for the experiments described in...   \n",
       "630     3743     4  For the language model, we used the KenLM tool...   \n",
       "631     3744     5  First used by Blitzer et al. (2007) , the MDS ...   \n",
       "632     3748     5  The most famous example would probably be the ...   \n",
       "633     3749     4  For language modeling, we trained a separate 5...   \n",
       "634     3750     5  They used the Web-based annotation tool brat (...   \n",
       "635     3751     5  The classification was conducted, using differ...   \n",
       "636     3752     4  The Europarl corpus (Koehn, 2005) is built fro...   \n",
       "637     3753     5  For language modeling, we trained a separate 5...   \n",
       "638     3754     5  The Spanish-English (S2E) training corpus was ...   \n",
       "639     3756     4  We use the Stanford CoreNLP caseless tagger fo...   \n",
       "640     3757     5  All of our models are trained using Nematus (S...   \n",
       "641     3758     5  We use GIZA++ (Och and Ney, 2003) with its def...   \n",
       "642     3759     4  We are working with standard tools as DISSECT ...   \n",
       "643     3760     5  They are based on Distributional Hypothesis wh...   \n",
       "644     3761     4  Its segmentation model is a class-based hidden...   \n",
       "645     3762     5  The edit distance kernel was trained with LIBS...   \n",
       "646     3763     5  The corpora are first tokenized and lowercased...   \n",
       "647     3764     5  For example, OntoNotes (Hovy et al., 2006 ), a...   \n",
       "648     3766     3  We use the open-source Moses toolkit (Koehn et...   \n",
       "649     3767     3  In our experiments, we use the LIBLINEAR packa...   \n",
       "650     3768     5  The Spanish-English (S2E) training corpus was ...   \n",
       "651     3769     5  The ICSI meeting corpus (Janin et al., 2003) i...   \n",
       "652     3770     5  We use ROUGE (Lin, 2004) for evaluating the co...   \n",
       "653     3771     2  The measure selected is the normalised Pearson...   \n",
       "654     3772     5  The main corpora we use are Europarl (Koehn, 2...   \n",
       "655     3773     4  We exploit this monolingual data for training ...   \n",
       "656     3774     5  We use the Stanford CoreNLP caseless tagger fo...   \n",
       "657     3775     5  This is a corpus-based metric relying on the d...   \n",
       "658     3776     3  Similarly, (Turian et al, 2010) evaluated thre...   \n",
       "659     3777     5  3.3.1 Reference System We compare a number of ...   \n",
       "660     3778     5  For training SVM classifiers we used LIBSVM pa...   \n",
       "661     3779     3  The word alignment was trained using GIZA++ (O...   \n",
       "662     3781     5  The corpus is first word-aligned using a word ...   \n",
       "663     3782     4  We perform bootstrap resampling with bounds es...   \n",
       "664     3784     5  We perform bootstrap resampling with bounds es...   \n",
       "665     3785     4  Markov Logic Networks (MLN) (Richardson and Do...   \n",
       "666     3786     5  with the training script of the Moses toolkit ...   \n",
       "667     3787     3  We train for 15 epochs using mini-batch stocha...   \n",
       "668     3788     3  This confirms the finding of Liu et al. (2012)...   \n",
       "669     3789     5  For this purpose we use the Europarl corpus (K...   \n",
       "670     3790     4  We exploit this monolingual data for training ...   \n",
       "671     3791     4  Our decoder is a stack decoder similar to Koeh...   \n",
       "672     3792     3  For the English- Spanish and French-English sy...   \n",
       "673     3794     5  The word alignment was trained using GIZA++ (O...   \n",
       "674     3795     4  Rhetorical Structure Theory (RST) (Mann and Th...   \n",
       "675     3796     4  The dataset used for the experiments reported ...   \n",
       "676     3797     5  A good data source for this is the Europarl Co...   \n",
       "677     3798     5  The most famous example would probably be the ...   \n",
       "678     3800     5  Previously (Socher et al, 2011)  used a recurs...   \n",
       "679     4302     4  The first source is the CoNLL 2003 shared task...   \n",
       "680     4303     4  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "681     4305     3  We therefore propose an alternative method bas...   \n",
       "682     4306     4  We used the implementation of the scikit-learn...   \n",
       "683     4307     4  Finally, the CoNLL-2003 shared task (Tjong Kim...   \n",
       "684     4308     5  Further, we sentence-split, tokenized, and lem...   \n",
       "685     4309     5  The corpora are first tokenized and lowercased...   \n",
       "686     4310     5  We use Adadelta (Zeiler, 2012) to update the p...   \n",
       "687     4311     3  The first source is the CoNLL 2003 shared task...   \n",
       "688     4313     5  We used a 2009 snapshot of Wikipedia, 2 which ...   \n",
       "689     4314     3  Recently, Hovy et al. (2013) utilized word emb...   \n",
       "690     4316     4  Finally, the CoNLL-2003 shared task (Tjong Kim...   \n",
       "691     4317     5  We run our experiments on Europarl (Koehn, 200...   \n",
       "692     4318     5  We applied bootstrap resampling (Koehn, 2004) ...   \n",
       "693     4319     5  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "694     4320     5  The statistical significance test is also carr...   \n",
       "695     4321     4  This model is motivated by Vector Space Model ...   \n",
       "696     4322     4  The data was segmented into baseNP parts and n...   \n",
       "697     4324     3  The NJU-Parser is based on the state-of-the ar...   \n",
       "698     4325     3  One is a 3-gram language model built using Ken...   \n",
       "699     4326     5  The word alignment was obtained by running Giz...   \n",
       "700     4327     5  Translations for English words in the lexical ...   \n",
       "701     4331     5  All the Language Models (LM) used in our exper...   \n",
       "702     4332     4  In all experiments, we use the SVM classifier ...   \n",
       "703     4333     5  We use the Stanford parser to generate a DG fo...   \n",
       "704     4334     4  Preprocessing: We tokenized the English side o...   \n",
       "705     4336     3  We use the Moses software package 5 to train a...   \n",
       "706     4337     3  In-domain SMT: we used the parallel corpus (Se...   \n",
       "707     4338     5  In-domain SMT: we used the parallel corpus (Se...   \n",
       "708     4339     5  The first competitive learning based system is...   \n",
       "709     4340     5  In-domain SMT: we used the parallel corpus (Se...   \n",
       "710     4341     3  We used predicted collapsed Stanford dependenc...   \n",
       "711     4342     4  We select as a general-purpose corpus Europarl...   \n",
       "712     4343     3  We used the lexicalized dependency parser in t...   \n",
       "713     4344     3  The result of the operation is equivalent to w...   \n",
       "714     4345     3  The distributional hypothesis of meaning (Harr...   \n",
       "715     4346     5  Preprocessing: We tokenized the English side o...   \n",
       "716     4347     3  One is from (Turian et al, 2010) , the dimensi...   \n",
       "717     4348     5  The thresholds were thoroughly selected depend...   \n",
       "718     4349     5  In-domain SMT: we used the parallel corpus (Se...   \n",
       "719     4350     5  The statistical significance test is also carr...   \n",
       "720     4351     3  We use the Moses software package 5 to train a...   \n",
       "721     4352     5  We used a 2009 snapshot of Wikipedia, 2 which ...   \n",
       "722     4353     4  The text was pre-processed using wp2txt 6 to r...   \n",
       "723     4354     5  We run our experiments on Europarl (Koehn, 200...   \n",
       "724     4355     4  We use Scikit-learn (Pedregosa et al., 2011 ),...   \n",
       "725     4356     5  The way they were added is similar to incorpor...   \n",
       "726     4358     5  We used the lexicalized dependency parser in t...   \n",
       "727     4359     4  We modified the implementation of the SWELL Ja...   \n",
       "728     4360     4  The Stanford dependency parser (De Marneffe et...   \n",
       "729     4361     4  The first work on this topic was done back in ...   \n",
       "730     4362     3  The OpenFst library is used to perform all of ...   \n",
       "731     4363     4  For our experiments, we used translated movie ...   \n",
       "732     4364     3  The Moses15 result is obtained by applying the...   \n",
       "733     4365     3  The OpenFst library is used to perform all of ...   \n",
       "734     4366     3  We select as a general-purpose corpus Europarl...   \n",
       "735     4367     4  We also compare our word embeddings with the E...   \n",
       "736     4368     4  The relationship between language and sentimen...   \n",
       "737     4371     3  Support vector machines (SVM) are one of the b...   \n",
       "738     4372     4  In Barankov and Tamchyna (2014), we experiment...   \n",
       "739     4373     4  To learn the parameters of the model we minimi...   \n",
       "740     4374     5  For training the translation model and for dec...   \n",
       "741     4375     5  The text was pre-processed using wp2txt 6 to r...   \n",
       "742     4376     3  The task reported on here is to produce PropBa...   \n",
       "743     4377     4  For both systems, the used training data is fr...   \n",
       "744     4378     4  We use the Stanford parser to generate a DG fo...   \n",
       "745     4379     4  We used predicted collapsed Stanford dependenc...   \n",
       "746     4380     4  We tokenize English data and segment Chinese d...   \n",
       "747     4381     3  The evaluation emphasis in multi-document summ...   \n",
       "748     4382     4  We conducted baseline experiments for phrase b...   \n",
       "749     4383     5  The similarity function is here the Smoothed P...   \n",
       "750     4384     5  We tokenize English data and segment Chinese d...   \n",
       "751     4385     3  The Stanford dependency parser (De Marneffe et...   \n",
       "752     4386     3  To demonstrate the effect of the proposed meth...   \n",
       "753     4387     5  The baseline systems are built with the openso...   \n",
       "754     4388     3  3.3.1 Reference System We compare a number of ...   \n",
       "755     4389     5  Preprocessing: We tokenized the English side o...   \n",
       "756     4391     3  Finally, it is also noticeable that the percen...   \n",
       "757     4392     3  As a learning algorithm we adopt a ranking SVM...   \n",
       "758     4393     2  Brain images are quite noisy, so we used the m...   \n",
       "759     4394     3  The training set is used to train the phrase-b...   \n",
       "760     4395     4  Results on English-French, English-Romanian, a...   \n",
       "761     4396     4  The work presented in Berger et al. (1996) tha...   \n",
       "762     4398     4  Our second method is based on the recurrent ne...   \n",
       "763     4399     4  The SUSANNE Corpus is a modified and condensed...   \n",
       "764     4400     5  The corpora are tokenised and truecased using ...   \n",
       "765     4901     5  English annotations were all produced using th...   \n",
       "766     4903     5  The parameters are estimated by Gibbs sampling...   \n",
       "767     4904     4  The meta-classifier is a linear SVM (Fan et al...   \n",
       "768     4905     3  We use Adadelta (Zeiler, 2012) to update the p...   \n",
       "769     4907     5  We train a Support Vector Machine (SVM) (Corte...   \n",
       "770     4908     5  Parameter tuning is carried out using Z- MERT ...   \n",
       "771     4909     4  Brin identifies the use of patterns in the dis...   \n",
       "772     4910     5     We adopt the setting of Socher et al. (2012) .   \n",
       "773     4912     5  The publicly available tool GIZA++ was used to...   \n",
       "774     4913     5          CRFSuite implementation (Okazaki, 2007 ).   \n",
       "775     4914     3  The HMM classifier used in our experiments fol...   \n",
       "776     4915     5  We computed 4-gram LMs with modified Kneser-Ne...   \n",
       "777     4916     5  Socher et al. (2011) come closest to our targe...   \n",
       "778     4917     3  We use Ridge Regression (RR) with l2-norm regu...   \n",
       "779     4918     3  As for the former (hereafter it is referred to...   \n",
       "780     4920     5  15 The significance tests were performed using...   \n",
       "781     4921     5  In this work, we focus on learning with Suppor...   \n",
       "782     4922     4  We used TreeTagger (Schmid, 1994) to obtain a ...   \n",
       "783     4923     5  To account for this constraint,  include infor...   \n",
       "784     4924     3  The system was trained on the English and Dani...   \n",
       "785     4925     3  We use linear SVMs from LIBLINEAR and SVMs wit...   \n",
       "786     4926     3  In current phrase-based statistical machine tr...   \n",
       "787     4928     5  We train a Support Vector Machine (SVM) (Corte...   \n",
       "788     4929     5  The word alignment was obtained by running Giz...   \n",
       "789     4930     3  For training SVM classifiers we used LIBSVM pa...   \n",
       "790     4931     5  We use the standard alignment tool Giza++ (Och...   \n",
       "791     4932     5  We use the standard alignment tool Giza++ (Och...   \n",
       "792     4933     3  In the Penn Discourse TreeBank 2.0 (Prasad et ...   \n",
       "793     4934     4  Weka (Hall et al., 2009) was used to apply lea...   \n",
       "794     4937     5  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "795     4938     5  We use the standard alignment tool Giza++ (Och...   \n",
       "796     4939     3  We used the GIZA++ software (Och and Ney, 2003...   \n",
       "797     4940     5  The language model used is the 5-gram corpus f...   \n",
       "798     4941     5  English annotations were all produced using th...   \n",
       "799     4942     3  For example, (Turian et al, 2010) compared Bro...   \n",
       "800     4943     3  Distributional representations encode an expre...   \n",
       "801     4944     4  (Li et al, 2013)  use crowdsourcing to build p...   \n",
       "802     4945     5  We use the standard alignment tool Giza++ (Och...   \n",
       "803     4946     4  The formalism that is used to represent the se...   \n",
       "804     4947     5  For French, Hungarian, Polish and Swedish we u...   \n",
       "805     4950     4  The English text was tokenized using the word ...   \n",
       "806     4951     4  We use the MOSES decoder (Koehn et al., 2007) ...   \n",
       "807     4952     4  Specifically, the sentence compression dataset...   \n",
       "808     4953     4  use the Stanford Parser (de Marneffe et al., 2...   \n",
       "809     4954     4  We conducted baseline experiments for phraseba...   \n",
       "810     4955     4  In practice, the decoder has to employ beam se...   \n",
       "811     4956     5  The source of bilingual data used in the exper...   \n",
       "812     4957     4  The tectogrammatical annotation layer is based...   \n",
       "813     4958     4  In this paper, we use the subjectivity corpus ...   \n",
       "814     4959     4  We conducted baseline experiments for phraseba...   \n",
       "815     4960     5  All our systems are contrasted with a standard...   \n",
       "816     4961     4  A common approach to computing similarity is t...   \n",
       "817     4962     4  We conducted baseline experiments for phraseba...   \n",
       "818     4963     3  The Stanford dependency parser (De Marneffe et...   \n",
       "819     4964     3  We tokenize and frequent-case the data with th...   \n",
       "820     4965     4  We conducted baseline experiments for phraseba...   \n",
       "821     4966     4  We conducted baseline experiments for phraseba...   \n",
       "822     4967     4  For training the translation model and for dec...   \n",
       "823     4968     4  We tokenize and frequent-case the data with th...   \n",
       "824     4969     5  We conducted baseline experiments for phraseba...   \n",
       "825     4970     5  We use the Moses software package 5 to train a...   \n",
       "826     4971     4  The corpora are tokenised and truecased using ...   \n",
       "827     4972     5  In contrast, (McClosky et al, 2006) focus on l...   \n",
       "828     4973     3  We use logistic regression with L2 regularizat...   \n",
       "829     4974     5  Since the phrase table contains lemmas, the Wi...   \n",
       "830     4975     4  We use logistic regression with L2 regularizat...   \n",
       "831     4976     4  The corpora are tokenised and truecased using ...   \n",
       "832     4977     4  For training the translation model and for dec...   \n",
       "833     4978     3  We use the Moses software package 5 to train a...   \n",
       "834     4980     4  We use the Moses software package 5 to train a...   \n",
       "835     4981     3  Europarl (Koehn, 2005) is a multilingual paral...   \n",
       "836     4982     5  For all syntactic parsers, we used the\" basic\"...   \n",
       "837     4983     3  The Stanford dependency parser (De Marneffe et...   \n",
       "838     4984     5  We built phrase-based machine translation syst...   \n",
       "839     4985     3  We use the cross-entropy loss function and min...   \n",
       "840     4986     4  We use the cross-entropy loss function and min...   \n",
       "841     4987     4  We use the AdaGrad method (Duchi et al., 2011)...   \n",
       "842     4988     5  Socher et al. (2011)  explored using recursive...   \n",
       "843     4989     5  2 Translation Model Moses is a state-of-the-ar...   \n",
       "844     4989     5  2 Translation Model Moses is a state-of-the-ar...   \n",
       "845     4990     5  2 Translation Model Moses is a state-of-the-ar...   \n",
       "846     4991     4  For training the translation model and for dec...   \n",
       "847     4992     5  All our systems are contrasted with a standard...   \n",
       "848     4993     5  For all syntactic parsers, we used the\" basic\"...   \n",
       "849     4994     4  can be evaluated by maximising the pseudo-like...   \n",
       "850     4995     5  The baseline systems are built with the openso...   \n",
       "851     4996     4  2 Translation Model Moses is a state-of-the-ar...   \n",
       "852     4997     3  In this approach we used the NERsuite software...   \n",
       "853     4998     4  2 Translation Model Moses is a state-of-the-ar...   \n",
       "854     4999     4  These efforts focused exclusively on the meron...   \n",
       "855     5000     4  Special forms of relatedness are represented i...   \n",
       "856     5501     5  We train our model on a subset of the WaCkyped...   \n",
       "857     5502     4  Since the first shared task on Recognising Tex...   \n",
       "858     5503     5  We used k-best batch MIRA (Cherry and Foster, ...   \n",
       "859     5504     3  We used the Mallet toolkit (McCallum, 2002) fo...   \n",
       "860     5505     3  We use the implementation provided by CRFsuite...   \n",
       "861     5506     5  glish source with target French by using GIZA+...   \n",
       "862     5507     5  We used ROUGE-N (Lin, 2004) for evaluation of ...   \n",
       "863     5508     4  1 with 2 -regularization using AdaGrad (Duchi ...   \n",
       "864     5509     4  We used Weka (Hall et al., 2009) for all our c...   \n",
       "865     5510     4  POS tagging was performed with TreeTagger (Sch...   \n",
       "866     5511     5  Word alignments on the parallel corpus are per...   \n",
       "867     5512     4  We used the implementation of the scikit-learn...   \n",
       "868     5514     3  The Lexical sample data was parsed using the C...   \n",
       "869     5515     5  For POS-tagging, we used the Stanford POS-tagg...   \n",
       "870     5516     5  We used the English side of the Europarl corpu...   \n",
       "871     5517     3  This data is part of the NUCLE corpus (Dahlmei...   \n",
       "872     5518     3  The training set is used to train the phrase-b...   \n",
       "873     5519     4  We used the word alignment produced by Giza (O...   \n",
       "874     5520     5  For this purpose we use the Europarl corpus (K...   \n",
       "875     5521     4  Word alignment is performed using GIZA++ (Och ...   \n",
       "876     5522     5  Word alignment was done with GIZA++ (Och and N...   \n",
       "877     5524     4  The evaluation results were provided by the or...   \n",
       "878     5525     3  In future work we can therefore incorporate un...   \n",
       "879     5526     4  glish source with target French) by using GIZA...   \n",
       "880     5527     3  We conducted statistical significance tests fo...   \n",
       "881     5528     5  The word alignment was obtained by running Giz...   \n",
       "882     5529     5  Word alignment was done with GIZA++ (Och and N...   \n",
       "883     5531     4  For the theory of Cho & Chai (2000) to be comp...   \n",
       "884     5532     5  In order to compare our method to a well under...   \n",
       "885     5534     4  We examine the quality of translations to Engl...   \n",
       "886     5535     4  Integer Linear Programming (ILP) has recently ...   \n",
       "887     5536     5  For that purpose, we use the word analogy task...   \n",
       "888     5537     3  The corpus is first word-aligned using a word ...   \n",
       "889     5538     3  For ranking, we use the SVM rank ranker (Joach...   \n",
       "890     5539     3  Parameters are updated using AdaGrad (Duchi et...   \n",
       "891     5540     3  The tagger we use is TnT (Brants, 2000) , a hi...   \n",
       "892     5541     3  Sentiment score of the last post of the observ...   \n",
       "893     5544     4  For German English we also have a system based...   \n",
       "894     5546     3  We use the Universal POS Tagset (UPOS) of Petr...   \n",
       "895     5547     3  We also considered an ensemble of our approach...   \n",
       "896     5548     3  One way to solve this problem is to use a kern...   \n",
       "897     5549     3  We use Adam (Kingma and Ba, 2015) for optimisa...   \n",
       "898     5550     3  We build our PB-SMT systems in a standard way ...   \n",
       "899     5551     4  The starting point for our model is the skipgr...   \n",
       "900     5553     4  We introduce a new anaphoricity detection mode...   \n",
       "901     5554     5  The realisation ranking component is an SVM ra...   \n",
       "902     5556     3  WSI is generally considered as an unsupervised...   \n",
       "903     5557     4  We run our experiments on Europarl (Koehn, 200...   \n",
       "904     5558     3  For the language model, we used the KenLM tool...   \n",
       "905     5559     3  The second collection is constituted by the GE...   \n",
       "906     5560     4  We introduce a new anaphoricity detection mode...   \n",
       "907     5563     5  The sentence aligned parallel data is first wo...   \n",
       "908     5564     3  All the Language Models (LM) used in our exper...   \n",
       "909     5565     4  They are based on Distributional Hypothesis wh...   \n",
       "910     5566     4  Finally, we consider the Europarl corpus v7 (K...   \n",
       "911     5568     3  For the contextual check we use the Google Web...   \n",
       "912     5569     3  By imposing constraints on the possible word r...   \n",
       "913     5571     5  Surdeanu et al. (2012) propose a two-layer mul...   \n",
       "914     5572     3  We have theoretically suggested that based on ...   \n",
       "915     5574     3  For the contextual check we use the Google Web...   \n",
       "916     5575     5  ROUGE (Lin, 2004) is a set of evaluation metri...   \n",
       "917     5576     5  Here we review the parameters of the standard ...   \n",
       "918     5577     4  The Spanish-English (S2E) training corpus was ...   \n",
       "919     5578     3  We convert the trees in both treebanks from co...   \n",
       "920     5579     3  We convert the trees in both treebanks from co...   \n",
       "921     5580     5  We use KenLM 3 (Heafield, 2011) for computing ...   \n",
       "922     5581     5  The most famous example would probably be the ...   \n",
       "923     5582     3  We use the Stanford parser with Stanford depen...   \n",
       "924     5583     4  Additionally, we train phrase-based machine tr...   \n",
       "925     5584     4  The training set is used to train the phrase-b...   \n",
       "926     5585     5  The morpho syntactically annotated corpus we u...   \n",
       "927     5586     3  There has been a large amount of work on senti...   \n",
       "928     5587     5  This task setup is further described in the ta...   \n",
       "929     5588     5  The baseline will be created by the Moses SMT ...   \n",
       "930     5591     3  Discourse structure in summarization Rhetorica...   \n",
       "931     5592     3  It builds on the C&C CCG parser (Clark and Cur...   \n",
       "932     5593     5  We use Boxer (Bos et al., 2004) to parse natur...   \n",
       "933     5594     5  The training set is used to train the phrase-b...   \n",
       "934     5595     4  We perform bootstrap resampling with bounds es...   \n",
       "935     5596     4  The major part of data comes from current and ...   \n",
       "936     5598     4  For Italian, we use the word2vec to train word...   \n",
       "937     5599     3  The GATE plugin-based architecture (Cunningham...   \n",
       "938     5600     3  We trained a 5-gram language model on the Xinh...   \n",
       "939     6101     5  We used GIZA++ (Och and Ney, 2003) along with ...   \n",
       "940     6102     3  Our previous MLN-based approach for joint disa...   \n",
       "941     6103     4  Pang et al. (2002) compare the performance of ...   \n",
       "942     6104     4  Jans et al. (2012) focused solely on the narra...   \n",
       "943     6105     4  We applied the Naive Bayes probabilistic super...   \n",
       "944     6106     4  We therefore seek to allow quick incremental u...   \n",
       "945     6107     3  We use the Moses software package 5 to train a...   \n",
       "946     6108     5  It has a much longer average sentence length t...   \n",
       "947     6109     4  The baseline systems are built with the openso...   \n",
       "948     6110     4  The corpora are tokenised and truecased using ...   \n",
       "949     6111     4  The dropout rate was set to 0.5, and the model...   \n",
       "950     6112     5  The dropout rate was set to 0.5, and the model...   \n",
       "951     6113     5  We built phrase-based machine translation syst...   \n",
       "952     6114     3  They had shown that the Penn Discourse TreeBan...   \n",
       "953     6115     5  They had shown that the Penn Discourse TreeBan...   \n",
       "954     6117     5  McDonald et al. (2005) present a technique for...   \n",
       "955     6118     3  We transformed the parse trees in OntoNotes in...   \n",
       "956     6119     5  2 Translation Model Moses is a state-of-the-ar...   \n",
       "957     6120     4  For the OntoNotes data sets, Same speaker (Lee...   \n",
       "958     6121     4  It was one of the best parsers in the CoNLL Sh...   \n",
       "959     6122     4  We transformed the parse trees in OntoNotes in...   \n",
       "960     6123     5  Results are reported on the test data using F1...   \n",
       "961     6125     5  The perplexity achieved by the 6-gram NN LM in...   \n",
       "962     6126     3  We address the QE problem as a regression task...   \n",
       "963     6127     5  Finally, it is also noticeable that the percen...   \n",
       "964     6128     4  Facts such as these are difficult to account f...   \n",
       "965     6130     4  All these reordering models are tested using M...   \n",
       "966     6131     5  @BULLET RAE-Subj: Socher et al. (2011) propose...   \n",
       "967     6132     5  We train a ridge regression model (Scikit-lear...   \n",
       "968     6133     4  We follow the protocols in Collobert et al. (2...   \n",
       "969     6134     4  This is equivalent to an SVM with the compound...   \n",
       "970     6135     4  All experiments are conducted using the Moses ...   \n",
       "971     6136     3  We use the Moses toolkit (Koehn et al., 2007) ...   \n",
       "972     6137     4  All experiments were on English part of speech...   \n",
       "973     6138     4  We use the word alignments to construct a phra...   \n",
       "974     6139     4  We estimated a hierarchical MT model for the t...   \n",
       "975     6140     4  A small amount of labeled data is used to map ...   \n",
       "976     6141     5  We use the word alignments to construct a phra...   \n",
       "977     6143     3  1 For a reference\" standard\" text we used the ...   \n",
       "978     6144     4  We worked with the Europarl corpus (Koehn, 200...   \n",
       "979     6145     5  The organization from SemEval-2013 Task 2: Sen...   \n",
       "980     6146     5  Pending a planned full evaluation using the Mo...   \n",
       "981     6147     5  The entity transition features are then used t...   \n",
       "982     6150     4  Our second method is based on the recurrent ne...   \n",
       "983     6153     4  We apply the Hidden Markov Model (HMM) (Viterb...   \n",
       "984     6155     5  We report BLEU (Papineni et al., 2001) of tran...   \n",
       "985     6156     5  It is a standard phrase-based machine translat...   \n",
       "986     6158     5  Finally, we used Moses toolkit as phrase-based...   \n",
       "987     6160     4  Concept similarity is computed using the edge-...   \n",
       "988     6161     5  Finally, we used Moses toolkit as phrase-based...   \n",
       "989     6163     5  It is a standard phrase-based machine translat...   \n",
       "990     6164     4  2 Translation Model Moses is a state-of-the-ar...   \n",
       "991     6165     5  Otherwise, it is measured by WordNet similarit...   \n",
       "992     6166     4  We use 5-gram language models with Kneser-Ney ...   \n",
       "993     6167     4  We adopt online learning, updating parameters ...   \n",
       "994     6169     3  We used the implementation of the scikit-learn...   \n",
       "995     6170     4  We adopt online learning, updating parameters ...   \n",
       "996     6172     5  Finally, we used Moses toolkit as phrase-based...   \n",
       "997     6173     3  Europarl (Koehn, 2005) is a multilingual paral...   \n",
       "998     6174     4  We leave the third-order models (Koo and Colli...   \n",
       "999     6175     4  We used the implementation of the scikit-learn...   \n",
       "1000    6176     5  For English and French a model was trained usi...   \n",
       "1001    6177     3  Empirically we show that our model beats the s...   \n",
       "1002    6178     4  We use logistic regression with L2 regularizat...   \n",
       "1003    6179     3  We use the New York Times Annotated Corpus (Sa...   \n",
       "1004    6180     4  We use the NMF and tf-idf implementations prov...   \n",
       "1005    6181     5  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "1006    6182     3  In addition, the data was tokenized, lemmatize...   \n",
       "1007    6183     5  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "1008    6184     5  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "1009    6185     5  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "1010    6186     4  Feature selection was performed using chi-squa...   \n",
       "1011      10     0  We use the Support Vector Machines implementat...   \n",
       "1012      32     5  Recently, there has been a successful attempt ...   \n",
       "1013      34     5  The first one is the WS-353 dataset (Finkelste...   \n",
       "1014      35     5  Abstract Meaning Representation (AMR) (Banares...   \n",
       "1015      36     5  We perform bootstrap resampling with bounds es...   \n",
       "1016      37     5  It is used to support semantic analyses in HPS...   \n",
       "1017    1201     4  We use Stanford parser (de Marneffe et al., 20...   \n",
       "1018    1202     3  Rooth et al. (1999) propose an Expectation-Max...   \n",
       "1019    1203     3  The Levenshtein distance (Levenshtein, 1966) b...   \n",
       "1020    1204     1  We use the Moses toolkit (Koehn et al., 2007) ...   \n",
       "1021      36     5  We perform bootstrap resampling with bounds es...   \n",
       "1022      37     5  It is used to support semantic analyses in HPS...   \n",
       "1023      38     3  It is used to support semantic analyses in the...   \n",
       "1024      39     4  It is used to support semantic analyses in HPS...   \n",
       "1025      40     4  It is used to support semantic analyses in the...   \n",
       "1026      43     3  We build upon our previous Markov Logic based ...   \n",
       "1027      44     2  Details about SVM and KFD can be found in (Tay...   \n",
       "1028      46     3  We learn the parameters using a quasi-Newton p...   \n",
       "1029      47     5  We use the SCFG decoder cdec (Dyer et al., 201...   \n",
       "1030      48     4  This is known as the Distributional Hypothesis...   \n",
       "1031      49     4  All our models , as well as the parser describ...   \n",
       "1032      51     4  For strings, many such kernel functions exist ...   \n",
       "1033      53     2  The estimation of the semantically Smoothed Pa...   \n",
       "1034      55     2  We train with the Adam optimizer (Kingma and B...   \n",
       "1035      56     3  In our experimental study, we use the freely a...   \n",
       "1036      58     2  More recently, (Carpineto and Romano, 2010) sh...   \n",
       "1037      59     4  They are based on the distributional hypothesi...   \n",
       "1038      60     4  Word alignment is performed using GIZA++ (Och ...   \n",
       "1039      61     4  Word alignment is performed using GIZA++ (Och ...   \n",
       "1040      62     3  We used Mallet software (McCallum, 2002) for C...   \n",
       "1041      63     3  The thesaurus consists of a hierarchy of 2,710...   \n",
       "1042      64     4  The detailed discussion is provided in the lon...   \n",
       "1043      66     4  The first one is the WS- 353 3 dataset (Finkel...   \n",
       "1044      68     3  A framework for human error analysis and error...   \n",
       "1045      69     4  MaxEnt classifier is a good example of this gr...   \n",
       "1046      70     4  Among these media, blog is one of the communic...   \n",
       "1047      73     2  For preprocessing, we used MADA (Morphological...   \n",
       "1048      74     4  We trained a 5-gram language model on the Xinh...   \n",
       "1049      75     4  To determine semantic type and subtype, we tra...   \n",
       "1050      76     2  We use the scikit implementation of Random For...   \n",
       "1051      78     3  Each term in the input text will be represente...   \n",
       "1052      79     5  An algorithm, the Kuhn-Munkres method (Kuhn, 1...   \n",
       "1053      81     3  We use Collapsed Gibbs Sampling (Griffiths and...   \n",
       "1054      82     5  We use the Stanford dependency parser (Marneff...   \n",
       "1055      83     5  Filter weights are initialized using Glorot-Be...   \n",
       "1056      85     4  Automatic sentence alignment of the training d...   \n",
       "1057      86     2  We use the AdaGrad optimizer (Duchi et al., 20...   \n",
       "1058      87     5  Then we did word alignment using GIZA++ (Och a...   \n",
       "1059      88     4  For example, DIRT (Lin and Pantel, 2001) aims ...   \n",
       "1060      89     3  The annotation was performed manually using th...   \n",
       "1061      91     4  System proposed by (Li et al., 2006 ), uses a ...   \n",
       "1062      92     3  This corpus contains around 11,000 NPs annotat...   \n",
       "1063      93     4  All modules take as input the corpus documents...   \n",
       "1064      95     4  From the pioneering work of (Rapp, 1995 ), con...   \n",
       "1065      96     5  Europarl (Koehn, 2005) is a multilingual paral...   \n",
       "1066      99     3  TESLA (Translation Evaluation of Sentences wit...   \n",
       "1067     100     4  For building the baseline SMT system, we used ...   \n",
       "1068     102     4  Rhetorical Structure Theory (RST) (Mann and Th...   \n",
       "1069     103     4  In addition, the fix-discount method in (Foste...   \n",
       "1070     104     5  Memory-based language processing (Daelemans an...   \n",
       "1071     105     4  We calculate statistical significance of perfo...   \n",
       "1072     106     4  For instance, machine translation (MT) systems...   \n",
       "1073     109     4  1 with 2 -regularization using AdaGrad (Duchi ...   \n",
       "1074     111     3  Why does the lr model outperform Berkeley 13 T...   \n",
       "1075     112     3  In the context of this paper we will be focusi...   \n",
       "1076     113     5  Europarl (Koehn, 2005) is a multilingual paral...   \n",
       "1077     114     4  We have used Foma, a free software tool to spe...   \n",
       "1078     116     3  We used the same test set used in Li et al. (2...   \n",
       "1079     117     3  The Penn Discourse Treebank (PDTB, Prasad et a...   \n",
       "1080     118     4  We used the implementation of the scikit-learn...   \n",
       "1081     119     3  Since the commonly used word similarity datase...   \n",
       "1082     120     5  The training data of the shared task is the NU...   \n",
       "1083     122     3  All system implementation was done using Pytho...   \n",
       "1084     123     3  It has been shown that a diverse set of predic...   \n",
       "1085     124     4  In the 2013 system, we had used SentiStrength ...   \n",
       "1086     125     3  Finally, we also compare the quality of the ca...   \n",
       "1087     126     4  These methods are based on the distributional ...   \n",
       "1088     127     5  All annotations were done using the BRAT rapid...   \n",
       "1089     128     4  Compared to WordNet (Fellbaum, 1998 ), there a...   \n",
       "1090     131     5  For training, we use Adam (Kingma and Ba, 2015...   \n",
       "1091     132     3  For our classifier, we use SVMs, specifically ...   \n",
       "1092     133     4  The first two experiments concern the predicti...   \n",
       "1093     136     2  We used only the non-ensembled left-to-right r...   \n",
       "1094     137     2  We used only the non-ensembled left-to-right r...   \n",
       "1095     138     4  The MSD morphological coding system was develo...   \n",
       "1096     139     5  The phrase tables were generated by means of s...   \n",
       "1097     140     5  Word alignment is performed using GIZA++ (Och ...   \n",
       "1098     141     4  We experimented with several levels of cluster...   \n",
       "1099     143     4  (Raghavan et al. (2007)) measure the benefit f...   \n",
       "1100     144     4  We built a modified Kneser-Ney smoothed 5-gram...   \n",
       "1101     146     4  A formal PAC-style analysis can be found in (A...   \n",
       "1102     147     4  The first model we introduce is based on the r...   \n",
       "1103     148     4  The SMT systems were built using the Moses too...   \n",
       "1104     149     3  The classifier experiments were carried out us...   \n",
       "1105     150     3  The training data of the shared task is the NU...   \n",
       "1106     151     5  SALDO (Borin et al., 2013) is the largest free...   \n",
       "1107     152     5  For the phrase-based SMT system, we adopted th...   \n",
       "1108     153     5  However, those string-to-tree systems run slow...   \n",
       "1109     154     4  The 5-gram target language model was trained u...   \n",
       "1110     155     5  For all experiments, we used the Moses SMT sys...   \n",
       "1111     156     4  We evaluate our method on the following data s...   \n",
       "1112     157     3  We use the Moses phrase-based translation syst...   \n",
       "1113     158     4  But we randomly selected 90% of the training d...   \n",
       "1114     159     3  The BLEU score measures the precision of n-gra...   \n",
       "1115     160     3  The training data of the shared task is the NU...   \n",
       "1116     161     3  Test data was drawn from the Open American Nat...   \n",
       "1117     162     5  We measure statistical significance using 95% ...   \n",
       "1118     163     4  We use the English portion of the ACE 2005 rel...   \n",
       "1119     164     5  This data was collected for the 2014 SemEval c...   \n",
       "1120     165     5  The parsing model used for intra-sentential pa...   \n",
       "1121     166     4  Latent Dirichlet Allocation (LDA) is a generat...   \n",
       "1122     167     5  Distributional hypothesis theory (Harris, 1954...   \n",
       "1123     168     5  Distributional hypothesis theory (Harris, 1954...   \n",
       "1124     169     5  In 2009, Yefang Wang (Wang et al., 2009) used ...   \n",
       "1125     170     3  We built a 5-gram language model on the Englis...   \n",
       "1126     171     3  The remaining three models are all Naive Bayes...   \n",
       "1127     172     4  We apply bootstrapping (Kozareva et al., 2008)...   \n",
       "1128     173     5  These methods are based on the distributional ...   \n",
       "1129     174     5  These results verify the benefit of using LTAG...   \n",
       "1130     175     5  RG-65: (Rubenstein and Goodenough, 1965) has 6...   \n",
       "1131     176     5  The significance tests were performed using th...   \n",
       "1132     177     5  The default Phrasal search algorithm is cube p...   \n",
       "1133     178     5  In-domain data is mainly used to solve the pro...   \n",
       "1134     179     4  All experiments were carried out using the ope...   \n",
       "1135     180     4  First, we apply heuristics to determine number...   \n",
       "1136     181     4  Rank SVM (Joachims, 2002) is a method based on...   \n",
       "1137     182     3  A more detailed description of the task can be...   \n",
       "1138     184     5  A Tree Kernel function is a convolution kernel...   \n",
       "1139     185     4  We build upon our previous approach for joint ...   \n",
       "1140     186     4  ROUGE-2 metric (Lin, 2004) is used for the eva...   \n",
       "1141     187     5  We used Mallet software (McCallum, 2002) for C...   \n",
       "1142     188     3  We exploit a transition-based framework with g...   \n",
       "1143     189     4  The annotation was performed using the BRAT 2 ...   \n",
       "1144     190     5  For building the word alignment models we use ...   \n",
       "1145     191     5  The reliability of the annotation was evaluate...   \n",
       "1146     192     5  It therefore follows the distributional hypoth...   \n",
       "1147     193     4  This dataset is composed of 35 triplets of sen...   \n",
       "1148     194     5  The significance tests were performed using th...   \n",
       "1149     195     5  We use the standard alignment tool Giza++ (Och...   \n",
       "1150     196     5  We use the standard alignment tool Giza++ (Och...   \n",
       "1151     198     3  The kernels are combined using Gaussian proces...   \n",
       "1152     199     5  To overcome this, Agirre et al. (2009) used Ma...   \n",
       "1153     200     5  We measure statistical significance using 95% ...   \n",
       "1154     701     4  We use the Stanford dependency parser (Chen an...   \n",
       "1155     702     3  Next, a tweet was tokenized and fed into MADAM...   \n",
       "1156     703     4  (Yarowsky ,(1995)) has proposed a bootstrappin...   \n",
       "1157     704     5  We also list the previous state-of-the-art per...   \n",
       "1158     705     4  We used Adam (Kingma and Ba, 2014) with a lear...   \n",
       "1159     706     2  Distributional semantics is based on the idea ...   \n",
       "1160     707     3  In order to estimate the basic lexical similar...   \n",
       "1161     709     5  We used the implementation of the scikit-learn...   \n",
       "1162     710     3  The translation model was trained by GIZA++ (O...   \n",
       "1163     711     5  This is a generalization of the operator Id in...   \n",
       "1164     712     4  We used standard classifiers available in scik...   \n",
       "1165     714     5  We used the implementation of the scikit-learn...   \n",
       "1166     715     5  For the phrase-based SMT system, we adopted th...   \n",
       "1167     716     3  4 Word alignments are created by aligning the ...   \n",
       "1168     717     4  The perplexity achieved by the 6- gram NN LM i...   \n",
       "1169     718     5  We used standard classifiers available in scik...   \n",
       "1170     719     5  Statistical machine translation is typically p...   \n",
       "1171     720     3  WordNet (Miller et al., 1990) is an on-line hi...   \n",
       "1172     721     4  We use the scikit implementation of Random For...   \n",
       "1173     724     4  We lemmatise the head of each constituent with...   \n",
       "1174     725     4  Our text processing uses the Natural Language ...   \n",
       "1175     726     5  They used the Web-based annotation tool brat (...   \n",
       "1176     729     3  Our system participated in SemEval-2013 Task 2...   \n",
       "1177     730     5  The English text was tokenized using the word ...   \n",
       "1178     731     4  The webpages were parsed using the Stanford Co...   \n",
       "1179     732     3  Statistical machine translation is typically p...   \n",
       "1180     733     3  The English side was tokenized using the Moses...   \n",
       "1181     734     5  We trained an English 5-gram language model us...   \n",
       "1182     735     5  All of the text data from Reddit was tokenized...   \n",
       "1183     736     4  Our machine translation systems are trained us...   \n",
       "1184     737     5  For the phrase-based SMT system, we adopted th...   \n",
       "1185     738     5  We build upon our previous approach for joint ...   \n",
       "1186     739     5  We used Adam (Kingma and Ba, 2014) with a lear...   \n",
       "1187     740     5  For all experiments, we used the Moses SMT sys...   \n",
       "1188     741     4  The SMT systems were built using the Moses too...   \n",
       "1189     742     5  For training the translation model and for dec...   \n",
       "1190     743     5  For training the translation model and for dec...   \n",
       "1191     744     5  For the phrase-based SMT system, we adopted th...   \n",
       "1192     745     5  We develop translation models using the phrase...   \n",
       "1193     746     4  We used TnT (Brants, 2000 ), trained on the Ne...   \n",
       "1194     747     3  The webpages were parsed using the Stanford Co...   \n",
       "1195     748     5  We develop translation models using the phrase...   \n",
       "1196     750     3  The term frequency count is normalized with th...   \n",
       "1197     751     4  We assessed the statistical significance of di...   \n",
       "1198     752     5  A framework for human error analysis and error...   \n",
       "1199     753     5  For example, Chang et al. (2009) found that th...   \n",
       "1200     754     4  On the Chinese side, we used the morphological...   \n",
       "1201     755     5  conducted using the Moses phrase-based decoder...   \n",
       "1202     756     3  conducted using the Moses phrase-based decoder...   \n",
       "1203     757     4  We conducted baseline experiments for phraseba...   \n",
       "1204     759     3  Then the processed data was performed for toke...   \n",
       "1205     760     5  We applied bootstrap resampling (Koehn, 2004) ...   \n",
       "1206     761     5  This system uses the attentional encoder-decod...   \n",
       "1207     762     3  The language model is a 5-gram KenLM (Heafield...   \n",
       "1208     763     5  Weighted Finite State Transducers (FSTs) used ...   \n",
       "1209     764     5  Weighted Finite State Transducers (FSTs) used ...   \n",
       "1210     765     3  Then the processed data was performed for toke...   \n",
       "1211     766     4  It is a modification of the model proposed by ...   \n",
       "1212     767     5  We use Scikit-learn (Pedregosa et al., 2011 ),...   \n",
       "1213     768     3  We use the Universal POS Tagset (UPOS) of Petr...   \n",
       "1214     768     3  We use the Universal POS Tagset (UPOS) of Petr...   \n",
       "1215     769     3  The CRF is trained using decisions from the fo...   \n",
       "1216     770     5  For language modeling, we trained a separate 5...   \n",
       "1217     771     5  with the training script of the Moses toolkit ...   \n",
       "1218     772     5  The word alignment was trained using GIZA++ (O...   \n",
       "1219     773     4  We use linear SVMs from LIBLINEAR and SVMs wit...   \n",
       "1220     774     5  We specify the hierarchical aligner in terms o...   \n",
       "1221     775     4  We use Stanford parser (de Marneffe et al., 20...   \n",
       "1222     776     4  In this work, we use the Stanford neural depen...   \n",
       "1223     778     5  We use Stanford parser (de Marneffe et al., 20...   \n",
       "1224     779     5  The learning algorithm used in our coreference...   \n",
       "1225     780     5  We use Stanford parser (de Marneffe et al., 20...   \n",
       "1226     781     5  Distributional semantics (see Cohen and Widdow...   \n",
       "1227     782     4  Their work is part of the state-of-the-art Ara...   \n",
       "1228     783     5  We use Stanford parser (de Marneffe et al., 20...   \n",
       "1229     784     4  Phrasal follows the log-linear approach to phr...   \n",
       "1230     785     5  Training data are based on a concatenation of ...   \n",
       "1231     786     3  In-domain SMT: we used the parallel corpus (Se...   \n",
       "1232     787     5  5-gram language models of Turkish and English ...   \n",
       "1233     788     5  We used the relation classification dataset of...   \n",
       "1234     789     4  We then run word alignment with GIZA++ (Och an...   \n",
       "1235     790     3  In-domain SMT: we used the parallel corpus (Se...   \n",
       "1236     791     5  The significance tests were performed using th...   \n",
       "1237     792     5  All experiments were carried out using the ope...   \n",
       "1238     793     5  These sentences have then be fed into an effic...   \n",
       "1239     794     4  The language model is a 5-gram KenLM (Heafield...   \n",
       "1240     795     4  Cohen et al. (2012)  present a spectral algori...   \n",
       "1241     796     5  Distributional hypothesis theory (Harris, 1954...   \n",
       "1242     797     5  Distributional hypothesis theory (Harris, 1954...   \n",
       "1243     799     3  We use the Stanford dependency parser (Marneff...   \n",
       "1244     800     3  We also replicated the experiment of Holmqvist...   \n",
       "1245    1301     4  We develop translation models using the phrase...   \n",
       "1246    1302     5  POS tagging was performed with TreeTagger (Sch...   \n",
       "1247    1303     5  We used the MaltParser (Nivre et al., 2007) fo...   \n",
       "1248    1304     4  Classification uses the scikit-learn Python pa...   \n",
       "1249    1305     5  We use the Stanford dependency parser (Marneff...   \n",
       "1250    1306     5  We used Adam (Kingma and Ba, 2014) with a lear...   \n",
       "1251    1307     4  Word alignment is performed using GIZA++ (Och ...   \n",
       "1252    1308     4  The test set was tagged with the French TreeTa...   \n",
       "1253    1309     4  For the phrase-based SMT system, we adopted th...   \n",
       "1254    1310     5  POS tagging was performed with TreeTagger (Sch...   \n",
       "1255    1311     5  For building the word alignment models we use ...   \n",
       "1256    1313     3  The Polish data is taken from the EUROPARL cor...   \n",
       "1257    1314     5  The German-to-English corpus is Europarl v7 (K...   \n",
       "1258    1315     5  Distributional models of meaning follow the di...   \n",
       "1259    1316     4  conducted using the Moses phrase-based decoder...   \n",
       "1260    1317     3  Europarl 2 (Koehn, 2005 ): it is a corpus of p...   \n",
       "1261    1318     4  We conducted statistical significance tests fo...   \n",
       "1262    1320     5  The English side was tokenized using the Moses...   \n",
       "1263    1321     5  We develop translation models using the phrase...   \n",
       "1264    1322     5  The baseline will be created by the Moses SMT ...   \n",
       "1265    1323     4  The resulting matrix is weighted using pointwi...   \n",
       "1266    1324     3  The SMT systems were built using the Moses too...   \n",
       "1267    1325     4  The morpho-syntactic tagging has been made wit...   \n",
       "1268    1326     4  For the determination of POS tags we use the S...   \n",
       "1269    1327     5  The Polish data is taken from the EUROPARL cor...   \n",
       "1270    1328     5  All annotations were done using the brat rapid...   \n",
       "1271    1329     4  The Spanish-English (S2E) training corpus was ...   \n",
       "1272    1330     4  We used TnT (Brants, 2000 ), trained on the Ne...   \n",
       "1273    1331     4  For all experiments, we used the Moses SMT sys...   \n",
       "1274    1332     5  Two baseNP data sets have been put forward by ...   \n",
       "1275    1333     5  Both of our systems were based on the Moses de...   \n",
       "1276    1334     5  For the phrase-based SMT system, we adopted th...   \n",
       "1277    1335     5  Corpus-based meaning representations rely on t...   \n",
       "1278    1336     4  Recently, Naim et al. (2014)  proposed an unsu...   \n",
       "1279    1337     5  A distributional similarity model is construct...   \n",
       "1280    1338     4  This paper describes the details of our system...   \n",
       "1281    1339     3  We used the phrase-based model Moses (Koehn et...   \n",
       "1282    1340     5  Combinatory Categorial grammar (CCG) is a ling...   \n",
       "1283    1341     4  We used the Random Forests implementation of s...   \n",
       "1284    1342     4  The proposed model extends the LDA framework o...   \n",
       "1285    1343     4  We used the training section of the dataset fr...   \n",
       "1286    1344     5  5-gram language models of Turkish and English ...   \n",
       "1287    1345     3  The corpora are first tokenized and lowercased...   \n",
       "1288    1346     5  The statistical significance test is also carr...   \n",
       "1289    1348     3  The realisation ranking component is an SVM ra...   \n",
       "1290    1351     5  The statistical significance test is also carr...   \n",
       "1291    1352     3  We use the NLTK toolkit (Loper and Bird, 2002)...   \n",
       "1292    1357     2  We mark the source tokens to which each target...   \n",
       "1293    1358     3  We rely on the hybrid aligned lexical semantic...   \n",
       "1294    1359     3  To help improve the information extraction too...   \n",
       "1295    1362     3  The module of coreference resolution included ...   \n",
       "1296    1363     5  We run our experiments on Europarl (Koehn, 200...   \n",
       "1297    1364     3  We used the scikit-learn (Pedregosa et al., 20...   \n",
       "1298    1365     3  We used the scikit-learn (Pedregosa et al., 20...   \n",
       "1299    1366     5  Automatic Multi-Document Summarization (MDS) a...   \n",
       "1300    1367     4  Additionally, we train phrase-based machine tr...   \n",
       "1301    1368     4  We used the scikit-learn (Pedregosa et al., 20...   \n",
       "1302    1369     4  Additionally, we train phrase-based machine tr...   \n",
       "1303    1370     4  We train classifiers for each of the above fea...   \n",
       "1304    1371     4  In addition, the corpus was lemmatised using t...   \n",
       "1305    1372     4  They are based on Distributional Hypothesis wh...   \n",
       "1306    1373     3  We used the same test set used in (Li et al. (...   \n",
       "1307    1374     4  We develop translation models using the phrase...   \n",
       "1308    1375     3  We used non-local features based on Finkel et ...   \n",
       "1309    1376     5  For building the baseline SMT system, we used ...   \n",
       "1310    1377     3  For translation, we use Moses (Koehn et al., 2...   \n",
       "1311    1378     4  To recognize explicit connectives, we construc...   \n",
       "1312    1379     3  To construct language models and measure perpl...   \n",
       "1313    1380     4  We use Gibbs sampling to estimate the distribu...   \n",
       "1314    1381     5  Arabizi is not a letter-based transliteration ...   \n",
       "1315    1382     4  To construct language models and measure perpl...   \n",
       "1316    1384     4  We build upon our previous Markov Logic based ...   \n",
       "1317    1385     4  We use the feedforward neural probabilistic la...   \n",
       "1318    1386     5  The SMT systems were built using the Moses too...   \n",
       "1319    1387     5  For building the baseline SMT system, we used ...   \n",
       "1320    1388     4  In-domain SMT: we used the parallel corpus (Se...   \n",
       "1321    1389     3  Following resource collection and construction...   \n",
       "1322    1390     4  One semiautomatic approach to evaluation is RO...   \n",
       "1323    1391     4  We use the Web 1T 5-gram corpus (Brants and Fr...   \n",
       "1324    1392     3  The SemEval-2015 Aspect Based Sentiment Analys...   \n",
       "1325    1393     4  The 2009 Bio NLP shared task (Kim et al., 2009...   \n",
       "1326    1394     4  The constituent context model (CCM) for induci...   \n",
       "1327    1395     4  The CCM is a generative model for the unsuperv...   \n",
       "1328    1397     3  We use the implementation provided by Tai et a...   \n",
       "1329    1398     3  Europarl (Koehn, 2005) is a multilingual paral...   \n",
       "1330    1399     5  The Stanford dependency parser (De Marneffe et...   \n",
       "1331    1400     5  The Stanford dependency parser (De Marneffe et...   \n",
       "1332    1901     4  The major part of data comes from current and ...   \n",
       "1333    1903     2  We compare the proposed model to our implement...   \n",
       "1334    1904     4  It is a phrase-based system built using the Mo...   \n",
       "1335    1905     4  In this section, we first discuss the hybrid t...   \n",
       "1336    1906     4  The Europarl corpus (Koehn, 2005) is built fro...   \n",
       "1337    1907     4  The Europarl corpus (Koehn, 2005) is built fro...   \n",
       "1338    1908     5  We use Adam (Kingma and Ba, 2015) for optimisa...   \n",
       "1339    1909     4  We use the open-source Moses toolkit (Koehn et...   \n",
       "1340    1910     4  We used the mkcls tool in GIZA++ (Och and Ney,...   \n",
       "1341    1911     3  We then use the phrase extraction utility in t...   \n",
       "1342    1912     3  Phrase pairs are extracted from IBM4 alignment...   \n",
       "1343    1914     5  Specifically, we build off the Bayesian block ...   \n",
       "1344    1915     4  Like (Cho & Chai (2000)) , our analysis also p...   \n",
       "1345    1917     5  We propose a relatively simple and nuanced uns...   \n",
       "1346    1918     4  Our work is also related to (Bunescu and Moone...   \n",
       "1347    1919     5  Motivated by previous work, we include a frequ...   \n",
       "1348    1920     5  The highest performance levels were achieved u...   \n",
       "1349    1921     4  In this rest of this paper, we discuss related...   \n",
       "1350    1922     5  In principle, classifiers trained on PDTB data...   \n",
       "1351    1926     5  In the next section we briefly review modeling...   \n",
       "1352    1927     4  This result is statistically significant at p ...   \n",
       "1353    1928     4  Starting with TextRank (Mihalcea and Tarau, 20...   \n",
       "1354    1929     5  Because of our experience with the Weka packag...   \n",
       "1355    1930     4  Dropout (Srivastava et al., 2014) is implement...   \n",
       "1356    1931     3  In-domain SMT: we used the parallel corpus (Se...   \n",
       "1357    1932     5  As mentioned in Section 3, we obtained depende...   \n",
       "1358    1933     5  We tokenize and truecase all of the corpora us...   \n",
       "1359    1935     5  To obtain these we use the Stanford dependency...   \n",
       "1360    1936     5  The NMT models are trained using Adam optimize...   \n",
       "1361    1937     4  The improved alignments gave a gain of Table 8...   \n",
       "1362    1938     5  The statistical significance tests using 95% c...   \n",
       "1363    1939     5  To obtain these we use the Stanford dependency...   \n",
       "1364    1941     4  We use the AdaGrad algorithm (Duchi et al., 20...   \n",
       "1365    1942     5  The other is from (Jeffrey Pennington et al. (...   \n",
       "1366    1943     4  We use the AdaGrad optimizer (Duchi et al., 20...   \n",
       "1367    1944     4  We experiment with the phrase-based statistica...   \n",
       "1368    1945     3  In-domain SMT: we used the parallel corpus (Se...   \n",
       "1369    1946     4  We follow the definition in Cohen et al. (2012...   \n",
       "1370    1947     3  The language model is a 5-gram KenLM (Heafield...   \n",
       "1371    1949     5  As mentioned in Section 3, we obtained depende...   \n",
       "1372    1950     4  Also, we evaluate on the RTE part of the SICK ...   \n",
       "1373    1951     3  Our model has a\" Siamese\" structure (Bromley e...   \n",
       "1374    1953     5  The dialogue act labelling of the corpus follo...   \n",
       "1375    1954     4  We also obtain the dependency parse of the sen...   \n",
       "1376    1955     4  The resulting matrix is weighted using pointwi...   \n",
       "1377    1956     5  We extract structured facts using two methods:...   \n",
       "1378    1957     4  Dropout (Srivastava et al., 2014) is implement...   \n",
       "1379    1958     3  The population distribution was estimated by t...   \n",
       "1380    1959     5  The statistical significance test is also carr...   \n",
       "1381    1960     3  The bootstrap sampling method provides a way f...   \n",
       "1382    1961     3  For medical we use the biomedical data from EM...   \n",
       "1383    1962     5  The MT experiments were carried out using the ...   \n",
       "1384    1963     4  For the theory of Cho & Chai (2000) to be comp...   \n",
       "1385    1964     5  3 With these trees fixed, the partial derivati...   \n",
       "1386    1965     5  Many researchers have considered generating pa...   \n",
       "1387    1966     5  We used Weka (Hall et al., 2009) for all our c...   \n",
       "1388    1967     3  Bilingual Corpora The corpus used in the follo...   \n",
       "1389    1968     4  First, we used the Moses toolkit (Koehn et al....   \n",
       "1390    1969     3  Another parallel corpus is the JRC-Acquis Mult...   \n",
       "1391    1970     3  The parallel corpus is wordaligned using GIZA+...   \n",
       "1392    1972     2  We thus cast MSC as a semantic sentence classi...   \n",
       "1393    1973     5  All these features are inherited from Moses (K...   \n",
       "1394    1974     3  For this purpose we use the Europarl corpus (K...   \n",
       "1395    1975     3  The texts were first automatically segmented a...   \n",
       "1396    1976     5  Secondly, Holmqvist et al. (2012) reordered so...   \n",
       "1397    1977     5  For language modeling, we use the English Giga...   \n",
       "1398    1978     5  For our LDA implementations, we use MALLET (Mc...   \n",
       "1399    1979     5  We use the standard Stanford-style set of depe...   \n",
       "1400    1980     4  Next we evaluate how well the complexity measu...   \n",
       "1401    1981     3  Syntax in EPEC is annotated following the depe...   \n",
       "1402    1983     5  We used the scikit-learn (Pedregosa et al., 20...   \n",
       "1403    1984     5  We used the scikit-learn toolkit to train our ...   \n",
       "1404    1985     5  The training set is used to train the phrase-b...   \n",
       "1405    1986     5  These features were obtained using the Stanfor...   \n",
       "1406    1987     3  We built a 5-gram language model on the Englis...   \n",
       "1407    1988     3  In the other side, the French corpus is part-o...   \n",
       "1408    1989     3  The COMLEX syntax dictionary (Grishman et al.,...   \n",
       "1409    1990     5  We lemmatise the head of each constituent with...   \n",
       "1410    1991     5          with the KenLM toolkit (Heafield, 2011 ).   \n",
       "1411    1992     5  We briefly review the HMM based word alignment...   \n",
       "1412    1993     4  We use the Stanford parser with Stanford depen...   \n",
       "1413    1995     5  We use the Stanford parser with Stanford depen...   \n",
       "1414    1996     4  All the summaries are evaluated using ROUGE (L...   \n",
       "1415    1997     4  The reported confidence intervals were estimat...   \n",
       "1416    1999     4  We implemented CharWNN using the Theano librar...   \n",
       "1417    2000     5  For the linear logistic regression implementat...   \n",
       "1418    2501     4  The tagger we use is TnT (Brants, 2000) , a hi...   \n",
       "1419    2502     3  We have used the implementation described in (...   \n",
       "1420    2503     3  We built a 5-gram language model on the Englis...   \n",
       "1421    2504     3  DELPH-IN Minimal Recursion Semantics (DM) As p...   \n",
       "1422    2505     5  Like the CoNLL-2006 shared task, the 2007 shar...   \n",
       "1423    2506     4  Like (Cho & Chai 2000 ), our analysis also pro...   \n",
       "1424    2507     4  The phrase table is extracted from a bilingual...   \n",
       "1425    2508     4  Google Web 1T (Brants and Franz, 2006) has bee...   \n",
       "1426    2509     5  We first use a dependency parser (de Marneffe ...   \n",
       "1427    2510     5  We first use a dependency parser (de Marneffe ...   \n",
       "1428    2511     5  These analyses provide an alternative but theo...   \n",
       "1429    2512     5  We experiment with the phrase-based statistica...   \n",
       "1430    2513     3  For our corpus, we randomly selected documents...   \n",
       "1431    2514     4  They used the Web-based annotation tool brat (...   \n",
       "1432    2515     4  All corpora were taken from the CHILDES databa...   \n",
       "1433    2516     5  The system generated tweets were evaluated usi...   \n",
       "1434    2517     5  The TOEFL synonym selection task is to select ...   \n",
       "1435    2518     4  We calculate our features using the KenLM tool...   \n",
       "1436    2519     5  We use the scikit implementation of Random For...   \n",
       "1437    2521     3  For this purpose we use the Europarl corpus (K...   \n",
       "1438    2522     5  The statistical significance tests using 95% c...   \n",
       "1439    2523     5  We then run word alignment with GIZA++ (Och an...   \n",
       "1440    2524     4  We built a 5-gram language model on the Englis...   \n",
       "1441    2525     5  We used GIZA++ (Och and Ney, 2003) to align th...   \n",
       "1442    2526     5  The phrase tables were generated by means of s...   \n",
       "1443    2527     5  Zhu et al. (2013) applied Kalman filter model ...   \n",
       "1444    2528     4  ( Koo et al. 2008)  have proposed to use word ...   \n",
       "1445    2529     4  More recently, (Pasha et al., 2014) created MA...   \n",
       "1446    2530     5  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "1447    2531     4  @BULLET Naive Bayes(NB): We use Binomial varia...   \n",
       "1448    2532     5  We use AdaGrad (Duchi et al., 2011)  with the ...   \n",
       "1449    2533     5  The MT experiments were carried out using the ...   \n",
       "1450    2534     5  We use the AdaGrad method (Duchi et al., 2011)...   \n",
       "1451    2535     4  Among the existing sense-tagged corpora, the S...   \n",
       "1452    2536     5  We use the Liblinear Support Vector Machine (S...   \n",
       "1453    2537     5  We build a state of the art phrase-based SMT s...   \n",
       "1454    2538     5  We built a source-to-target PB-SMT model from ...   \n",
       "1455    2539     5  The dictionaries are automatically generated v...   \n",
       "1456    2540     3  There are two main approaches to processing no...   \n",
       "1457    2541     4  We conducted baseline experiments for phraseba...   \n",
       "1458    2542     4  The task of identifying mentions to medical co...   \n",
       "1459    2543     5  We built a source-to-target PB-SMT model from ...   \n",
       "1460    2545     5  We also use MADA+TOKAN (Habash et al., 2009) t...   \n",
       "1461    2546     5  Both training and testing data consist of PubM...   \n",
       "1462    2547     4  We conducted baseline experiments for phraseba...   \n",
       "1463    2548     5  To extract our part-of-speech (POS) features, ...   \n",
       "1464    2549     5  The corpora are tokenised and truecased using ...   \n",
       "1465    2550     5  We trained a model using Moses toolkit (Koehn ...   \n",
       "1466    2551     4  For seed and test paradigms we used verbal inf...   \n",
       "1467    2552     5  Then we did word alignment using GIZA++ (Och a...   \n",
       "1468    2553     4  ( Pang et al. 2002) have reported the effectiv...   \n",
       "1469    2555     5  We train the concept identification stage usin...   \n",
       "1470    2556     3  For the contextual check we use the Google Web...   \n",
       "1471    2557     5  We built a source-to-target PB-SMT model from ...   \n",
       "1472    2558     4  @BULLET Naive Bayes(NB): We use Binomial varia...   \n",
       "1473    2559     4  We built a source-to-target PB-SMT model from ...   \n",
       "1474    2560     5  An unpruned, modified Kneser-Ney-smoothed 4-gr...   \n",
       "1475    2561     4  For native data, several teams make use of the...   \n",
       "1476    2562     4  We measure statistical significance using 95% ...   \n",
       "1477    2563     4  1 The models are constructed using C4.5 decisi...   \n",
       "1478    2564     5  It therefore follows the distributional hypoth...   \n",
       "1479    2565     4  Consider the two examples below, drawn from th...   \n",
       "1480    2566     5  BLE is based on the distributional hypothesis ...   \n",
       "1481    2567     5  We use the BLEU score as primary criterion whi...   \n",
       "1482    2568     4  The relation prediction task of Science IE is ...   \n",
       "1483    2569     3  Machine translation system settings: We used a...   \n",
       "1484    2570     5  We use the MRS analyses that are produced by t...   \n",
       "1485    2571     5  In each case, the improvement of EBMT TM + SMT...   \n",
       "1486    2572     4  To capture typical part-of bridging (see Examp...   \n",
       "1487    2574     5  The surfacesyntactic representation ? p was a ...   \n",
       "1488    2575     5  In our work, like Hernault et al. (2010) , we ...   \n",
       "1489    2576     5  We use the data that were recorded and preproc...   \n",
       "1490    2577     5  We trained non-projective dependency parsers f...   \n",
       "1491    2578     4  Our LP constraints based on the new type marke...   \n",
       "1492    2579     2  The WSJ grammar covers the UPenn Wall Street J...   \n",
       "1493    2580     5  Statistically significant results, calculated ...   \n",
       "1494    2581     4  In testing, we used minimum Bayes risk decodin...   \n",
       "1495    2582     3  selects the translation with minimum Bayes ris...   \n",
       "1496    2583     4  We use a prototype-based selectional preferenc...   \n",
       "1497    2584     4  We used 10-fold cross-validation, set the conf...   \n",
       "1498    2585     3  The Brown Corpus tagged with WordNet senses (M...   \n",
       "1499    2586     5  As an implementation, we use SVM light (Joachi...   \n",
       "1500    2587     4  For translation tables , the Moses system (Koe...   \n",
       "1501    2588     5  The particle filter of Canini et al. (2009) re...   \n",
       "1502    2589     4  We describe an approximation to the BLEU score...   \n",
       "1503    2590     5  We applied the Naive Bayes probabilistic super...   \n",
       "1504    2593     4  We minimize the cross entropy loss using gradi...   \n",
       "1505    2594     5  Statistical significance in BLEU score differe...   \n",
       "1506    2595     5  We measure statistical significance using 95% ...   \n",
       "1507    2596     3  3 As verbs, we take all tags that map to V in ...   \n",
       "1508    2597     5  We use the MRS analyses that are produced by t...   \n",
       "1509    2598     5  We measure statistical significance using 95% ...   \n",
       "1510    2599     5  On Semantic Role Labeling Gildea and Jurafsky ...   \n",
       "1511    2600     4  We apply the stochastic gradient descent algor...   \n",
       "1512    3101     4  We used the Linear SVM implementation (with de...   \n",
       "1513    3102     5  We test the statistical significance of differ...   \n",
       "1514    3103     5  In particular, the parser implements the arc-s...   \n",
       "1515    3104     5  We use Adadelta (Zeiler, 2012) to update the p...   \n",
       "1516    3105     4  We used the English side of the Europarl corpu...   \n",
       "1517    3106     3  CCG is a lexicalized theory of grammar (Steedm...   \n",
       "1518    3107     5  The results for TESLA-M and TESLA-F have previ...   \n",
       "1519    3108     3  The semantic representation is Minimal Recursi...   \n",
       "1520    3109     4  GermaNet (GN) is the German counterpart to WN ...   \n",
       "1521    3110     3  (Baroni et al 2002)  report that 47% of the vo...   \n",
       "1522    3111     4  Both language models use modified Kneser-Ney s...   \n",
       "1523    3112     5  We used MALLET (McCallum, 2002) for this exper...   \n",
       "1524    3114     5  We calculated significance using paired bootst...   \n",
       "1525    3115     5  Additionally, we train phrase-based machine tr...   \n",
       "1526    3116     5  For building the baseline SMT system, we used ...   \n",
       "1527    3117     3  This results in the semantic triple shot(man,b...   \n",
       "1528    3119     4  We used standard classifiers available in scik...   \n",
       "1529    3120     3  Rhetorical Structure Theory (Mann and Thompson...   \n",
       "1530    3121     4  Both language models use modified Kneser-Ney s...   \n",
       "1531    3123     5  We train the concept identification stage usin...   \n",
       "1532    3124     5  For training the translation model and for dec...   \n",
       "1533    3125     5  We use ROUGE score as our evaluation metric (L...   \n",
       "1534    3126     5  For building the baseline SMT system, we used ...   \n",
       "1535    3127     4  We use the Europarl parallel corpus (Koehn, 20...   \n",
       "1536    3128     3  We use Ridge Regression (RR) with l2-norm regu...   \n",
       "1537    3129     4  All data used in our experiments are sentence-...   \n",
       "1538    3130     4  Since the phrase table contains lemmas, the Wi...   \n",
       "1539    3131     5  For this purpose we used the logistic regressi...   \n",
       "1540    3132     5  All our systems are contrasted with a standard...   \n",
       "1541    3133     4  We test our metrics in the setting of the WMT ...   \n",
       "1542    3134     4  These classifiers have been used in related wo...   \n",
       "1543    3135     4  One is from (Turian et al, 2010) , the dimensi...   \n",
       "1544    3136     5  Word alignment using GIZA++ toolkit (Och and N...   \n",
       "1545    3137     5  We found that using AdaGrad (Duchi et al., 201...   \n",
       "1546    3138     5  The Penn Discourse Treebank (PDTB, Prasad et a...   \n",
       "1547    3139     5  The system was trained on the English and Dani...   \n",
       "1548    3140     5  For example, Turian et al. (2010) compared Bro...   \n",
       "1549    3141     5  We use the Moses software package 5 to train a...   \n",
       "1550    3142     3  Past experiences on this system have shown tha...   \n",
       "1551    3143     5  We trained a number of French-English SMT syst...   \n",
       "1552    3144     5  @BULLET Zhang2015: Zhang et al. (2015) propose...   \n",
       "1553    3145     4  All data used in our experiments are sentence-...   \n",
       "1554    3146     5  The major part of data comes from current and ...   \n",
       "1555    3147     5  We expect this restriction is more consistent ...   \n",
       "1556    3148     4  Additionally, we train phrase-based machine tr...   \n",
       "1557    3149     5  Additionally, we train phrase-based machine tr...   \n",
       "1558    3150     4  The decoder searches for the best translation ...   \n",
       "1559    3151     5  The training set is used to train the phrase-b...   \n",
       "1560    3152     5  The usages from the ukWaC are tokenised and le...   \n",
       "1561    3153     4  To quantify the redundancy of structures, we p...   \n",
       "1562    3154     5  Baseline word alignments were obtained by runn...   \n",
       "1563    3156     3  The results displayed in Table 3 are obtained ...   \n",
       "1564    3157     5  We trained a number of French-English SMT syst...   \n",
       "1565    3158     3  The performance of our algorithm is compared w...   \n",
       "1566    3159     3  We use the scikit implementation of SVM (Pedre...   \n",
       "1567    3160     4  For example, Turian et al. (2010) showed that ...   \n",
       "1568    3161     3  EASE uses NLTK (Bird et al., 2009) for POS tag...   \n",
       "1569    3163     5  All our translation systems are based on Moses...   \n",
       "1570    3164     5  The training set is used to train the phrase-b...   \n",
       "1571    3165     5  Additionally, we train phrase-based machine tr...   \n",
       "1572    3166     4  We thank Gbor Recski (HAS Research Institute f...   \n",
       "1573    3167     4  It has been shown in previous work on relation...   \n",
       "1574    3168     4  These algorithms were used to participate in t...   \n",
       "1575    3169     5  Phrase-Based SMT system: a standard non factor...   \n",
       "1576    3171     4  For the theory of Cho & Chai (2000) to be comp...   \n",
       "1577    3172     5  All linguistic annotations needed for features...   \n",
       "1578    3173     3  Algorithm 5 shows a Passive-Aggressive algorit...   \n",
       "1579    3175     4  The task is part of the Semantic Evaluation 20...   \n",
       "1580    3176     4  We used the same annotation guidelines as Zaid...   \n",
       "1581    3177     4  The Stanford parser 1 (Marneffe et al., 2006) ...   \n",
       "1582    3178     5  1 After tokenization , we lemmatize and stem t...   \n",
       "1583    3179     5  Machine translation system settings: We used a...   \n",
       "1584    3180     4  The phrase table is extracted from a bilingual...   \n",
       "1585    3181     3  This architecture is very similar to the frame...   \n",
       "1586    3182     5  Besides using SentiStrength, we use the lexico...   \n",
       "1587    3183     4  Run1: We firstly use the Stanford CoreNLP tool...   \n",
       "1588    3184     4  By setting (n inw and enw(n)=1 for all nodes, ...   \n",
       "1589    3185     5  2 We tested the difference in performance for ...   \n",
       "1590    3186     4  Run1: We firstly use the Stanford CoreNLP tool...   \n",
       "1591    3187     5  All linguistic annotations needed for features...   \n",
       "1592    3188     4  In (Erkan and Radev, 2004 ), the concept of gr...   \n",
       "1593    3189     5  Significance was tested using a paired bootstr...   \n",
       "1594    3194     4  For seed and test paradigms we used verbal inf...   \n",
       "1595    3195     5  We used KenLM (Heafield, 2011) to create 3-gra...   \n",
       "1596    3196     5  We built a 5-gram language model on the Englis...   \n",
       "1597    3197     4  An estimate of the likelihood of a verb taking...   \n",
       "1598    3198     4  We use the Web 1T 5-gram corpus (Brants and Fr...   \n",
       "1599    3199     5  Machine translation system settings: We used a...   \n",
       "1600    3200     5  Machine translation system settings: We used a...   \n",
       "1601    3701     4  A chunk is a minimal , non-recursive structure...   \n",
       "1602    3702     5  MC-30: A subset of RG-65 dataset with 30 word ...   \n",
       "1603    3703     5  We obtained news-peg judgments using the Brat ...   \n",
       "1604    3704     4  We transformed the parse trees in OntoNotes in...   \n",
       "1605    3705     4  Gradient clipping heuristic to prevent the\" ex...   \n",
       "1606    3706     5  We obtained news-peg judgments using the Brat ...   \n",
       "1607    3707     5  We then describe in more detail a modern Chine...   \n",
       "1608    3708     5  In order to assess statistical significance of...   \n",
       "1609    3710     5  Statistical significance in BLEU score differe...   \n",
       "1610    3711     4  The article system builds on the elements of t...   \n",
       "1611    3712     5  For calculating the required frequencies, we u...   \n",
       "1612    3713     3  Phrase pairs were extracted from symmetrized w...   \n",
       "1613    3714     4  We ran all of our experiments in Weka (Hall et...   \n",
       "1614    3715     4  Both corpora were extracted from the open para...   \n",
       "1615    3716     4  We ran all of our experiments in Weka (Hall et...   \n",
       "1616    3717     4  The Spanish-English (S2E) training corpus was ...   \n",
       "1617    3718     4  Type System extends the type system that is bu...   \n",
       "1618    3719     4  For other languages we use the corpora made av...   \n",
       "1619    3720     5  Table 5 compares our reordering model with a r...   \n",
       "1620    3720     4  Table 5 compares our reordering model with a r...   \n",
       "1621    3721     3  These results contradict those given in Zelenk...   \n",
       "1622    3722     5  For example, OntoNotes (Hovy et al., 2006 ), a...   \n",
       "1623    3723     4  As for EJ translation, we use the Stanford par...   \n",
       "1624    3724     4  For both English and German we used the part-o...   \n",
       "1625    3725     4  We train our model on a subset of the WaCkyped...   \n",
       "1626    3726     3  The questions are translated using a phrase-ba...   \n",
       "1627    3727     3  The questions are translated using a phrase-ba...   \n",
       "1628    3728     5  We measure statistical significance using 95% ...   \n",
       "1629    3729     4  As for EJ translation, we use the Stanford par...   \n",
       "1630    3730     3  Words were downcased and lemmatized using the ...   \n",
       "1631    3731     3  English sentences are parsed into dependency s...   \n",
       "1632    3732     5  Both of our systems were based on the Moses de...   \n",
       "1633    3733     4  We also used ANEW (Bradley and Lang, 1999) for...   \n",
       "1634    3734     3  translation at the DiscoMT 2015 workshop (Hard...   \n",
       "1635    3735     3  Establishing and maintaining common ground is ...   \n",
       "1636    3736     4  SentiWordNet score (senti) We used the Senti- ...   \n",
       "1637    3737     3  Europarl 2 (Koehn, 2005 ): it is a corpus of p...   \n",
       "1638    3738     4  We also carried out a chunk-reordering PB-SMT ...   \n",
       "1639    3739     5  WSI is generally considered as an unsupervised...   \n",
       "1640    3740     3  The data used for the experiments described in...   \n",
       "1641    3743     4  For the language model, we used the KenLM tool...   \n",
       "1642    3744     5  First used by Blitzer et al. (2007) , the MDS ...   \n",
       "1643    3748     5  The most famous example would probably be the ...   \n",
       "1644    3749     4  For language modeling, we trained a separate 5...   \n",
       "1645    3750     5  They used the Web-based annotation tool brat (...   \n",
       "1646    3751     5  The classification was conducted, using differ...   \n",
       "1647    3752     4  The Europarl corpus (Koehn, 2005) is built fro...   \n",
       "1648    3753     5  For language modeling, we trained a separate 5...   \n",
       "1649    3754     5  The Spanish-English (S2E) training corpus was ...   \n",
       "1650    3756     4  We use the Stanford CoreNLP caseless tagger fo...   \n",
       "1651    3757     5  All of our models are trained using Nematus (S...   \n",
       "1652    3758     5  We use GIZA++ (Och and Ney, 2003) with its def...   \n",
       "1653    3759     4  We are working with standard tools as DISSECT ...   \n",
       "1654    3760     5  They are based on Distributional Hypothesis wh...   \n",
       "1655    3761     4  Its segmentation model is a class-based hidden...   \n",
       "1656    3762     5  The edit distance kernel was trained with LIBS...   \n",
       "1657    3763     5  The corpora are first tokenized and lowercased...   \n",
       "1658    3764     5  For example, OntoNotes (Hovy et al., 2006 ), a...   \n",
       "1659    3766     3  We use the open-source Moses toolkit (Koehn et...   \n",
       "1660    3767     3  In our experiments, we use the LIBLINEAR packa...   \n",
       "1661    3768     5  The Spanish-English (S2E) training corpus was ...   \n",
       "1662    3769     5  The ICSI meeting corpus (Janin et al., 2003) i...   \n",
       "1663    3770     5  We use ROUGE (Lin, 2004) for evaluating the co...   \n",
       "1664    3771     2  The measure selected is the normalised Pearson...   \n",
       "1665    3772     5  The main corpora we use are Europarl (Koehn, 2...   \n",
       "1666    3773     4  We exploit this monolingual data for training ...   \n",
       "1667    3774     5  We use the Stanford CoreNLP caseless tagger fo...   \n",
       "1668    3775     5  This is a corpus-based metric relying on the d...   \n",
       "1669    3776     3  Similarly, (Turian et al, 2010) evaluated thre...   \n",
       "1670    3777     5  3.3.1 Reference System We compare a number of ...   \n",
       "1671    3778     5  For training SVM classifiers we used LIBSVM pa...   \n",
       "1672    3779     3  The word alignment was trained using GIZA++ (O...   \n",
       "1673    3781     5  The corpus is first word-aligned using a word ...   \n",
       "1674    3782     4  We perform bootstrap resampling with bounds es...   \n",
       "1675    3784     5  We perform bootstrap resampling with bounds es...   \n",
       "1676    3785     4  Markov Logic Networks (MLN) (Richardson and Do...   \n",
       "1677    3786     5  with the training script of the Moses toolkit ...   \n",
       "1678    3787     3  We train for 15 epochs using mini-batch stocha...   \n",
       "1679    3788     3  This confirms the finding of Liu et al. (2012)...   \n",
       "1680    3789     5  For this purpose we use the Europarl corpus (K...   \n",
       "1681    3790     4  We exploit this monolingual data for training ...   \n",
       "1682    3791     4  Our decoder is a stack decoder similar to Koeh...   \n",
       "1683    3792     3  For the English- Spanish and French-English sy...   \n",
       "1684    3794     5  The word alignment was trained using GIZA++ (O...   \n",
       "1685    3795     4  Rhetorical Structure Theory (RST) (Mann and Th...   \n",
       "1686    3796     4  The dataset used for the experiments reported ...   \n",
       "1687    3797     5  A good data source for this is the Europarl Co...   \n",
       "1688    3798     5  The most famous example would probably be the ...   \n",
       "1689    3800     5  Previously (Socher et al, 2011)  used a recurs...   \n",
       "1690    4302     4  The first source is the CoNLL 2003 shared task...   \n",
       "1691    4303     4  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "1692    4305     3  We therefore propose an alternative method bas...   \n",
       "1693    4306     4  We used the implementation of the scikit-learn...   \n",
       "1694    4307     4  Finally, the CoNLL-2003 shared task (Tjong Kim...   \n",
       "1695    4308     5  Further, we sentence-split, tokenized, and lem...   \n",
       "1696    4309     5  The corpora are first tokenized and lowercased...   \n",
       "1697    4310     5  We use Adadelta (Zeiler, 2012) to update the p...   \n",
       "1698    4311     3  The first source is the CoNLL 2003 shared task...   \n",
       "1699    4313     5  We used a 2009 snapshot of Wikipedia, 2 which ...   \n",
       "1700    4314     3  Recently, Hovy et al. (2013) utilized word emb...   \n",
       "1701    4316     4  Finally, the CoNLL-2003 shared task (Tjong Kim...   \n",
       "1702    4317     5  We run our experiments on Europarl (Koehn, 200...   \n",
       "1703    4318     5  We applied bootstrap resampling (Koehn, 2004) ...   \n",
       "1704    4319     5  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "1705    4320     5  The statistical significance test is also carr...   \n",
       "1706    4321     4  This model is motivated by Vector Space Model ...   \n",
       "1707    4322     4  The data was segmented into baseNP parts and n...   \n",
       "1708    4324     3  The NJU-Parser is based on the state-of-the ar...   \n",
       "1709    4325     3  One is a 3-gram language model built using Ken...   \n",
       "1710    4326     5  The word alignment was obtained by running Giz...   \n",
       "1711    4327     5  Translations for English words in the lexical ...   \n",
       "1712    4331     5  All the Language Models (LM) used in our exper...   \n",
       "1713    4332     4  In all experiments, we use the SVM classifier ...   \n",
       "1714    4333     5  We use the Stanford parser to generate a DG fo...   \n",
       "1715    4334     4  Preprocessing: We tokenized the English side o...   \n",
       "1716    4336     3  We use the Moses software package 5 to train a...   \n",
       "1717    4337     3  In-domain SMT: we used the parallel corpus (Se...   \n",
       "1718    4338     5  In-domain SMT: we used the parallel corpus (Se...   \n",
       "1719    4339     5  The first competitive learning based system is...   \n",
       "1720    4340     5  In-domain SMT: we used the parallel corpus (Se...   \n",
       "1721    4341     3  We used predicted collapsed Stanford dependenc...   \n",
       "1722    4342     4  We select as a general-purpose corpus Europarl...   \n",
       "1723    4343     3  We used the lexicalized dependency parser in t...   \n",
       "1724    4344     3  The result of the operation is equivalent to w...   \n",
       "1725    4345     3  The distributional hypothesis of meaning (Harr...   \n",
       "1726    4346     5  Preprocessing: We tokenized the English side o...   \n",
       "1727    4347     3  One is from (Turian et al, 2010) , the dimensi...   \n",
       "1728    4348     5  The thresholds were thoroughly selected depend...   \n",
       "1729    4349     5  In-domain SMT: we used the parallel corpus (Se...   \n",
       "1730    4350     5  The statistical significance test is also carr...   \n",
       "1731    4351     3  We use the Moses software package 5 to train a...   \n",
       "1732    4352     5  We used a 2009 snapshot of Wikipedia, 2 which ...   \n",
       "1733    4353     4  The text was pre-processed using wp2txt 6 to r...   \n",
       "1734    4354     5  We run our experiments on Europarl (Koehn, 200...   \n",
       "1735    4355     4  We use Scikit-learn (Pedregosa et al., 2011 ),...   \n",
       "1736    4356     5  The way they were added is similar to incorpor...   \n",
       "1737    4358     5  We used the lexicalized dependency parser in t...   \n",
       "1738    4359     4  We modified the implementation of the SWELL Ja...   \n",
       "1739    4360     4  The Stanford dependency parser (De Marneffe et...   \n",
       "1740    4361     4  The first work on this topic was done back in ...   \n",
       "1741    4362     3  The OpenFst library is used to perform all of ...   \n",
       "1742    4363     4  For our experiments, we used translated movie ...   \n",
       "1743    4364     3  The Moses15 result is obtained by applying the...   \n",
       "1744    4365     3  The OpenFst library is used to perform all of ...   \n",
       "1745    4366     3  We select as a general-purpose corpus Europarl...   \n",
       "1746    4367     4  We also compare our word embeddings with the E...   \n",
       "1747    4368     4  The relationship between language and sentimen...   \n",
       "1748    4371     3  Support vector machines (SVM) are one of the b...   \n",
       "1749    4372     4  In Barankov and Tamchyna (2014), we experiment...   \n",
       "1750    4373     4  To learn the parameters of the model we minimi...   \n",
       "1751    4374     5  For training the translation model and for dec...   \n",
       "1752    4375     5  The text was pre-processed using wp2txt 6 to r...   \n",
       "1753    4376     3  The task reported on here is to produce PropBa...   \n",
       "1754    4377     4  For both systems, the used training data is fr...   \n",
       "1755    4378     4  We use the Stanford parser to generate a DG fo...   \n",
       "1756    4379     4  We used predicted collapsed Stanford dependenc...   \n",
       "1757    4380     4  We tokenize English data and segment Chinese d...   \n",
       "1758    4381     3  The evaluation emphasis in multi-document summ...   \n",
       "1759    4382     4  We conducted baseline experiments for phrase b...   \n",
       "1760    4383     5  The similarity function is here the Smoothed P...   \n",
       "1761    4384     5  We tokenize English data and segment Chinese d...   \n",
       "1762    4385     3  The Stanford dependency parser (De Marneffe et...   \n",
       "1763    4386     3  To demonstrate the effect of the proposed meth...   \n",
       "1764    4387     5  The baseline systems are built with the openso...   \n",
       "1765    4388     3  3.3.1 Reference System We compare a number of ...   \n",
       "1766    4389     5  Preprocessing: We tokenized the English side o...   \n",
       "1767    4391     3  Finally, it is also noticeable that the percen...   \n",
       "1768    4392     3  As a learning algorithm we adopt a ranking SVM...   \n",
       "1769    4393     2  Brain images are quite noisy, so we used the m...   \n",
       "1770    4394     3  The training set is used to train the phrase-b...   \n",
       "1771    4395     4  Results on English-French, English-Romanian, a...   \n",
       "1772    4396     4  The work presented in Berger et al. (1996) tha...   \n",
       "1773    4398     4  Our second method is based on the recurrent ne...   \n",
       "1774    4399     4  The SUSANNE Corpus is a modified and condensed...   \n",
       "1775    4400     5  The corpora are tokenised and truecased using ...   \n",
       "1776    4901     5  English annotations were all produced using th...   \n",
       "1777    4903     5  The parameters are estimated by Gibbs sampling...   \n",
       "1778    4904     4  The meta-classifier is a linear SVM (Fan et al...   \n",
       "1779    4905     3  We use Adadelta (Zeiler, 2012) to update the p...   \n",
       "1780    4907     5  We train a Support Vector Machine (SVM) (Corte...   \n",
       "1781    4908     5  Parameter tuning is carried out using Z- MERT ...   \n",
       "1782    4909     4  Brin identifies the use of patterns in the dis...   \n",
       "1783    4910     5     We adopt the setting of Socher et al. (2012) .   \n",
       "1784    4912     5  The publicly available tool GIZA++ was used to...   \n",
       "1785    4913     5          CRFSuite implementation (Okazaki, 2007 ).   \n",
       "1786    4914     3  The HMM classifier used in our experiments fol...   \n",
       "1787    4915     5  We computed 4-gram LMs with modified Kneser-Ne...   \n",
       "1788    4916     5  Socher et al. (2011) come closest to our targe...   \n",
       "1789    4917     3  We use Ridge Regression (RR) with l2-norm regu...   \n",
       "1790    4918     3  As for the former (hereafter it is referred to...   \n",
       "1791    4920     5  15 The significance tests were performed using...   \n",
       "1792    4921     5  In this work, we focus on learning with Suppor...   \n",
       "1793    4922     4  We used TreeTagger (Schmid, 1994) to obtain a ...   \n",
       "1794    4923     5  To account for this constraint,  include infor...   \n",
       "1795    4924     3  The system was trained on the English and Dani...   \n",
       "1796    4925     3  We use linear SVMs from LIBLINEAR and SVMs wit...   \n",
       "1797    4926     3  In current phrase-based statistical machine tr...   \n",
       "1798    4928     5  We train a Support Vector Machine (SVM) (Corte...   \n",
       "1799    4929     5  The word alignment was obtained by running Giz...   \n",
       "1800    4930     3  For training SVM classifiers we used LIBSVM pa...   \n",
       "1801    4931     5  We use the standard alignment tool Giza++ (Och...   \n",
       "1802    4932     5  We use the standard alignment tool Giza++ (Och...   \n",
       "1803    4933     3  In the Penn Discourse TreeBank 2.0 (Prasad et ...   \n",
       "1804    4934     4  Weka (Hall et al., 2009) was used to apply lea...   \n",
       "1805    4937     5  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "1806    4938     5  We use the standard alignment tool Giza++ (Och...   \n",
       "1807    4939     3  We used the GIZA++ software (Och and Ney, 2003...   \n",
       "1808    4940     5  The language model used is the 5-gram corpus f...   \n",
       "1809    4941     5  English annotations were all produced using th...   \n",
       "1810    4942     3  For example, (Turian et al, 2010) compared Bro...   \n",
       "1811    4943     3  Distributional representations encode an expre...   \n",
       "1812    4944     4  (Li et al, 2013)  use crowdsourcing to build p...   \n",
       "1813    4945     5  We use the standard alignment tool Giza++ (Och...   \n",
       "1814    4946     4  The formalism that is used to represent the se...   \n",
       "1815    4947     5  For French, Hungarian, Polish and Swedish we u...   \n",
       "1816    4950     4  The English text was tokenized using the word ...   \n",
       "1817    4951     4  We use the MOSES decoder (Koehn et al., 2007) ...   \n",
       "1818    4952     4  Specifically, the sentence compression dataset...   \n",
       "1819    4953     4  use the Stanford Parser (de Marneffe et al., 2...   \n",
       "1820    4954     4  We conducted baseline experiments for phraseba...   \n",
       "1821    4955     4  In practice, the decoder has to employ beam se...   \n",
       "1822    4956     5  The source of bilingual data used in the exper...   \n",
       "1823    4957     4  The tectogrammatical annotation layer is based...   \n",
       "1824    4958     4  In this paper, we use the subjectivity corpus ...   \n",
       "1825    4959     4  We conducted baseline experiments for phraseba...   \n",
       "1826    4960     5  All our systems are contrasted with a standard...   \n",
       "1827    4961     4  A common approach to computing similarity is t...   \n",
       "1828    4962     4  We conducted baseline experiments for phraseba...   \n",
       "1829    4963     3  The Stanford dependency parser (De Marneffe et...   \n",
       "1830    4964     3  We tokenize and frequent-case the data with th...   \n",
       "1831    4965     4  We conducted baseline experiments for phraseba...   \n",
       "1832    4966     4  We conducted baseline experiments for phraseba...   \n",
       "1833    4967     4  For training the translation model and for dec...   \n",
       "1834    4968     4  We tokenize and frequent-case the data with th...   \n",
       "1835    4969     5  We conducted baseline experiments for phraseba...   \n",
       "1836    4970     5  We use the Moses software package 5 to train a...   \n",
       "1837    4971     4  The corpora are tokenised and truecased using ...   \n",
       "1838    4972     5  In contrast, (McClosky et al, 2006) focus on l...   \n",
       "1839    4973     3  We use logistic regression with L2 regularizat...   \n",
       "1840    4974     5  Since the phrase table contains lemmas, the Wi...   \n",
       "1841    4975     4  We use logistic regression with L2 regularizat...   \n",
       "1842    4976     4  The corpora are tokenised and truecased using ...   \n",
       "1843    4977     4  For training the translation model and for dec...   \n",
       "1844    4978     3  We use the Moses software package 5 to train a...   \n",
       "1845    4980     4  We use the Moses software package 5 to train a...   \n",
       "1846    4981     3  Europarl (Koehn, 2005) is a multilingual paral...   \n",
       "1847    4982     5  For all syntactic parsers, we used the\" basic\"...   \n",
       "1848    4983     3  The Stanford dependency parser (De Marneffe et...   \n",
       "1849    4984     5  We built phrase-based machine translation syst...   \n",
       "1850    4985     3  We use the cross-entropy loss function and min...   \n",
       "1851    4986     4  We use the cross-entropy loss function and min...   \n",
       "1852    4987     4  We use the AdaGrad method (Duchi et al., 2011)...   \n",
       "1853    4988     5  Socher et al. (2011)  explored using recursive...   \n",
       "1854    4989     5  2 Translation Model Moses is a state-of-the-ar...   \n",
       "1855    4989     5  2 Translation Model Moses is a state-of-the-ar...   \n",
       "1856    4990     5  2 Translation Model Moses is a state-of-the-ar...   \n",
       "1857    4991     4  For training the translation model and for dec...   \n",
       "1858    4992     5  All our systems are contrasted with a standard...   \n",
       "1859    4993     5  For all syntactic parsers, we used the\" basic\"...   \n",
       "1860    4994     4  can be evaluated by maximising the pseudo-like...   \n",
       "1861    4995     5  The baseline systems are built with the openso...   \n",
       "1862    4996     4  2 Translation Model Moses is a state-of-the-ar...   \n",
       "1863    4997     3  In this approach we used the NERsuite software...   \n",
       "1864    4998     4  2 Translation Model Moses is a state-of-the-ar...   \n",
       "1865    4999     4  These efforts focused exclusively on the meron...   \n",
       "1866    5000     4  Special forms of relatedness are represented i...   \n",
       "1867    5501     5  We train our model on a subset of the WaCkyped...   \n",
       "1868    5502     4  Since the first shared task on Recognising Tex...   \n",
       "1869    5503     5  We used k-best batch MIRA (Cherry and Foster, ...   \n",
       "1870    5504     3  We used the Mallet toolkit (McCallum, 2002) fo...   \n",
       "1871    5505     3  We use the implementation provided by CRFsuite...   \n",
       "1872    5506     5  glish source with target French by using GIZA+...   \n",
       "1873    5507     5  We used ROUGE-N (Lin, 2004) for evaluation of ...   \n",
       "1874    5508     4  1 with 2 -regularization using AdaGrad (Duchi ...   \n",
       "1875    5509     4  We used Weka (Hall et al., 2009) for all our c...   \n",
       "1876    5510     4  POS tagging was performed with TreeTagger (Sch...   \n",
       "1877    5511     5  Word alignments on the parallel corpus are per...   \n",
       "1878    5512     4  We used the implementation of the scikit-learn...   \n",
       "1879    5514     3  The Lexical sample data was parsed using the C...   \n",
       "1880    5515     5  For POS-tagging, we used the Stanford POS-tagg...   \n",
       "1881    5516     5  We used the English side of the Europarl corpu...   \n",
       "1882    5517     3  This data is part of the NUCLE corpus (Dahlmei...   \n",
       "1883    5518     3  The training set is used to train the phrase-b...   \n",
       "1884    5519     4  We used the word alignment produced by Giza (O...   \n",
       "1885    5520     5  For this purpose we use the Europarl corpus (K...   \n",
       "1886    5521     4  Word alignment is performed using GIZA++ (Och ...   \n",
       "1887    5522     5  Word alignment was done with GIZA++ (Och and N...   \n",
       "1888    5524     4  The evaluation results were provided by the or...   \n",
       "1889    5525     3  In future work we can therefore incorporate un...   \n",
       "1890    5526     4  glish source with target French) by using GIZA...   \n",
       "1891    5527     3  We conducted statistical significance tests fo...   \n",
       "1892    5528     5  The word alignment was obtained by running Giz...   \n",
       "1893    5529     5  Word alignment was done with GIZA++ (Och and N...   \n",
       "1894    5531     4  For the theory of Cho & Chai (2000) to be comp...   \n",
       "1895    5532     5  In order to compare our method to a well under...   \n",
       "1896    5534     4  We examine the quality of translations to Engl...   \n",
       "1897    5535     4  Integer Linear Programming (ILP) has recently ...   \n",
       "1898    5536     5  For that purpose, we use the word analogy task...   \n",
       "1899    5537     3  The corpus is first word-aligned using a word ...   \n",
       "1900    5538     3  For ranking, we use the SVM rank ranker (Joach...   \n",
       "1901    5539     3  Parameters are updated using AdaGrad (Duchi et...   \n",
       "1902    5540     3  The tagger we use is TnT (Brants, 2000) , a hi...   \n",
       "1903    5541     3  Sentiment score of the last post of the observ...   \n",
       "1904    5544     4  For German English we also have a system based...   \n",
       "1905    5546     3  We use the Universal POS Tagset (UPOS) of Petr...   \n",
       "1906    5547     3  We also considered an ensemble of our approach...   \n",
       "1907    5548     3  One way to solve this problem is to use a kern...   \n",
       "1908    5549     3  We use Adam (Kingma and Ba, 2015) for optimisa...   \n",
       "1909    5550     3  We build our PB-SMT systems in a standard way ...   \n",
       "1910    5551     4  The starting point for our model is the skipgr...   \n",
       "1911    5553     4  We introduce a new anaphoricity detection mode...   \n",
       "1912    5554     5  The realisation ranking component is an SVM ra...   \n",
       "1913    5556     3  WSI is generally considered as an unsupervised...   \n",
       "1914    5557     4  We run our experiments on Europarl (Koehn, 200...   \n",
       "1915    5558     3  For the language model, we used the KenLM tool...   \n",
       "1916    5559     3  The second collection is constituted by the GE...   \n",
       "1917    5560     4  We introduce a new anaphoricity detection mode...   \n",
       "1918    5563     5  The sentence aligned parallel data is first wo...   \n",
       "1919    5564     3  All the Language Models (LM) used in our exper...   \n",
       "1920    5565     4  They are based on Distributional Hypothesis wh...   \n",
       "1921    5566     4  Finally, we consider the Europarl corpus v7 (K...   \n",
       "1922    5568     3  For the contextual check we use the Google Web...   \n",
       "1923    5569     3  By imposing constraints on the possible word r...   \n",
       "1924    5571     5  Surdeanu et al. (2012) propose a two-layer mul...   \n",
       "1925    5572     3  We have theoretically suggested that based on ...   \n",
       "1926    5574     3  For the contextual check we use the Google Web...   \n",
       "1927    5575     5  ROUGE (Lin, 2004) is a set of evaluation metri...   \n",
       "1928    5576     5  Here we review the parameters of the standard ...   \n",
       "1929    5577     4  The Spanish-English (S2E) training corpus was ...   \n",
       "1930    5578     3  We convert the trees in both treebanks from co...   \n",
       "1931    5579     3  We convert the trees in both treebanks from co...   \n",
       "1932    5580     5  We use KenLM 3 (Heafield, 2011) for computing ...   \n",
       "1933    5581     5  The most famous example would probably be the ...   \n",
       "1934    5582     3  We use the Stanford parser with Stanford depen...   \n",
       "1935    5583     4  Additionally, we train phrase-based machine tr...   \n",
       "1936    5584     4  The training set is used to train the phrase-b...   \n",
       "1937    5585     5  The morpho syntactically annotated corpus we u...   \n",
       "1938    5586     3  There has been a large amount of work on senti...   \n",
       "1939    5587     5  This task setup is further described in the ta...   \n",
       "1940    5588     5  The baseline will be created by the Moses SMT ...   \n",
       "1941    5591     3  Discourse structure in summarization Rhetorica...   \n",
       "1942    5592     3  It builds on the C&C CCG parser (Clark and Cur...   \n",
       "1943    5593     5  We use Boxer (Bos et al., 2004) to parse natur...   \n",
       "1944    5594     5  The training set is used to train the phrase-b...   \n",
       "1945    5595     4  We perform bootstrap resampling with bounds es...   \n",
       "1946    5596     4  The major part of data comes from current and ...   \n",
       "1947    5598     4  For Italian, we use the word2vec to train word...   \n",
       "1948    5599     3  The GATE plugin-based architecture (Cunningham...   \n",
       "1949    5600     3  We trained a 5-gram language model on the Xinh...   \n",
       "1950    6101     5  We used GIZA++ (Och and Ney, 2003) along with ...   \n",
       "1951    6102     3  Our previous MLN-based approach for joint disa...   \n",
       "1952    6103     4  Pang et al. (2002) compare the performance of ...   \n",
       "1953    6104     4  Jans et al. (2012) focused solely on the narra...   \n",
       "1954    6105     4  We applied the Naive Bayes probabilistic super...   \n",
       "1955    6106     4  We therefore seek to allow quick incremental u...   \n",
       "1956    6107     3  We use the Moses software package 5 to train a...   \n",
       "1957    6108     5  It has a much longer average sentence length t...   \n",
       "1958    6109     4  The baseline systems are built with the openso...   \n",
       "1959    6110     4  The corpora are tokenised and truecased using ...   \n",
       "1960    6111     4  The dropout rate was set to 0.5, and the model...   \n",
       "1961    6112     5  The dropout rate was set to 0.5, and the model...   \n",
       "1962    6113     5  We built phrase-based machine translation syst...   \n",
       "1963    6114     3  They had shown that the Penn Discourse TreeBan...   \n",
       "1964    6115     5  They had shown that the Penn Discourse TreeBan...   \n",
       "1965    6117     5  McDonald et al. (2005) present a technique for...   \n",
       "1966    6118     3  We transformed the parse trees in OntoNotes in...   \n",
       "1967    6119     5  2 Translation Model Moses is a state-of-the-ar...   \n",
       "1968    6120     4  For the OntoNotes data sets, Same speaker (Lee...   \n",
       "1969    6121     4  It was one of the best parsers in the CoNLL Sh...   \n",
       "1970    6122     4  We transformed the parse trees in OntoNotes in...   \n",
       "1971    6123     5  Results are reported on the test data using F1...   \n",
       "1972    6125     5  The perplexity achieved by the 6-gram NN LM in...   \n",
       "1973    6126     3  We address the QE problem as a regression task...   \n",
       "1974    6127     5  Finally, it is also noticeable that the percen...   \n",
       "1975    6128     4  Facts such as these are difficult to account f...   \n",
       "1976    6130     4  All these reordering models are tested using M...   \n",
       "1977    6131     5  @BULLET RAE-Subj: Socher et al. (2011) propose...   \n",
       "1978    6132     5  We train a ridge regression model (Scikit-lear...   \n",
       "1979    6133     4  We follow the protocols in Collobert et al. (2...   \n",
       "1980    6134     4  This is equivalent to an SVM with the compound...   \n",
       "1981    6135     4  All experiments are conducted using the Moses ...   \n",
       "1982    6136     3  We use the Moses toolkit (Koehn et al., 2007) ...   \n",
       "1983    6137     4  All experiments were on English part of speech...   \n",
       "1984    6138     4  We use the word alignments to construct a phra...   \n",
       "1985    6139     4  We estimated a hierarchical MT model for the t...   \n",
       "1986    6140     4  A small amount of labeled data is used to map ...   \n",
       "1987    6141     5  We use the word alignments to construct a phra...   \n",
       "1988    6143     3  1 For a reference\" standard\" text we used the ...   \n",
       "1989    6144     4  We worked with the Europarl corpus (Koehn, 200...   \n",
       "1990    6145     5  The organization from SemEval-2013 Task 2: Sen...   \n",
       "1991    6146     5  Pending a planned full evaluation using the Mo...   \n",
       "1992    6147     5  The entity transition features are then used t...   \n",
       "1993    6150     4  Our second method is based on the recurrent ne...   \n",
       "1994    6153     4  We apply the Hidden Markov Model (HMM) (Viterb...   \n",
       "1995    6155     5  We report BLEU (Papineni et al., 2001) of tran...   \n",
       "1996    6156     5  It is a standard phrase-based machine translat...   \n",
       "1997    6158     5  Finally, we used Moses toolkit as phrase-based...   \n",
       "1998    6160     4  Concept similarity is computed using the edge-...   \n",
       "1999    6161     5  Finally, we used Moses toolkit as phrase-based...   \n",
       "2000    6163     5  It is a standard phrase-based machine translat...   \n",
       "2001    6164     4  2 Translation Model Moses is a state-of-the-ar...   \n",
       "2002    6165     5  Otherwise, it is measured by WordNet similarit...   \n",
       "2003    6166     4  We use 5-gram language models with Kneser-Ney ...   \n",
       "2004    6167     4  We adopt online learning, updating parameters ...   \n",
       "2005    6169     3  We used the implementation of the scikit-learn...   \n",
       "2006    6170     4  We adopt online learning, updating parameters ...   \n",
       "2007    6172     5  Finally, we used Moses toolkit as phrase-based...   \n",
       "2008    6173     3  Europarl (Koehn, 2005) is a multilingual paral...   \n",
       "2009    6174     4  We leave the third-order models (Koo and Colli...   \n",
       "2010    6175     4  We used the implementation of the scikit-learn...   \n",
       "2011    6176     5  For English and French a model was trained usi...   \n",
       "2012    6177     3  Empirically we show that our model beats the s...   \n",
       "2013    6178     4  We use logistic regression with L2 regularizat...   \n",
       "2014    6179     3  We use the New York Times Annotated Corpus (Sa...   \n",
       "2015    6180     4  We use the NMF and tf-idf implementations prov...   \n",
       "2016    6181     5  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "2017    6182     3  In addition, the data was tokenized, lemmatize...   \n",
       "2018    6183     5  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "2019    6184     5  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "2020    6185     5  We used the Moses toolkit (Koehn et al., 2007)...   \n",
       "2021    6186     4  Feature selection was performed using chi-squa...   \n",
       "2022    4753     0  Barzilay and Lapata (2008)  propose an entity ...   \n",
       "2023    2789     0  The differences in BLEU points are 0.14 and 0....   \n",
       "2024    2746     0  This type of non-local feature was not used by...   \n",
       "2025    1534     0  This type of non-local feature was not used by...   \n",
       "2026     706     0  Distributional semantics is based on the idea ...   \n",
       "\n",
       "                                              sentence2  \n",
       "0     Recently, there has been a successful attempt ...  \n",
       "1     The first one is the WS-353 dataset (Finkelste...  \n",
       "2     Abstract Meaning Representation (AMR) (Banares...  \n",
       "3     We perform bootstrap resampling with bounds es...  \n",
       "4     It is used to support semantic analyses in the...  \n",
       "5     We first use a dependency parser (de Marneffe ...  \n",
       "6     Alternatively, Rooth et al. (1999)  propose an...  \n",
       "7     The Levenshtein distance gives an indication o...  \n",
       "8     We built phrase-based machine translation syst...  \n",
       "9     We perform bootstrap resampling with bounds es...  \n",
       "10    It is used to support semantic analyses in the...  \n",
       "11    It is used to support semantic analyses in the...  \n",
       "12    It is used to support semantic analyses in the...  \n",
       "13    It is used to support semantic analyses in the...  \n",
       "14    We build upon our previous approach for joint ...  \n",
       "15    Details about SVM and KRR can be found in (Tay...  \n",
       "16    We learn the parameters  using a quasi-Newton ...  \n",
       "17    For direct translation, we use the SCFG decode...  \n",
       "18    This is known as the Distributional Hypothesis...  \n",
       "19    The models , as well as the parser described i...  \n",
       "20    For strings, a lot of such kernel functions ex...  \n",
       "21    The estimation of the semantically Smoothed Pa...  \n",
       "22    We train with the Adam optimizer (Kingma and B...  \n",
       "23    In our experimental study, we use the freely a...  \n",
       "24    OPTIMSRC: (Carpineto and Romano, 2010) showed ...  \n",
       "25    These methods rely on the distributional hypot...  \n",
       "26    Word alignment is done using GIZA++ (Och and N...  \n",
       "27    Word alignment is done using GIZA++ (Och and N...  \n",
       "28    We used Mallet software (McCallum, 2002) for S...  \n",
       "29    The ontology, GoiTaikei, consists of a hierarc...  \n",
       "30    A detailed discussion on the results is provid...  \n",
       "31    The first one is the WS-353 dataset (Finkelste...  \n",
       "32    A framework for human error analysis has been ...  \n",
       "33    MaxEnt classifier is an example of this group ...  \n",
       "34    Blog is one of the crucial, communicative and ...  \n",
       "35                                                       \n",
       "36    For this purpose, we use MADA (Morphological A...  \n",
       "37    Our 5-gram language model was trained on the X...  \n",
       "38    To determine semantic types and subtypes, we t...  \n",
       "39    We use the scikit implementation of SVM (Pedre...  \n",
       "40    Each term in the input text is represented by ...  \n",
       "41    An algorithm, the Kuhn-Munkres method (Kuhn, 1...  \n",
       "42    We use collapsed Gibbs sampling (Griffiths and...  \n",
       "43    We use the Stanford parser with Stanford depen...  \n",
       "44    Weights are initialized using Glorot-Bengio st...  \n",
       "45    Automatic sentence alignment of the training d...  \n",
       "46    We use AdaGrad (Duchi et al., 2011)  with the ...  \n",
       "47    We performed word alignment using GIZA++ (Och ...  \n",
       "48    For example, DIRT (Lin and Pantel, 2001) aims ...  \n",
       "49    The annotation was performed using the BRAT 2 ...  \n",
       "50    A similar semantic similarity measure, propose...  \n",
       "51    It consists of 50 texts taken from the WSJ por...  \n",
       "52    All components take as input the corpus docume...  \n",
       "53    From the pioneering work of (Rapp, 1995 ), BLE...  \n",
       "54    We run our experiments on Europarl (Koehn, 200...  \n",
       "55      TESLA-F was called TESLA in Liu et al. (2010) .  \n",
       "56    For building our SMT systems, the open-source ...  \n",
       "57    Rhetorical Structure Theory (RST) (Mann and Th...  \n",
       "58    In addition, the fix-discount method (Foster e...  \n",
       "59    Memory-based language processing (Daelemans an...  \n",
       "60    We tested the significance of differences usin...  \n",
       "61    In addition, machine translation (MT) systems ...  \n",
       "62    1 -regularization using AdaGrad (Duchi et al.,...  \n",
       "63    The MUC score (Vilain et al., 1995) counts the...  \n",
       "64    We will focus on the syntactic tree kernel des...  \n",
       "65    The Europarl corpus (Koehn, 2005) is built fro...  \n",
       "66    The module was implemented using Foma, a free ...  \n",
       "67    We used the same test data as in Li et al. (20...  \n",
       "68    Penn Discourse Treebank The Penn Discourse Tre...  \n",
       "69    We used a GB implementation of the scikit-lear...  \n",
       "70    The second is the MEN dataset (Bruni et al., 2...  \n",
       "71    The training data released by the task organiz...  \n",
       "72    All of the machine learning was done using sci...  \n",
       "73    It has been long identified in NLP that a dive...  \n",
       "74    In our system, we used the sentiment lexicon p...  \n",
       "75    We also compared the quality of the candidate ...  \n",
       "76    Corpus-based VSMs follow the standard distribu...  \n",
       "77    The annotations were made using the BRAT rapid...  \n",
       "78    Compared to WordNet (Fellbaum, 1998 ), there a...  \n",
       "79    We use Adam (Kingma and Ba, 2015) for optimiza...  \n",
       "80    Specifically, we use the LIBLINEAR SVM package...  \n",
       "81    The first two experiments involve predicting t...  \n",
       "82    We used only the non-ensembled left-to-right r...  \n",
       "83    We used only the non-ensembled left-to-right r...  \n",
       "84    The MSD morphological coding system is a posit...  \n",
       "85    The phrase table was generated employing symme...  \n",
       "86    Word alignment is performed with Giza++ (Och a...  \n",
       "87    Following Koo et al. (2008) , we also experime...  \n",
       "88    Raghavan et al. (2007)  evaluate benefit from ...  \n",
       "89    Training and querying of a modified Kneser-Ney...  \n",
       "90    The formal derivation can be found in (Ando an...  \n",
       "91    The model as described thus far is identical t...  \n",
       "92    The baseline systems are built with the openso...  \n",
       "93    The classifier evaluations were carried out us...  \n",
       "94    The training data for the task is from the NUC...  \n",
       "95    SALDO (Borin et al., 2013) is the most compreh...  \n",
       "96    For our SMT experiments, we use the Moses tool...  \n",
       "97    However such string-to-tree systems run slowly...  \n",
       "98    We trained an English 5-gram language model us...  \n",
       "99    For our SMT experiments, we use the Moses tool...  \n",
       "100   @BULLET OntoNotes-Test: Test set of the OntoNo...  \n",
       "101   We use the state-of-the-art phrase-based machi...  \n",
       "102   Since the training data used in Li et al. (200...  \n",
       "103   BLEU score: This score measures the precision ...  \n",
       "104   The training data provided for the task is a s...  \n",
       "105   We selected the dataset of Jurgens and Klapaft...  \n",
       "106   The statistical significance tests using 95% c...  \n",
       "107   We evaluate our relation extraction system on ...  \n",
       "108   Sentences Involving Compositional Knowledge (S...  \n",
       "109   Our novel parsing model is the Dynamic Conditi...  \n",
       "110   Combination of latent topics ent Dirichlet All...  \n",
       "111   It therefore follows the distributional hypoth...  \n",
       "112   It therefore follows the distributional hypoth...  \n",
       "113   In 2009, Yefang Wang (Wang et al., 2009) used ...  \n",
       "114   We built a modified Kneser-Ney smoothed 5-gram...  \n",
       "115   The other models are trained on native English...  \n",
       "116   We then apply bootstrapping (Kozareva et al., ...  \n",
       "117   It therefore follows the distributional hypoth...  \n",
       "118   Our hypothesis is that the LTAG based features...  \n",
       "119   RG-65: (Rubenstein and Goodenough, 1965) is se...  \n",
       "120   Statistical significance tests are performed u...  \n",
       "121   The search is typically carried out using the ...  \n",
       "122   In-domain data only solves the problem of data...  \n",
       "123   All the experiments are carried out in Moses t...  \n",
       "124   We then apply heuristics to determine number a...  \n",
       "125   For this task we use RankSVM (Joachims, 2002) ...  \n",
       "126   A precise description of the corpus and metric...  \n",
       "127   Tree Kernel (TK) functions are convolution ker...  \n",
       "128   For disambiguation and clustering we build upo...  \n",
       "129   We used the ROUGE-1 evaluation metric (Lin, 20...  \n",
       "130   We used Mallet toolkit (McCallum, 2002) for CR...  \n",
       "131   Our joint parsing model exploits a transition-...  \n",
       "132   The annotations were made using the BRAT rapid...  \n",
       "133   To build the word alignment models we used the...  \n",
       "134   We evaluated annotation reliability by using t...  \n",
       "135   Distributional models of meaning follow the di...  \n",
       "136   This dataset is composed of 32 sentence quadru...  \n",
       "137   A statistical significance test was performed ...  \n",
       "138   We use the Giza++ tool (Och and Ney, 2003) to ...  \n",
       "139   We use the Giza++ tool (Och and Ney, 2003) to ...  \n",
       "140   Gaussian process regression (GPR) (Rasmussen a...  \n",
       "141   In another work, a corpus of roughly 1.6 Teraw...  \n",
       "142   We calculated significance using paired bootst...  \n",
       "143   In this work, we use the Stanford neural depen...  \n",
       "144   MADAMIRA (Pasha et al., 2014) is a morphologic...  \n",
       "145   Yarowsky (1995) proposed such a method for wor...  \n",
       "146   We also list the results from SMT model (Durra...  \n",
       "147   We use the Adam (Kingma and Ba, 2014) algorith...  \n",
       "148   Word co-occurence statistics\" You shall know a...  \n",
       "149   The co-occurrence Word Space is acquired throu...  \n",
       "150   We used the scikit-learn toolkit to train our ...  \n",
       "151   The probability p(E) is computed using a simpl...  \n",
       "152   This is similar to the operator Intro in (Kapl...  \n",
       "153   We used a GB implementation of the scikit-lear...  \n",
       "154   For the linear logistic regression implementat...  \n",
       "155   Finally, we used Moses toolkit as phrase-based...  \n",
       "156   Baseline word alignments were obtained by runn...  \n",
       "157   The language model is a 5-gram with interpolat...  \n",
       "158   We used the scikit-learn toolkit to train our ...  \n",
       "159   It is a standard phrase-based machine translat...  \n",
       "160   The WordNet on-line lexical database (Miller e...  \n",
       "161   We used a GB implementation of the scikit-lear...  \n",
       "162   We used the Stuttgart TreeTagger (Schmid, 1994...  \n",
       "163   Part-of-speech tagging was accomplished using ...  \n",
       "164   The annotation was performed manually using th...  \n",
       "165   We participated in both subtask A and B of Sem...  \n",
       "166   We tokenise the text using the default tokenis...  \n",
       "167   In addition, the data was tokenized, lemmatize...  \n",
       "168   We built phrase-based machine translation syst...  \n",
       "169   The corpora are tokenised and truecased using ...  \n",
       "170   We built a trigram language model with Kneser-...  \n",
       "171   We tokenise the text using the default tokenis...  \n",
       "172   We built phrase-based machine translation syst...  \n",
       "173   For training the translation model and for dec...  \n",
       "174   Scope-ignorant (Disambig.): Our previous MLN-b...  \n",
       "175   We use ADAM (Kingma and Ba, 2014) with a learn...  \n",
       "176   For training the translation model and for dec...  \n",
       "177   The corpora are tokenised and truecased using ...  \n",
       "178   For our SMT experiments, we use the Moses tool...  \n",
       "179   First, we used the Moses toolkit (Koehn et al....  \n",
       "180   The baseline systems are built with the openso...  \n",
       "181   We built phrase-based machine translation syst...  \n",
       "182   We employed the TnT tagger (Brants, 2000) whic...  \n",
       "183   In addition, the data was tokenized, lemmatize...  \n",
       "184   We build a state of the art phrase-based SMT s...  \n",
       "185   TF-IDF is a standard statistical method that c...  \n",
       "186   We assess statistical significance of the diff...  \n",
       "187   A framework for human error analysis has been ...  \n",
       "188   Chang et al. (2009) stated that one reason is ...  \n",
       "189   The MMA system (Kruengkrai et al., 2009) train...  \n",
       "190   We build a state of the art phrase-based SMT s...  \n",
       "191   We built phrase-based machine translation syst...  \n",
       "192   conducted using the Moses phrase-based decoder...  \n",
       "193   In addition, the data was tokenized, lemmatize...  \n",
       "194   We measure significance of results using boots...  \n",
       "195   We followed the encoder-decoder architecture w...  \n",
       "196   We built a trigram language model with Kneser-...  \n",
       "197   The decoder is implemented with Weighted Finit...  \n",
       "198   The decoder is implemented with Weighted Finit...  \n",
       "199   In addition, the data was tokenized, lemmatize...  \n",
       "200   Our first baseline is MI09, a distantly superv...  \n",
       "201   We used the Scikit-learn machine learning libr...  \n",
       "202   This in turn relies on the same underlying fea...  \n",
       "203   This in turn relies on the same underlying fea...  \n",
       "204   MADAMIRA (Pasha et al., 2014) is a morphologic...  \n",
       "205   We built a modified Kneser- Ney smoothed 5-gra...  \n",
       "206   We preprocessed the training corpora with scri...  \n",
       "207   We performed word alignment using GIZA++ (Och ...  \n",
       "208   As our learner, we use LIBSVM with a linear ke...  \n",
       "209   We specify our dynamic programming algorithm a...  \n",
       "210   We use the Stanford dependency parser (de Marn...  \n",
       "211   In order to detect the object pronouns, we emp...  \n",
       "212   We also obtain the dependency parse of the sen...  \n",
       "213   The question classifier used in the experiment...  \n",
       "214   We use the Stanford dependency parser (de Marn...  \n",
       "215   The former that is the most popular relies on ...  \n",
       "216   MADAMIRA (Pasha et al., 2014) is a morphologic...  \n",
       "217   We also obtain the dependency parse of the sen...  \n",
       "218   The log-linear approach to phrase-based transl...  \n",
       "219   Both CDS corpora are available from the CHILDE...  \n",
       "220   For the phrase-based SMT system, we adopted th...  \n",
       "221   We built a 5-gram language model on the Englis...  \n",
       "222   We evaluated our model on a semantic relation ...  \n",
       "223   We performed word alignment using GIZA++ (Och ...  \n",
       "224   For all experiments, we used the Moses SMT sys...  \n",
       "225   The statistical significance tests using 95% c...  \n",
       "226   The SMT systems were built using the Moses too...  \n",
       "227   The sentences are fed into the PET HPSG parser...  \n",
       "228   The 5-gram target language model was trained u...  \n",
       "229   First, we present an algorithm for estimating ...  \n",
       "230   Similar row vectors in T indicate similar cont...  \n",
       "231   Similar row vectors in T indicate similar cont...  \n",
       "232   We first use a dependency parser (de Marneffe ...  \n",
       "233   We also carried out a chunk-reordering PB-SMT ...  \n",
       "234   conducted using the Moses phrase-based decoder...  \n",
       "235   The test set was tagged with the French TreeTa...  \n",
       "236   For the parsing experimens I used MaltParser (...  \n",
       "237   We used a GB implementation of the scikit-lear...  \n",
       "238   These features were obtained using the Stanfor...  \n",
       "239   We used Adam as the optimizer (Kingma and Ba, ...  \n",
       "240   This is done using IBM Model 1 (Brown et al., ...  \n",
       "241   For the French side, the TreeTagger (Schmid, 1...  \n",
       "242   conducted using the Moses phrase-based decoder...  \n",
       "243   POS tagging is performed using the IMS Tree Ta...  \n",
       "244   For word alignments, we used Mgiza++ (Gao and ...  \n",
       "245     All the data come from Europarl (Koehn, 2005 ).  \n",
       "246   The Polish data is taken from the EUROPARL cor...  \n",
       "247   Distributional similarity relies on the distri...  \n",
       "248   The SMT systems were built using the Moses too...  \n",
       "249   In our MT experiments, we translate French int...  \n",
       "250   Statistical significance is tested on the BLEU...  \n",
       "251   The baseline will be created by the Moses SMT ...  \n",
       "252   We use the Moses phrase-based translation syst...  \n",
       "253   For our SMT experiments, we use the Moses tool...  \n",
       "254   A popular measure of this association is point...  \n",
       "255   The questions are translated using a phrase-ba...  \n",
       "256   POS tagging is performed using the IMS Tree Ta...  \n",
       "257   For both we use TreeTagger (Schmid, 1994) with...  \n",
       "258   The Europarl corpus (Koehn, 2005) is built fro...  \n",
       "259   The annotation was performed using the BRAT 2 ...  \n",
       "260   The Polish data is taken from the EUROPARL cor...  \n",
       "261   HCRC is tagged with TnT (Brants, 2000 ), train...  \n",
       "262   For German-English we also have a system based...  \n",
       "263   An alternative representation for baseNPs has ...  \n",
       "264   The SMT systems were built using the Moses too...  \n",
       "265   The baseline will be created by the Moses SMT ...  \n",
       "266   Distributional models of meaning follow the di...  \n",
       "267   Recently, Naim et al. (2014) proposed a fully ...  \n",
       "268   Distributional models of meaning follow the di...  \n",
       "269   In the following we will describe the system w...  \n",
       "270   It was built with the Moses toolkit (Koehn et ...  \n",
       "271   Combinatory Categorial Grammar (CCG) (Steedman...  \n",
       "272   We used the Linear SVM implementation (with de...  \n",
       "273   The article of Blei et al. (2003) compares LDA...  \n",
       "274   Gimpel et al. (2011)  provided a dataset of PO...  \n",
       "275   In order to evaluate the fluency of each syste...  \n",
       "276   The corpus was then automatically tagged with ...  \n",
       "277   The improvement is statistically significant a...  \n",
       "278   The learner is implemented as a ranking compon...  \n",
       "279   A statistical significance test was performed ...  \n",
       "280   We use the Punkt sentence splitter from NLTK (...  \n",
       "281   We find this method provides an additional 1.5...  \n",
       "282   In particular, the contribution of this paper ...  \n",
       "283   The BioScope corpus is a manually annotated co...  \n",
       "284   We apply the Stanford coreference resolution s...  \n",
       "285   Europarl 2 (Koehn, 2005 ): it is a corpus of p...  \n",
       "286   We used a GB implementation of the scikit-lear...  \n",
       "287   We used the scikit-learn toolkit to train our ...  \n",
       "288   Automatic text summarization aims to automatic...  \n",
       "289   For our SMT experiments, we use the Moses tool...  \n",
       "290   We used the scikit-learn (Pedregosa et al., 20...  \n",
       "291   For the phrase-based SMT system, we adopted th...  \n",
       "292   We train Random Forest classifiers (Breiman, 2...  \n",
       "293   1 The corpus is lemmatised and tagged by part-...  \n",
       "294   Distributional models of meaning follow the di...  \n",
       "295   But we randomly selected 90% of the training d...  \n",
       "296   Additionally, we train phrase-based machine tr...  \n",
       "297   For example, Finkel et al. (2005) enabled the ...  \n",
       "298   For all experiments, we used the Moses SMT sys...  \n",
       "299   For our SMT experiments, we use the Moses tool...  \n",
       "300   In addition, we show that the latent represent...  \n",
       "301   Both language models use modified Kneser-Ney s...  \n",
       "302   We use the Gibbs sampling based LDA (Griffiths...  \n",
       "303   1 Arabic transliteration is presented in the B...  \n",
       "304   Both language models use modified Kneser-Ney s...  \n",
       "305   Scope-ignorant (Disambig.): Our previous MLN-b...  \n",
       "306   We follow the neural network architecture of V...  \n",
       "307   We trained a number of French-English SMT syst...  \n",
       "308   For our SMT experiments, we use the Moses tool...  \n",
       "309   SMT translations: a phrase-based SMT system bu...  \n",
       "310   We trained a model using Moses toolkit (Koehn ...  \n",
       "311   The method, ROUGE (Lin and Hovy, 2003 ), is ba...  \n",
       "312   We used the unigram counts from the Web 1T 5-g...  \n",
       "313   This paper describes the approach of the SemaN...  \n",
       "314   The recent BioNLP 2009 shared task (BioNLP09ST...  \n",
       "315   The CCM is a generative model for the unsuperv...  \n",
       "316   The idea of representing a constituent by its ...  \n",
       "317   We use the dependency tree long short-term mem...  \n",
       "318   We use the French- English parallel corpus (ap...  \n",
       "319   We used the Stanford Dependency Parser (de Mar...  \n",
       "320   We used the Stanford Dependency Parser (de Mar...  \n",
       "321   The data used for the experiments described in...  \n",
       "322   The model architecture, shown in figure 1 , is...  \n",
       "323   SMT translations: a phrase-based SMT system bu...  \n",
       "324   In Lu et al. (2008) , the mixgram model (an in...  \n",
       "325   We use the French- English parallel corpus (ap...  \n",
       "326   We use the French- English parallel corpus (ap...  \n",
       "327   Each model was trained during 50 epochs using ...  \n",
       "328   We use the Moses phrase-based translation syst...  \n",
       "329   We also used Giza++ word alignment tool (Och a...  \n",
       "330   We use the Moses phrase-based translation syst...  \n",
       "331   Phrase pairs were extracted from symmetrized w...  \n",
       "332   Our work is motivated by the Bayesian HMM appr...  \n",
       "333   Given the LP constraints in (4), (7), and (8),...  \n",
       "334   As mentioned above, we extend the WMF model pr...  \n",
       "335   The dependency path is the shortest path betwe...  \n",
       "336   Outside of the rhetorical features, the discou...  \n",
       "337   Table 8 to Table 12 show the Macro-Average (F ...  \n",
       "338   This paper describes the details of our system...  \n",
       "339   We used the English side of the Europarl corpu...  \n",
       "340   We briefly review the HMM based word alignment...  \n",
       "341   The improvement is statistically significant a...  \n",
       "342   In the unsupervised approach, graph-based rank...  \n",
       "343   Especially, we use the WEKA (Hall et al., 2009...  \n",
       "344   Dropout (Srivastava et al., 2014) is a very ef...  \n",
       "345   For our SMT experiments, we use the Moses tool...  \n",
       "346   We make use of dependency parse information fr...  \n",
       "347   We preprocessed the training corpora with scri...  \n",
       "348   We use the Stanford dependency parser (de Marn...  \n",
       "349   All parameters are learned by Adam optimizer (...  \n",
       "350   One of the phrase-based systems moreover utili...  \n",
       "351   A statistical significance test was performed ...  \n",
       "352   We use the Stanford dependency parser (de Marn...  \n",
       "353   We use online learning to train model paramete...  \n",
       "354   We use the 100-dimensional GloVe word embeddin...  \n",
       "355   We use online learning to train model paramete...  \n",
       "356   We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "357   The SMT systems were built using the Moses too...  \n",
       "358   We also use these perturbation schemes to crea...  \n",
       "359   We trained an English 5-gram language model us...  \n",
       "360   We make use of dependency parse information fr...  \n",
       "361   The second is the RTE part of the SICK dataset...  \n",
       "362   Most previous work use sentence modeling with ...  \n",
       "363   The classes used to train the DATE tagger are ...  \n",
       "364   To examine the effect of normalization on depe...  \n",
       "365   We construct word-word co-occurrence matrix X;...  \n",
       "366   We extract facts from captions using Clausie (...  \n",
       "367   Next, the output of the max-pooling layer is p...  \n",
       "368   The bootstrap sampling method provides a way f...  \n",
       "369   The significance testing is performed by paire...  \n",
       "370   The population distribution was estimated by t...  \n",
       "371   For French-English experiments, we used the EM...  \n",
       "372   The translation system is trained using the we...  \n",
       "373   Then we revise the two LP constraints of Cho &...  \n",
       "374   Derivatives are computed efficiently via backp...  \n",
       "375   Distributional models of meaning follow the di...  \n",
       "376   We use the WEKA toolkit (Hall et al., 2009) fo...  \n",
       "377   The experiments are carried out on a subset of...  \n",
       "378   The Moses toolkit (Koehn et al., 2007) is used...  \n",
       "379                 corpus (Steinberger et al., 2006 ).  \n",
       "380   This is done using IBM Model 1 (Brown et al., ...  \n",
       "381   We compare the proposed model to our implement...  \n",
       "382   All the experiments are carried out in Moses t...  \n",
       "383   For our experiments, we use 40,000 sentences f...  \n",
       "384   The data was tagged using TnT (Brants, 2000 ),...  \n",
       "385   Holmqvist et al. (2012) presented a method whe...  \n",
       "386   Language Model: For all 3-SCFG systems we use ...  \n",
       "387   For training bilingual topic models, we use Ma...  \n",
       "388   We use the Stanford parser with Stanford depen...  \n",
       "389   We also compare annotation strategies in terms...  \n",
       "390   For the German experiments, we used the NEGRA ...  \n",
       "391   We use the scikit implementation of SVM (Pedre...  \n",
       "392   For Indep-Logistic, we used scikit-learn (Pedr...  \n",
       "393   We trained a standard Moses baseline (Koehn et...  \n",
       "394   We use the Stanford parser with Stanford depen...  \n",
       "395   One is a 3-gram language model built using Ken...  \n",
       "396   1 The corpus is lemmatised and tagged by part-...  \n",
       "397   The COMLEX syntax dictionary (Grishman et al.,...  \n",
       "398   POS tagging was performed with the TreeTagger ...  \n",
       "399   For language modeling we used the KenLM toolki...  \n",
       "400   Och is the HMM alignment model of (Och and Ney...  \n",
       "401   These features were obtained using the Stanfor...  \n",
       "402   We use the standard Stanford-style set of depe...  \n",
       "403   For the summarization task, we compare results...  \n",
       "404   We calculated significance using paired bootst...  \n",
       "405   Gradients computed using the automatic differe...  \n",
       "406   For Indep-Logistic, we used scikit-learn (Pedr...  \n",
       "407   The data was tagged using TnT (Brants, 2000 ),...  \n",
       "408   1 Regarding the learning algorithm, we used ge...  \n",
       "409   We build our PB-SMT systems in a standard way ...  \n",
       "410   As output, the grammar delivers detailed seman...  \n",
       "411   One of the first venues at which domain adapta...  \n",
       "412   However, like Cho & Chai (2000) , our analysis...  \n",
       "413   A core component of every PBSMT system is the ...  \n",
       "414   The Google Web 1T data (Brants and Franz, 2006...  \n",
       "415   A complete set of parser tags and the method u...  \n",
       "416   A complete set of parser tags and the method u...  \n",
       "417   Liang et al. (2006) observe that standard upda...  \n",
       "418   We present a system that takes a general Moses...  \n",
       "419   New York Times consists of 500 random sentence...  \n",
       "420   We consider the following types of implicit re...  \n",
       "421   The eval-uation set was comprised of adult utt...  \n",
       "422   The summaries from the above algorithm for the...  \n",
       "423   The TOEFL synonym dataset (Landauer and Dumais...  \n",
       "424   1 We used the KenLM toolkit (Heafield, 2011) t...  \n",
       "425   The regressor used is a Random Forest Regresso...  \n",
       "426   The statistical dictionary for this task was e...  \n",
       "427   We used bootstrap resampling for testing stati...  \n",
       "428   We used the GIZA++ software (Och and Ney, 2003...  \n",
       "429   We calculate our features using the KenLM tool...  \n",
       "430   We then run word alignment with GIZA++ (Och an...  \n",
       "431   The word alignment was obtained by running Giz...  \n",
       "432   In the scenario of human-computer interactive ...  \n",
       "433   In order to reduce the amount of annotated dat...  \n",
       "434   Next, the files are processed with the morphol...  \n",
       "435   For building our SMT systems, the open-source ...  \n",
       "436   We use the Bernoulli Naive Bayes classifier in...  \n",
       "437   We use online learning to train model paramete...  \n",
       "438   The baseline systems are built with the openso...  \n",
       "439   We use online learning to train model paramete...  \n",
       "440   The SEMCOR corpus (Miller et al., 1994) is one...  \n",
       "441   A linear-kernel Support Vector Machine (Chang ...  \n",
       "442   We tokenize and truecase all of the corpora us...  \n",
       "443   We build a state of the art phrase-based SMT s...  \n",
       "444   We then made use of the GIZA++ software (Och a...  \n",
       "445   There are two approaches proposed in the liter...  \n",
       "446   We trained a model using Moses toolkit (Koehn ...  \n",
       "447   We found the largest of such corpus to be the ...  \n",
       "448   We use the Moses software package 5 to train a...  \n",
       "449   We use the MADA package (Habash et al., 2009) ...  \n",
       "450   The data provided for the shared task is prepa...  \n",
       "451   We built a source-to-target PB-SMT model from ...  \n",
       "452   We used the Brill tagger provided by NLTK for ...  \n",
       "453   We tokenize and truecase all of the corpora us...  \n",
       "454   Our baseline is a phrase-based MT system train...  \n",
       "455   For English, we used the CELEX database (Baaye...  \n",
       "456   We then run word alignment with GIZA++ (Och an...  \n",
       "457   Pang et al. (2002)  use machine learning metho...  \n",
       "458   We use online learning to train model paramete...  \n",
       "459   The Web1T corpus (Brants and Franz, 2006) is a...  \n",
       "460   The corpora are tokenised and truecased using ...  \n",
       "461   @BULLET Logistic Regression(LR): We use Logist...  \n",
       "462   We tokenize and frequent-case the data with th...  \n",
       "463   The language model is a standard 5-gram model ...  \n",
       "464   We used the unigram counts from the Web 1T 5-g...  \n",
       "465   The column\" pair-CI\" shows 95% confidence inte...  \n",
       "466   The Weka SMO implementation of SVM (Hall et al...  \n",
       "467   Distributional similarity relies on the distri...  \n",
       "468   Pitler and Nenkova (2008) used discourse relat...  \n",
       "469   Distributional similarity relies on the distri...  \n",
       "470   The optimization is done using the Downhill Si...  \n",
       "471   The SemEval-2010 Task 8 dataset is a widely us...  \n",
       "472   We compare the model against the Moses phrase-...  \n",
       "473   The experiments are carried out on a broad-cov...  \n",
       "474   In all cases, results are statistically signif...  \n",
       "475   We extract a list containing around 4,000 rela...  \n",
       "476   The number of features extracted from the PDT ...  \n",
       "477   Similar to the work of Hernault et al. (2010) ...  \n",
       "478   1 Full details of the experimental protocol, d...  \n",
       "479   For non-projective parsing experiments, four l...  \n",
       "480   For the theory of Cho & Chai (2000) to be comp...  \n",
       "481   The approach presented in this paper is a firs...  \n",
       "482   We calculated significance using paired bootst...  \n",
       "483   selects the translation with minimum Bayes ris...  \n",
       "484   Additionally, we will compare two decision rul...  \n",
       "485   We build on a recent selectional preference mo...  \n",
       "486   ROUGE-2 metric (Lin, 2004) is used for the eva...  \n",
       "487   The senses in WordNet are ordered according to...  \n",
       "488   As a supervised classifier for VSM and WIKI, w...  \n",
       "489   We compare the model against the Moses phrase-...  \n",
       "490   The particle filter studied empirically by Can...  \n",
       "491   We here describe a linear approximation to the...  \n",
       "492   We conducted experiments using Multinomial Nai...  \n",
       "493   The network is trained using SGD with shuffled...  \n",
       "494   Statistical significance of the difference bet...  \n",
       "495   For all results, we computed their confidence ...  \n",
       "496   In the POS tag level, we basically used the un...  \n",
       "497   Here, we use (1) the Link Grammar Parser 8 and...  \n",
       "498   For statistical significance testing, we use a...  \n",
       "499   Gildea and Jurafsky (2002)  were the first to ...  \n",
       "500   We use a minibatch size of 100, and use AdaDel...  \n",
       "501   We use the scikit implementation of SVM (Pedre...  \n",
       "502   Statistical significance tests are performed u...  \n",
       "503   1 The parser implements the arc-standard algor...  \n",
       "504   In the training procedure, we use AdaDelta (Ze...  \n",
       "505   The systems for the English ? Spanish translat...  \n",
       "506   The input of the Boxer system is a syntactic a...  \n",
       "507   TESLA (Translation Evaluation of Sentences wit...  \n",
       "508   The ERG produces Minimal Recursion Semantics (...  \n",
       "509   Future work could be to extend the German data...  \n",
       "510   Baroni et al. (2002)  analyzed the 28 million ...  \n",
       "511   5-gram language models are trained over the ta...  \n",
       "512   10 We used the PLTM implementation in Mallet (...  \n",
       "513   The statistical significance test is also carr...  \n",
       "514   Finally, we used Moses toolkit as phrase-based...  \n",
       "515   We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "516   We also lemmatized all words using Stanford Co...  \n",
       "517   3 We used logistic regression (though linear S...  \n",
       "518   The closest area to our work consists of inves...  \n",
       "519   5-gram language models are trained over the ta...  \n",
       "520              1 using AdaGrad (Duchi et al., 2011 ).  \n",
       "521   All our translation systems are based on Moses...  \n",
       "522   We expect this restriction is more consistent ...  \n",
       "523   For training the translation model and for dec...  \n",
       "524   We extract our paraphrase grammar from the Fre...  \n",
       "525   @BULLET Logistic Regression(LR): We use Logist...  \n",
       "526   We tokenize English data and segment Chinese d...  \n",
       "527   1 The corpus is lemmatised and tagged by part-...  \n",
       "528   We use logistic regression with L2 regularizat...  \n",
       "529   Our machine translation systems this year are ...  \n",
       "530   The compared systems are evaluated on the Engl...  \n",
       "531   We evaluated our method with movie review docu...  \n",
       "532   For example, Turian et al. (2010)  showed that...  \n",
       "533   Word alignment is performed by GIZA++ (Och and...  \n",
       "534              1 using AdaGrad (Duchi et al., 2011 ).  \n",
       "535   One of the most important resources for discou...  \n",
       "536   The EuroParl data set consists of 707 sentence...  \n",
       "537   Turian et al. (2010) applied word embeddings t...  \n",
       "538   In particular, we use Moses (Koehn et al., 200...  \n",
       "539   Our submitted system for the second task is ba...  \n",
       "540   We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "541   Zhang et al. (2015) explore a shallow convolut...  \n",
       "542   We tokenize English data and segment Chinese d...  \n",
       "543   The source of bilingual data used in the exper...  \n",
       "544   ROUGE (Lin, 2004) is the fully automatic metri...  \n",
       "545   We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "546   The baseline systems are built with the openso...  \n",
       "547   The log-linear approach to phrase-based transl...  \n",
       "548   We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "549   3 All English data are POS tagged and lemmatis...  \n",
       "550   The trigram target language model is trained f...  \n",
       "551   Translation models were trained over the bilin...  \n",
       "552   The core model is a decision tree classifier t...  \n",
       "553   We built phrase-based machine translation syst...  \n",
       "554   Note that word sense disambiguation is perform...  \n",
       "555   We use a well-known scikitlearn (Pedregosa et ...  \n",
       "556   Turian et al. (2010) applied word embeddings t...  \n",
       "557   We used the Brill tagger provided by NLTK for ...  \n",
       "558   All our systems are contrasted with a standard...  \n",
       "559   We use the Moses software package 5 to train a...  \n",
       "560   For training the translation model and for dec...  \n",
       "561   We employed the state-of-the-art sentiment ann...  \n",
       "562   Our work is also related to (Bunescu and Moone...  \n",
       "563   The system that obtained the best performance ...  \n",
       "564   SMT translations: a phrase-based SMT system bu...  \n",
       "565   Given the LP constraints in (4), (7), and (8),...  \n",
       "566   Part of speech tagging and named entity recogn...  \n",
       "567   To build a parser, we use a structured classif...  \n",
       "568   Semantic Textual Similarity (STS) is the task ...  \n",
       "569   We also use A0 to compare against the\" masking...  \n",
       "570   The same kind of process was applied to the Pe...  \n",
       "571   For Reuters we segmented and tokenized the dat...  \n",
       "572   We use the Moses phrase-based translation syst...  \n",
       "573   The corpus is first word-aligned using a word ...  \n",
       "574   3 DKPro is a collection of software components...  \n",
       "575   In order to construct the lexical prior knowle...  \n",
       "576   We use the Stanford CoreNLP caseless tagger fo...  \n",
       "577   The proof is similar to the proof for the tree...  \n",
       "578   We calculate statistical significance of perfo...  \n",
       "579   We use the Stanford CoreNLP caseless tagger fo...  \n",
       "580   Part of speech tagging and named entity recogn...  \n",
       "581   It was also the model used to rank sentences i...  \n",
       "582   Statistical significance is tested on the BLEU...  \n",
       "583   To validate this measure, we computed the cosi...  \n",
       "584   1 We used the KenLM toolkit (Heafield, 2011) t...  \n",
       "585   We used KenLM (Heafield, 2011) to create 3-gra...  \n",
       "586   In the News column , we show the statistics of...  \n",
       "587   Since the GoogleAPI is not available any more,...  \n",
       "588   The Moses SMT toolkit (Koehn et al., 2007) pro...  \n",
       "589   We trained a model using Moses toolkit (Koehn ...  \n",
       "590   The chunk label tagset is a coarser version of...  \n",
       "591   We use a set of 30 word pairs from a study car...  \n",
       "592   All annotations were done using the brat rapid...  \n",
       "593   We use the Stanford CoreNLP caseless tagger fo...  \n",
       "594   For the exploding gradient problem, numerical ...  \n",
       "595   The annotations were made using the BRAT rapid...  \n",
       "596   For Chinese, we use the Penn Chinese Treebank ...  \n",
       "597   For statistical significance testing, we use a...  \n",
       "598   Significance was tested using a paired bootstr...  \n",
       "599   The preposition classifier uses a combined sys...  \n",
       "600   As a practical approximation, we use bigram co...  \n",
       "601   Word alignments were created using GIZA++ (Och...  \n",
       "602   Weka (Hall et al., 2009) which contains the im...  \n",
       "603   The parallel data were taken from OPUS (Tiedem...  \n",
       "604   We conducted experiments using Multinomial Nai...  \n",
       "605   The experiments focus on translation from Germ...  \n",
       "606   This architecture is very similar to the frame...  \n",
       "607   We use the CoNLL-X (Buchholz and Marsi, 2006) ...  \n",
       "608   We note that our model outperforms the model p...  \n",
       "609   We note that our model outperforms the model p...  \n",
       "610   Zelenko et al. (2003) have shown the contiguou...  \n",
       "611   To this end, a recent large-scale annotation e...  \n",
       "612   We use Stanford parser (de Marneffe et al., 20...  \n",
       "613   Only for German data did we used the TreeTagge...  \n",
       "614   We built a knowledge base (V 2 R) 1 using the ...  \n",
       "615   The first two baselines are standard systems u...  \n",
       "616   The decoder is built on top of an open-source ...  \n",
       "617   Significance was tested using a paired bootstr...  \n",
       "618   We use Stanford parser (de Marneffe et al., 20...  \n",
       "619   For Reuters we segmented and tokenized the dat...  \n",
       "620   The grammatical relations are all the collapse...  \n",
       "621   The decoder is built on top of an open-source ...  \n",
       "622   Finally the ANEW lexicon (Bradley and Lang, 19...  \n",
       "623   The NLP Group of the Idiap Research Institute ...  \n",
       "624   An example of such a pragmatic factor is commo...  \n",
       "625   We then have assigned a sentiment score using ...  \n",
       "626   We used a subset of the data provided for the ...  \n",
       "627   Holmqvist et al. (2012) presented a method whe...  \n",
       "628   Distributional models of meaning follow the di...  \n",
       "629    Europarl 2 (Koehn, 2005 ): it is a corpus ...  \n",
       "630   Language Model: For all 3-SCFG systems we use ...  \n",
       "631   To evaluate DA for sentiment classification, w...  \n",
       "632   For this purpose we use the Europarl corpus (K...  \n",
       "633   In order to evaluate the fluency of each syste...  \n",
       "634   The annotations were made using the BRAT rapid...  \n",
       "635   The SVM models were trained using the Scikit-l...  \n",
       "636   We used the English side of the Europarl corpu...  \n",
       "637   The language model is a 5-gram KenLM (Heafield...  \n",
       "638   We used the English side of the Europarl corpu...  \n",
       "639   We also lemmatized all words using Stanford Co...  \n",
       "640   We trained our basic neural machine translatio...  \n",
       "641   We used the GIZA++ software (Och and Ney, 2003...  \n",
       "642   The matrix is weighted with PPMI as implemente...  \n",
       "643   Corpus-based VSMs follow the standard\" distrib...  \n",
       "644   For Chinese, a segmentation model (Zhang et al...  \n",
       "645   As our learner, we use LIBSVM with a linear ke...  \n",
       "646   The corpus was converted from XML to raw text,...  \n",
       "647   For example, the OntoNotes (Hovy et al., 2006)...  \n",
       "648   The Moses toolkit (Koehn et al., 2007) is used...  \n",
       "649   Specifically, we use the LIBLINEAR SVM package...  \n",
       "650   For this purpose we use the Europarl corpus (K...  \n",
       "651   The ICSI Meeting Corpus: The ICSI Meeting Corp...  \n",
       "652   We use ROUGE metric (Lin, 2004) to evaluate ge...  \n",
       "653   The task is part of the Semantic Evaluation 20...  \n",
       "654   For this purpose we use the Europarl corpus (K...  \n",
       "655   We also used automatically back-translated in-...  \n",
       "656   We also lemmatized all words using Stanford Co...  \n",
       "657   Corpus-based meaning representations rely on t...  \n",
       "658   Turian et al. (2010)  evaluate different techn...  \n",
       "659   We then use the phrase extraction utility in t...  \n",
       "660   We used LIBSVM to implement our own SVM for re...  \n",
       "661   The word alignment was obtained by running Giz...  \n",
       "662   The word alignment was obtained by running Giz...  \n",
       "663   We used bootstrap resampling for testing stati...  \n",
       "664   We used bootstrap resampling for testing stati...  \n",
       "665   Markov Logic Networks (MLN) (Richardson and Do...  \n",
       "666   We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "667   We initialize parameters uniformly, using the ...  \n",
       "668   More recently, Liu et al. (2012) have proposed...  \n",
       "669   We evaluate our method by means of the Europar...  \n",
       "670   We also used automatically back-translated in-...  \n",
       "671   Phrase extraction was performed following Koeh...  \n",
       "672   In our low-resource condition, we trained an S...  \n",
       "673   Word alignment was done with GIZA++ (Och and N...  \n",
       "674   The closest area to our work consists of inves...  \n",
       "675   The other groups have been used already (for e...  \n",
       "676   For this purpose we use the Europarl corpus (K...  \n",
       "677   We used the English side of the Europarl corpu...  \n",
       "678   For the MSRP task, Socher et al. (2011) used a...  \n",
       "679   AIDA/YAGO is derived from the CoNLL-2003 share...  \n",
       "680   For this purpose, we use the Moses toolkit to ...  \n",
       "681   We measure translation quality via the BLEU sc...  \n",
       "682   We used the Linear SVM implementation (with de...  \n",
       "683   AIDA/YAGO is derived from the CoNLL-2003 share...  \n",
       "684   We also lemmatized all words using Stanford Co...  \n",
       "685   All novels were lemmatized and POS-tagged usin...  \n",
       "686   We also use Adadelta (Zeiler, 2012) to optimiz...  \n",
       "687   AIDA/YAGO is derived from the CoNLL-2003 share...  \n",
       "688   All novels were lemmatized and POS-tagged usin...  \n",
       "689   Hovy et al. (2013) applied tree kernels to met...  \n",
       "690   AIDA/YAGO is derived from the CoNLL-2003 share...  \n",
       "691   We used the English side of the Europarl corpu...  \n",
       "692   We used bootstrap resampling for testing stati...  \n",
       "693   The Moses toolkit (Koehn et al., 2007) is used...  \n",
       "694   We used bootstrap resampling for testing stati...  \n",
       "695   One of the best-known methods of representing ...  \n",
       "696   We have used the baseNP data presented in (Ram...  \n",
       "697   The second algorithm, denoted GloTr, is the Ch...  \n",
       "698   Training and querying of a modified Kneser-Ney...  \n",
       "699   Baseline word alignments were obtained by runn...  \n",
       "700   In order to improve the robustness of the word...  \n",
       "701   The language models are estimated using the Ke...  \n",
       "702   For all experiments, we use a decision-tree cl...  \n",
       "703   We used the lexicalized dependency parser in t...  \n",
       "704   We built phrase-based machine translation syst...  \n",
       "705   We use Moses 7 (Koehn et al., 2007) to impleme...  \n",
       "706   For training the translation model and for dec...  \n",
       "707   The baseline systems are built with the openso...  \n",
       "708   To solve coreference, we used a variation of t...  \n",
       "709   We built phrase-based machine translation syst...  \n",
       "710   We extract syntactic dependencies using Stanfo...  \n",
       "711   We run our experiments on Europarl (Koehn, 200...  \n",
       "712   In our experiments , we used the Stanford pars...  \n",
       "713   The OpenFst library is used to perform all of ...  \n",
       "714   This is a corpus-based metric relying on the d...  \n",
       "715   We build a state of the art phrase-based SMT s...  \n",
       "716   For BROWN, the features are the prefix feature...  \n",
       "717   For a pair of words, WordNet provides a series...  \n",
       "718   Our baseline is a phrase-based MT system train...  \n",
       "719   We measure significance of results using boots...  \n",
       "720   3.3.1 Reference System We compare a number of ...  \n",
       "721   3 All English data are POS tagged and lemmatis...  \n",
       "722   In addition, the data was tokenized, lemmatize...  \n",
       "723   Europarl (Koehn, 2005) is a multilingual paral...  \n",
       "724   Logistic regression, implemented in Python wit...  \n",
       "725   In this paper, we use the subjectivity corpus ...  \n",
       "726   In our experiments , we used the Stanford pars...  \n",
       "727   We also compare our word embeddings with the E...  \n",
       "728   We used the lexicalized dependency parser in t...  \n",
       "729   The Dutch version was tagged automatically usi...  \n",
       "730   The decoder is implemented with Weighted Finit...  \n",
       "731   For Chinese-English experiments, we used the O...  \n",
       "732   The baseline systems are built with the openso...  \n",
       "733   The decoder is implemented with Weighted Finit...  \n",
       "734   We run our experiments on Europarl (Koehn, 200...  \n",
       "735   Dhillon et al. (2015) used CCA to derive word ...  \n",
       "736   The rapid growth of user-generated content, mu...  \n",
       "737   These classifiers are based on a discriminativ...  \n",
       "738   We build a state of the art phrase-based SMT s...  \n",
       "739   For optimization, we employed the Adam 6 http:...  \n",
       "740   was used for word alignment and phrase transla...  \n",
       "741   In addition, the data was tokenized, lemmatize...  \n",
       "742   A standard for predicate argument annotation i...  \n",
       "743   The source of bilingual data used in the exper...  \n",
       "744   We used the lexicalized dependency parser in t...  \n",
       "745   We extract syntactic dependencies using Stanfo...  \n",
       "746   2 Further, we sentence-split, tokenized, and l...  \n",
       "747   ROUGE (Lin and Hovy, 2003) has been adopted as...  \n",
       "748   All experiments were carried out using the ope...  \n",
       "749   We directly apply the Smoothed Partial Tree-Ke...  \n",
       "750   2 Further, we sentence-split, tokenized, and l...  \n",
       "751   We used the lexicalized dependency parser in t...  \n",
       "752   We build a state of the art phrase-based SMT s...  \n",
       "753   The Moses toolkit (Koehn et al., 2007) is used...  \n",
       "754   We build a state of the art phrase-based SMT s...  \n",
       "755   We tokenize and frequent-case the data with th...  \n",
       "756   Baroni et al. (2002)  report that 47% of the v...  \n",
       "757   To train a mention-pair classifier, we use the...  \n",
       "758   Following the evaluation paradigm of Mitchell ...  \n",
       "759   This system is the Moses decoder (Koehn et al....  \n",
       "760   Our alignment model is based on a simple varia...  \n",
       "761   The original reordering constraint in Berger e...  \n",
       "762   Mikolov et al. (2013a) proposed a faster skip-...  \n",
       "763   The corpus consists of a subset of the Brown C...  \n",
       "764   All these features are inherited from Moses (K...  \n",
       "765   We also lemmatized all words using Stanford Co...  \n",
       "766   10 We used the PLTM implementation in Mallet (...  \n",
       "767   LIBLINEAR (Fan et al., 2008 ), a library for l...  \n",
       "768   We use a mini-batch stochastic gradient descen...  \n",
       "769   Specifically, we use Support Vector Machine (S...  \n",
       "770   Feature weights, based on BLEU, are then tuned...  \n",
       "771   Brin proposed the bootstrapping method for rel...  \n",
       "772   Neural networks are first used in this task in...  \n",
       "773   We used the GIZA++ software (Och and Ney, 2003...  \n",
       "774   We trained a CRF tagger using CRFSuite 1 (Okaz...  \n",
       "775   The HMM classifier used in the experiments in ...  \n",
       "776   The language model is a 5-gram with interpolat...  \n",
       "777   We plan to adapt ideas from Socher et al. (201...  \n",
       "778   We use the Support Vector Machines implementat...  \n",
       "779   For argument, a dependency version of the prun...  \n",
       "780   We used bootstrap resampling for testing stati...  \n",
       "781   Specifically, we use Support Vector Machine (S...  \n",
       "782   We used the Stuttgart TreeTagger (Schmid, 1994...  \n",
       "783   Earlier models made use of latent semantic ana...  \n",
       "784   In principle, classifiers trained on PDTB data...  \n",
       "785   The edit distance kernel was trained with LIBS...  \n",
       "786   It is a standard phrase-based machine translat...  \n",
       "787   Specifically, we use Support Vector Machine (S...  \n",
       "788   The word alignment is created by GIZA++ (Och a...  \n",
       "789   The SVM implementation used was LIBSVM (Chang ...  \n",
       "790   We used GIZA++ (Och and Ney, 2003) to align th...  \n",
       "791   We used the GIZA++ software (Och and Ney, 2003...  \n",
       "792   In contrast, the set of discourse markers in o...  \n",
       "793   The SVM implementation of Weka (Hall et al., 2...  \n",
       "794   This was done with a specific tool provided wi...  \n",
       "795   We used the GIZA++ software (Och and Ney, 2003...  \n",
       "796   We performed word alignment using GIZA++ (Och ...  \n",
       "797   We used Google Books ngrams (Michel et al., 20...  \n",
       "798   We also lemmatized all words using Stanford Co...  \n",
       "799   We evaluate C&W word embeddings with 25, 50 an...  \n",
       "800   This is often called distributional semantics ...  \n",
       "801   Our neural network is similar to that of Li et...  \n",
       "802   We used GIZA++ (Och and Ney, 2003) to align th...  \n",
       "803   Meaning representation and composition in the ...  \n",
       "804   For this purpose we use the Europarl corpus (K...  \n",
       "805   The stopwords are taken from the stop-word lis...  \n",
       "806   We build a state of the art phrase-based SMT s...  \n",
       "807   One idea is to apply it to the language model ...  \n",
       "808   We extract syntactic dependencies using Stanfo...  \n",
       "809   The corpora are tokenised and truecased using ...  \n",
       "810   To make the exponential algorithm practical, b...  \n",
       "811   The EuroParl data set consists of 707 sentence...  \n",
       "812   Our approach is based on the Czech linguistic ...  \n",
       "813   These classifiers have been used in related wo...  \n",
       "814   We build a baseline error correction system, u...  \n",
       "815   We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "816   This sense similarity measure is inspired by t...  \n",
       "817   The baseline systems are built with the openso...  \n",
       "818   In our experiments , we used the Stanford pars...  \n",
       "819   We implement MERT and MIRA 1 , and directly us...  \n",
       "820   We tokenize and frequent-case the data with th...  \n",
       "821   Our baseline is a phrase-based MT system train...  \n",
       "822   The corpora are tokenised and truecased using ...  \n",
       "823   We built phrase-based machine translation syst...  \n",
       "824   We build a state of the art phrase-based SMT s...  \n",
       "825   We build a state of the art phrase-based SMT s...  \n",
       "826   We built phrase-based machine translation syst...  \n",
       "827   McClosky et al. (2006)  use self-training in c...  \n",
       "828   @BULLET Logistic Regression(LR): We use Logist...  \n",
       "829   3 All English data are POS tagged and lemmatis...  \n",
       "830   We use the Bernoulli Naive Bayes classifier in...  \n",
       "831   Both systems are based on the Moses SMT toolki...  \n",
       "832   We tokenize and frequent-case the data with th...  \n",
       "833   We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "834   We built phrase-based machine translation syst...  \n",
       "835   We extract our paraphrase grammar from the Fre...  \n",
       "836   In our experiments , we used the Stanford pars...  \n",
       "837   In our experiments , we used the Stanford pars...  \n",
       "838   All our systems are contrasted with a standard...  \n",
       "839   We use AdaGrad (Duchi et al., 2011)  with the ...  \n",
       "840   We train the concept identification stage usin...  \n",
       "841   We use the cross-entropy loss function and min...  \n",
       "842   Socher et al. (2011)  use recursive auto-encod...  \n",
       "843   We build a state of the art phrase-based SMT s...  \n",
       "844   We build a state of the art phrase-based SMT s...  \n",
       "845   Our baseline is a phrase-based MT system train...  \n",
       "846   We built phrase-based machine translation syst...  \n",
       "847   We build a state of the art phrase-based SMT s...  \n",
       "848   In our experiments , we used the Stanford pars...  \n",
       "849   The parameters can be efficiently estimated fr...  \n",
       "850   We build a state of the art phrase-based SMT s...  \n",
       "851   All our systems are contrasted with a standard...  \n",
       "852   NERsuite is a NER system that is built on top ...  \n",
       "853   We built phrase-based machine translation syst...  \n",
       "854   Experts can manually specify the attributes of...  \n",
       "855   Experts can manually specify the attributes of...  \n",
       "856   We used the publicly available Wacky corpus (B...  \n",
       "857   Although there had been research on reasoning ...  \n",
       "858   We tune the systems using kbest batch MIRA (Ch...  \n",
       "859   We use the MALLET package (McCallum, 2002) for...  \n",
       "860   We use CRFsuite (Okazaki, 2007) as an implemen...  \n",
       "861   Word alignments were created using GIZA++ (Och...  \n",
       "862   We expect this restriction is more consistent ...  \n",
       "863   We adopt online learning, updating parameters ...  \n",
       "864   Weka (Hall et al., 2009) which contains the im...  \n",
       "865   TreeTagger is a statistical, decision tree-bas...  \n",
       "866   The parallel corpus is wordaligned using GIZA+...  \n",
       "867   Classification uses the scikit-learn Python pa...  \n",
       "868   It builds on the C&C CCG parser (Clark and Cur...  \n",
       "869   Next, we replace all nouns with their POS tag;...  \n",
       "870     All the data come from Europarl (Koehn, 2005 ).  \n",
       "871   The training data released by the task organiz...  \n",
       "872   conducted using the Moses phrase-based decoder...  \n",
       "873   Och is the HMM alignment model of (Och and Ney...  \n",
       "874     All the data come from Europarl (Koehn, 2005 ).  \n",
       "875   glish source with target French) by using GIZA...  \n",
       "876   Word alignment is performed with Giza++ (Och a...  \n",
       "877   The models were evaluated using M2 scorer (Dah...  \n",
       "878   Han et al. (2012) introduced a dictionary base...  \n",
       "879   Word alignment is done using GIZA++ (Och and N...  \n",
       "880   We used bootstrap resampling for testing stati...  \n",
       "881   Word alignment is performed with Giza++ (Och a...  \n",
       "882   Word alignment is performed using GIZA++ (Och ...  \n",
       "883   In section 3, we present a solution to the all...  \n",
       "884   In our experiments we investigated both weak a...  \n",
       "885   To calculate the number of changes, we used a ...  \n",
       "886   Second, to avoid the error propagation problem...  \n",
       "887   Mikolov et al. (2013a) proposed a faster skip-...  \n",
       "888   A core component of every PBSMT system is the ...  \n",
       "889   We use the SVM rank implementation (Joachims, ...  \n",
       "890   All models were trained using Adagrad (Duchi e...  \n",
       "891   HCRC is tagged with TnT (Brants, 2000 ), train...  \n",
       "892   We trained the classifiers using the LIBLINEAR...  \n",
       "893   For reference, we also show the MT performance...  \n",
       "894   For example, Petrov et al. (2012)  build super...  \n",
       "895   We chose a threshold such that our approach pr...  \n",
       "896   The proof is similar to the proof for the tree...  \n",
       "897   We train with the Adam optimizer (Kingma and B...  \n",
       "898   We use KenLM 3 (Heafield, 2011) for computing ...  \n",
       "899   Our model is an extension of the contextual ba...  \n",
       "900   We generated embeddings by training a characte...  \n",
       "901   The advantage of an SVM rank model is that it ...  \n",
       "902   According to the distributional hypothesis of ...  \n",
       "903   Our experiments were carried out on the Europa...  \n",
       "904   The language model is a 5-gram KenLM (Heafield...  \n",
       "905   GENIA (Kim et al., 2003) is a collection of 20...  \n",
       "906   We generated embeddings by training a characte...  \n",
       "907   Baseline word alignments were obtained by runn...  \n",
       "908   For the language model, we used all monolingua...  \n",
       "909   Many researchers have considered generating pa...  \n",
       "910   Our experiments were carried out on the Europa...  \n",
       "911   org/wiki/Fog_Index 8 For word frequency we use...  \n",
       "912   The first is a reimplementation of the stack-b...  \n",
       "913   Surdeanu et al. (2012) proposed a novel approa...  \n",
       "914   Then we revise the two LP constraints of Cho &...  \n",
       "915   We used the unigram counts from the Web 1T 5-g...  \n",
       "916   We expect this restriction is more consistent ...  \n",
       "917   The training set is used to train the phrase-b...  \n",
       "918   The systems for the English ? Spanish translat...  \n",
       "919   We use the Stanford parser with Stanford depen...  \n",
       "920   We use the Stanford parser with Stanford depen...  \n",
       "921   We use a 5-gram LM trained on the Gigaword cor...  \n",
       "922   The system was trained on the English and Dani...  \n",
       "923   We convert the trees in both treebanks from co...  \n",
       "924   Here we review the parameters of the standard ...  \n",
       "925   The Moses decoder (Koehn et al., 2007) was use...  \n",
       "926   This corresponds to a new version of the Frenc...  \n",
       "927   For a detailed survey of the field of sentimen...  \n",
       "928   A complete description of the training and tes...  \n",
       "929   18 Our baseline is the SMT toolkit Moses (Koeh...  \n",
       "930   Causal relations are among discourse relations...  \n",
       "931   It receives CCG derivations from the C&C parse...  \n",
       "932   In transforming natural language text to logic...  \n",
       "933   We use the Moses phrase-based translation syst...  \n",
       "934   We also report 95% confidence intervals (CI) m...  \n",
       "935   The Europarl corpus (Koehn, 2005) is built fro...  \n",
       "936   7 http://opennlp.apache.org 5.2.3.), we use th...  \n",
       "937   GATE (Cunningham et al., 2002a) is an architec...  \n",
       "938   Both language models use modified Kneser-Ney s...  \n",
       "939   We automatically word-aligned the German part ...  \n",
       "940   Joint disambiguation and clustering of mention...  \n",
       "941   Pang et al. (2002)  use machine learning metho...  \n",
       "942   We refer to the system of Jans et al. (2012) a...  \n",
       "943   Naive Bayes and Decision Tree models were buil...  \n",
       "944   We build a state of the art phrase-based SMT s...  \n",
       "945   We compare the model against the Moses phrase-...  \n",
       "946   We then describe in more detail a modern Chine...  \n",
       "947   The first two baselines are standard systems u...  \n",
       "948   The first two baselines are standard systems u...  \n",
       "949   We use the cross-entropy loss function and min...  \n",
       "950   We use AdaGrad (Duchi et al., 2011)  with the ...  \n",
       "951   We compare the model against the Moses phrase-...  \n",
       "952   The Penn Discourse TreeBank (PDTB; Prasad et a...  \n",
       "953   Pitler and Nenkova (2008) used discourse relat...  \n",
       "954   The details of parsing model were presented in...  \n",
       "955   In addition, the data was tokenized, lemmatize...  \n",
       "956   The decoder is built on top of an open-source ...  \n",
       "957   The mention detection of the Stanford corefere...  \n",
       "958   CTB6 is used as the Chinese data set in the Co...  \n",
       "959   In addition, the data was tokenized, lemmatize...  \n",
       "960   We report F1 performance scored using the offi...  \n",
       "961   The results of this experiment appear in Table...  \n",
       "962   We use LibSVM (Chang and Lin, 2011) as the SVM...  \n",
       "963   Baroni et al. (2002) also pointed out that the...  \n",
       "964   The most influential of the semantic approache...  \n",
       "965   We refer to that model as Moses en-es-100k , b...  \n",
       "966   We follow the formulation of vector compositio...  \n",
       "967   Our system is a linear model estimated using r...  \n",
       "968   We use SRL Collobert et al. (2011) to determin...  \n",
       "969   The sparsity of lexical features can also be t...  \n",
       "970   For the comparisons of translation quality, th...  \n",
       "971   We refer to that model as Moses en-es-100k , b...  \n",
       "972   These techniques were evaluated in experiments...  \n",
       "973   We use the intersection of direct and reverse ...  \n",
       "974   We also compare with the standard phrase-based...  \n",
       "975   Thus, inducing a number of clusters similar to...  \n",
       "976   We use the intersection of direct and reverse ...  \n",
       "977   The evaluation corpus is a subset of an ungram...  \n",
       "978   Translations for English words in the lexical ...  \n",
       "979   We participated in both subtask A and B of Sem...  \n",
       "980   To test our method, we conducted two lowresour...  \n",
       "981   We use the support vector machine (SVM) rank a...  \n",
       "982   In all of the above tasks, we compare the neur...  \n",
       "983   We use Viterbi algorithm (Viterbi, 1967) to de...  \n",
       "984   We measure translation quality via the BLEU sc...  \n",
       "985   All our systems are contrasted with a standard...  \n",
       "986   The baseline systems are built with the openso...  \n",
       "987   The degree of similarity between two similar w...  \n",
       "988   We built phrase-based machine translation syst...  \n",
       "989   We built phrase-based machine translation syst...  \n",
       "990   Finally, we used Moses toolkit as phrase-based...  \n",
       "991   The degree of similarity between two similar w...  \n",
       "992   The language models are estimated using the Ke...  \n",
       "993   We use the cross-entropy loss function and min...  \n",
       "994   We used SVM implementations from scikit-learn ...  \n",
       "995   We train the concept identification stage usin...  \n",
       "996   We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "997   We used the English side of the Europarl corpu...  \n",
       "998   \" Koo10\" stands for the Model 1 in (Koo and Co...  \n",
       "999   Specifically, we used the standard Gradient Bo...  \n",
       "1000  For this purpose we use the Europarl corpus (K...  \n",
       "1001  Very recently Rush et al. (2015) proposed a ne...  \n",
       "1002  We use the scikit implementation of Random For...  \n",
       "1003  Of the remaining twelve, eight came from a lar...  \n",
       "1004  We used the implementation of the scikit-learn...  \n",
       "1005  We built phrase-based machine translation syst...  \n",
       "1006  We also lemmatized all words using Stanford Co...  \n",
       "1007  The baseline systems are built with the openso...  \n",
       "1008  For training the translation model and for dec...  \n",
       "1009  @BULLET Decoder: Moses (Koehn et al., 2007) wi...  \n",
       "1010  We train and evaluate the classifiers in a 10-...  \n",
       "1011                                                     \n",
       "1012  Recently, there has been a successful attempt ...  \n",
       "1013  The first one is the WS-353 dataset (Finkelste...  \n",
       "1014  Abstract Meaning Representation (AMR) (Banares...  \n",
       "1015  We perform bootstrap resampling with bounds es...  \n",
       "1016  It is used to support semantic analyses in the...  \n",
       "1017  We first use a dependency parser (de Marneffe ...  \n",
       "1018  Alternatively, Rooth et al. (1999)  propose an...  \n",
       "1019  The Levenshtein distance gives an indication o...  \n",
       "1020  We built phrase-based machine translation syst...  \n",
       "1021  We perform bootstrap resampling with bounds es...  \n",
       "1022  It is used to support semantic analyses in the...  \n",
       "1023  It is used to support semantic analyses in the...  \n",
       "1024  It is used to support semantic analyses in the...  \n",
       "1025  It is used to support semantic analyses in the...  \n",
       "1026  We build upon our previous approach for joint ...  \n",
       "1027  Details about SVM and KRR can be found in (Tay...  \n",
       "1028  We learn the parameters  using a quasi-Newton ...  \n",
       "1029  For direct translation, we use the SCFG decode...  \n",
       "1030  This is known as the Distributional Hypothesis...  \n",
       "1031  The models , as well as the parser described i...  \n",
       "1032  For strings, a lot of such kernel functions ex...  \n",
       "1033  The estimation of the semantically Smoothed Pa...  \n",
       "1034  We train with the Adam optimizer (Kingma and B...  \n",
       "1035  In our experimental study, we use the freely a...  \n",
       "1036  OPTIMSRC: (Carpineto and Romano, 2010) showed ...  \n",
       "1037  These methods rely on the distributional hypot...  \n",
       "1038  Word alignment is done using GIZA++ (Och and N...  \n",
       "1039  Word alignment is done using GIZA++ (Och and N...  \n",
       "1040  We used Mallet software (McCallum, 2002) for S...  \n",
       "1041  The ontology, GoiTaikei, consists of a hierarc...  \n",
       "1042  A detailed discussion on the results is provid...  \n",
       "1043  The first one is the WS-353 dataset (Finkelste...  \n",
       "1044  A framework for human error analysis has been ...  \n",
       "1045  MaxEnt classifier is an example of this group ...  \n",
       "1046  Blog is one of the crucial, communicative and ...  \n",
       "1047  For this purpose, we use MADA (Morphological A...  \n",
       "1048  Our 5-gram language model was trained on the X...  \n",
       "1049  To determine semantic types and subtypes, we t...  \n",
       "1050  We use the scikit implementation of SVM (Pedre...  \n",
       "1051  Each term in the input text is represented by ...  \n",
       "1052  An algorithm, the Kuhn-Munkres method (Kuhn, 1...  \n",
       "1053  We use collapsed Gibbs sampling (Griffiths and...  \n",
       "1054  We use the Stanford parser with Stanford depen...  \n",
       "1055  Weights are initialized using Glorot-Bengio st...  \n",
       "1056  Automatic sentence alignment of the training d...  \n",
       "1057  We use AdaGrad (Duchi et al., 2011)  with the ...  \n",
       "1058  We performed word alignment using GIZA++ (Och ...  \n",
       "1059  For example, DIRT (Lin and Pantel, 2001) aims ...  \n",
       "1060  The annotation was performed using the BRAT 2 ...  \n",
       "1061  A similar semantic similarity measure, propose...  \n",
       "1062  It consists of 50 texts taken from the WSJ por...  \n",
       "1063  All components take as input the corpus docume...  \n",
       "1064  From the pioneering work of (Rapp, 1995 ), BLE...  \n",
       "1065  We run our experiments on Europarl (Koehn, 200...  \n",
       "1066    TESLA-F was called TESLA in Liu et al. (2010) .  \n",
       "1067  For building our SMT systems, the open-source ...  \n",
       "1068  Rhetorical Structure Theory (RST) (Mann and Th...  \n",
       "1069  In addition, the fix-discount method (Foster e...  \n",
       "1070  Memory-based language processing (Daelemans an...  \n",
       "1071  We tested the significance of differences usin...  \n",
       "1072  In addition, machine translation (MT) systems ...  \n",
       "1073  1 -regularization using AdaGrad (Duchi et al.,...  \n",
       "1074  The MUC score (Vilain et al., 1995) counts the...  \n",
       "1075  We will focus on the syntactic tree kernel des...  \n",
       "1076  The Europarl corpus (Koehn, 2005) is built fro...  \n",
       "1077  The module was implemented using Foma, a free ...  \n",
       "1078  We used the same test data as in Li et al. (20...  \n",
       "1079  Penn Discourse Treebank The Penn Discourse Tre...  \n",
       "1080  We used a GB implementation of the scikit-lear...  \n",
       "1081  The second is the MEN dataset (Bruni et al., 2...  \n",
       "1082  The training data released by the task organiz...  \n",
       "1083  All of the machine learning was done using sci...  \n",
       "1084  It has been long identified in NLP that a dive...  \n",
       "1085  In our system, we used the sentiment lexicon p...  \n",
       "1086  We also compared the quality of the candidate ...  \n",
       "1087  Corpus-based VSMs follow the standard distribu...  \n",
       "1088  The annotations were made using the BRAT rapid...  \n",
       "1089  Compared to WordNet (Fellbaum, 1998 ), there a...  \n",
       "1090  We use Adam (Kingma and Ba, 2015) for optimiza...  \n",
       "1091  Specifically, we use the LIBLINEAR SVM package...  \n",
       "1092  The first two experiments involve predicting t...  \n",
       "1093  We used only the non-ensembled left-to-right r...  \n",
       "1094  We used only the non-ensembled left-to-right r...  \n",
       "1095  The MSD morphological coding system is a posit...  \n",
       "1096  The phrase table was generated employing symme...  \n",
       "1097  Word alignment is performed with Giza++ (Och a...  \n",
       "1098  Following Koo et al. (2008) , we also experime...  \n",
       "1099  Raghavan et al. (2007)  evaluate benefit from ...  \n",
       "1100  Training and querying of a modified Kneser-Ney...  \n",
       "1101  The formal derivation can be found in (Ando an...  \n",
       "1102  The model as described thus far is identical t...  \n",
       "1103  The baseline systems are built with the openso...  \n",
       "1104  The classifier evaluations were carried out us...  \n",
       "1105  The training data for the task is from the NUC...  \n",
       "1106  SALDO (Borin et al., 2013) is the most compreh...  \n",
       "1107  For our SMT experiments, we use the Moses tool...  \n",
       "1108  However such string-to-tree systems run slowly...  \n",
       "1109  We trained an English 5-gram language model us...  \n",
       "1110  For our SMT experiments, we use the Moses tool...  \n",
       "1111  @BULLET OntoNotes-Test: Test set of the OntoNo...  \n",
       "1112  We use the state-of-the-art phrase-based machi...  \n",
       "1113  Since the training data used in Li et al. (200...  \n",
       "1114  BLEU score: This score measures the precision ...  \n",
       "1115  The training data provided for the task is a s...  \n",
       "1116  We selected the dataset of Jurgens and Klapaft...  \n",
       "1117  The statistical significance tests using 95% c...  \n",
       "1118  We evaluate our relation extraction system on ...  \n",
       "1119  Sentences Involving Compositional Knowledge (S...  \n",
       "1120  Our novel parsing model is the Dynamic Conditi...  \n",
       "1121  Combination of latent topics ent Dirichlet All...  \n",
       "1122  It therefore follows the distributional hypoth...  \n",
       "1123  It therefore follows the distributional hypoth...  \n",
       "1124  In 2009, Yefang Wang (Wang et al., 2009) used ...  \n",
       "1125  We built a modified Kneser-Ney smoothed 5-gram...  \n",
       "1126  The other models are trained on native English...  \n",
       "1127  We then apply bootstrapping (Kozareva et al., ...  \n",
       "1128  It therefore follows the distributional hypoth...  \n",
       "1129  Our hypothesis is that the LTAG based features...  \n",
       "1130  RG-65: (Rubenstein and Goodenough, 1965) is se...  \n",
       "1131  Statistical significance tests are performed u...  \n",
       "1132  The search is typically carried out using the ...  \n",
       "1133  In-domain data only solves the problem of data...  \n",
       "1134  All the experiments are carried out in Moses t...  \n",
       "1135  We then apply heuristics to determine number a...  \n",
       "1136  For this task we use RankSVM (Joachims, 2002) ...  \n",
       "1137  A precise description of the corpus and metric...  \n",
       "1138  Tree Kernel (TK) functions are convolution ker...  \n",
       "1139  For disambiguation and clustering we build upo...  \n",
       "1140  We used the ROUGE-1 evaluation metric (Lin, 20...  \n",
       "1141  We used Mallet toolkit (McCallum, 2002) for CR...  \n",
       "1142  Our joint parsing model exploits a transition-...  \n",
       "1143  The annotations were made using the BRAT rapid...  \n",
       "1144  To build the word alignment models we used the...  \n",
       "1145  We evaluated annotation reliability by using t...  \n",
       "1146  Distributional models of meaning follow the di...  \n",
       "1147  This dataset is composed of 32 sentence quadru...  \n",
       "1148  A statistical significance test was performed ...  \n",
       "1149  We use the Giza++ tool (Och and Ney, 2003) to ...  \n",
       "1150  We use the Giza++ tool (Och and Ney, 2003) to ...  \n",
       "1151  Gaussian process regression (GPR) (Rasmussen a...  \n",
       "1152  In another work, a corpus of roughly 1.6 Teraw...  \n",
       "1153  We calculated significance using paired bootst...  \n",
       "1154  In this work, we use the Stanford neural depen...  \n",
       "1155  MADAMIRA (Pasha et al., 2014) is a morphologic...  \n",
       "1156  Yarowsky (1995) proposed such a method for wor...  \n",
       "1157  We also list the results from SMT model (Durra...  \n",
       "1158  We use the Adam (Kingma and Ba, 2014) algorith...  \n",
       "1159  Word co-occurence statistics\" You shall know a...  \n",
       "1160  The co-occurrence Word Space is acquired throu...  \n",
       "1161  We used the scikit-learn toolkit to train our ...  \n",
       "1162  The probability p(E) is computed using a simpl...  \n",
       "1163  This is similar to the operator Intro in (Kapl...  \n",
       "1164  We used a GB implementation of the scikit-lear...  \n",
       "1165  For the linear logistic regression implementat...  \n",
       "1166  Finally, we used Moses toolkit as phrase-based...  \n",
       "1167  Baseline word alignments were obtained by runn...  \n",
       "1168  The language model is a 5-gram with interpolat...  \n",
       "1169  We used the scikit-learn toolkit to train our ...  \n",
       "1170  It is a standard phrase-based machine translat...  \n",
       "1171  The WordNet on-line lexical database (Miller e...  \n",
       "1172  We used a GB implementation of the scikit-lear...  \n",
       "1173  We used the Stuttgart TreeTagger (Schmid, 1994...  \n",
       "1174  Part-of-speech tagging was accomplished using ...  \n",
       "1175  The annotation was performed manually using th...  \n",
       "1176  We participated in both subtask A and B of Sem...  \n",
       "1177  We tokenise the text using the default tokenis...  \n",
       "1178  In addition, the data was tokenized, lemmatize...  \n",
       "1179  We built phrase-based machine translation syst...  \n",
       "1180  The corpora are tokenised and truecased using ...  \n",
       "1181  We built a trigram language model with Kneser-...  \n",
       "1182  We tokenise the text using the default tokenis...  \n",
       "1183  We built phrase-based machine translation syst...  \n",
       "1184  For training the translation model and for dec...  \n",
       "1185  Scope-ignorant (Disambig.): Our previous MLN-b...  \n",
       "1186  We use ADAM (Kingma and Ba, 2014) with a learn...  \n",
       "1187  For training the translation model and for dec...  \n",
       "1188  The corpora are tokenised and truecased using ...  \n",
       "1189  For our SMT experiments, we use the Moses tool...  \n",
       "1190  First, we used the Moses toolkit (Koehn et al....  \n",
       "1191  The baseline systems are built with the openso...  \n",
       "1192  We built phrase-based machine translation syst...  \n",
       "1193  We employed the TnT tagger (Brants, 2000) whic...  \n",
       "1194  In addition, the data was tokenized, lemmatize...  \n",
       "1195  We build a state of the art phrase-based SMT s...  \n",
       "1196  TF-IDF is a standard statistical method that c...  \n",
       "1197  We assess statistical significance of the diff...  \n",
       "1198  A framework for human error analysis has been ...  \n",
       "1199  Chang et al. (2009) stated that one reason is ...  \n",
       "1200  The MMA system (Kruengkrai et al., 2009) train...  \n",
       "1201  We build a state of the art phrase-based SMT s...  \n",
       "1202  We built phrase-based machine translation syst...  \n",
       "1203  conducted using the Moses phrase-based decoder...  \n",
       "1204  In addition, the data was tokenized, lemmatize...  \n",
       "1205  We measure significance of results using boots...  \n",
       "1206  We followed the encoder-decoder architecture w...  \n",
       "1207  We built a trigram language model with Kneser-...  \n",
       "1208  The decoder is implemented with Weighted Finit...  \n",
       "1209  The decoder is implemented with Weighted Finit...  \n",
       "1210  In addition, the data was tokenized, lemmatize...  \n",
       "1211  Our first baseline is MI09, a distantly superv...  \n",
       "1212  We used the Scikit-learn machine learning libr...  \n",
       "1213  This in turn relies on the same underlying fea...  \n",
       "1214  This in turn relies on the same underlying fea...  \n",
       "1215  MADAMIRA (Pasha et al., 2014) is a morphologic...  \n",
       "1216  We built a modified Kneser- Ney smoothed 5-gra...  \n",
       "1217  We preprocessed the training corpora with scri...  \n",
       "1218  We performed word alignment using GIZA++ (Och ...  \n",
       "1219  As our learner, we use LIBSVM with a linear ke...  \n",
       "1220  We specify our dynamic programming algorithm a...  \n",
       "1221  We use the Stanford dependency parser (de Marn...  \n",
       "1222  In order to detect the object pronouns, we emp...  \n",
       "1223  We also obtain the dependency parse of the sen...  \n",
       "1224  The question classifier used in the experiment...  \n",
       "1225  We use the Stanford dependency parser (de Marn...  \n",
       "1226  The former that is the most popular relies on ...  \n",
       "1227  MADAMIRA (Pasha et al., 2014) is a morphologic...  \n",
       "1228  We also obtain the dependency parse of the sen...  \n",
       "1229  The log-linear approach to phrase-based transl...  \n",
       "1230  Both CDS corpora are available from the CHILDE...  \n",
       "1231  For the phrase-based SMT system, we adopted th...  \n",
       "1232  We built a 5-gram language model on the Englis...  \n",
       "1233  We evaluated our model on a semantic relation ...  \n",
       "1234  We performed word alignment using GIZA++ (Och ...  \n",
       "1235  For all experiments, we used the Moses SMT sys...  \n",
       "1236  The statistical significance tests using 95% c...  \n",
       "1237  The SMT systems were built using the Moses too...  \n",
       "1238  The sentences are fed into the PET HPSG parser...  \n",
       "1239  The 5-gram target language model was trained u...  \n",
       "1240  First, we present an algorithm for estimating ...  \n",
       "1241  Similar row vectors in T indicate similar cont...  \n",
       "1242  Similar row vectors in T indicate similar cont...  \n",
       "1243  We first use a dependency parser (de Marneffe ...  \n",
       "1244  We also carried out a chunk-reordering PB-SMT ...  \n",
       "1245  conducted using the Moses phrase-based decoder...  \n",
       "1246  The test set was tagged with the French TreeTa...  \n",
       "1247  For the parsing experimens I used MaltParser (...  \n",
       "1248  We used a GB implementation of the scikit-lear...  \n",
       "1249  These features were obtained using the Stanfor...  \n",
       "1250  We used Adam as the optimizer (Kingma and Ba, ...  \n",
       "1251  This is done using IBM Model 1 (Brown et al., ...  \n",
       "1252  For the French side, the TreeTagger (Schmid, 1...  \n",
       "1253  conducted using the Moses phrase-based decoder...  \n",
       "1254  POS tagging is performed using the IMS Tree Ta...  \n",
       "1255  For word alignments, we used Mgiza++ (Gao and ...  \n",
       "1256    All the data come from Europarl (Koehn, 2005 ).  \n",
       "1257  The Polish data is taken from the EUROPARL cor...  \n",
       "1258  Distributional similarity relies on the distri...  \n",
       "1259  The SMT systems were built using the Moses too...  \n",
       "1260  In our MT experiments, we translate French int...  \n",
       "1261  Statistical significance is tested on the BLEU...  \n",
       "1262  The baseline will be created by the Moses SMT ...  \n",
       "1263  We use the Moses phrase-based translation syst...  \n",
       "1264  For our SMT experiments, we use the Moses tool...  \n",
       "1265  A popular measure of this association is point...  \n",
       "1266  The questions are translated using a phrase-ba...  \n",
       "1267  POS tagging is performed using the IMS Tree Ta...  \n",
       "1268  For both we use TreeTagger (Schmid, 1994) with...  \n",
       "1269  The Europarl corpus (Koehn, 2005) is built fro...  \n",
       "1270  The annotation was performed using the BRAT 2 ...  \n",
       "1271  The Polish data is taken from the EUROPARL cor...  \n",
       "1272  HCRC is tagged with TnT (Brants, 2000 ), train...  \n",
       "1273  For German-English we also have a system based...  \n",
       "1274  An alternative representation for baseNPs has ...  \n",
       "1275  The SMT systems were built using the Moses too...  \n",
       "1276  The baseline will be created by the Moses SMT ...  \n",
       "1277  Distributional models of meaning follow the di...  \n",
       "1278  Recently, Naim et al. (2014) proposed a fully ...  \n",
       "1279  Distributional models of meaning follow the di...  \n",
       "1280  In the following we will describe the system w...  \n",
       "1281  It was built with the Moses toolkit (Koehn et ...  \n",
       "1282  Combinatory Categorial Grammar (CCG) (Steedman...  \n",
       "1283  We used the Linear SVM implementation (with de...  \n",
       "1284  The article of Blei et al. (2003) compares LDA...  \n",
       "1285  Gimpel et al. (2011)  provided a dataset of PO...  \n",
       "1286  In order to evaluate the fluency of each syste...  \n",
       "1287  The corpus was then automatically tagged with ...  \n",
       "1288  The improvement is statistically significant a...  \n",
       "1289  The learner is implemented as a ranking compon...  \n",
       "1290  A statistical significance test was performed ...  \n",
       "1291  We use the Punkt sentence splitter from NLTK (...  \n",
       "1292  We find this method provides an additional 1.5...  \n",
       "1293  In particular, the contribution of this paper ...  \n",
       "1294  The BioScope corpus is a manually annotated co...  \n",
       "1295  We apply the Stanford coreference resolution s...  \n",
       "1296  Europarl 2 (Koehn, 2005 ): it is a corpus of p...  \n",
       "1297  We used a GB implementation of the scikit-lear...  \n",
       "1298  We used the scikit-learn toolkit to train our ...  \n",
       "1299  Automatic text summarization aims to automatic...  \n",
       "1300  For our SMT experiments, we use the Moses tool...  \n",
       "1301  We used the scikit-learn (Pedregosa et al., 20...  \n",
       "1302  For the phrase-based SMT system, we adopted th...  \n",
       "1303  We train Random Forest classifiers (Breiman, 2...  \n",
       "1304  1 The corpus is lemmatised and tagged by part-...  \n",
       "1305  Distributional models of meaning follow the di...  \n",
       "1306  But we randomly selected 90% of the training d...  \n",
       "1307  Additionally, we train phrase-based machine tr...  \n",
       "1308  For example, Finkel et al. (2005) enabled the ...  \n",
       "1309  For all experiments, we used the Moses SMT sys...  \n",
       "1310  For our SMT experiments, we use the Moses tool...  \n",
       "1311  In addition, we show that the latent represent...  \n",
       "1312  Both language models use modified Kneser-Ney s...  \n",
       "1313  We use the Gibbs sampling based LDA (Griffiths...  \n",
       "1314  1 Arabic transliteration is presented in the B...  \n",
       "1315  Both language models use modified Kneser-Ney s...  \n",
       "1316  Scope-ignorant (Disambig.): Our previous MLN-b...  \n",
       "1317  We follow the neural network architecture of V...  \n",
       "1318  We trained a number of French-English SMT syst...  \n",
       "1319  For our SMT experiments, we use the Moses tool...  \n",
       "1320  SMT translations: a phrase-based SMT system bu...  \n",
       "1321  We trained a model using Moses toolkit (Koehn ...  \n",
       "1322  The method, ROUGE (Lin and Hovy, 2003 ), is ba...  \n",
       "1323  We used the unigram counts from the Web 1T 5-g...  \n",
       "1324  This paper describes the approach of the SemaN...  \n",
       "1325  The recent BioNLP 2009 shared task (BioNLP09ST...  \n",
       "1326  The CCM is a generative model for the unsuperv...  \n",
       "1327  The idea of representing a constituent by its ...  \n",
       "1328  We use the dependency tree long short-term mem...  \n",
       "1329  We use the French- English parallel corpus (ap...  \n",
       "1330  We used the Stanford Dependency Parser (de Mar...  \n",
       "1331  We used the Stanford Dependency Parser (de Mar...  \n",
       "1332  The data used for the experiments described in...  \n",
       "1333  The model architecture, shown in figure 1 , is...  \n",
       "1334  SMT translations: a phrase-based SMT system bu...  \n",
       "1335  In Lu et al. (2008) , the mixgram model (an in...  \n",
       "1336  We use the French- English parallel corpus (ap...  \n",
       "1337  We use the French- English parallel corpus (ap...  \n",
       "1338  Each model was trained during 50 epochs using ...  \n",
       "1339  We use the Moses phrase-based translation syst...  \n",
       "1340  We also used Giza++ word alignment tool (Och a...  \n",
       "1341  We use the Moses phrase-based translation syst...  \n",
       "1342  Phrase pairs were extracted from symmetrized w...  \n",
       "1343  Our work is motivated by the Bayesian HMM appr...  \n",
       "1344  Given the LP constraints in (4), (7), and (8),...  \n",
       "1345  As mentioned above, we extend the WMF model pr...  \n",
       "1346  The dependency path is the shortest path betwe...  \n",
       "1347  Outside of the rhetorical features, the discou...  \n",
       "1348  Table 8 to Table 12 show the Macro-Average (F ...  \n",
       "1349  This paper describes the details of our system...  \n",
       "1350  We used the English side of the Europarl corpu...  \n",
       "1351  We briefly review the HMM based word alignment...  \n",
       "1352  The improvement is statistically significant a...  \n",
       "1353  In the unsupervised approach, graph-based rank...  \n",
       "1354  Especially, we use the WEKA (Hall et al., 2009...  \n",
       "1355  Dropout (Srivastava et al., 2014) is a very ef...  \n",
       "1356  For our SMT experiments, we use the Moses tool...  \n",
       "1357  We make use of dependency parse information fr...  \n",
       "1358  We preprocessed the training corpora with scri...  \n",
       "1359  We use the Stanford dependency parser (de Marn...  \n",
       "1360  All parameters are learned by Adam optimizer (...  \n",
       "1361  One of the phrase-based systems moreover utili...  \n",
       "1362  A statistical significance test was performed ...  \n",
       "1363  We use the Stanford dependency parser (de Marn...  \n",
       "1364  We use online learning to train model paramete...  \n",
       "1365  We use the 100-dimensional GloVe word embeddin...  \n",
       "1366  We use online learning to train model paramete...  \n",
       "1367  We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "1368  The SMT systems were built using the Moses too...  \n",
       "1369  We also use these perturbation schemes to crea...  \n",
       "1370  We trained an English 5-gram language model us...  \n",
       "1371  We make use of dependency parse information fr...  \n",
       "1372  The second is the RTE part of the SICK dataset...  \n",
       "1373  Most previous work use sentence modeling with ...  \n",
       "1374  The classes used to train the DATE tagger are ...  \n",
       "1375  To examine the effect of normalization on depe...  \n",
       "1376  We construct word-word co-occurrence matrix X;...  \n",
       "1377  We extract facts from captions using Clausie (...  \n",
       "1378  Next, the output of the max-pooling layer is p...  \n",
       "1379  The bootstrap sampling method provides a way f...  \n",
       "1380  The significance testing is performed by paire...  \n",
       "1381  The population distribution was estimated by t...  \n",
       "1382  For French-English experiments, we used the EM...  \n",
       "1383  The translation system is trained using the we...  \n",
       "1384  Then we revise the two LP constraints of Cho &...  \n",
       "1385  Derivatives are computed efficiently via backp...  \n",
       "1386  Distributional models of meaning follow the di...  \n",
       "1387  We use the WEKA toolkit (Hall et al., 2009) fo...  \n",
       "1388  The experiments are carried out on a subset of...  \n",
       "1389  The Moses toolkit (Koehn et al., 2007) is used...  \n",
       "1390                corpus (Steinberger et al., 2006 ).  \n",
       "1391  This is done using IBM Model 1 (Brown et al., ...  \n",
       "1392  We compare the proposed model to our implement...  \n",
       "1393  All the experiments are carried out in Moses t...  \n",
       "1394  For our experiments, we use 40,000 sentences f...  \n",
       "1395  The data was tagged using TnT (Brants, 2000 ),...  \n",
       "1396  Holmqvist et al. (2012) presented a method whe...  \n",
       "1397  Language Model: For all 3-SCFG systems we use ...  \n",
       "1398  For training bilingual topic models, we use Ma...  \n",
       "1399  We use the Stanford parser with Stanford depen...  \n",
       "1400  We also compare annotation strategies in terms...  \n",
       "1401  For the German experiments, we used the NEGRA ...  \n",
       "1402  We use the scikit implementation of SVM (Pedre...  \n",
       "1403  For Indep-Logistic, we used scikit-learn (Pedr...  \n",
       "1404  We trained a standard Moses baseline (Koehn et...  \n",
       "1405  We use the Stanford parser with Stanford depen...  \n",
       "1406  One is a 3-gram language model built using Ken...  \n",
       "1407  1 The corpus is lemmatised and tagged by part-...  \n",
       "1408  The COMLEX syntax dictionary (Grishman et al.,...  \n",
       "1409  POS tagging was performed with the TreeTagger ...  \n",
       "1410  For language modeling we used the KenLM toolki...  \n",
       "1411  Och is the HMM alignment model of (Och and Ney...  \n",
       "1412  These features were obtained using the Stanfor...  \n",
       "1413  We use the standard Stanford-style set of depe...  \n",
       "1414  For the summarization task, we compare results...  \n",
       "1415  We calculated significance using paired bootst...  \n",
       "1416  Gradients computed using the automatic differe...  \n",
       "1417  For Indep-Logistic, we used scikit-learn (Pedr...  \n",
       "1418  The data was tagged using TnT (Brants, 2000 ),...  \n",
       "1419  1 Regarding the learning algorithm, we used ge...  \n",
       "1420  We build our PB-SMT systems in a standard way ...  \n",
       "1421  As output, the grammar delivers detailed seman...  \n",
       "1422  One of the first venues at which domain adapta...  \n",
       "1423  However, like Cho & Chai (2000) , our analysis...  \n",
       "1424  A core component of every PBSMT system is the ...  \n",
       "1425  The Google Web 1T data (Brants and Franz, 2006...  \n",
       "1426  A complete set of parser tags and the method u...  \n",
       "1427  A complete set of parser tags and the method u...  \n",
       "1428  Liang et al. (2006) observe that standard upda...  \n",
       "1429  We present a system that takes a general Moses...  \n",
       "1430  New York Times consists of 500 random sentence...  \n",
       "1431  We consider the following types of implicit re...  \n",
       "1432  The eval-uation set was comprised of adult utt...  \n",
       "1433  The summaries from the above algorithm for the...  \n",
       "1434  The TOEFL synonym dataset (Landauer and Dumais...  \n",
       "1435  1 We used the KenLM toolkit (Heafield, 2011) t...  \n",
       "1436  The regressor used is a Random Forest Regresso...  \n",
       "1437  The statistical dictionary for this task was e...  \n",
       "1438  We used bootstrap resampling for testing stati...  \n",
       "1439  We used the GIZA++ software (Och and Ney, 2003...  \n",
       "1440  We calculate our features using the KenLM tool...  \n",
       "1441  We then run word alignment with GIZA++ (Och an...  \n",
       "1442  The word alignment was obtained by running Giz...  \n",
       "1443  In the scenario of human-computer interactive ...  \n",
       "1444  In order to reduce the amount of annotated dat...  \n",
       "1445  Next, the files are processed with the morphol...  \n",
       "1446  For building our SMT systems, the open-source ...  \n",
       "1447  We use the Bernoulli Naive Bayes classifier in...  \n",
       "1448  We use online learning to train model paramete...  \n",
       "1449  The baseline systems are built with the openso...  \n",
       "1450  We use online learning to train model paramete...  \n",
       "1451  The SEMCOR corpus (Miller et al., 1994) is one...  \n",
       "1452  A linear-kernel Support Vector Machine (Chang ...  \n",
       "1453  We tokenize and truecase all of the corpora us...  \n",
       "1454  We build a state of the art phrase-based SMT s...  \n",
       "1455  We then made use of the GIZA++ software (Och a...  \n",
       "1456  There are two approaches proposed in the liter...  \n",
       "1457  We trained a model using Moses toolkit (Koehn ...  \n",
       "1458  We found the largest of such corpus to be the ...  \n",
       "1459  We use the Moses software package 5 to train a...  \n",
       "1460  We use the MADA package (Habash et al., 2009) ...  \n",
       "1461  The data provided for the shared task is prepa...  \n",
       "1462  We built a source-to-target PB-SMT model from ...  \n",
       "1463  We used the Brill tagger provided by NLTK for ...  \n",
       "1464  We tokenize and truecase all of the corpora us...  \n",
       "1465  Our baseline is a phrase-based MT system train...  \n",
       "1466  For English, we used the CELEX database (Baaye...  \n",
       "1467  We then run word alignment with GIZA++ (Och an...  \n",
       "1468  Pang et al. (2002)  use machine learning metho...  \n",
       "1469  We use online learning to train model paramete...  \n",
       "1470  The Web1T corpus (Brants and Franz, 2006) is a...  \n",
       "1471  The corpora are tokenised and truecased using ...  \n",
       "1472  @BULLET Logistic Regression(LR): We use Logist...  \n",
       "1473  We tokenize and frequent-case the data with th...  \n",
       "1474  The language model is a standard 5-gram model ...  \n",
       "1475  We used the unigram counts from the Web 1T 5-g...  \n",
       "1476  The column\" pair-CI\" shows 95% confidence inte...  \n",
       "1477  The Weka SMO implementation of SVM (Hall et al...  \n",
       "1478  Distributional similarity relies on the distri...  \n",
       "1479  Pitler and Nenkova (2008) used discourse relat...  \n",
       "1480  Distributional similarity relies on the distri...  \n",
       "1481  The optimization is done using the Downhill Si...  \n",
       "1482  The SemEval-2010 Task 8 dataset is a widely us...  \n",
       "1483  We compare the model against the Moses phrase-...  \n",
       "1484  The experiments are carried out on a broad-cov...  \n",
       "1485  In all cases, results are statistically signif...  \n",
       "1486  We extract a list containing around 4,000 rela...  \n",
       "1487  The number of features extracted from the PDT ...  \n",
       "1488  Similar to the work of Hernault et al. (2010) ...  \n",
       "1489  1 Full details of the experimental protocol, d...  \n",
       "1490  For non-projective parsing experiments, four l...  \n",
       "1491  For the theory of Cho & Chai (2000) to be comp...  \n",
       "1492  The approach presented in this paper is a firs...  \n",
       "1493  We calculated significance using paired bootst...  \n",
       "1494  selects the translation with minimum Bayes ris...  \n",
       "1495  Additionally, we will compare two decision rul...  \n",
       "1496  We build on a recent selectional preference mo...  \n",
       "1497  ROUGE-2 metric (Lin, 2004) is used for the eva...  \n",
       "1498  The senses in WordNet are ordered according to...  \n",
       "1499  As a supervised classifier for VSM and WIKI, w...  \n",
       "1500  We compare the model against the Moses phrase-...  \n",
       "1501  The particle filter studied empirically by Can...  \n",
       "1502  We here describe a linear approximation to the...  \n",
       "1503  We conducted experiments using Multinomial Nai...  \n",
       "1504  The network is trained using SGD with shuffled...  \n",
       "1505  Statistical significance of the difference bet...  \n",
       "1506  For all results, we computed their confidence ...  \n",
       "1507  In the POS tag level, we basically used the un...  \n",
       "1508  Here, we use (1) the Link Grammar Parser 8 and...  \n",
       "1509  For statistical significance testing, we use a...  \n",
       "1510  Gildea and Jurafsky (2002)  were the first to ...  \n",
       "1511  We use a minibatch size of 100, and use AdaDel...  \n",
       "1512  We use the scikit implementation of SVM (Pedre...  \n",
       "1513  Statistical significance tests are performed u...  \n",
       "1514  1 The parser implements the arc-standard algor...  \n",
       "1515  In the training procedure, we use AdaDelta (Ze...  \n",
       "1516  The systems for the English ? Spanish translat...  \n",
       "1517  The input of the Boxer system is a syntactic a...  \n",
       "1518  TESLA (Translation Evaluation of Sentences wit...  \n",
       "1519  The ERG produces Minimal Recursion Semantics (...  \n",
       "1520  Future work could be to extend the German data...  \n",
       "1521  Baroni et al. (2002)  analyzed the 28 million ...  \n",
       "1522  5-gram language models are trained over the ta...  \n",
       "1523  10 We used the PLTM implementation in Mallet (...  \n",
       "1524  The statistical significance test is also carr...  \n",
       "1525  Finally, we used Moses toolkit as phrase-based...  \n",
       "1526  We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "1527  We also lemmatized all words using Stanford Co...  \n",
       "1528  3 We used logistic regression (though linear S...  \n",
       "1529  The closest area to our work consists of inves...  \n",
       "1530  5-gram language models are trained over the ta...  \n",
       "1531             1 using AdaGrad (Duchi et al., 2011 ).  \n",
       "1532  All our translation systems are based on Moses...  \n",
       "1533  We expect this restriction is more consistent ...  \n",
       "1534  For training the translation model and for dec...  \n",
       "1535  We extract our paraphrase grammar from the Fre...  \n",
       "1536  @BULLET Logistic Regression(LR): We use Logist...  \n",
       "1537  We tokenize English data and segment Chinese d...  \n",
       "1538  1 The corpus is lemmatised and tagged by part-...  \n",
       "1539  We use logistic regression with L2 regularizat...  \n",
       "1540  Our machine translation systems this year are ...  \n",
       "1541  The compared systems are evaluated on the Engl...  \n",
       "1542  We evaluated our method with movie review docu...  \n",
       "1543  For example, Turian et al. (2010)  showed that...  \n",
       "1544  Word alignment is performed by GIZA++ (Och and...  \n",
       "1545             1 using AdaGrad (Duchi et al., 2011 ).  \n",
       "1546  One of the most important resources for discou...  \n",
       "1547  The EuroParl data set consists of 707 sentence...  \n",
       "1548  Turian et al. (2010) applied word embeddings t...  \n",
       "1549  In particular, we use Moses (Koehn et al., 200...  \n",
       "1550  Our submitted system for the second task is ba...  \n",
       "1551  We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "1552  Zhang et al. (2015) explore a shallow convolut...  \n",
       "1553  We tokenize English data and segment Chinese d...  \n",
       "1554  The source of bilingual data used in the exper...  \n",
       "1555  ROUGE (Lin, 2004) is the fully automatic metri...  \n",
       "1556  We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "1557  The baseline systems are built with the openso...  \n",
       "1558  The log-linear approach to phrase-based transl...  \n",
       "1559  We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "1560  3 All English data are POS tagged and lemmatis...  \n",
       "1561  The trigram target language model is trained f...  \n",
       "1562  Translation models were trained over the bilin...  \n",
       "1563  The core model is a decision tree classifier t...  \n",
       "1564  We built phrase-based machine translation syst...  \n",
       "1565  Note that word sense disambiguation is perform...  \n",
       "1566  We use a well-known scikitlearn (Pedregosa et ...  \n",
       "1567  Turian et al. (2010) applied word embeddings t...  \n",
       "1568  We used the Brill tagger provided by NLTK for ...  \n",
       "1569  All our systems are contrasted with a standard...  \n",
       "1570  We use the Moses software package 5 to train a...  \n",
       "1571  For training the translation model and for dec...  \n",
       "1572  We employed the state-of-the-art sentiment ann...  \n",
       "1573  Our work is also related to (Bunescu and Moone...  \n",
       "1574  The system that obtained the best performance ...  \n",
       "1575  SMT translations: a phrase-based SMT system bu...  \n",
       "1576  Given the LP constraints in (4), (7), and (8),...  \n",
       "1577  Part of speech tagging and named entity recogn...  \n",
       "1578  To build a parser, we use a structured classif...  \n",
       "1579  Semantic Textual Similarity (STS) is the task ...  \n",
       "1580  We also use A0 to compare against the\" masking...  \n",
       "1581  The same kind of process was applied to the Pe...  \n",
       "1582  For Reuters we segmented and tokenized the dat...  \n",
       "1583  We use the Moses phrase-based translation syst...  \n",
       "1584  The corpus is first word-aligned using a word ...  \n",
       "1585  3 DKPro is a collection of software components...  \n",
       "1586  In order to construct the lexical prior knowle...  \n",
       "1587  We use the Stanford CoreNLP caseless tagger fo...  \n",
       "1588  The proof is similar to the proof for the tree...  \n",
       "1589  We calculate statistical significance of perfo...  \n",
       "1590  We use the Stanford CoreNLP caseless tagger fo...  \n",
       "1591  Part of speech tagging and named entity recogn...  \n",
       "1592  It was also the model used to rank sentences i...  \n",
       "1593  Statistical significance is tested on the BLEU...  \n",
       "1594  To validate this measure, we computed the cosi...  \n",
       "1595  1 We used the KenLM toolkit (Heafield, 2011) t...  \n",
       "1596  We used KenLM (Heafield, 2011) to create 3-gra...  \n",
       "1597  In the News column , we show the statistics of...  \n",
       "1598  Since the GoogleAPI is not available any more,...  \n",
       "1599  The Moses SMT toolkit (Koehn et al., 2007) pro...  \n",
       "1600  We trained a model using Moses toolkit (Koehn ...  \n",
       "1601  The chunk label tagset is a coarser version of...  \n",
       "1602  We use a set of 30 word pairs from a study car...  \n",
       "1603  All annotations were done using the brat rapid...  \n",
       "1604  We use the Stanford CoreNLP caseless tagger fo...  \n",
       "1605  For the exploding gradient problem, numerical ...  \n",
       "1606  The annotations were made using the BRAT rapid...  \n",
       "1607  For Chinese, we use the Penn Chinese Treebank ...  \n",
       "1608  For statistical significance testing, we use a...  \n",
       "1609  Significance was tested using a paired bootstr...  \n",
       "1610  The preposition classifier uses a combined sys...  \n",
       "1611  As a practical approximation, we use bigram co...  \n",
       "1612  Word alignments were created using GIZA++ (Och...  \n",
       "1613  Weka (Hall et al., 2009) which contains the im...  \n",
       "1614  The parallel data were taken from OPUS (Tiedem...  \n",
       "1615  We conducted experiments using Multinomial Nai...  \n",
       "1616  The experiments focus on translation from Germ...  \n",
       "1617  This architecture is very similar to the frame...  \n",
       "1618  We use the CoNLL-X (Buchholz and Marsi, 2006) ...  \n",
       "1619  We note that our model outperforms the model p...  \n",
       "1620  We note that our model outperforms the model p...  \n",
       "1621  Zelenko et al. (2003) have shown the contiguou...  \n",
       "1622  To this end, a recent large-scale annotation e...  \n",
       "1623  We use Stanford parser (de Marneffe et al., 20...  \n",
       "1624  Only for German data did we used the TreeTagge...  \n",
       "1625  We built a knowledge base (V 2 R) 1 using the ...  \n",
       "1626  The first two baselines are standard systems u...  \n",
       "1627  The decoder is built on top of an open-source ...  \n",
       "1628  Significance was tested using a paired bootstr...  \n",
       "1629  We use Stanford parser (de Marneffe et al., 20...  \n",
       "1630  For Reuters we segmented and tokenized the dat...  \n",
       "1631  The grammatical relations are all the collapse...  \n",
       "1632  The decoder is built on top of an open-source ...  \n",
       "1633  Finally the ANEW lexicon (Bradley and Lang, 19...  \n",
       "1634  The NLP Group of the Idiap Research Institute ...  \n",
       "1635  An example of such a pragmatic factor is commo...  \n",
       "1636  We then have assigned a sentiment score using ...  \n",
       "1637  We used a subset of the data provided for the ...  \n",
       "1638  Holmqvist et al. (2012) presented a method whe...  \n",
       "1639  Distributional models of meaning follow the di...  \n",
       "1640   Europarl 2 (Koehn, 2005 ): it is a corpus ...  \n",
       "1641  Language Model: For all 3-SCFG systems we use ...  \n",
       "1642  To evaluate DA for sentiment classification, w...  \n",
       "1643  For this purpose we use the Europarl corpus (K...  \n",
       "1644  In order to evaluate the fluency of each syste...  \n",
       "1645  The annotations were made using the BRAT rapid...  \n",
       "1646  The SVM models were trained using the Scikit-l...  \n",
       "1647  We used the English side of the Europarl corpu...  \n",
       "1648  The language model is a 5-gram KenLM (Heafield...  \n",
       "1649  We used the English side of the Europarl corpu...  \n",
       "1650  We also lemmatized all words using Stanford Co...  \n",
       "1651  We trained our basic neural machine translatio...  \n",
       "1652  We used the GIZA++ software (Och and Ney, 2003...  \n",
       "1653  The matrix is weighted with PPMI as implemente...  \n",
       "1654  Corpus-based VSMs follow the standard\" distrib...  \n",
       "1655  For Chinese, a segmentation model (Zhang et al...  \n",
       "1656  As our learner, we use LIBSVM with a linear ke...  \n",
       "1657  The corpus was converted from XML to raw text,...  \n",
       "1658  For example, the OntoNotes (Hovy et al., 2006)...  \n",
       "1659  The Moses toolkit (Koehn et al., 2007) is used...  \n",
       "1660  Specifically, we use the LIBLINEAR SVM package...  \n",
       "1661  For this purpose we use the Europarl corpus (K...  \n",
       "1662  The ICSI Meeting Corpus: The ICSI Meeting Corp...  \n",
       "1663  We use ROUGE metric (Lin, 2004) to evaluate ge...  \n",
       "1664  The task is part of the Semantic Evaluation 20...  \n",
       "1665  For this purpose we use the Europarl corpus (K...  \n",
       "1666  We also used automatically back-translated in-...  \n",
       "1667  We also lemmatized all words using Stanford Co...  \n",
       "1668  Corpus-based meaning representations rely on t...  \n",
       "1669  Turian et al. (2010)  evaluate different techn...  \n",
       "1670  We then use the phrase extraction utility in t...  \n",
       "1671  We used LIBSVM to implement our own SVM for re...  \n",
       "1672  The word alignment was obtained by running Giz...  \n",
       "1673  The word alignment was obtained by running Giz...  \n",
       "1674  We used bootstrap resampling for testing stati...  \n",
       "1675  We used bootstrap resampling for testing stati...  \n",
       "1676  Markov Logic Networks (MLN) (Richardson and Do...  \n",
       "1677  We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "1678  We initialize parameters uniformly, using the ...  \n",
       "1679  More recently, Liu et al. (2012) have proposed...  \n",
       "1680  We evaluate our method by means of the Europar...  \n",
       "1681  We also used automatically back-translated in-...  \n",
       "1682  Phrase extraction was performed following Koeh...  \n",
       "1683  In our low-resource condition, we trained an S...  \n",
       "1684  Word alignment was done with GIZA++ (Och and N...  \n",
       "1685  The closest area to our work consists of inves...  \n",
       "1686  The other groups have been used already (for e...  \n",
       "1687  For this purpose we use the Europarl corpus (K...  \n",
       "1688  We used the English side of the Europarl corpu...  \n",
       "1689  For the MSRP task, Socher et al. (2011) used a...  \n",
       "1690  AIDA/YAGO is derived from the CoNLL-2003 share...  \n",
       "1691  For this purpose, we use the Moses toolkit to ...  \n",
       "1692  We measure translation quality via the BLEU sc...  \n",
       "1693  We used the Linear SVM implementation (with de...  \n",
       "1694  AIDA/YAGO is derived from the CoNLL-2003 share...  \n",
       "1695  We also lemmatized all words using Stanford Co...  \n",
       "1696  All novels were lemmatized and POS-tagged usin...  \n",
       "1697  We also use Adadelta (Zeiler, 2012) to optimiz...  \n",
       "1698  AIDA/YAGO is derived from the CoNLL-2003 share...  \n",
       "1699  All novels were lemmatized and POS-tagged usin...  \n",
       "1700  Hovy et al. (2013) applied tree kernels to met...  \n",
       "1701  AIDA/YAGO is derived from the CoNLL-2003 share...  \n",
       "1702  We used the English side of the Europarl corpu...  \n",
       "1703  We used bootstrap resampling for testing stati...  \n",
       "1704  The Moses toolkit (Koehn et al., 2007) is used...  \n",
       "1705  We used bootstrap resampling for testing stati...  \n",
       "1706  One of the best-known methods of representing ...  \n",
       "1707  We have used the baseNP data presented in (Ram...  \n",
       "1708  The second algorithm, denoted GloTr, is the Ch...  \n",
       "1709  Training and querying of a modified Kneser-Ney...  \n",
       "1710  Baseline word alignments were obtained by runn...  \n",
       "1711  In order to improve the robustness of the word...  \n",
       "1712  The language models are estimated using the Ke...  \n",
       "1713  For all experiments, we use a decision-tree cl...  \n",
       "1714  We used the lexicalized dependency parser in t...  \n",
       "1715  We built phrase-based machine translation syst...  \n",
       "1716  We use Moses 7 (Koehn et al., 2007) to impleme...  \n",
       "1717  For training the translation model and for dec...  \n",
       "1718  The baseline systems are built with the openso...  \n",
       "1719  To solve coreference, we used a variation of t...  \n",
       "1720  We built phrase-based machine translation syst...  \n",
       "1721  We extract syntactic dependencies using Stanfo...  \n",
       "1722  We run our experiments on Europarl (Koehn, 200...  \n",
       "1723  In our experiments , we used the Stanford pars...  \n",
       "1724  The OpenFst library is used to perform all of ...  \n",
       "1725  This is a corpus-based metric relying on the d...  \n",
       "1726  We build a state of the art phrase-based SMT s...  \n",
       "1727  For BROWN, the features are the prefix feature...  \n",
       "1728  For a pair of words, WordNet provides a series...  \n",
       "1729  Our baseline is a phrase-based MT system train...  \n",
       "1730  We measure significance of results using boots...  \n",
       "1731  3.3.1 Reference System We compare a number of ...  \n",
       "1732  3 All English data are POS tagged and lemmatis...  \n",
       "1733  In addition, the data was tokenized, lemmatize...  \n",
       "1734  Europarl (Koehn, 2005) is a multilingual paral...  \n",
       "1735  Logistic regression, implemented in Python wit...  \n",
       "1736  In this paper, we use the subjectivity corpus ...  \n",
       "1737  In our experiments , we used the Stanford pars...  \n",
       "1738  We also compare our word embeddings with the E...  \n",
       "1739  We used the lexicalized dependency parser in t...  \n",
       "1740  The Dutch version was tagged automatically usi...  \n",
       "1741  The decoder is implemented with Weighted Finit...  \n",
       "1742  For Chinese-English experiments, we used the O...  \n",
       "1743  The baseline systems are built with the openso...  \n",
       "1744  The decoder is implemented with Weighted Finit...  \n",
       "1745  We run our experiments on Europarl (Koehn, 200...  \n",
       "1746  Dhillon et al. (2015) used CCA to derive word ...  \n",
       "1747  The rapid growth of user-generated content, mu...  \n",
       "1748  These classifiers are based on a discriminativ...  \n",
       "1749  We build a state of the art phrase-based SMT s...  \n",
       "1750  For optimization, we employed the Adam 6 http:...  \n",
       "1751  was used for word alignment and phrase transla...  \n",
       "1752  In addition, the data was tokenized, lemmatize...  \n",
       "1753  A standard for predicate argument annotation i...  \n",
       "1754  The source of bilingual data used in the exper...  \n",
       "1755  We used the lexicalized dependency parser in t...  \n",
       "1756  We extract syntactic dependencies using Stanfo...  \n",
       "1757  2 Further, we sentence-split, tokenized, and l...  \n",
       "1758  ROUGE (Lin and Hovy, 2003) has been adopted as...  \n",
       "1759  All experiments were carried out using the ope...  \n",
       "1760  We directly apply the Smoothed Partial Tree-Ke...  \n",
       "1761  2 Further, we sentence-split, tokenized, and l...  \n",
       "1762  We used the lexicalized dependency parser in t...  \n",
       "1763  We build a state of the art phrase-based SMT s...  \n",
       "1764  The Moses toolkit (Koehn et al., 2007) is used...  \n",
       "1765  We build a state of the art phrase-based SMT s...  \n",
       "1766  We tokenize and frequent-case the data with th...  \n",
       "1767  Baroni et al. (2002)  report that 47% of the v...  \n",
       "1768  To train a mention-pair classifier, we use the...  \n",
       "1769  Following the evaluation paradigm of Mitchell ...  \n",
       "1770  This system is the Moses decoder (Koehn et al....  \n",
       "1771  Our alignment model is based on a simple varia...  \n",
       "1772  The original reordering constraint in Berger e...  \n",
       "1773  Mikolov et al. (2013a) proposed a faster skip-...  \n",
       "1774  The corpus consists of a subset of the Brown C...  \n",
       "1775  All these features are inherited from Moses (K...  \n",
       "1776  We also lemmatized all words using Stanford Co...  \n",
       "1777  10 We used the PLTM implementation in Mallet (...  \n",
       "1778  LIBLINEAR (Fan et al., 2008 ), a library for l...  \n",
       "1779  We use a mini-batch stochastic gradient descen...  \n",
       "1780  Specifically, we use Support Vector Machine (S...  \n",
       "1781  Feature weights, based on BLEU, are then tuned...  \n",
       "1782  Brin proposed the bootstrapping method for rel...  \n",
       "1783  Neural networks are first used in this task in...  \n",
       "1784  We used the GIZA++ software (Och and Ney, 2003...  \n",
       "1785  We trained a CRF tagger using CRFSuite 1 (Okaz...  \n",
       "1786  The HMM classifier used in the experiments in ...  \n",
       "1787  The language model is a 5-gram with interpolat...  \n",
       "1788  We plan to adapt ideas from Socher et al. (201...  \n",
       "1789  We use the Support Vector Machines implementat...  \n",
       "1790  For argument, a dependency version of the prun...  \n",
       "1791  We used bootstrap resampling for testing stati...  \n",
       "1792  Specifically, we use Support Vector Machine (S...  \n",
       "1793  We used the Stuttgart TreeTagger (Schmid, 1994...  \n",
       "1794  Earlier models made use of latent semantic ana...  \n",
       "1795  In principle, classifiers trained on PDTB data...  \n",
       "1796  The edit distance kernel was trained with LIBS...  \n",
       "1797  It is a standard phrase-based machine translat...  \n",
       "1798  Specifically, we use Support Vector Machine (S...  \n",
       "1799  The word alignment is created by GIZA++ (Och a...  \n",
       "1800  The SVM implementation used was LIBSVM (Chang ...  \n",
       "1801  We used GIZA++ (Och and Ney, 2003) to align th...  \n",
       "1802  We used the GIZA++ software (Och and Ney, 2003...  \n",
       "1803  In contrast, the set of discourse markers in o...  \n",
       "1804  The SVM implementation of Weka (Hall et al., 2...  \n",
       "1805  This was done with a specific tool provided wi...  \n",
       "1806  We used the GIZA++ software (Och and Ney, 2003...  \n",
       "1807  We performed word alignment using GIZA++ (Och ...  \n",
       "1808  We used Google Books ngrams (Michel et al., 20...  \n",
       "1809  We also lemmatized all words using Stanford Co...  \n",
       "1810  We evaluate C&W word embeddings with 25, 50 an...  \n",
       "1811  This is often called distributional semantics ...  \n",
       "1812  Our neural network is similar to that of Li et...  \n",
       "1813  We used GIZA++ (Och and Ney, 2003) to align th...  \n",
       "1814  Meaning representation and composition in the ...  \n",
       "1815  For this purpose we use the Europarl corpus (K...  \n",
       "1816  The stopwords are taken from the stop-word lis...  \n",
       "1817  We build a state of the art phrase-based SMT s...  \n",
       "1818  One idea is to apply it to the language model ...  \n",
       "1819  We extract syntactic dependencies using Stanfo...  \n",
       "1820  The corpora are tokenised and truecased using ...  \n",
       "1821  To make the exponential algorithm practical, b...  \n",
       "1822  The EuroParl data set consists of 707 sentence...  \n",
       "1823  Our approach is based on the Czech linguistic ...  \n",
       "1824  These classifiers have been used in related wo...  \n",
       "1825  We build a baseline error correction system, u...  \n",
       "1826  We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "1827  This sense similarity measure is inspired by t...  \n",
       "1828  The baseline systems are built with the openso...  \n",
       "1829  In our experiments , we used the Stanford pars...  \n",
       "1830  We implement MERT and MIRA 1 , and directly us...  \n",
       "1831  We tokenize and frequent-case the data with th...  \n",
       "1832  Our baseline is a phrase-based MT system train...  \n",
       "1833  The corpora are tokenised and truecased using ...  \n",
       "1834  We built phrase-based machine translation syst...  \n",
       "1835  We build a state of the art phrase-based SMT s...  \n",
       "1836  We build a state of the art phrase-based SMT s...  \n",
       "1837  We built phrase-based machine translation syst...  \n",
       "1838  McClosky et al. (2006)  use self-training in c...  \n",
       "1839  @BULLET Logistic Regression(LR): We use Logist...  \n",
       "1840  3 All English data are POS tagged and lemmatis...  \n",
       "1841  We use the Bernoulli Naive Bayes classifier in...  \n",
       "1842  Both systems are based on the Moses SMT toolki...  \n",
       "1843  We tokenize and frequent-case the data with th...  \n",
       "1844  We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "1845  We built phrase-based machine translation syst...  \n",
       "1846  We extract our paraphrase grammar from the Fre...  \n",
       "1847  In our experiments , we used the Stanford pars...  \n",
       "1848  In our experiments , we used the Stanford pars...  \n",
       "1849  All our systems are contrasted with a standard...  \n",
       "1850  We use AdaGrad (Duchi et al., 2011)  with the ...  \n",
       "1851  We train the concept identification stage usin...  \n",
       "1852  We use the cross-entropy loss function and min...  \n",
       "1853  Socher et al. (2011)  use recursive auto-encod...  \n",
       "1854  We build a state of the art phrase-based SMT s...  \n",
       "1855  We build a state of the art phrase-based SMT s...  \n",
       "1856  Our baseline is a phrase-based MT system train...  \n",
       "1857  We built phrase-based machine translation syst...  \n",
       "1858  We build a state of the art phrase-based SMT s...  \n",
       "1859  In our experiments , we used the Stanford pars...  \n",
       "1860  The parameters can be efficiently estimated fr...  \n",
       "1861  We build a state of the art phrase-based SMT s...  \n",
       "1862  All our systems are contrasted with a standard...  \n",
       "1863  NERsuite is a NER system that is built on top ...  \n",
       "1864  We built phrase-based machine translation syst...  \n",
       "1865  Experts can manually specify the attributes of...  \n",
       "1866  Experts can manually specify the attributes of...  \n",
       "1867  We used the publicly available Wacky corpus (B...  \n",
       "1868  Although there had been research on reasoning ...  \n",
       "1869  We tune the systems using kbest batch MIRA (Ch...  \n",
       "1870  We use the MALLET package (McCallum, 2002) for...  \n",
       "1871  We use CRFsuite (Okazaki, 2007) as an implemen...  \n",
       "1872  Word alignments were created using GIZA++ (Och...  \n",
       "1873  We expect this restriction is more consistent ...  \n",
       "1874  We adopt online learning, updating parameters ...  \n",
       "1875  Weka (Hall et al., 2009) which contains the im...  \n",
       "1876  TreeTagger is a statistical, decision tree-bas...  \n",
       "1877  The parallel corpus is wordaligned using GIZA+...  \n",
       "1878  Classification uses the scikit-learn Python pa...  \n",
       "1879  It builds on the C&C CCG parser (Clark and Cur...  \n",
       "1880  Next, we replace all nouns with their POS tag;...  \n",
       "1881    All the data come from Europarl (Koehn, 2005 ).  \n",
       "1882  The training data released by the task organiz...  \n",
       "1883  conducted using the Moses phrase-based decoder...  \n",
       "1884  Och is the HMM alignment model of (Och and Ney...  \n",
       "1885    All the data come from Europarl (Koehn, 2005 ).  \n",
       "1886  glish source with target French) by using GIZA...  \n",
       "1887  Word alignment is performed with Giza++ (Och a...  \n",
       "1888  The models were evaluated using M2 scorer (Dah...  \n",
       "1889  Han et al. (2012) introduced a dictionary base...  \n",
       "1890  Word alignment is done using GIZA++ (Och and N...  \n",
       "1891  We used bootstrap resampling for testing stati...  \n",
       "1892  Word alignment is performed with Giza++ (Och a...  \n",
       "1893  Word alignment is performed using GIZA++ (Och ...  \n",
       "1894  In section 3, we present a solution to the all...  \n",
       "1895  In our experiments we investigated both weak a...  \n",
       "1896  To calculate the number of changes, we used a ...  \n",
       "1897  Second, to avoid the error propagation problem...  \n",
       "1898  Mikolov et al. (2013a) proposed a faster skip-...  \n",
       "1899  A core component of every PBSMT system is the ...  \n",
       "1900  We use the SVM rank implementation (Joachims, ...  \n",
       "1901  All models were trained using Adagrad (Duchi e...  \n",
       "1902  HCRC is tagged with TnT (Brants, 2000 ), train...  \n",
       "1903  We trained the classifiers using the LIBLINEAR...  \n",
       "1904  For reference, we also show the MT performance...  \n",
       "1905  For example, Petrov et al. (2012)  build super...  \n",
       "1906  We chose a threshold such that our approach pr...  \n",
       "1907  The proof is similar to the proof for the tree...  \n",
       "1908  We train with the Adam optimizer (Kingma and B...  \n",
       "1909  We use KenLM 3 (Heafield, 2011) for computing ...  \n",
       "1910  Our model is an extension of the contextual ba...  \n",
       "1911  We generated embeddings by training a characte...  \n",
       "1912  The advantage of an SVM rank model is that it ...  \n",
       "1913  According to the distributional hypothesis of ...  \n",
       "1914  Our experiments were carried out on the Europa...  \n",
       "1915  The language model is a 5-gram KenLM (Heafield...  \n",
       "1916  GENIA (Kim et al., 2003) is a collection of 20...  \n",
       "1917  We generated embeddings by training a characte...  \n",
       "1918  Baseline word alignments were obtained by runn...  \n",
       "1919  For the language model, we used all monolingua...  \n",
       "1920  Many researchers have considered generating pa...  \n",
       "1921  Our experiments were carried out on the Europa...  \n",
       "1922  org/wiki/Fog_Index 8 For word frequency we use...  \n",
       "1923  The first is a reimplementation of the stack-b...  \n",
       "1924  Surdeanu et al. (2012) proposed a novel approa...  \n",
       "1925  Then we revise the two LP constraints of Cho &...  \n",
       "1926  We used the unigram counts from the Web 1T 5-g...  \n",
       "1927  We expect this restriction is more consistent ...  \n",
       "1928  The training set is used to train the phrase-b...  \n",
       "1929  The systems for the English ? Spanish translat...  \n",
       "1930  We use the Stanford parser with Stanford depen...  \n",
       "1931  We use the Stanford parser with Stanford depen...  \n",
       "1932  We use a 5-gram LM trained on the Gigaword cor...  \n",
       "1933  The system was trained on the English and Dani...  \n",
       "1934  We convert the trees in both treebanks from co...  \n",
       "1935  Here we review the parameters of the standard ...  \n",
       "1936  The Moses decoder (Koehn et al., 2007) was use...  \n",
       "1937  This corresponds to a new version of the Frenc...  \n",
       "1938  For a detailed survey of the field of sentimen...  \n",
       "1939  A complete description of the training and tes...  \n",
       "1940  18 Our baseline is the SMT toolkit Moses (Koeh...  \n",
       "1941  Causal relations are among discourse relations...  \n",
       "1942  It receives CCG derivations from the C&C parse...  \n",
       "1943  In transforming natural language text to logic...  \n",
       "1944  We use the Moses phrase-based translation syst...  \n",
       "1945  We also report 95% confidence intervals (CI) m...  \n",
       "1946  The Europarl corpus (Koehn, 2005) is built fro...  \n",
       "1947  7 http://opennlp.apache.org 5.2.3.), we use th...  \n",
       "1948  GATE (Cunningham et al., 2002a) is an architec...  \n",
       "1949  Both language models use modified Kneser-Ney s...  \n",
       "1950  We automatically word-aligned the German part ...  \n",
       "1951  Joint disambiguation and clustering of mention...  \n",
       "1952  Pang et al. (2002)  use machine learning metho...  \n",
       "1953  We refer to the system of Jans et al. (2012) a...  \n",
       "1954  Naive Bayes and Decision Tree models were buil...  \n",
       "1955  We build a state of the art phrase-based SMT s...  \n",
       "1956  We compare the model against the Moses phrase-...  \n",
       "1957  We then describe in more detail a modern Chine...  \n",
       "1958  The first two baselines are standard systems u...  \n",
       "1959  The first two baselines are standard systems u...  \n",
       "1960  We use the cross-entropy loss function and min...  \n",
       "1961  We use AdaGrad (Duchi et al., 2011)  with the ...  \n",
       "1962  We compare the model against the Moses phrase-...  \n",
       "1963  The Penn Discourse TreeBank (PDTB; Prasad et a...  \n",
       "1964  Pitler and Nenkova (2008) used discourse relat...  \n",
       "1965  The details of parsing model were presented in...  \n",
       "1966  In addition, the data was tokenized, lemmatize...  \n",
       "1967  The decoder is built on top of an open-source ...  \n",
       "1968  The mention detection of the Stanford corefere...  \n",
       "1969  CTB6 is used as the Chinese data set in the Co...  \n",
       "1970  In addition, the data was tokenized, lemmatize...  \n",
       "1971  We report F1 performance scored using the offi...  \n",
       "1972  The results of this experiment appear in Table...  \n",
       "1973  We use LibSVM (Chang and Lin, 2011) as the SVM...  \n",
       "1974  Baroni et al. (2002) also pointed out that the...  \n",
       "1975  The most influential of the semantic approache...  \n",
       "1976  We refer to that model as Moses en-es-100k , b...  \n",
       "1977  We follow the formulation of vector compositio...  \n",
       "1978  Our system is a linear model estimated using r...  \n",
       "1979  We use SRL Collobert et al. (2011) to determin...  \n",
       "1980  The sparsity of lexical features can also be t...  \n",
       "1981  For the comparisons of translation quality, th...  \n",
       "1982  We refer to that model as Moses en-es-100k , b...  \n",
       "1983  These techniques were evaluated in experiments...  \n",
       "1984  We use the intersection of direct and reverse ...  \n",
       "1985  We also compare with the standard phrase-based...  \n",
       "1986  Thus, inducing a number of clusters similar to...  \n",
       "1987  We use the intersection of direct and reverse ...  \n",
       "1988  The evaluation corpus is a subset of an ungram...  \n",
       "1989  Translations for English words in the lexical ...  \n",
       "1990  We participated in both subtask A and B of Sem...  \n",
       "1991  To test our method, we conducted two lowresour...  \n",
       "1992  We use the support vector machine (SVM) rank a...  \n",
       "1993  In all of the above tasks, we compare the neur...  \n",
       "1994  We use Viterbi algorithm (Viterbi, 1967) to de...  \n",
       "1995  We measure translation quality via the BLEU sc...  \n",
       "1996  All our systems are contrasted with a standard...  \n",
       "1997  The baseline systems are built with the openso...  \n",
       "1998  The degree of similarity between two similar w...  \n",
       "1999  We built phrase-based machine translation syst...  \n",
       "2000  We built phrase-based machine translation syst...  \n",
       "2001  Finally, we used Moses toolkit as phrase-based...  \n",
       "2002  The degree of similarity between two similar w...  \n",
       "2003  The language models are estimated using the Ke...  \n",
       "2004  We use the cross-entropy loss function and min...  \n",
       "2005  We used SVM implementations from scikit-learn ...  \n",
       "2006  We train the concept identification stage usin...  \n",
       "2007  We used the Moses toolkit (Koehn et al., 2007)...  \n",
       "2008  We used the English side of the Europarl corpu...  \n",
       "2009  \" Koo10\" stands for the Model 1 in (Koo and Co...  \n",
       "2010  Specifically, we used the standard Gradient Bo...  \n",
       "2011  For this purpose we use the Europarl corpus (K...  \n",
       "2012  Very recently Rush et al. (2015) proposed a ne...  \n",
       "2013  We use the scikit implementation of Random For...  \n",
       "2014  Of the remaining twelve, eight came from a lar...  \n",
       "2015  We used the implementation of the scikit-learn...  \n",
       "2016  We built phrase-based machine translation syst...  \n",
       "2017  We also lemmatized all words using Stanford Co...  \n",
       "2018  The baseline systems are built with the openso...  \n",
       "2019  For training the translation model and for dec...  \n",
       "2020  @BULLET Decoder: Moses (Koehn et al., 2007) wi...  \n",
       "2021  We train and evaluate the classifiers in a 10-...  \n",
       "2022  Cheung and Penn (2010) extend the approach of ...  \n",
       "2023  Statistically significant results, calculated ...  \n",
       "2024  For example, Finkel et al. (2005) enabled the ...  \n",
       "2025  The resulting performance of the proposed algo...  \n",
       "2026  Word co-occurence statistics\" You shall know a...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#membaca data json\n",
    "df = pd.read_json('data2knewfix.json')\n",
    "#mengambil kolom data\n",
    "dff = df['data'][2]\n",
    "#merubah list data menjadi dataframe\n",
    "dt=pd.DataFrame(dff,columns=['id_case','class','sentence1','sentence2'])\n",
    "pd.set_option('display.max_rows', dt.shape[0]+1)\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_tandabaca(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_space(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.strip()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def text_cleaning(words):\n",
    "    words = remove_tandabaca(words)\n",
    "    words = remove_space(words)\n",
    "    return words\n",
    "\n",
    "def preprosessing(words):\n",
    "    words = to_lowercase(words)\n",
    "    words = text_cleaning(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    822\n",
       "4    714\n",
       "3    451\n",
       "2     32\n",
       "0      6\n",
       "1      2\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cek jumlah dataset dari setiap kelas\n",
    "dt['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027\n"
     ]
    }
   ],
   "source": [
    "y = dt['class']\n",
    "y = to_categorical(y)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027\n",
      "314\n"
     ]
    }
   ],
   "source": [
    "#panjang kalimat \n",
    "text1 = dt['sentence1'].tolist()\n",
    "text2 = dt['sentence2'].tolist()\n",
    "text = [] \n",
    "for i in range(0, len(text1)): \n",
    "    text.append(text1[i]+text2[i]) \n",
    "print(len(text))\n",
    "print(len(text[1]))\n",
    "words = preprosessing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recently there has been a successful attempt to harmonize the linguistic principles behind the coding systems msd and kr farkas et al 2010 recently there has been a successful attempt to harmonize the coding systems msd and kr farkas et al 2010',\n",
       " 'the first one is the ws353 dataset finkelstein et al 2001 containing 353 pairs of english words that have been assigned similarity ratings by humansthe first one is the ws353 dataset finkelstein et al 2001  which contains 353 pairs of english words that have been assigned similarity ratings by humans',\n",
       " 'abstract meaning representation amr banarescu et al 2013 is a semantic formalism where the meaning of a sentence is encoded as a rooted directed graphabstract meaning representation amr banarescu et al 2013 is a semantic formalism encoding the meaning of a sentence as a rooted directed graph',\n",
       " 'we perform bootstrap resampling with bounds estimation as described in koehn 2004 we perform bootstrap resampling with bounds estimation as described by koehn 2004',\n",
       " 'it is used to support semantic analyses in hpsg english resource  grammar erg copestake and flickinger 2000  but also in other grammar formalisms like lfgit is used to support semantic analyses in the hpsg english resource grammar  copestake and flickinger 2000  but also in other grammar formalisms like lfg',\n",
       " 'we use stanford parser de marneffe et al 2006 to obtain parse trees and dependency relationswe first use a dependency parser de marneffe et al 2006 to parse each sentence and extract the set of dependency relations associated with the sentence',\n",
       " 'rooth et al 1999 propose an expectationmaximization em clustering algorithm for selectional preference acquisition based on a probabilistic latent variable modelalternatively rooth et al 1999  propose an embased clustering smooth for sp',\n",
       " 'the levenshtein distance levenshtein 1966 between two strings is defined as the minimum number of editing operations substitutions deletions and insertionsthe levenshtein distance gives an indication of the similarity between two strings levenshtein 1966',\n",
       " 'we use the moses toolkit koehn et al 2007 to create a statistical phrasebased machine translation model built on the best preprocessed data as described abovewe built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " 'we perform bootstrap resampling with bounds estimation as described in koehn 2004 we perform bootstrap resampling with bounds estimation as described by koehn 2004',\n",
       " 'it is used to support semantic analyses in hpsg english resource  grammar erg copestake and flickinger 2000  but also in other grammar formalisms like lfgit is used to support semantic analyses in the hpsg english resource grammar  copestake and flickinger 2000  but also in other grammar formalisms like lfg',\n",
       " 'it is used to support semantic analyses in the english hpsg grammar erg copestake and flickinger 2000  but also in other grammar formalisms like lfgit is used to support semantic analyses in the hpsg english resource grammar copestake and flickinger 2000  but also in other grammar formalisms like lfg',\n",
       " 'it is used to support semantic analyses in hpsg english grammar erg copestake and flickinger 2000  but also in other grammar formalisms like lfgit is used to support semantic analyses in the hpsg english resource grammar copestake and flickinger 2000  but also in other grammar formalisms like lfg',\n",
       " 'it is used to support semantic analyses in the english hpsg grammar erg copestake and flickinger 2000  but also in other grammar formalisms like lfgit is used to support semantic analyses in the hpsg english resource grammar  copestake and flickinger 2000  but also in other grammar formalisms like lfg',\n",
       " 'we build upon our previous markov logic based approach for joint concept disambiguation and clustering fahrni and strube 2012 we build upon our previous approach for joint concept disambiguation and clustering fahrni and strube 2012',\n",
       " 'details about svm and kfd can be found in taylor and cristianini 2004 details about svm and krr can be found in taylor and cristianini 2004',\n",
       " 'we learn the parameters using a quasinewton procedure with l 1 lasso regularization andrew and gao 2007 we learn the parameters  using a quasinewton qn procedure with l 1 lasso regularization andrew and gao 2007',\n",
       " 'we use the scfg decoder cdec dyer et al 2010 4 and build grammars using its implementation of the suffix array extraction method described in lopez 2007for direct translation we use the scfg decoder cdec dyer et al 2010 4 and build grammars using its implementation of the suffix array extraction method described in lopez 2007',\n",
       " 'this is known as the distributional hypothesis harris 1968 this is known as the distributional hypothesis in linguistics harris 1968',\n",
       " 'all our models  as well as the parser described in henderson 2003  are run only oncethe models  as well as the parser described in henderson 2003  are run only once',\n",
       " 'for strings many such kernel functions exist with various applications in computational biology and computational linguistics taylor and cristianini 2004 for strings a lot of such kernel functions exist with many applications in computational biology and computational linguistics taylor and cristianini 2004',\n",
       " 'the estimation of the semantically smoothed partial tree kernel sptk is made available by an extended version of svmlighttk software 5 moschitti 2006 the estimation of the semantically smoothed partial tree kernel sptk is made available by an extended version of svmlighttk software 7 moschitti 2006',\n",
       " 'we train with the adam optimizer kingma and ba 2015  a learning rate of 00001 batch size of 50 and dropout with probability 02 applied to the hidden layerwe train with the adam optimizer kingma and ba 2015  a learning rate of 00001 batch size of 50 and dropout with probability 02 applied to the hidden layer',\n",
       " 'in our experimental study we use the freely available implementations in weka witten and frank 2005 in our experimental study we use the freely available implementation of svm in weka witten and frank 2005',\n",
       " 'more recently carpineto and romano 2010 showed that the characteristics of the outputs returned by src algorithms suggest the adoption of a meta clustering approachoptimsrc carpineto and romano 2010 showed that the characteristics of the outputs returned by prc algorithms suggest the adoption of a meta clustering approach',\n",
       " 'they are based on the distributional hypothesis harris 1968 and by looking at a set of event expressions whose argument fillers have a similar distribution they try to recognize synonymous eventthese methods rely on the distributional hypothesis harris 1968  and by looking at a set of event expressions whose argument fillers have a similar distribution try to recognize synonymous event',\n",
       " 'word alignment is performed using giza och and ney 2003 word alignment is done using giza och and ney 2003',\n",
       " 'word alignment is performed using giza och and ney 2003 word alignment is done using giza och and ney 2003',\n",
       " 'we used mallet software mccallum 2002 for crf experimentswe used mallet software mccallum 2002 for sscrf experiments',\n",
       " 'the thesaurus consists of a hierarchy of 2710 semantic classes defined for over 264312 nouns with a maximum depth of twelve ikehara et al 1997 the ontology goitaikei consists of a hierarchy of 2710 semantic classes defined for over 264312 nouns with a maximum depth of 12 ikehara et al 1997',\n",
       " 'the detailed discussion is provided in the longer version of the paper kim et al 2013 a detailed discussion on the results is provided in the longer version of the paper kim et al 2013',\n",
       " 'the first one is the ws 353 3 dataset finkelstein et al 2001 containing 353 pairs of english words that have been assigned similarity ratings by humansthe first one is the ws353 dataset finkelstein et al 2001  which contains 353 pairs of english words that have been assigned similarity ratings by humans',\n",
       " 'a framework for human error analysis and error classification has been proposed in vilar et al 2006  but like human evaluation  this is also a time consuming taska framework for human error analysis has been proposed in vilar et al 2006  but as every human evaluation this is also a time consuming task',\n",
       " 'maxent classifier is a good example of this group mani et al 2006 maxent classifier is an example of this group mani et al 2006',\n",
       " 'among these media blog is one of the communicative and informative repository of text based emotional contents in the web 20 lin et al 2007 blog is one of the crucial communicative and informative repository of text based emotional contents in the web 20 lin et al 2007',\n",
       " 'for the gold preprocessing and all 5k settings we refer the reader to the shared task overview paper seddah et al 2013',\n",
       " 'for preprocessing we used mada morphological analysis and disambiguation for arabic habash et al 2009 which is one of the most accurate arabic preprocessing toolkitsfor this purpose we use mada morphological analysis and disambiguation for arabic habash et al 2009 which is one of the most accurate arabic preprocessing toolkits',\n",
       " 'we trained a 5gram language model on the xinhua section of the english gigaword corpus 306 million words using the srilm toolkit stolcke 2002 with the modified kneserney smoothingour 5gram language model was trained on the xinhua section of the english gigaword corpus 306 million words using the srilm toolkit stolcke 2002  with modified kneserney smoothing',\n",
       " 'to determine semantic type and subtype we train two svm multiclass classifiers using svm multiclass tsochantaridis et al 2004 to determine semantic types and subtypes we train two svm multiclass classifiers using svm multiclass tsochantaridis et al 2004',\n",
       " 'we use the scikit implementation of random forest pedregosa et al 2011 we use the scikit implementation of svm pedregosa et al 2011',\n",
       " 'each term in the input text will be represented by its stem and pos tag in the following format stempos using buckwalter transliteration buckwalter 2002 each term in the input text is represented by its stem and pos tag using buckwalter transliteration buckwalter 2002',\n",
       " 'an algorithm the kuhnmunkres method kuhn 1955  can find solutions to the optimum assignment problem in polynomial timean algorithm the kuhnmunkres method kuhn 1955  has been proposed that can find a solution to the optimum assignment problem in polynomial time',\n",
       " 'we use collapsed gibbs sampling griffiths and steyvers 2004 to infer the parameters of the model and the latent violent categories and topics assignments for tweets given observed data d gibbswe use collapsed gibbs sampling griffiths and steyvers 2004 to infer the parameters of the model  given observed data d gibbs sampling is a markov chain monte carlo method which allows us repeat',\n",
       " 'we use the stanford dependency parser marneffe et al 2006 we use the stanford parser with stanford dependencies de marneffe et al 2006',\n",
       " 'filter weights are initialized using glorotbengio strategy glorot and bengio 2010 weights are initialized using glorotbengio strategy glorot and bengio 2010',\n",
       " 'automatic sentence alignment of the training data was provided by ulrich german and the hand alignments of the words in the test data were created by franz och and hermann ney och and ney 2003 automatic sentence alignment of the training data was provided by ulrich german and the hand alignments of the words in the trial and test data were created by franz och and hermann ney och and ney 2003',\n",
       " 'we use the adagrad optimizer duchi et al 2011  with initial learning rate set to 01we use adagrad duchi et al 2011  with the initial learning rate set to 05',\n",
       " 'then we did word alignment using giza och and ney 2003 with the default growdiagfinaland alignment symmetrization methodwe performed word alignment using giza och and ney 2003 with the default growdiagfinaland alignment symmetrization method',\n",
       " 'for example dirt lin and pantel 2001 aims to discover different representations of the same semantic relation ie similar dependency pathsfor example dirt lin and pantel 2001 aims to discover different representations of the same semantic relation using distributional similarity of dependency paths',\n",
       " 'the annotation was performed manually using the brat annotation tool stenetorp et al 2012 the annotation was performed using the brat 2 tool stenetorp et al 2012',\n",
       " 'system proposed by li et al 2006  uses a semanticvector approach to measure sentence similaritya similar semantic similarity measure proposed by li et al 2006  uses a semanticvector approach to measure sentence similarity',\n",
       " 'this corpus contains around 11000 nps annotated for information status including 663 bridging nps and their antecedents in 50 texts taken from the wsj portion of the ontonotes corpus  weischedel et al 2011it consists of 50 texts taken from the wsj portion of the ontonotes corpus weischedel et al 2011 with almost 11000 nps annotated for information status including 663 bridging nps and their antecedent',\n",
       " 'all modules take as input the corpus documents preprocessed with a partofspeech tagger 4 and shallow parser 5 punyakanok and roth 2001 all components take as input the corpus documents preprocessed with a partofspeech tagger 2 and shallow parser 3 punyakanok and roth 2001',\n",
       " 'from the pioneering work of rapp 1995  contextual similarity has been used for ble for a long timefrom the pioneering work of rapp 1995  ble from comparable corpora has been studied for a long time',\n",
       " 'europarl koehn 2005 is a multilingual parallel corpus extracted from the proceedings of the european parliamentwe run our experiments on europarl koehn 2005  a multilingual parallel corpus extracted from the proceedings of the european parliament',\n",
       " 'tesla translation evaluation of sentences with linearprogrammingbased analysis was first proposed in liu et al 2010 teslaf was called tesla in liu et al 2010',\n",
       " 'for building the baseline smt system we used the opensource smt toolkit moses koehn et al 2007  in its standard setupfor building our smt systems the opensource smt toolkit moses koehn et al 2007 was used in its standard setup',\n",
       " 'rhetorical structure theory rst mann and thompson 1988  one of the most influential theories of discourse represents texts by labeled hierarchical structures called discourse trees dtsrhetorical structure theory rst mann and thompson 1988  one of the most influential theories of discourse posits a tree representation of a discourse known as a discourse tree dt',\n",
       " 'in addition the fixdiscount method in foster et al 2006 for phrase table smoothing is also usedin addition the fixdiscount method foster et al 2006 for phrase table smoothing was also used',\n",
       " 'memorybased language processing daelemans and van den bosch 2005 is based on the idea that nlp problems can be solved by reuse of solved examples of the problem stored in memorymemorybased language processing daelemans and van den bosch 2005 is based on the idea that nlp problems can be solved by reuse of solved examples of the problem in memory',\n",
       " 'we calculate statistical significance of performance differences using stratified shuffling yeh 2000 we tested the significance of differences using stratified shuffling yeh 2000',\n",
       " 'for instance machine translation mt systems can benefit from training on sentences extracted from parallel or comparable documents retrieved from the web munteanu and marcu 2005 in addition machine translation mt systems can be improved by training on sentences extracted from parallel or comparable documents mined from the web munteanu and marcu 2005',\n",
       " '1 with 2 regularization using adagrad duchi et al 2011 1 regularization using adagrad duchi et al 2011',\n",
       " 'why does the lr model outperform berkeley 13 the muc vilain et al 1995 score is the minimum number of links between mentions to be inserted or deleted when mapping the output to a gold standardthe muc score vilain et al 1995 counts the minimum number of links between mentions to be inserted or deleted when mapping a system response to a gold standard key set',\n",
       " 'in the context of this paper we will be focusing on the subset tree sst kernel described in collins and duffy 2002  which relies on a fragment definition that does not allow to break productionwe will focus on the syntactic tree kernel described in collins and duffy 2002  which relies on a fragment definition that does not allow to break production rules',\n",
       " 'europarl koehn 2005 is a multilingual parallel corpus extracted from the proceedings of the european parliamentthe europarl corpus koehn 2005 is built from the proceedings of the european parliament',\n",
       " 'we have used foma a free software tool to specify finitestate automata and transducers hulden 2009 the module was implemented using foma a free software tool to specify finitestate automata and transducers hulden 2009',\n",
       " 'we used the same test set used in li et al 2004 for our testing 5 we used the same test data as in li et al 2004',\n",
       " 'the penn discourse treebank pdtb prasad et al 2008 is a large corpus annotated with discourse relations covering the wall street journal part of the penn treebankpenn discourse treebank the penn discourse treebank pdtb is a corpus of wall street journal articles annotated with discourse relations prasad et al 2008',\n",
       " 'we used the implementation of the scikitlearn 2 module pedregosa et al 2011 we used a gb implementation of the scikitlearn package pedregosa et al 2011',\n",
       " 'since the commonly used word similarity datasets contain a small number of word pairs we also use the men dataset bruni et al 2012 of 3000 word pairs sampled from words that occur at least 700 times in a large web corpusthe second is the men dataset bruni et al 2012 of 3000 words pairs sampled from words that occur at least 700 times in a large web corpus',\n",
       " 'the training data of the shared task is the nucle corpus dahlmeier et al 2013  which contains essays written by learners of english the training data released by the task organizers comes from the nucle corpus dahlmeier et al 2013  which contains essays written by learners of english as a foreign language',\n",
       " 'all system implementation was done using python and the opensource machine learning toolkit scikitlearn pedregosa et al 2011 all of the machine learning was done using scikitlearn pedregosa et al 2011',\n",
       " 'it has been shown that a diverse set of predictions can be used to help improve decoder accuracy for various problems in nlp henderson and brill 1999 it has been long identified in nlp that a diverse set of solutions from a decoder can be reranked or recombined in order to improve the accuracy in various problems henderson and brill 1999',\n",
       " 'in the 2013 system we had used sentistrength lexicon thelwall et al 2010 in our system we used the sentiment lexicon provided by sentistrength thelwall et al 2010',\n",
       " 'finally we also compare the quality of the candidate phrase embeddings with word embeddings dhillon et al 2011 by adding them as features in a crf based sequence taggerwe also compared the quality of the candidate phrase embeddings with the wordlevel embeddings by adding them as features dhillon et al 2011 along with the baseline features in the crf tagger',\n",
       " 'these methods are based on the distributional hypothesis which states that words appearing in the same contexts tend to have similar meaning harris 1954 corpusbased vsms follow the standard distributional hypothesis which states that words appearing in the same contexts tend to have similar meaning harris 1954',\n",
       " 'all annotations were done using the brat rapid annotation tool stenetorp et al 2012 the annotations were made using the brat rapid annotation tool stenetorp et al 2012',\n",
       " 'compared to wordnet fellbaum 1998  there are similarities but also significant differencescompared to wordnet fellbaum 1998  there are similarities as well as considerable differences',\n",
       " 'for training we use adam kingma and ba 2015 for optimization with an initial learning rate of 0001we use adam kingma and ba 2015 for optimization with initial learning rate of 0001',\n",
       " 'for our classifier we use svms specifically the liblinear svm software package fan et al 2008  which is wellsuited to text classification tasks with large numbers of features and large numberspecifically we use the liblinear svm package fan et al 2008 as it is wellsuited to text classification tasks with large numbers of features and texts',\n",
       " 'the first two experiments concern the prediction of the sentiment of movie reviews in the stanford sentiment treebank socher et al 2013the first two experiments involve predicting the sentiment of movie reviews socher et al 2013',\n",
       " 'we used only the nonensembled lefttoright run ie no righttoleft rescoring as done by sennrich et al 2016a with beam size of 5 5 taking just the singlebest outputwe used only the nonensembled lefttoright run ie no righttoleft rescoring as done by sennrich et al 2016a with beam size of 12 default value',\n",
       " 'we used only the nonensembled lefttoright run sennrich et al 2016a with beam size of 5 5 taking just the singlebest outputwe used only the nonensembled lefttoright run sennrich et al 2016a with beam size of 12 default value',\n",
       " 'the msd morphological coding system was developed for a bunch of languages including hungarian erjavec 2004 the msd morphological coding system is a positional coding system developed for several languages erjavec 2004',\n",
       " 'the phrase tables were generated by means of symmetrised word alignments obtained with giza och and ney 2003 the phrase table was generated employing symmetrised word alignments obtained with giza och and ney 2003',\n",
       " 'word alignment is performed using giza och and ney 2003 word alignment is performed with giza och and ney 2003',\n",
       " 'we experimented with several levels of cluster granularity using development data and following koo et al 2008following koo et al 2008  we also experimented with using two sets of cluster labels with different levels of granularity',\n",
       " 'raghavan et al 2007 measure the benefit from feature feedback as the gain in the learning speed with feature feedbackraghavan et al 2007  evaluate benefit from feature feedback in terms of the gain in learning speed',\n",
       " 'we built a modified kneserney smoothed 5gram language model using the english side of the training data and performed querying with kenlm heafield 2011 training and querying of a modified kneserney smoothed 5 gram language model are done on the english side of the training data using kenlm heafield 2011',\n",
       " 'a formal pacstyle analysis can be found in ando and zhang 2004 the formal derivation can be found in ando and zhang 2004',\n",
       " 'the first model we introduce is based on the recurrent neural network language model of mikolov et al 2010 the model as described thus far is identical to the recurrent neural network language model rnnlm of mikolov et al 2010',\n",
       " 'the smt systems were built using the moses toolkit koehn et al 2007 the baseline systems are built with the opensource phrasebased smt toolkit moses koehn et al 2007',\n",
       " 'the classifier experiments were carried out using the svmlight software joachims 1999 available at httpsvmlightjoachimsorg with a polynomial kernel 2 degree3the classifier evaluations were carried out using the svmlight software joachims 1999 available at httpsvmlightjoachimsorg with the default linear kernel for the standard feature evaluation',\n",
       " 'the training data of the shared task is the nucle corpus dahlmeier et al 2013  which contains essays written by learners of englishthe training data for the task is from the nucle corpus dahlmeier et al 2013  an errortagged collection of essays written by nonnative learners of english',\n",
       " 'saldo borin et al 2013 is the largest freely available lexical resource for swedishsaldo borin et al 2013 is the most comprehensive open lexical resource for swedish',\n",
       " 'for the phrasebased smt system we adopted the moses toolkit koehn et al 2007 for our smt experiments we use the moses toolkit koehn et al 2007',\n",
       " 'however those stringtotree systems run slowly in cubic time huang et al 2006 however such stringtotree systems run slowly in cubic time huang et al 2006',\n",
       " 'the 5gram target language model was trained using kenlm heafield 2011 we trained an english 5gram language model using kenlm heafield 2011',\n",
       " 'for all experiments we used the moses smt system koehn et al 2007 for our smt experiments we use the moses toolkit koehn et al 2007',\n",
       " 'we evaluate our method on the following data sets bullet ontonotesdev development set of the ontonotes data provided by the conll2012 shared task pradhan et al 2012 bullet ontonotestest test set of the ontonotes data provided by the conll2012 shared task pradhan et al 2012',\n",
       " 'we use the moses phrasebased translation system koehn et al 2007 to implement our modelswe use the stateoftheart phrasebased machine translation system moses koehn et al 2007 to perform our machine translation experiments',\n",
       " 'but we randomly selected 90 of the training data used in li et al 2004 as our training data and the remainder as the development data as shown in table 5 since the training data used in li et al 2004 is identical as the union of our training and development data we denoted it as traindev in table 6',\n",
       " 'the bleu score measures the precision of ngrams over all n to 4 in our case with respect to a reference translation with a penalty for short translations papineni et al 2001 bleu score this score measures the precision of unigrams bigrams trigrams and fourgrams with respect to a whole set of reference translations with a penalty for too short sentences papineni et al 2001',\n",
       " 'the training data of the shared task is the nucle corpus dahlmeier et al 2013  which contains essays written by learners of englishthe training data provided for the task is a subset of the nucle v23 corpus dahlmeier et al 2013  which comprises essays written in english by students at the national university of singapore',\n",
       " 'test data was drawn from the open american national corpus ide and suderman 2004  oanc across a variety of genres and from both the spoken and written portions of the corpuswe selected the dataset of jurgens and klapaftis 2013 which was drawn from the open american national corpus oanc ide and suderman 2004 across a variety of genres and from both the spoken and written portions of the corpus',\n",
       " 'we measure statistical significance using 95 confidence intervals computed with paired bootstrap resampling koehn 2004 the statistical significance tests using 95 confidence interval are measured with paired bootstrap resampling koehn 2004',\n",
       " 'we use the english portion of the ace 2005 relation extraction dataset walker et al 2006 we evaluate our relation extraction system on the english portion of the ace 2005 corpus walker et al 2006',\n",
       " 'this data was collected for the 2014 semeval competition marelli et al 2014 and consists of 9927 sentence pairs with 4500 for training 500 as a development setsentences involving compositional knowledge sick is from task 1 of the 2014 semeval competition marelli et al 2014 and consists of 9927 annotated sentence pairs with 4500 for training 500 as a development set',\n",
       " 'the parsing model used for intrasentential parsing is a dynamic conditional random field dcrf sutton et al 2007 shown in figure 7 our novel parsing model is the dynamic conditional random field dcrf sutton et al 2007 shown in figure 2',\n",
       " 'latent dirichlet allocation lda is a generative model which considers a document model salton 1989 as a mixture probability of latent topics combination of latent topics ent dirichlet allocation lda is a generative model which considers a document model seen as a bag of words salton 1989 as a mixture probability of latent topics',\n",
       " 'distributional hypothesis theory harris 1954 indicates that words that occur in the same context tend to have similar meaningsit therefore follows the distributional hypothesis harris 1954 which states that words that occur in the same contexts tend to have similar meanings',\n",
       " 'distributional hypothesis theory harris 1954 indicates that words that occur in the same context tend to have similar meaningsit therefore follows the distributional hypothesis harris 1954 which states that words that occur in the same contexts tend to have similar meanings',\n",
       " 'in 2009 yefang wang wang et al 2009 used cascading classifiers on manually annotated data which fetched fscore of 0832in 2009 yefang wang wang et al 2009 used cascading classifiers on manually annotated data and achieved around 832 accuracy',\n",
       " 'we built a 5gram language model on the english side of qcatrain using kenlm heafield 2011 we built a modified kneserney smoothed 5gram language model using the english side of the training data and performed querying with kenlm heafield 2011',\n",
       " 'the remaining three models are all naive bayes classifiers trained on the google web 1t 5gram corpus henceforth google corpus  brants and franz 2006 the other models are trained on native english data the google web 1t 5gram corpus henceforth google brants and franz 2006  with the naive bayes nb algorithm',\n",
       " 'we apply bootstrapping kozareva et al 2008 on the word graphs by manually selecting 10 seeds for concrete and abstract words see table 10 we then apply bootstrapping kozareva et al 2008 on the noun and adjective graphs by selecting 10 seeds for visual and nonvisual nouns and adjectives see table 1',\n",
       " 'these methods are based on the distributional hypothesis which states that words appearing in the same contexts tend to have similar meaning harris 1954 it therefore follows the distributional hypothesis harris 1954 which states that words that occur in the same contexts tend to have similar meanings',\n",
       " 'these results verify the benefit of using ltag based features and confirm the hypothesis that ltag based features provide a novel set of abstract features that complement the hand selected features collins 2000 our hypothesis is that the ltag based features provide a novel set of abstract features that complement the hand selected features from collins 2000 and the ltag based features will help improve',\n",
       " 'rg65 rubenstein and goodenough 1965 has 65 word pairsrg65 rubenstein and goodenough 1965 is set of 65 word pairs',\n",
       " 'the significance tests were performed using the bootstrap resampling method koehn 2004 statistical significance tests are performed using bootstrap resampling koehn 2004',\n",
       " 'the default phrasal search algorithm is cube pruning huang and chiang 2007 the search is typically carried out using the cube pruning algorithm huang and chiang 2007',\n",
       " 'indomain data is mainly used to solve the problem of data sparseness sun and xu 2011 indomain data only solves the problem of data sparseness sun and xu 2011',\n",
       " 'all experiments were carried out using the opensource smt toolkit moses koehn et al 2007 all the experiments are carried out in moses toolkit koehn et al 2007',\n",
       " 'first we apply heuristics to determine number and gender based on word lists wordnet miller 1990 and partofspeech tagswe then apply heuristics to determine number and gender for the characters based on word lists word net miller 1990 and pos tags',\n",
       " 'rank svm joachims 2002 is a method based on support vector machines svms for which we use only linear kernels to keep complexity lowfor this task we use ranksvm joachims 2002 which is a method based on support vector machines svms',\n",
       " 'a more detailed description of the task can be found in nakov et al 2017 a precise description of the corpus and metrics can be found in task3 description paper nakov et al 2017',\n",
       " 'a tree kernel function is a convolution kernel haussler 1999 defined over pairs of treestree kernel tk functions are convolution kernels haussler 1999 defined over pairs of trees',\n",
       " 'we build upon our previous approach for joint concept disambiguation and clustering fahrni and strube 2012 for disambiguation and clustering we build upon our previous work fahrni and strube 2012',\n",
       " 'rouge2 metric lin 2004 is used for the evaluation we used the rouge1 evaluation metric lin 2004',\n",
       " 'we used mallet software mccallum 2002 for crf experimentswe used mallet toolkit mccallum 2002 for crf implementation',\n",
       " 'we exploit a transitionbased framework with global learning and beamsearch decoding to implement the joint model zhang and clark 2011 our joint parsing model exploits a transitionbased framework with global learning and beamsearch decoding zhang and clark 2011  extended from a arcstandard transitionbased parsing model',\n",
       " 'the annotation was performed using the brat 2 tool stenetorp et al 2012 the annotations were made using the brat rapid annotation tool stenetorp et al 2012',\n",
       " 'for building the word alignment models we use mgiza gao and vogel 2008 to build the word alignment models we used the mgiza package gao and vogel 2008',\n",
       " 'the reliability of the annotation was evaluated using the kappa statistic carletta 1996 we evaluated annotation reliability by using the kappa statistic carletta 1996',\n",
       " 'it therefore follows the distributional hypothesis harris 1954 which states that words that occur in the same contexts tend to have similar meaningsdistributional models of meaning follow the distributional hypothesis harris 1954  which states that two words that occur in similar contexts have similar meanings',\n",
       " 'this dataset is composed of 35 triplets of sentences from the eyetracking experiment experiment 1 in traxler et al 2002  for a total of 105 sentencesthis dataset is composed of 32 sentence quadruplets from experiments 2 eyetracking and 3 selfpaced reading in traxler et al 2002  for a total of 120 sentences',\n",
       " 'the significance tests were performed using the bootstrap resampling method koehn 2004 a statistical significance test was performed using the bootstrap resampling method koehn 2004',\n",
       " 'we use the standard alignment tool giza och and ney 2003 to word align the parallel datawe use the giza tool och and ney 2003 to align words in our parallel corpora',\n",
       " 'we use the standard alignment tool giza och and ney 2003 to word align the parallel datawe use the giza tool och and ney 2003 to align words in our parallel corpora',\n",
       " 'the kernels are combined using gaussian process regression gpr rasmussen and williams 2006 gaussian process regression gpr rasmussen and williams 2006',\n",
       " 'to overcome this agirre et al 2009 used mapreduce infrastructure with 2000 cores to compute pairwise similarities of words on a corpus of roughly 16 terawordsin another work a corpus of roughly 16 terawords was used by agirre et al 2009  to compute pairwise similarities of the words in the test sets using the mapreduce infrastructure on 2000 cores',\n",
       " 'we measure statistical significance using 95 confidence intervals computed with paired bootstrap resampling koehn 2004 we calculated significance using paired bootstrap resampling koehn 2004',\n",
       " 'we use the stanford dependency parser chen and manning 2014 at this stage and have not experimented with alternatives in this work we use the stanford neural dependency parser chen and manning 2014',\n",
       " 'next a tweet was tokenized and fed into madamira pasha et al 2014  a morphological analysis tool for arabic textmadamira pasha et al 2014 is a morphological analysis and disambiguation tool of arabic',\n",
       " 'yarowsky 1995 has proposed a bootstrapping method for word sense disambiguation yarowsky 1995 proposed such a method for word sense disambiguation which we refer to as monolingual bootstrapping',\n",
       " 'we also list the previous stateoftheart performance from a conventional smt system durrani et al 2014 with the bleu of 370we also list the results from smt model durrani et al 2014 as a comparison',\n",
       " 'we used adam kingma and ba 2014 with a learning rate of 00002we use the adam kingma and ba 2014 algorithm to minimize the sum of the loss for p asv and p af  with a learning rate of 10',\n",
       " 'distributional semantics is based on the idea that firth 1957 in other words the meaning of a word is related to the contexts it appears inword cooccurence statistics you shall know a word by the company it keeps firth 1957',\n",
       " 'in order to estimate the basic lexical similarity function employed in the sum ssc and sptk operators a cooccurrence word space is acquired through the distributional analysis of the ukwac corpus baroni et al 2009 the cooccurrence word space is acquired through the distributional analysis of the ukwac corpus baroni et al 2009',\n",
       " 'we used the implementation of the scikitlearn 2 module pedregosa et al 2011 we used the scikitlearn toolkit to train our classifiers pedregosa et al 2011',\n",
       " 'the translation model was trained by giza och and ney 2003  and the trigram was trained by the cmucambridge statistical language modeling toolkit v2 clarkson and rosenfeld 1997 the probability pe is computed using a simple trigram language model that was trained using the cmu language modeling toolkit clarkson and rosenfeld 1997',\n",
       " 'this is a generalization of the operator id in kaplan and kay 1994 this is similar to the operator intro in kaplan and kay 1994',\n",
       " 'we used standard classifiers available in scikitlearn package pedregosa et al 2011 we used a gb implementation of the scikitlearn package pedregosa et al 2011',\n",
       " 'we used the implementation of the scikitlearn 2 module pedregosa et al 2011 for the linear logistic regression implementation we used scikitlearn pedregosa et al 2011',\n",
       " 'for the phrasebased smt system we adopted the moses toolkit koehn et al 2007 finally we used moses toolkit as phrasebased reference koehn et al 2007',\n",
       " '4 word alignments are created by aligning the data in both directions with giza 5 and symmetrizing the two trained alignments och and ney 2003 baseline word alignments were obtained by running giza in both directions and symmetrizing using the growdiagfinaland heuristic och and ney 2003',\n",
       " 'the perplexity achieved by the 6 gram nn lm in the spanish newstest08 development set was 116 versus 94 obtained with a standard 6gram language model with interpolation and kneserney smoothing kneser and ney 1995 the language model is a 5gram with interpolation and kneserney smoothing kneser and ney 1995',\n",
       " 'we used standard classifiers available in scikitlearn package pedregosa et al 2011 we used the scikitlearn toolkit to train our classifiers pedregosa et al 2011',\n",
       " 'statistical machine translation is typically performed using phrasebased systems koehn et al 2007 it is a standard phrasebased machine translation model koehn et al 2007',\n",
       " 'wordnet miller et al 1990 is an online hierarchical lexical database which contains semantic information about english wordsthe wordnet online lexical database miller et al 1990',\n",
       " 'we use the scikit implementation of random forest pedregosa et al 2011 we used a gb implementation of the scikitlearn package pedregosa et al 2011',\n",
       " 'we lemmatise the head of each constituent with treetagger schmid 1994 we used the stuttgart treetagger schmid 1994 to lemmatise constituent heads',\n",
       " 'our text processing uses the natural language toolkit nltk bird et al 2009 partofspeech tagging was accomplished using the natural language toolkit nltk bird et al 2009',\n",
       " 'they used the webbased annotation tool brat stenetorp et al 2012 for the annotation the annotation was performed manually using the brat annotation tool stenetorp et al 2012',\n",
       " 'our system participated in semeval2013 task 2 sentiment analysis in twitter wilson et al 2013 we participated in both subtask a and b of semeval2013 task 2 sentiment analysis in twitter wilson et al 2013 with an adaptation of our existing system',\n",
       " 'the english text was tokenized using the word tokenize routine from nltk bird et al 2009 we tokenise the text using the default tokeniser from nltk bird et al 2009',\n",
       " 'the webpages were parsed using the stanford corenlp software manning et al 2014 in addition the data was tokenized lemmatized and parsed using stanford corenlp manning et al 2014',\n",
       " 'statistical machine translation is typically performed using phrasebased systems koehn et al 2007 we built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " 'the english side was tokenized using the moses toolkit koehn et al 2007 the corpora are tokenised and truecased using scripts from the moses toolkit koehn et al 2007',\n",
       " 'we trained an english 5gram language model using kenlm heafield 2011 we built a trigram language model with kneserney smoothing using kenlm toolkit heafield 2011',\n",
       " 'all of the text data from reddit was tokenized using the nltk tokenizer bird et al 2009 we tokenise the text using the default tokeniser from nltk bird et al 2009',\n",
       " 'our machine translation systems are trained using moses 3 koehn et al 2007 we built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " 'for the phrasebased smt system we adopted the moses toolkit koehn et al 2007 for training the translation model and for decoding we used the moses toolkit koehn et al 2007',\n",
       " 'we build upon our previous approach for joint concept disambiguation and clustering fahrni and strube 2012 scopeignorant disambig our previous mlnbased approach for concept disambiguation fahrni and strube 2012',\n",
       " 'we used adam kingma and ba 2014 with a learning rate of 00002 we use adam kingma and ba 2014 with a learning rate of 4e4 and a batch size of 32',\n",
       " 'for all experiments we used the moses smt system koehn et al 2007 for training the translation model and for decoding we used the moses toolkit koehn et al 2007',\n",
       " 'the smt systems were built using the moses toolkit koehn et al 2007 the corpora are tokenised and truecased using scripts from the moses toolkit koehn et al 2007',\n",
       " 'for training the translation model and for decoding we used the moses toolkit koehn et al 2007 for our smt experiments we use the moses toolkit koehn et al 2007',\n",
       " 'for training the translation model and for decoding we used the moses toolkit koehn et al 2007 first we used the moses toolkit koehn et al 2007 for statistical machine translation',\n",
       " 'for the phrasebased smt system we adopted the moses toolkit koehn et al 2007 the baseline systems are built with the opensource phrasebased smt toolkit moses koehn et al 2007',\n",
       " 'we develop translation models using the phrasebased moses koehn et al 2007 smt systemwe built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " 'we used tnt brants 2000  trained on the negra training setwe employed the tnt tagger brants 2000 which was trained on the spective conll training data',\n",
       " 'the webpages were parsed using the stanford corenlp software manning et al 2014 in addition the data was tokenized lemmatized and parsed using stanford corenlp manning et al 2014',\n",
       " 'we develop translation models using the phrasebased moses koehn et al 2007 smt systemwe build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " 'the term frequency count is normalized with the inverse document frequency in the test collection salton and buckley 1988 tfidf is a standard statistical method that combines the frequency of a term in a particular document with its inverse document frequency in general use salton and buckley 1988',\n",
       " 'we assessed the statistical significance of differences in score with an approximate randomization test 8 noreen 1989  indicating a significant impact in bold fontwe assess statistical significance of the difference in f 1 score for two approaches via an approximate randomization test noreen 1989',\n",
       " 'a framework for human error analysis and error classification has been proposed in vilar et al 2006 and a detailed analysis of the obtained results has been carried outa framework for human error analysis has been proposed in vilar et al 2006  but as every human evaluation this is also a time consuming task',\n",
       " 'for example chang et al 2009 found that the probability of heldout documents is not always a good predictor of human judgmentschang et al 2009 stated that one reason is that the objective function of topic models does not always correlate well with human judgments',\n",
       " 'on the chinese side we used the morphological analyzer described in kruengkrai et al 2009 trained on the training data of ctb tp to perform word segmentation and pos tagging and used the firstthe mma system kruengkrai et al 2009 trained on the training data was used to perform word segmentation and tagging and the baseline parser was used to parse the sentences in the gigaword corpus',\n",
       " 'conducted using the moses phrasebased decoder koehn et al 2007 we build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " 'conducted using the moses phrasebased decoder koehn et al 2007 we built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " 'we conducted baseline experiments for phrasebased machine translation using the moses toolkit koehn et al 2007 conducted using the moses phrasebased decoder koehn et al 2007',\n",
       " 'then the processed data was performed for tokenization pos tagging parsing stemming and lemmatization using stanford corenlp manning et al 2014 in addition the data was tokenized lemmatized and parsed using stanford corenlp manning et al 2014',\n",
       " 'we applied bootstrap resampling koehn 2004 to measure statistical significance  p  005 of our models compared to a baselinewe measure significance of results using bootstrap resampling at p  005 koehn 2004',\n",
       " 'this system uses the attentional encoderdecoder architecture described by bahdanau et al 2015  building on work by sutskever et al 2014 we followed the encoderdecoder architecture with attention proposed by bahdanau et al 2015',\n",
       " 'the language model is a 5gram kenlm heafield 2011 model trained using lmplz with modified kneserney smoothing and no pruningwe built a trigram language model with kneserney smoothing using kenlm toolkit heafield 2011',\n",
       " 'weighted finite state transducers fsts used in our model are constructed with openfst allauzen et al 2007 the decoder is implemented with weighted finite state transducers wfsts using standard operations available in the openfst libraries allauzen et al 2007',\n",
       " 'weighted finite state transducers fsts used in our model are constructed with openfst allauzen et al 2007 the decoder is implemented with weighted finite state transducers wfsts using standard operations available in the openfst libraries allauzen et al 2007',\n",
       " 'then the processed data was performed for tokenization pos tagging parsing stemming and lemmatization using stanford corenlp manning et al 2014 in addition the data was tokenized lemmatized and parsed using stanford corenlp manning et al 2014',\n",
       " 'it is a modification of the model proposed by mintz et al 2009 our first baseline is mi09 a distantly supervised classifier based on the work of mintz et al 2009',\n",
       " 'we use scikitlearn pedregosa et al 2011  the machine learning library for python for implementing the different approacheswe used the scikitlearn machine learning library pedregosa et al 2011 for both implementing our classification models and performing statistical feature selection',\n",
       " 'we use the universal pos tagset upos of petrov et al 2012 this in turn relies on the same underlying feature model typically drawing from a shared partofspeech pos representation such as the universal pos tagset of petrov et al 2012',\n",
       " 'we use the universal pos tagset upos of petrov et al 2012 this in turn relies on the same underlying feature model typically drawing from a shared partofspeech pos representation such as the universal pos tagset of petrov et al 2012',\n",
       " 'the crf is trained using decisions from the following underlying components bullet madamira is a publicly available tool for morphological analysis and disambiguation of eda and msa text pasha et al 2014madamira pasha et al 2014 is a morphological analysis and disambiguation tool of arabic',\n",
       " 'for language modeling we trained a separate 5gram kneserney smoothed lm model on the target ie english side of the training bitext using kenlm heafield 2011 we built a modified kneser ney smoothed 5gram language model using the english side of the training data and performed querying with kenlm heafield 2011 7',\n",
       " 'with the training script of the moses toolkit koehn et al 2007 we preprocessed the training corpora with scripts included in the moses toolkit koehn et al 2007',\n",
       " 'the word alignment was trained using giza och and ney 2003 with the configuration growdiagfinaland alignment symmetrization methodwe performed word alignment using giza och and ney 2003 with the default growdiagfinaland alignment symmetrization method',\n",
       " 'we use linear svms from liblinear and svms with rbf kernel from libsvm chang and lin 2011 as our learner we use libsvm with a linear kernel chang and lin 2011',\n",
       " 'we specify the hierarchical aligner in terms of a deduction system shieber et al 1995 we specify our dynamic programming algorithm as a deduction system shieber et al 1995',\n",
       " 'we use stanford parser de marneffe et al 2006 to obtain parse trees and dependency relationswe use the stanford dependency parser de marneffe et al 2006 for extracting dependency path and partofspeech features',\n",
       " 'in this work we use the stanford neural dependency parser chen and manning 2014 in order to detect the object pronouns we employ stanford parser chen and manning 2014',\n",
       " 'we use stanford parser de marneffe et al 2006 to obtain parse trees and dependency relationswe also obtain the dependency parse of the sentences using the stanford parser de marneffe et al 2006',\n",
       " 'the learning algorithm used in our coreference engine is c45 quinlan 1993 the question classifier used in the experiments is the c45 decision tree classifier quinlan 1993',\n",
       " 'we use stanford parser de marneffe et al 2006 to obtain parse trees and dependency relationswe use the stanford dependency parser de marneffe et al 2006 for extracting dependency path and partofspeech features',\n",
       " 'distributional semantics see cohen and widdows 2009 for an overview is based on the observation that words that occur in similar contexts tend to be semantically related harris 1954 the former that is the most popular relies on the distributional hypothesis that puts forward the idea that words with similar meaning tend to occur in similar contexts harris 1954',\n",
       " 'their work is part of the stateoftheart arabic morphological tagger madamira pasha et al 2014 madamira pasha et al 2014 is a morphological analysis and disambiguation tool of arabic',\n",
       " 'we use stanford parser de marneffe et al 2006 to obtain parse trees and dependency relationswe also obtain the dependency parse of the sentences using the stanford parser de marneffe et al 2006',\n",
       " 'phrasal follows the loglinear approach to phrasebased translation och and ney 2004 in which the decision rule has the familiar linear form  arg max e w e f 1the loglinear approach to phrasebased translation och and ney 2004 directly models the predictive translation distribution pef  w  1 zf exp w e f 1 where e is the target string',\n",
       " 'training data are based on a concatenation of 18 postagged english corpora 2 from the childes database macwhinney 2000 both cds corpora are available from the childes database macwhinney 2000',\n",
       " 'indomain smt we used the parallel corpus section 3 to train an enpt phrasebased smt system using the moses toolkit koehn et al 2007 for the phrasebased smt system we adopted the moses toolkit koehn et al 2007',\n",
       " '5gram language models of turkish and english were trained using kenlm heafield 2011 we built a 5gram language model on the english side of qcatrain using kenlm heafield 2011',\n",
       " 'we used the relation classification dataset of the semeval 2010 task 8 hendrickx et al 2010 we evaluated our model on a semantic relation classification task semeval 2010 task 8 hendrickx et al 2010',\n",
       " 'we then run word alignment with giza och and ney 2003 in both directions with the default parameters used in moseswe performed word alignment using giza och and ney 2003 with the default growdiagfinaland alignment symmetrization method',\n",
       " 'indomain smt we used the parallel corpus section 3 to train an enpt phrasebased smt system using the moses toolkit koehn et al 2007 for all experiments we used the moses smt system koehn et al 2007',\n",
       " 'the significance tests were performed using the bootstrap resampling method koehn 2004 the statistical significance tests using 95 confidence interval are measured with paired bootstrap resampling koehn 2004',\n",
       " 'all experiments were carried out using the opensource smt toolkit moses koehn et al 2007  in its standard nonmonotonic configuration the smt systems were built using the moses toolkit koehn et al 2007',\n",
       " 'these sentences have then be fed into an efficient hpsg parser pet callmeier 2000  with erg loadedthe sentences are fed into the pet hpsg parser callmeier 2000 with the gg loaded',\n",
       " 'the language model is a 5gram kenlm heafield 2011 model trained using lmplz with modified kneserney smoothing and no pruningthe 5gram target language model was trained using kenlm heafield 2011',\n",
       " 'cohen et al 2012  present a spectral algorithm for lpcfg estimation but the transformation of the lpcfg model and its spectral algorithm to rhmms is awkward and opaquefirst we present an algorithm for estimating lpcfgs akin to the spectral algorithm of cohen et al 2012  but simpler to understand and implement',\n",
       " 'distributional hypothesis theory harris 1954 indicates that words that occur in the same context tend to have similar meaningssimilar row vectors in t indicate similar context of two terms in the domain and terms that occur in the same contexts tend to have similar meanings harris 1954',\n",
       " 'distributional hypothesis theory harris 1954 indicates that words that occur in the same context tend to have similar meaningssimilar row vectors in t indicate similar context of two terms in the domain and terms that occur in the same contexts tend to have similar meanings harris 1954',\n",
       " 'we use the stanford dependency parser marneffe et al 2006 we first use a dependency parser de marneffe et al 2006 to parse each sentence and extract the set of dependency relations associated with the sentence',\n",
       " 'we also replicated the experiment of holmqvist et al 2012 on this datasetwe also carried out a chunkreordering pbsmt experiment where the chunks are reordered based on the final alignments obtained by 1pass experiment of holmqvist et al 2012',\n",
       " 'we develop translation models using the phrasebased moses koehn et al 2007 smt systemconducted using the moses phrasebased decoder koehn et al 2007',\n",
       " 'pos tagging was performed with treetagger schmid 1994 the test set was tagged with the french treetagger schmid 1994',\n",
       " 'we used the maltparser nivre et al 2007 for parsing experimentsfor the parsing experimens i used maltparser nivre et al 2007  version 181',\n",
       " 'classification uses the scikitlearn python package pedregosa et al 2011 we used a gb implementation of the scikitlearn package pedregosa et al 2011',\n",
       " 'we use the stanford dependency parser marneffe et al 2006 these features were obtained using the stanford parser 2 marneffe et al 2006',\n",
       " 'we used adam kingma and ba 2014 with a learning rate of 00002we used adam as the optimizer kingma and ba 2014',\n",
       " 'word alignment is performed using giza och and ney 2003 this is done using ibm model 1 brown et al 1993 and giza och and ney 2003',\n",
       " 'the test set was tagged with the french treetagger schmid 1994 for the french side the treetagger schmid 1994 was used',\n",
       " 'for the phrasebased smt system we adopted the moses toolkit koehn et al 2007 conducted using the moses phrasebased decoder koehn et al 2007',\n",
       " 'pos tagging was performed with treetagger schmid 1994 pos tagging is performed using the ims tree tagger schmid 1994',\n",
       " 'for building the word alignment models we use mgiza gao and vogel 2008 for word alignments we used mgiza gao and vogel 2008',\n",
       " 'the polish data is taken from the europarl corpus koehn 2005 all the data come from europarl koehn 2005',\n",
       " 'the germantoenglish corpus is europarl v7 koehn 2005 the polish data is taken from the europarl corpus koehn 2005',\n",
       " 'distributional models of meaning follow the distributional hypothesis harris 1954  which states that two words that occur in similar contexts have similar meaningsdistributional similarity relies on the distributional hypothesis that similar terms appear in similar contexts harris 1954',\n",
       " 'conducted using the moses phrasebased decoder koehn et al 2007 the smt systems were built using the moses toolkit koehn et al 2007',\n",
       " 'europarl 2 koehn 2005  it is a corpus of parallel texts in 11 languages from the proceedings of the european parliament in our mt experiments we translate french into spanish and use the following corpora to learn our translation systems bullet europarl corpus koehn 2005  the europarl parallel corpus is extract',\n",
       " 'we conducted statistical significance tests for bleu between our best domainadapted system the baseline and the three thirdparty systems using paired bootstrap resampling koehn 2004 with 1000 statistical significance is tested on the bleu metric using paired bootstrap resampling koehn 2004 with n  1000 and p  005',\n",
       " 'the english side was tokenized using the moses toolkit koehn et al 2007 the baseline will be created by the moses smt toolkit koehn et al 2007',\n",
       " 'we develop translation models using the phrasebased moses koehn et al 2007 smt systemwe use the moses phrasebased translation system koehn et al 2007 to implement our models',\n",
       " 'the baseline will be created by the moses smt toolkit koehn et al 2007 for our smt experiments we use the moses toolkit koehn et al 2007',\n",
       " 'the resulting matrix is weighted using pointwise mutual information church and hanks 1990 a popular measure of this association is pointwise mutual information pmi church and hanks 1990',\n",
       " 'the smt systems were built using the moses toolkit koehn et al 2007 the questions are translated using a phrasebased system built using moses koehn et al 2007 the mo set',\n",
       " 'the morphosyntactic tagging has been made with the tree tagger schmid 1994 pos tagging is performed using the ims tree tagger schmid 1994',\n",
       " 'for the determination of pos tags we use the stuttgart treetagger schmid 1994 for both we use treetagger schmid 1994 with language dependent models ie french model for french texts english for english texts',\n",
       " 'the polish data is taken from the europarl corpus koehn 2005 the europarl corpus koehn 2005 is built from the proceedings of the european parliament',\n",
       " 'all annotations were done using the brat rapid annotation tool stenetorp et al 2012 the annotation was performed using the brat 2 tool stenetorp et al 2012',\n",
       " 'the spanishenglish s2e training corpus was drawn from the europarl collection koehn 2005 the polish data is taken from the europarl corpus koehn 2005',\n",
       " 'we used tnt brants 2000  trained on the negra training sethcrc is tagged with tnt brants 2000  trained on the full ptb',\n",
       " 'for all experiments we used the moses smt system koehn et al 2007 for germanenglish we also have a system based on moses koehn et al 2007',\n",
       " 'two basenp data sets have been put forward by ramshaw and marcus 1995 an alternative representation for basenps has been put forward by ramshaw and marcus 1995',\n",
       " 'both of our systems were based on the moses decoder koehn et al 2007 the smt systems were built using the moses toolkit koehn et al 2007',\n",
       " 'for the phrasebased smt system we adopted the moses toolkit koehn et al 2007 the baseline will be created by the moses smt toolkit koehn et al 2007',\n",
       " 'corpusbased meaning representations rely on the distributional hypothesis which assumes that words occurring in a similar set of contexts are also similar in meaning harris 1954 distributional models of meaning follow the distributional hypothesis harris 1954  which states that two words that occur in similar contexts have similar meanings',\n",
       " 'recently naim et al 2014  proposed an unsupervised learning algorithm for automatically aligning sentences in a document with corresponding video segmentsrecently naim et al 2014 proposed a fully unsupervised approach for aligning wetlab experiment videos with associated text protocols without any direct supervision',\n",
       " 'a distributional similarity model is constructed based on the distributional hypothesis harris 1954  words that occur in the same contexts tend to share similar meaningsdistributional models of meaning follow the distributional hypothesis harris 1954  which states that two words that occur in similar contexts have similar meanings',\n",
       " 'this paper describes the details of our system that participated in the subtask a of semeval2014 task 9 sentiment analysis in twitter rosenthal et al 2014 in the following we will describe the system with which we participated in the message polarity classification subtask of sentiment analysis in twitter task 9 of semeval 2014 rosenthal et al 2014',\n",
       " 'we used the phrasebased model moses koehn et al 2007 for the experiments with all the standard settings including a lexicalized reordering model and a 5gram language modelit was built with the moses toolkit koehn et al 2007 using the 14 standard core features including a 5gram language model',\n",
       " 'combinatory categorial grammar ccg is a linguistic formalism that represents both the syntax and semantics of language steedman 1996 combinatory categorial grammar ccg steedman 1996 is a lexicalized grammar formalism that has been used for both broad coverage syntactic parsing and semantic parsing',\n",
       " 'we used the random forests implementation of scikitlearn toolkit pedregosa et al 2011 with 50 estimatorswe used the linear svm implementation with default parameters and random forest with 50 trees both available at scikitlearn pedregosa et al 2011',\n",
       " 'the proposed model extends the lda framework of blei et al 2003 the article of blei et al 2003 compares lda with plsi and mixture unigram models using the perplexity of the model',\n",
       " 'we used the training section of the dataset from gimpel et al 2011 gimpel et al 2011  provided a dataset of postagged tweets consisting almost entirely of tweets sampled from one particular day october 27 2010',\n",
       " '5gram language models of turkish and english were trained using kenlm heafield 2011 in order to evaluate the fluency of each system we train 5gram language models for each language using kenlm heafield 2011',\n",
       " 'the corpora are first tokenized and lowercased using the moses scripts then lemmatized and tagged by partofspeech pos using the treetagger schmid 1994 the corpus was then automatically tagged with partofspeech information using treetagger schmid 1994',\n",
       " 'the statistical significance test is also carried out with paired bootstrap resampling method with p  0001 intervals koehn 2004 the improvement is statistically significant according to paired bootstrap resampling test koehn 2004',\n",
       " 'the realisation ranking component is an svm ranking model implemented with svmrank a support vector machinebased learning tool joachims 2006 the learner is implemented as a ranking component trained with svmrank joachims 2006',\n",
       " 'the statistical significance test is also carried out with paired bootstrap resampling method with p  0001 intervals koehn 2004 a statistical significance test was performed using the bootstrap resampling method koehn 2004',\n",
       " 'we use the nltk toolkit loper and bird 2002 to extract the numerical quantity from each sentencewe use the punkt sentence splitter from nltk loper and bird 2002 to perform both sentence and word segmentation on each text chunk',\n",
       " 'we mark the source tokens to which each target unk symbol is most aligned with the method of luong et al 2015 we find this method provides an additional 15 bleu points which is consistent with the conclusion in luong et al 2015',\n",
       " 'we rely on the hybrid aligned lexical semantic resource proposed by faralli et al 2016 to perform wsdin particular the contribution of this paper is a new unsupervised knowledgebased approach to wsd based on the hybrid aligned resource har introduced by faralli et al 2016',\n",
       " 'to help improve the information extraction tools a corpus  called bioscope has been annotated for speculation negation and its linguistic scopes in biomedical texts szarvas et al 2008 the bioscope corpus is a manually annotated corpus for speculation and negation keywords token level and their linguistic scopes sentence level szarvas et al 2008',\n",
       " 'the module of coreference resolution included in the ixa pipeline is loosely based on the stanford multi sieve pass system lee et al 2013 we apply the stanford coreference resolution system lee et al 2013',\n",
       " 'we run our experiments on europarl koehn 2005  a multilingual parallel corpus extracted from the proceedings of the european parliamenteuroparl 2 koehn 2005  it is a corpus of parallel texts in 11 languages from the proceedings of the european parliament',\n",
       " 'we used the scikitlearn pedregosa et al 2011 implementation of these classifiers with their default parameter settings for our experimentswe used a gb implementation of the scikitlearn package pedregosa et al 2011',\n",
       " 'we used the scikitlearn pedregosa et al 2011 implementation of these classifiers with their default parameter settings for our experimentswe used the scikitlearn toolkit to train our classifiers pedregosa et al 2011',\n",
       " 'automatic multidocument summarization mds aims at selecting the relevant information from multiple documents on the same topic to produce a summary mani 2001 automatic text summarization aims to automatically produce a short and wellorganized summary of single or multiple documents mani 2001',\n",
       " 'additionally we train phrasebased machine translation models with our alignments using the popular moses toolkit koehn et al 2007 for our smt experiments we use the moses toolkit koehn et al 2007',\n",
       " 'we used the scikitlearn pedregosa et al 2011 implementation of these classifiers with their default parameter settings for our experimentswe used the scikitlearn pedregosa et al 2011 implementation of svrs and the skll toolkit',\n",
       " 'additionally we train phrasebased machine translation models with our alignments using the popular moses toolkit koehn et al 2007 for the phrasebased smt system we adopted the moses toolkit koehn et al 2007',\n",
       " 'we train classifiers for each of the above feature types and for the full feature set on the training set of each corpus using the default configuration of the naive bayes implementation of weka  hall et al 2009 we train random forest classifiers breiman 2001 using weka hall et al 2009 for each step and also for the joint model',\n",
       " 'in addition the corpus was lemmatised using the treetagger lemmatizer schmid 1994 1 the corpus is lemmatised and tagged by partofspeech on both sides using the treetagger schmid 1994',\n",
       " 'they are based on distributional hypothesis which works under the assumption that similar words occur in similar contexts harris 1954 distributional models of meaning follow the distributional hypothesis harris 1954  which states that two words that occur in similar contexts have similar meanings',\n",
       " 'we used the same test set used in li et al 2004 for our testing 5 but we randomly selected 90 of the training data used in li et al 2004 as our training data and the remainder as the development data as shown in table 5',\n",
       " 'we develop translation models using the phrasebased moses koehn et al 2007 smt systemadditionally we train phrasebased machine translation models with our alignments using the popular moses toolkit koehn et al 2007',\n",
       " 'we used nonlocal features based on finkel et al 2005 for example finkel et al 2005 enabled the use of nonlocal features by using gibbs sampling',\n",
       " 'for building the baseline smt system we used the opensource smt toolkit moses koehn et al 2007  in its standard setupfor all experiments we used the moses smt system koehn et al 2007',\n",
       " 'for translation we use moses koehn et al 2007 with lexicalized reordering step and the proposed model with latent derivations laderfor our smt experiments we use the moses toolkit koehn et al 2007',\n",
       " 'to recognize explicit connectives we construct a list of existing connectives labeled in the penn discourse treebank prasad et al 2008a in addition we show that the latent representation coheres well with the characterization of discourse connectives in the penn discourse treebank prasad et al 2008',\n",
       " 'to construct language models and measure perplexity  we use srilm stolcke 2002 with interpolated modified kneserney discounting chen and goodman 1996 and with a fixed vocabulary both language models use modified kneserney smoothing chen and goodman 1996',\n",
       " 'we use gibbs sampling to estimate the distributions of n and m  integrating out the multinomial parameters griffiths and steyvers 2004 we use the gibbs sampling based lda griffiths and steyvers 2004',\n",
       " 'arabizi is not a letterbased transliteration from the arabic script as is for example the buckwalter transliteration buckwalter 2004 1 arabic transliteration is presented in the buckwalter scheme buckwalter 2004',\n",
       " 'to construct language models and measure perplexity  we use srilm stolcke 2002 with interpolated modified kneserney discounting chen and goodman 1996 and with a fixed vocabulary both language models use modified kneserney smoothing chen and goodman 1996',\n",
       " 'we build upon our previous markov logic based approach for joint concept disambiguation and clustering fahrni and strube 2012 scopeignorant disambig our previous mlnbased approach for concept disambiguation fahrni and strube 2012',\n",
       " 'we use the feedforward neural probabilistic language model architecture of vaswani et al 2013  as shown in figure 4 we follow the neural network architecture of vaswani et al 2013  using two hidden layers of rectified linear units',\n",
       " 'the smt systems were built using the moses toolkit koehn et al 2007 we trained a number of frenchenglish smt systems using the moses toolkit koehn et al 2007 in its default setting',\n",
       " 'for building the baseline smt system we used the opensource smt toolkit moses koehn et al 2007  in its standard setupfor our smt experiments we use the moses toolkit koehn et al 2007',\n",
       " 'indomain smt we used the parallel corpus section 3 to train an enpt phrasebased smt system using the moses toolkit koehn et al 2007 smt translations a phrasebased smt system built using the moses toolkit koehn et al 2007 and the whole spanishenglish dataset except the sentences in the test set was used to translated the',\n",
       " 'following resource collection and construction a smt model for englishbrazilianportuguese was trained using the moses toolkit koehn et al 2007 using its baseline settingswe trained a model using moses toolkit koehn et al 2007 on the training data as our baseline system',\n",
       " 'one semiautomatic approach to evaluation is rouge lin and hovy 2003  which is primarily based on ngram cooccurrence between automatic and human summariesthe method rouge lin and hovy 2003  is based on ngram overlap between the systemproduced and ideal summaries',\n",
       " 'we use the web 1t 5gram corpus brants and franz 2006 to compute the language model score for a sentencewe used the unigram counts from the web 1t 5gram corpus brants and franz 2006 to determine the frequency of use of each candidate synonym',\n",
       " 'the semeval2015 aspect based sentiment analysis task is a continuation of semeval2014 task 4 pontiki et al 2014 this paper describes the approach of the semantic analyis project snap to task 4 of semeval 2014 aspect based sentiment analysis pontiki et al 2014',\n",
       " 'the 2009 bio nlp shared task kim et al 2009 aimed at extracting biologicalevents where one of the event types was gene expressionthe recent bionlp 2009 shared task bionlp09st on event extraction kim et al 2009 focused on event types of varying complexity',\n",
       " 'the constituent context model ccm for inducing constituency parses klein and manning 2002 was the first unsupervised approach to surpass a rightbranching baseline the ccm is a generative model for the unsupervised induction of binary constituency parses over sequences of partofspeech pos tags klein and manning 2002',\n",
       " 'the ccm is a generative model for the unsupervised induction of binary constituency parses over sequences of partofspeech pos tags klein and manning 2002 the idea of representing a constituent by its yield and a different definition of context is used by the ccm unsupervised parsing model klein and manning 2002',\n",
       " 'we use the implementation provided by tai et al 2015  changing only the dependency parses that are fed to their modelwe use the dependency tree long shortterm memory network treelstm proposed by tai et al 2015  simply replacing their default dependency parser with our version that maps unseen words',\n",
       " 'europarl koehn 2005 is a multilingual parallel corpus extracted from the proceedings of the european parliamentwe use the french english parallel corpus approximately 12 million sentences from the corpus of european parliamentary proceedings koehn 2005 as the data on which pivoting is performed to text',\n",
       " 'the stanford dependency parser de marneffe et al 2006 is used for extracting features from the dependency parse treeswe used the stanford dependency parser de marneffe et al 2006 to parse the sentences that contain a candidate speculation keyword and extracted the following features from the dependency parse trees',\n",
       " 'the stanford dependency parser de marneffe et al 2006 is used for extracting features from the dependency parse treeswe used the stanford dependency parser de marneffe et al 2006 to parse the sentences that contain a candidate speculation keyword and extracted the following features from the dependency parse trees',\n",
       " 'the major part of data comes from current and upcoming full releases of the europarl data set koehn 2005 the data used for the experiments described in this paper comes predominantly from bible translations  wikipedia and the europarl corpus of european parliamentary proceedings koehn 2005',\n",
       " 'we compare the proposed model to our implementation of the iobesbased model described in collobert et al 2011  applied to mwe tagging the model architecture shown in figure 1  is a slight variant of the cnn architecture of collobert et al 2011',\n",
       " 'it is a phrasebased system built using the moses toolkit koehn et al 2007 and trainedtuned using only the preprocessed tokenised lowercased parallel data provided for the shared tasksmt translations a phrasebased smt system built using the moses toolkit koehn et al 2007 and the whole spanishenglish dataset except the sentences in the test set was used to translated the shared task',\n",
       " 'in this section we first discuss the hybrid tree model of lu et al 2008  and introduce a novel extensionin lu et al 2008  the mixgram model an interpolation between the unigram model and the bigram model was also considered when parsing novel sentences which yielded a better performance',\n",
       " 'the europarl corpus koehn 2005 is built from the proceedings of the european parliamentwe use the french english parallel corpus approximately 12 million sentences from the corpus of european parliamentary proceedings koehn 2005 as the data on which pivoting is performed to text',\n",
       " 'the europarl corpus koehn 2005 is built from the proceedings of the european parliamentwe use the french english parallel corpus approximately 12 million sentences from the corpus of european parliamentary proceedings koehn 2005 as the data on which pivoting is performed to text',\n",
       " 'we use adam kingma and ba 2015 for optimisation with initial learning rate of 0001each model was trained during 50 epochs using the adaptive variant of stochastic gradient descent adam kingma and ba 2015 with an initial learning rate of 0001',\n",
       " 'we use the opensource moses toolkit koehn et al 2007 to build a standard phrasebased smt system which extracts up to 8 words phrases in the moses phrase tablewe use the moses phrasebased translation system koehn et al 2007 to implement our models',\n",
       " 'we used the mkcls tool in giza och and ney 2003 to learn the word classeswe also used giza word alignment tool och and ney 2003 on the same files and collected figures pertaining to the alignment of proper names in hindi and english',\n",
       " 'we then use the phrase extraction utility in the moses statistical machine translation system koehn et al 2007 to extract a phrase table which operates over characters we use the moses phrasebased translation system koehn et al 2007 to implement our models',\n",
       " 'phrase pairs are extracted from ibm4 alignments obtained with giza och and ney 2003 phrase pairs were extracted from symmetrized word alignments and distortions generated by giza och and ney 2003 using the combination of heuristics growdiagfinaland and msdbidirectiona',\n",
       " 'specifically we build off the bayesian block hmms used by ritter et al 2010 for modeling twitter conversations which will be our primary baselineour work is motivated by the bayesian hmm approach of ritter et al 2010   the model we refer to as the block hmm bhmm  and we consider this our primary baseline',\n",
       " 'like cho  chai 2000  our analysis also provides the same explanation for various scrambled sentences such as the double accusative construction dac in koreangiven the lp constraints in 4 7 and 8 we can provide a simpler explanation to various scrambled sentences including the alleged counterexamples to the analysis of cho  chai 2000',\n",
       " 'we propose a relatively simple and nuanced unsupervised model inspired by the monolingual weighted matrix factorization wmf model proposed in guo and diab 2012  which we extend to the crosslinas mentioned above we extend the wmf model proposed in guo and diab 2012 to bilingual and multilingual settings by forcing the two monolingual components to use a shared factor',\n",
       " 'our work is also related to bunescu and mooney 2005  where the similarity between the words on the path connecting two entities in the dependency graph is used to devise a kernel functionthe dependency path is the shortest path between the two entities in a dependency parse graph and has been shown to be important for relation extraction bunescu and mooney 2005',\n",
       " 'motivated by previous work we include a frequency count of 17 discourse markers which were found to be the most common across the argue corpus abbott et al 2011 outside of the rhetorical features the discourse markers which are found to be the most useful in our experiments agree with those found in the ar gue corpus abbott et al 2011',\n",
       " 'the highest performance levels were achieved using a sequential minimal optimization algorithm for training a support vector classifier using polynomial kernels platt 1998 table 8 to table 12 show the macroaverage f macroavg scores obtained after 10 crossvalidation using sequential minimal optimization algorithm j platt 1998 for training a support vector mach',\n",
       " 'in this rest of this paper we discuss related work the methods for each system and experiments and results for each subtask using the data provided by semeval2014 task 9 sentiment analysis in twithis paper describes the details of our system that participated in the subtask a of semeval2014 task 9 sentiment analysis in twitter rosenthal et al 2014',\n",
       " 'in principle classifiers trained on pdtb data can be applied directly to label connectives over the english side of the europarl corpus koehn 2005 used for training and testing smtwe used the english side of the europarl corpus koehn 2005',\n",
       " 'in the next section we briefly review modeling of transition probabilities in a conventional hmm alignment model vogel et al 1996 och and ney 2000a we briefly review the hmm based word alignment models vogel 1996 och and ney 2000a',\n",
       " 'this result is statistically significant at p  005 according to bootstrap resampling test koehn 2004 the improvement is statistically significant according to paired bootstrap resampling test koehn 2004',\n",
       " 'starting with textrank mihalcea and tarau 2004  graphbased ranking methods are becoming the most widely used unsupervised approach for keyphrase extractionin the unsupervised approach graphbased ranking methods are stateoftheart mihalcea and tarau 2004',\n",
       " 'because of our experience with the weka package hall et al 2009 we chose this tool for implementation especially we use the weka hall et al 2009 implementation of the simple kmeans for our experiments',\n",
       " 'dropout srivastava et al 2014 is implemented with a dropout rate of 02 to prevent the model from overfittingdropout srivastava et al 2014 is a very effective regularization technique to prevent overfitting of a network',\n",
       " 'indomain smt we used the parallel corpus section 3 to train an enpt phrasebased smt system using the moses toolkit koehn et al 2007 for our smt experiments we use the moses toolkit koehn et al 2007',\n",
       " 'as mentioned in section 3 we obtained dependencies from the output of the stanford parser de marneffe and manning 2008 we make use of dependency parse information from the stanford dependency parser de marneffe and manning 2008',\n",
       " 'we tokenize and truecase all of the corpora using code released with moses koehn et al 2007 we preprocessed the training corpora with scripts included in the moses toolkit koehn et al 2007',\n",
       " 'to obtain these we use the stanford dependency parser de marneffe et al 2006 and the forced alignment from section 39we use the stanford dependency parser de marneffe et al 2006 for extracting dependency path and partofspeech features',\n",
       " 'the nmt models are trained using adam optimizer kingma and ba 2014 with an initial learning rate of 00001all parameters are learned by adam optimizer kingma and ba 2014 with the learning rate 0001',\n",
       " 'the improved alignments gave a gain of table 8  hierarchical lexicalized reordering model galley and manning 2008 one of the phrasebased systems moreover utilizes a lexicalized reordering model galley and manning 2008',\n",
       " 'the statistical significance tests using 95 confidence interval are measured with paired bootstrap resampling koehn 2004 a statistical significance test was performed using the bootstrap resampling method koehn 2004',\n",
       " 'to obtain these we use the stanford dependency parser de marneffe et al 2006 and the forced alignment from section 39we use the stanford dependency parser de marneffe et al 2006 for extracting dependency path and partofspeech features',\n",
       " 'we use the adagrad algorithm duchi et al 2011 to optimize the conditional marginal loglikelihood of the datawe use online learning to train model parameters  updating the parameters using the adagrad algorithm duchi et al 2011',\n",
       " 'the other is from jeffrey pennington et al 2014  the dimension of word embedding is 100we use the 100dimensional glove word embeddings from jeffrey pennington et al 2014',\n",
       " 'we use the adagrad optimizer duchi et al 2011  with initial learning rate set to 01we use online learning to train model parameters  updating the parameters using the adagrad algorithm duchi et al 2011',\n",
       " 'we experiment with the phrasebased statistical machine translation toolkit moses koehn et al 2007 in order to train a japanese english system and to show the influence of the expanded parallelwe used the moses toolkit koehn et al 2007 to build a phrase based machine translation system with a traditional 5gram lm trained on the target side of our bitext',\n",
       " 'indomain smt we used the parallel corpus section 3 to train an enpt phrasebased smt system using the moses toolkit koehn et al 2007 the smt systems were built using the moses toolkit koehn et al 2007',\n",
       " 'we follow the definition in cohen et al 2012 of lpcfgswe also use these perturbation schemes to create multiple models for the algorithm of cohen et al 2012',\n",
       " 'the language model is a 5gram kenlm heafield 2011 model trained using lmplz with modified kneserney smoothing and no pruningwe trained an english 5gram language model using kenlm heafield 2011',\n",
       " 'as mentioned in section 3 we obtained dependencies from the output of the stanford parser de marneffe and manning 2008 we make use of dependency parse information from the stanford dependency parser de marneffe and manning 2008',\n",
       " 'also we evaluate on the rte part of the sick dataset marelli et al 2014 and show that our approach leads to improvementsthe second is the rte part of the sick dataset marelli et al 2014',\n",
       " 'our model has a siamese structure bromley et al 1993 with two subnetworks each processing a sentence in parallelmost previous work use sentence modeling with a siamese structure bromley et al 1993',\n",
       " 'the dialogue act labelling of the corpus follows the date tagging scheme walker et al 2001 the classes used to train the date tagger are derived directly from the date tagging scheme walker et al 2001c',\n",
       " 'we also obtain the dependency parse of the sentences using the stanford parser de marneffe et al 2006 to examine the effect of normalization on dependency parsing we employ the stanford dependency parser 3 marneffe et al 2006',\n",
       " 'the resulting matrix is weighted using pointwise mutual information church and hanks 1990 we construct wordword cooccurrence matrix x every element in the matrix is the pointwise mutual information between the two words church and hanks 1990',\n",
       " 'we extract structured facts using two methods clausie del corro and gemulla 2013 and sedona detailed later in sec 4 also see fig 1we extract facts from captions using clausie del corro and gemulla 2013 and our proposed sedonanlp system',\n",
       " 'dropout srivastava et al 2014 is implemented with a dropout rate of 02 to prevent the model from overfittingnext the output of the maxpooling layer is passed to a dropout layer srivastava et al 2014',\n",
       " 'the population distribution was estimated by the bootstrap method cohen 1995 the bootstrap sampling method provides a way for artificially establishing a sampling distribution for a statistic when the distribution is not known cohen 1995',\n",
       " 'the statistical significance test is also carried out with paired bootstrap resampling method with p  0001 intervals koehn 2004 the significance testing is performed by paired bootstrap resampling koehn 2004',\n",
       " 'the bootstrap sampling method provides a way for artificially establishing a sampling distribution for a statistic when the distribution is not known cohen 1995 the population distribution was estimated by the bootstrap method cohen 1995',\n",
       " 'for medical we use the biomedical data from emea tiedemann 2009 for frenchenglish experiments we used the emea parallel corpus tiedemann 2009  which are medical documents from the european medecines agency',\n",
       " 'the mt experiments were carried out using the standard loglinear phrasebased smt toolkit moses koehn et al 2007 the translation system is trained using the well known moses toolkit koehn et al 2007',\n",
       " 'for the theory of cho  chai 2000 to be complete we have proposed a new type of marker and the adjunct lp constraint in conjunction with their argument lp constraintthen we revise the two lp constraints of cho  chai 2000 and add the adjunct lp constraint',\n",
       " '3 with these trees fixed the partial derivatives with respect to parameters are computed via the backpropagation through structures algorithm goller and kuchler 1996 derivatives are computed efficiently via backpropagation through structure goller and kuchler 1996',\n",
       " 'many researchers have considered generating paraphrases by mining the web guided by the distributional hypothesis which states that words occurring in similar contexts tend to have similar meanings harris 1954 distributional models of meaning follow the distributional hypothesis harris 1954  which states that two words that occur in similar contexts have similar meanings',\n",
       " 'we used weka hall et al 2009 for all our classification experimentswe use the weka toolkit hall et al 2009 for our supervised learning experiments',\n",
       " 'bilingual corpora the corpus used in the following experiments is the basic travel expression corpus takezawa et al 2002 the experiments are carried out on a subset of the basic travel expression corpus btec takezawa et al 2002  as it is used for the supplied data track condition of the iwslt evaluation campaign',\n",
       " 'first we used the moses toolkit koehn et al 2007 for statistical machine translationthe moses toolkit koehn et al 2007 is used for phrase extraction',\n",
       " 'another parallel corpus is the jrcacquis multilingual parallel corpus steinberger et al 2006 corpus steinberger et al 2006',\n",
       " 'the parallel corpus is wordaligned using giza och and ney 2003 this is done using ibm model 1 brown et al 1993 and giza och and ney 2003',\n",
       " 'we thus cast msc as a semantic sentence classification task in a cnn architecture adopting the onelayer cnn model of kim 2014 a variant of collobert et al 2011 we compare the proposed model to our implementation of the iobesbased model described in collobert et al 2011  applied to mwe tagging',\n",
       " 'all these features are inherited from moses koehn et al 2007 all the experiments are carried out in moses toolkit koehn et al 2007',\n",
       " 'for this purpose we use the europarl corpus koehn 2005 for our experiments we use 40000 sentences from europarl koehn 2005 for each language pair following the basic setup of tiedemann 2014',\n",
       " 'the texts were first automatically segmented and tokenized 10 and then they were partofspeech tagged by tnt tagger brants 2000  which was trained on the respective conll training data the filethe data was tagged using tnt brants 2000  using a model trained on the wall street journal',\n",
       " 'secondly holmqvist et al 2012 reordered source words based on word alignment whereas we suggest reordering source chunksholmqvist et al 2012 presented a method where source text is reordered to replicate the target word order based on word alignment',\n",
       " 'for language modeling we use the english gigaword corpus with 5gram lm implemented with the kenlm toolkit heafield 2011 language model for all 3scfg systems we use a 4gram kneserney smoothed language model trained using the kenlm toolkit heafield 2011',\n",
       " 'for our lda implementations we use mallet mccallum 2002 for training bilingual topic models we use mallet toolkit mccallum 2002',\n",
       " 'we use the standard stanfordstyle set of dependency labels de marneffe et al 2006 we use the stanford parser with stanford dependencies de marneffe et al 2006',\n",
       " 'next we evaluate how well the complexity measures proposed in  raghavan et al 2007 correlate with improvement in performance and improvement in learning ratewe also compare annotation strategies in terms of the learning rate similar to raghavan et al 2007  except that we estimate and compare the maximum improvement in the learning rate',\n",
       " 'syntax in epec is annotated following the dependency based formalism used in the prague dependency treebank which was also used in the german negra corpus skut et al 1997 for the german experiments we used the negra corpus skut et al 1997',\n",
       " 'we used the scikitlearn pedregosa et al 2011 implementation of svrs and the skll toolkitwe use the scikit implementation of svm pedregosa et al 2011',\n",
       " 'we used the scikitlearn toolkit to train our classifiers pedregosa et al 2011 for indeplogistic we used scikitlearn pedregosa et al 2011 to train classifiers',\n",
       " 'the training set is used to train the phrasebased translation model and language model for moses koehn et al 2007 we trained a standard moses baseline koehn et al 2007 on the same training data and used the same 4 gram language model to generate responses',\n",
       " 'these features were obtained using the stanford parser 2 marneffe et al 2006 we use the stanford parser with stanford dependencies de marneffe et al 2006',\n",
       " 'we built a 5gram language model on the english side of qcatrain using kenlm heafield 2011 one is a 3gram language model built using kenlm heafield 2011 and trained over a modified version of the annotated corpus in which every it is concatenated with its type eg it event',\n",
       " 'in the other side the french corpus is partofspeech pos tagged by using treetagger tool schmid 1994 for annotating text with partofspeech and lemma information1 the corpus is lemmatised and tagged by partofspeech on both sides using the treetagger schmid 1994',\n",
       " 'the comlex syntax dictionary grishman et al 1994 the comlex syntax dictionary grishman et al 1994 brown corpus kuera and francis 1967 has been used as a reference corpus in many computational applications',\n",
       " 'we lemmatise the head of each constituent with treetagger schmid 1994 pos tagging was performed with the treetagger schmid 1994',\n",
       " 'with the kenlm toolkit heafield 2011 for language modeling we used the kenlm toolkit heafield 2011 for standard ngram modeling with an ngram length of 5',\n",
       " 'we briefly review the hmm based word alignment models vogel 1996 och and ney 2000 och is the hmm alignment model of och and ney 2000',\n",
       " 'we use the stanford parser with stanford dependencies marneffe et al 2006 these features were obtained using the stanford parser 2 marneffe et al 2006',\n",
       " 'we use the stanford parser with stanford dependencies de marneffe et al 2006 we use the standard stanfordstyle set of dependency labels de marneffe et al 2006',\n",
       " 'all the summaries are evaluated using rouge lin 2004 for the summarization task we compare results using rouge lin 2004',\n",
       " 'the reported confidence intervals were estimated using bootstrap resampling koehn 2004 we calculated significance using paired bootstrap resampling koehn 2004',\n",
       " 'we implemented charwnn using the theano library bergstra et al 2010 gradients computed using the automatic differentiation facilities of theano bergstra et al 2010 which implements a generalized bptt',\n",
       " 'for the linear logistic regression implementation we used scikitlearn pedregosa et al 2011 for indeplogistic we used scikitlearn pedregosa et al 2011 to train classifiers',\n",
       " 'the tagger we use is tnt brants 2000  a hidden markov trigram tagger which was trained on the spoken dutch corpus cgn internal release 6the data was tagged using tnt brants 2000  using a model trained on the wall street journal',\n",
       " 'we have used the implementation described in schapire and singer 1999 with decision trees of depth fixed to 31 regarding the learning algorithm we used generalized adaboost with realvalued weak classifiers which constructs an ensemble of decision trees of fixed depth schapire and singer 1999',\n",
       " 'we built a 5gram language model on the english side of qcatrain using kenlm heafield 2011 we build our pbsmt systems in a standard way using the moses system  kenlm for language modelling heafield 2011  and standard lexical reordering model',\n",
       " 'delphin minimal recursion semantics dm as part of the full hpsg sign the erg also makes available a logicalform representation of propositional semantics in the format of minimal recursion semantas output the grammar delivers detailed semantic representations in the form of minimal recursion semantics copestake et al 2005',\n",
       " 'like the conll2006 shared task the 2007 shared task focuses on dependency parsing and aims at comparing stateoftheart machine learning algorithms applied to this task nivre et al 2007 one of the first venues at which domain adaptation was targeted was the 2007 conll shared task on dependency parsing nivre et al 2007',\n",
       " 'like cho  chai 2000  our analysis also provides the same explanation for various scrambled sentences such as the double accusative construction dac in koreanhowever like cho  chai 2000  our analysis can also predict that the scrambled sentence 19c is grammatical whereas 19b is ungrammatical',\n",
       " 'the phrase table is extracted from a bilingual text aligned on the word level using eg giza  och and ney 2003 a core component of every pbsmt system is the phrase table which contains bilingual phrase pairs extracted from a bilingual corpus after word alignment och and ney 2003',\n",
       " 'google web 1t brants and franz 2006 has been used to calculate term idf which is used as a measure of the importance of the termsthe google web 1t data brants and franz 2006 has been shown to give a higher score however this data was not available during the course of this research',\n",
       " 'we first use a dependency parser de marneffe et al 2006 to parse each sentence and extract the set of dependency relations associated with the sentence a complete set of parser tags and the method used to map from a constituent to a typed dependency grammar can be found in de marneffe et al 2006',\n",
       " 'we first use a dependency parser de marneffe et al 2006 to parse each sentence and extract the set of dependency relations associated with the sentence a complete set of parser tags and the method used to map from a constituent to a typed dependency grammar can be found in de marneffe et al 2006',\n",
       " 'these analyses provide an alternative but theoretically more reasonable explanation to the findings of liang et al 2006  while they blame unreasonable gold derivations for the failure of standliang et al 2006 observe that standard update performs worse than local update which they attribute to the fact that the former often update towards a gold derivation made up of unreasonable r',\n",
       " 'we experiment with the phrasebased statistical machine translation toolkit moses koehn et al 2007 in order to train a japanese english system and to show the influence of the expanded parallelwe present a system that takes a general moses statistical machine translation system koehn et al 2007 and adapts it to the biomedical domain',\n",
       " 'for our corpus we randomly selected documents from the washington section of the new york times corpus sandhaus 2008 from the year 2007new york times consists of 500 random sentences from the new york times corpus sandhaus 2008  year 2007 we selected only sentences that contained at least one named entity according to the stanf',\n",
       " 'they used the webbased annotation tool brat stenetorp et al 2012 for the annotation we consider the following types of implicit referents  the brat tool stenetorp et al 2012 was used for annotation',\n",
       " 'all corpora were taken from the childes database macwhinney 2000 the evaluation set was comprised of adult utterances from the brown 1973 data of the childes database macwhinney 2000',\n",
       " 'the system generated tweets were evaluated using rouge measures lin 2004 the summaries from the above algorithm for the qfmds were evaluated based on rouge metrics lin 2004',\n",
       " 'the toefl synonym selection task is to select the semantically closest word to a target from a list of four candidates landauer and dumais 1997 the toefl synonym dataset landauer and dumais 1997 consists of 80 question words for each of which 4 answer words are given and the task is to select the answer word most similar to the question',\n",
       " 'we calculate our features using the kenlm toolkit heafield 2011 1 we used the kenlm toolkit heafield 2011 to build all language models used in this work ie both for data selection and for the mt systems used for extrinsic evaluation',\n",
       " 'we use the scikit implementation of random forest pedregosa et al 2011 the regressor used is a random forest regressor in the implementation provided by scikitlearn pedregosa et al 2011',\n",
       " 'for this purpose we use the europarl corpus koehn 2005 the statistical dictionary for this task was extracted from the englishfrench europarl 7 corpus koehn 2005',\n",
       " 'the statistical significance tests using 95 confidence interval are measured with paired bootstrap resampling koehn 2004 we used bootstrap resampling for testing statistical significance koehn 2004',\n",
       " 'we then run word alignment with giza och and ney 2003 in both directions with the default parameters used in moseswe used the giza software och and ney 2003 to do the word alignments',\n",
       " 'we built a 5gram language model on the english side of qcatrain using kenlm heafield 2011 we calculate our features using the kenlm toolkit heafield 2011',\n",
       " 'we used giza och and ney 2003 to align the words in the corpuswe then run word alignment with giza och and ney 2003 in both directions with the default parameters used in moses',\n",
       " 'the phrase tables were generated by means of symmetrised word alignments obtained with giza och and ney 2003 the word alignment was obtained by running giza och and ney 2003',\n",
       " 'zhu et al 2013 applied kalman filter model to learn and estimate user intentions in their humancomputer interactive word segmentation frameworkin the scenario of humancomputer interactive chinese word segmentation the kalman filter approach proposed by zhu et al 2013  the md value and the horizontal axis denotes the occurrence of',\n",
       " 'koo et al 2008  have proposed to use word clusters as features to improve graphbased statistical dependency parsing for english and czechin order to reduce the amount of annotated data to train a dependency parser koo et al 2008 used word clusters computed from unlabelled data as features for training a parser',\n",
       " 'more recently pasha et al 2014 created madamira a system for morphological analysis and disambiguation of arabic this system can be used to improve the accuracy of spelling checking system esnext the files are processed with the morphological analysis and disambiguation system madamira pasha et al 2014 that corrects a common class of spelling errors',\n",
       " 'we used the moses toolkit koehn et al 2007 with its default settingsfor building our smt systems the opensource smt toolkit moses koehn et al 2007 was used in its standard setup',\n",
       " 'bullet naive bayesnb we use binomial variant with laplace smoothing parameter  1 pedregosa et al 2011 we use the bernoulli naive bayes classifier in scikit with the default settings pedregosa et al 2011',\n",
       " 'we use adagrad duchi et al 2011  with the initial learning rate set to   05we use online learning to train model parameters  updating the parameters using the adagrad algorithm duchi et al 2011',\n",
       " 'the mt experiments were carried out using the standard loglinear phrasebased smt toolkit moses koehn et al 2007 the baseline systems are built with the opensource phrasebased smt toolkit moses koehn et al 2007',\n",
       " 'we use the adagrad method duchi et al 2011 to automatically update the learning rate for each parameterwe use online learning to train model parameters  updating the parameters using the adagrad algorithm duchi et al 2011',\n",
       " 'among the existing sensetagged corpora the semcor corpus miller et al 1994 is one of the most widely usedthe semcor corpus miller et al 1994 is one of the few currently available manually senseannotated corpora for wsd',\n",
       " 'we use the liblinear support vector machine svm chang and lin 2011 classifier for training and run 5fold crossvalidation for evaluationa linearkernel support vector machine chang and lin 2011 classifier is trained on the available training data',\n",
       " 'we build a state of the art phrasebased smt system using moses koehn et al 2007 we tokenize and truecase all of the corpora using code released with moses koehn et al 2007 1',\n",
       " 'we built a sourcetotarget pbsmt model from the bilingual domain corpus using the moses toolkit koehn et al 2007 we build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " 'the dictionaries are automatically generated via word alignment using giza och and ney 2000 on parallel corporawe then made use of the giza software och and ney 2000 to perform word alignment on the parallel corpora',\n",
       " 'there are two main approaches to processing nonstandard data normalization and domain adaptation eisenstein 2013 there are two approaches proposed in the literature to handle this problem eisenstein 2013',\n",
       " 'we conducted baseline experiments for phrasebased machine translation using the moses toolkit koehn et al 2007 we trained a model using moses toolkit koehn et al 2007 on the training data as our baseline system',\n",
       " 'the task of identifying mentions to medical concepts in free text and mapping these mentions to a knowledge base was recently proposed in shareclef ehealth evaluation lab 2013 suominen et al 2013 we found the largest of such corpus to be the dataset from the task to recognize disorder mentions in clinical text initially organized by shareclef ehealth evaluation lab shel in 2013  suominen et al 2013',\n",
       " 'we built a sourcetotarget pbsmt model from the bilingual domain corpus using the moses toolkit koehn et al 2007 we use the moses software package 5 to train a pbmt model koehn et al 2007',\n",
       " 'we also use madatokan habash et al 2009 to preprocess and tokenize the arabic side of the corpuswe use the mada package habash et al 2009 to collect the stem and the morphological features of the hypothesis and reference translation',\n",
       " 'both training and testing data consist of pubmed abstracts extracted from the genia corpus kim et al 2008 the data provided for the shared task is prepared from the genia corpus kim et al 2008',\n",
       " 'we conducted baseline experiments for phrasebased machine translation using the moses toolkit koehn et al 2007 we built a sourcetotarget pbsmt model from the bilingual domain corpus using the moses toolkit koehn et al 2007',\n",
       " 'to extract our partofspeech pos features we first tag the transcripts using the nltk pos tagger bird et al 2009 we used the brill tagger provided by nltk for our pos tagging bird et al 2009',\n",
       " 'the corpora are tokenised and truecased using scripts from the moses toolkit koehn et al 2007 we tokenize and truecase all of the corpora using code released with moses koehn et al 2007 1',\n",
       " 'we trained a model using moses toolkit koehn et al 2007 on the training data as our baseline systemour baseline is a phrasebased mt system trained using the moses toolkit koehn et al 2007',\n",
       " 'for seed and test paradigms we used verbal inflectional paradigms from the celex morphological database baayen et al 1995 for english we used the celex database baayen et al 1995 to segment english words into morphemes',\n",
       " 'then we did word alignment using giza och and ney 2003 with the default growdiagfinaland alignment symmetrization methodwe then run word alignment with giza och and ney 2003 in both directions with the default parameters used in moses',\n",
       " 'pang et al 2002 have reported the effectiveness of applying machine learning techniques to the pn classificationpang et al 2002  use machine learning methods nb svm and maxent to detect sentiments on movie reviews',\n",
       " 'we train the concept identification stage using infinite ramp loss 1 with adagrad duchi et al 2011 we use online learning to train model parameters  updating the parameters using the adagrad algorithm duchi et al 2011',\n",
       " 'for the contextual check we use the google web 1t 5gram corpus brants and franz 2006 which contains counts for ngrams from unigrams through to fivegrams obtained from over 1 trillion word tokethe web1t corpus brants and franz 2006 is a dataset consisting of the counts for ngrams obtained from 1 trillion 10 12 words of english web text subject to a minimum occurrence threshold 20',\n",
       " 'we built a sourcetotarget pbsmt model from the bilingual domain corpus using the moses toolkit koehn et al 2007 the corpora are tokenised and truecased using scripts from the moses toolkit koehn et al 2007',\n",
       " 'bullet naive bayesnb we use binomial variant with laplace smoothing parameter  1 pedregosa et al 2011 bullet logistic regressionlr we use logistic regression with l2 loss penalty and c1 pedregosa et al 2011',\n",
       " 'we built a sourcetotarget pbsmt model from the bilingual domain corpus using the moses toolkit koehn et al 2007 we tokenize and frequentcase the data with the standard scripts from the moses toolkit koehn et al 2007',\n",
       " 'an unpruned modified kneserneysmoothed 4gram language model is estimated using the kenlm toolkit heafield et al 2013 the language model is a standard 5gram model estimated from the monolingual data using modified kneserney smoothing without pruning applying kenlm tools heafield et al 2013',\n",
       " 'for native data several teams make use of the web 1t 5gram corpus henceforth web1t brants and franz 2006 we used the unigram counts from the web 1t 5gram corpus brants and franz 2006 to determine the frequency of use of each candidate synonym',\n",
       " 'we measure statistical significance using 95 confidence intervals computed with paired bootstrap resampling koehn 2004 the column pairci shows 95 confidence intervals relative to the primary system using the paired bootstrap resampling method koehn 2004',\n",
       " '1 the models are constructed using c45 decision tree classifiers as implemented within weka hall et al 2009  with default parameter settingsthe weka smo implementation of svm hall et al 2009 was used as classifier with default parameter settings',\n",
       " 'it therefore follows the distributional hypothesis harris 1954 which states that words that occur in the same contexts tend to have similar meaningsdistributional similarity relies on the distributional hypothesis that similar terms appear in similar contexts harris 1954',\n",
       " 'consider the two examples below drawn from the penn discourse treebank pdtb prasad et al 2008  of a causal and a contrast relation respectively pitler and nenkova 2008 used discourse relations of the penn discourse treebank prasad et al 2008 as a feature',\n",
       " 'ble is based on the distributional hypothesis harris 1954  stating that words with similar meaning have similar distributions across languages distributional similarity relies on the distributional hypothesis that similar terms appear in similar contexts harris 1954',\n",
       " 'we use the bleu score as primary criterion which is optimized on the development set using the downhill simplex algorithm press et al 2002 the optimization is done using the downhill simplex algorithm from the numerical recipes book press et al 2002',\n",
       " 'the relation prediction task of science ie is challenging and quite different from other semantic relation prediction task like semeval2010 task 8 hendrickx et al 2009 the semeval2010 task 8 dataset is a widely used benchmark for relation classification hendrickx et al 2009',\n",
       " 'machine translation system settings we used a phrasebased statistical machine translation model as implemented in the moses toolkit koehn et al 2007 for machine translationwe compare the model against the moses phrasebased translation system koehn et al 2007  applied to phoneme sequences',\n",
       " 'we use the mrs analyses that are produced by the hpsg english resource grammar erg flickinger 2000 the experiments are carried out on a broadcoverage linguisticallyprecise hpsg grammar for english the lingo english resource grammar erg copestake and flickinger 2000',\n",
       " 'in each case the improvement of ebmt tm  smt over the baseline smt is statistically significant reliability of 98 using bootstrap resampling koehn 2004 in all cases results are statistically significant 99 following the pair bootstrap resampling koehn 2004',\n",
       " 'to capture typical partof bridging see example 2 we extract a list of 45 nouns which specify building parts eg room or roof from the general inquirer lexicon stone et al 1966 we extract a list containing around 4000 relational nouns from wordnet and a list containing around 500 nouns that specify professional roles from the general inquirer lexicon stone et al 1966',\n",
       " 'the surfacesyntactic representation  p was a standard firstorder edge factorization using the same features as mcdonald et al 2005 the number of features extracted from the pdt training set was 13 450 672 using the feature set outlined by mcdonald et al 2005',\n",
       " 'in our work like hernault et al 2010  we also consider the discourse segmentation task as a sequence labeling problemsimilar to the work of hernault et al 2010  our base model uses conditional random fields 1 to learn a sequence labeling model',\n",
       " 'we use the data that were recorded and preprocessed by mitchell et al 2008  available for download in their supporting online material1 full details of the experimental protocol data acquisition and preprocessing can be found in mitchell et al 2008 and the supporting material',\n",
       " 'we trained nonprojective dependency parsers for 6 languages using the conllx shared task datasets buchholz and marsi 2006  arabic danish dutch japanese slovene and spanishfor nonprojective parsing experiments four languages from the conll x shared task are used danish dutch slovene and swedish buchholz and marsi 2006',\n",
       " 'our lp constraints based on the new type marker are compatible with those of cho  chai 2000 on the one hand and are able to deal with the scrambling between adjuncts and arguments on the other handfor the theory of cho  chai 2000 to be complete we have proposed a new type of marker and the adjunct lp constraint in conjunction with their argument lp constraint',\n",
       " 'the wsj grammar covers the upenn wall street journal wsj treebank sentences marcus et al 1994 the approach presented in this paper is a first attempt to scale up stochastic parsing systems based on linguistically finegrained handcoded grammars to the upenn wall street journal henceforth wsj',\n",
       " 'statistically significant results calculated with paired bootstrap resampling koehn 2004 for bleu and nist are indicated with symbols p  001 and p  005we calculated significance using paired bootstrap resampling koehn 2004',\n",
       " 'in testing we used minimum bayes risk decoding kumar and byrne 2004  cube pruning and the operation sequence model durrani et al 2011 selects the translation with minimum bayes risk kumar and byrne 2004',\n",
       " 'selects the translation with minimum bayes risk kumar and byrne 2004 additionally we will compare two decision rules the common maximum aposteriori map decision rule and the minimum bayes risk mbr decision rule kumar and byrne 2004',\n",
       " 'we use a prototypebased selectional preference model erk 2007 we build on a recent selectional preference model erk 2007 that bases its generalisations on word similarity in a vector space',\n",
       " 'we used 10fold crossvalidation set the confidence interval to 95 and used the jackknifing procedure for multiannotation evaluation lin 2004 rouge2 metric lin 2004 is used for the evaluation',\n",
       " 'the brown corpus tagged with wordnet senses miller et al 1993 the senses in wordnet are ordered according to the frequency data in the manually tagged resource sem cor miller et al 1993',\n",
       " 'as an implementation we use svm light joachims 1999 as a supervised classifier for vsm and wiki we chose support vector machines using svm light joachims 1999',\n",
       " 'for translation tables  the moses system koehn et al 2007 as well as portage offer model filtering moses offline portage offline andor at load timewe compare the model against the moses phrasebased translation system koehn et al 2007  applied to phoneme sequences',\n",
       " 'the particle filter of canini et al 2009 rejuvenates over independent draws from the history by storing all past observations and statesthe particle filter studied empirically by canini et al 2009  stored the entire history incurring linear storage complexity in the size of the stream',\n",
       " 'we describe an approximation to the bleu score papineni et al 2001 that will satisfy these conditions we here describe a linear approximation to the logbleu score papineni et al 2001 which allows such a decomposition',\n",
       " 'we applied the naive bayes probabilistic supervised learning algorithm from the weka machine learning library hall et al 2009 we conducted experiments using multinomial naive bayes classifier implemented in the weka toolkit hall et al 2009',\n",
       " 'we minimize the cross entropy loss using gradientbased optimization and the adam update rule kingma and ba 2014 the network is trained using sgd with shuffled minibatches using the adam update rule kingma and ba 2014',\n",
       " 'statistical significance in bleu score difference was measured by using paired bootstrap resampling koehn 2004 statistical significance of the difference between systems is computed with paired bootstrap resampling koehn 2004 p  005 1 000 iterations',\n",
       " 'we measure statistical significance using 95 confidence intervals computed with paired bootstrap resampling koehn 2004 for all results we computed their confidence intervals p  005 by means of bootstrap resampling koehn 2004',\n",
       " '3 as verbs we take all tags that map to v in the universal tag mappings from petrov et al 2012 in the pos tag level we basically used the universal tagset proposed by petrov et al 2012  in mapping original tags into universal ones',\n",
       " 'we use the mrs analyses that are produced by the hpsg english resource grammar erg flickinger 2000 here we use 1 the link grammar parser 8 and 2 the hpsg english resource grammar copestake and flickinger 2000 and pet parser',\n",
       " 'we measure statistical significance using 95 confidence intervals computed with paired bootstrap resampling koehn 2004 for statistical significance testing we use a paired bootstrap resampling method proposed in koehn 2004',\n",
       " 'on semantic role labeling gildea and jurafsky 2002 first presented a system based on a statistical classifier which is trained on a handannotated corpora framenetgildea and jurafsky 2002  were the first to describe a statistical system trained on the data from the framenet project to automatically assign semantic roles',\n",
       " 'we apply the stochastic gradient descent algorithm with minibatches and the adadelta update rule zeiler 2012 we use a minibatch size of 100 and use adadelta zeiler 2012 as the gradient update method',\n",
       " 'we used the linear svm implementation with default parameters and random forest with 50 trees both available at scikitlearn pedregosa et al 2011 we use the scikit implementation of svm pedregosa et al 2011',\n",
       " 'we test the statistical significance of differences between various mt systems using the bootstrap resampling method koehn 2004 statistical significance tests are performed using bootstrap resampling koehn 2004',\n",
       " 'in particular the parser implements the arcstandard parsing algorithm nivre 2004 1 the parser implements the arcstandard algorithm nivre 2004 and it therefore makes use of a stack and a buffer',\n",
       " 'we use adadelta zeiler 2012 to update the parameters during trainingin the training procedure we use adadelta zeiler 2012 to update model parameters with a minibatch size 80',\n",
       " 'we used the english side of the europarl corpus koehn 2005 the systems for the english  spanish translation tasks were trained on the sentencealigned europarl corpus koehn 2005',\n",
       " 'ccg is a lexicalized theory of grammar steedman 2001 the input of the boxer system is a syntactic analysis in the form of a derivation of combinatorial categorial grammar ccg steedman 2001',\n",
       " 'the results for teslam and teslaf have previously been reported in liu et al 2010 3 tesla translation evaluation of sentences with linearprogrammingbased analysis was first proposed in liu et al 2010',\n",
       " 'the semantic representation is minimal recursion semantics copestake et al 2005 the erg produces minimal recursion semantics mrs copestake et al 2005 analyses which are flat structures that explicitly encode predicate argument relations and other data',\n",
       " 'germanet gn is the german counterpart to wn hamp and feldweg 1997 future work could be to extend the german dataset by adding additional resources to the llr for instance germanet hamp and feldweg 1997',\n",
       " 'baroni et al 2002  report that 47 of the vocabulary types in the apa corpus 2 were compounds baroni et al 2002  analyzed the 28 million words german apa news corpus and discovered that compounds account for 47 of the word types but only 7 of the overall token count with 83 of compounds',\n",
       " 'both language models use modified kneserney smoothing chen and goodman 1996 5gram language models are trained over the targetside of the training data using srilm stolcke 2002 with modified kneserney discounting chen and goodman 1996',\n",
       " 'we used mallet mccallum 2002 for this experiment and the same trainingtesting partitions used in the experiment reported in table 3 10 we used the pltm implementation in mallet mccallum 2002',\n",
       " 'we calculated significance using paired bootstrap resampling koehn 2004 the statistical significance test is also carried out with paired bootstrap resampling method with p  0001 intervals koehn 2004',\n",
       " 'additionally we train phrasebased machine translation models with our alignments using the popular moses toolkit koehn et al 2007 finally we used moses toolkit as phrasebased reference koehn et al 2007',\n",
       " 'for building the baseline smt system we used the opensource smt toolkit moses koehn et al 2007  in its standard setupwe used the moses toolkit koehn et al 2007 with its default settings',\n",
       " 'this results in the semantic triple shotmanbird lemmatized to shootmanbird using the stanford corenlp lemmatizer manning et al 2014 we also lemmatized all words using stanford corenlp manning et al 2014',\n",
       " 'we used standard classifiers available in scikitlearn package pedregosa et al 2011 3 we used logistic regression though linear svm showed almost the same results for the final reranking the implementation was taken from scikitlearn package pedregosa et al 2011',\n",
       " 'rhetorical structure theory mann and thompson 1988 belongs to the first sortthe closest area to our work consists of investigations of discourse relations in the context of rhetorical structure theory mann and thompson 1988',\n",
       " 'both language models use modified kneserney smoothing chen and goodman 1996 5gram language models are trained over the targetside of the training data using srilm stolcke 2002 with modified kneserney discounting chen and goodman 1996',\n",
       " 'we train the concept identification stage using infinite ramp loss with adagrad duchi et al 2011 1 using adagrad duchi et al 2011',\n",
       " 'for training the translation model and for decoding we used the moses toolkit koehn et al 2007 all our translation systems are based on moses koehn et al 2007 and standard components for training and tuning the models',\n",
       " 'we use rouge score as our evaluation metric lin 2004 with standard options 8 we expect this restriction is more consistent with the rouge evaluation metric used for summarization lin 2004',\n",
       " 'for building the baseline smt system we used the opensource smt toolkit moses koehn et al 2007  in its standard setupfor training the translation model and for decoding we used the moses toolkit koehn et al 2007',\n",
       " 'we use the europarl parallel corpus koehn 2005 as the bitext from which to extract the auto matic bilingual lexiconswe extract our paraphrase grammar from the frenchenglish portion of the europarl corpus version 5 koehn 2005',\n",
       " 'we use ridge regression rr with l2norm regularization and support vector regression svr with an rbf kernel from scikitlearn pedregosa et al 2011 bullet logistic regressionlr we use logistic regression with l2 loss penalty and c1 pedregosa et al 2011',\n",
       " 'all data used in our experiments are sentencesplit lowercased and tokenize using the corenlp toolkit manning et al 2014 we tokenize english data and segment chinese data using the stanford corenlp toolkit manning et al 2014',\n",
       " 'since the phrase table contains lemmas the wikipedia corpus was lemmatised using the treetagger schmid 1994 1 the corpus is lemmatised and tagged by partofspeech on both sides using the treetagger schmid 1994',\n",
       " 'for this purpose we used the logistic regression classifier from scikitlearn with l2 regularisation pedregosa et al 2011 1 we use logistic regression with l2 regularization implemented using the scikitlearn toolkit pedregosa et al 2011',\n",
       " 'all our systems are contrasted with a standard phrasebased system built with moses koehn et al 2007 our machine translation systems this year are a departure from our previous moses koehn et al 2007 based systems from wmt16',\n",
       " 'we test our metrics in the setting of the wmt 2009 evaluation task callisonburch et al 2009 the compared systems are evaluated on the englishtogerman 13 news translation task of wmt 2009 callisonburch et al 2009',\n",
       " 'these classifiers have been used in related work by pang et al 2002 we evaluated our method with movie review documents that were used in pang et al 2002',\n",
       " 'one is from turian et al 2010  the dimension of word embedding is 50for example turian et al 2010  showed that the optimal dimensionality for word embeddings is taskspecific',\n",
       " 'word alignment using giza toolkit och and ney 2000  the default configuration as available in training scripts for mosesword alignment is performed by giza och and ney 2000 in both directions with the default setting',\n",
       " 'we found that using adagrad duchi et al 2011 to update the parameters is very effective 1 using adagrad duchi et al 2011',\n",
       " 'the penn discourse treebank pdtb prasad et al 2008 is a large corpus annotated with discourse relations covering the wall street journal part of the penn treebankone of the most important resources for discourse connectives in english is the penn discourse treebank prasad et al 2008',\n",
       " 'the system was trained on the english and danish part of the europarl corpus version 3 koehn 2005 the europarl data set consists of 707 sentences of the german part of the europarl corpus koehn 2005',\n",
       " 'for example turian et al 2010 compared brown clusters cw embeddings and hlbl embeddings in ner and chunking tasksturian et al 2010 applied word embeddings to chunking and named entity recognition ner',\n",
       " 'we use the moses software package 5 to train a pbmt model koehn et al 2007 in particular we use moses koehn et al 2007 to train a monolingual phrasebased mt system on the paralex corpus',\n",
       " 'past experiences on this system have shown that the random forest breiman 2001 outperforms other regression algorithms like support vector regression or multilayer perceptronour submitted system for the second task is based on random forest breiman 2001',\n",
       " 'we trained a number of frenchenglish smt systems using the moses toolkit koehn et al 2007 in its default settingwe used the moses toolkit koehn et al 2007 to train standard phrasebased systems with default configurations',\n",
       " 'bullet zhang2015 zhang et al 2015 proposed to use shallow convolutional neural networks to model two arguments respectivelyzhang et al 2015 explore a shallow convolutional neural network and achieve competitive performance',\n",
       " 'all data used in our experiments are sentencesplit lowercased and tokenize using the corenlp toolkit manning et al 2014 we tokenize english data and segment chinese data using the stanford corenlp toolkit manning et al 2014',\n",
       " 'the major part of data comes from current and upcoming film releases of the europarl dataset koehn 2005 the source of bilingual data used in the experiments is the europarl collection koehn 2005',\n",
       " 'we expect this restriction is more consistent with the rouge evaluation metric used for summarization lin 2004 rouge lin 2004 is the fully automatic metric commonly used to evaluate the text summarization results',\n",
       " 'additionally we train phrasebased machine translation models with our alignments using the popular moses toolkit koehn et al 2007 we used the moses toolkit koehn et al 2007 to train standard phrasebased systems with default configurations',\n",
       " 'additionally we train phrasebased machine translation models with our alignments using the popular moses toolkit koehn et al 2007 the baseline systems are built with the opensource phrasebased smt toolkit moses koehn et al 2007',\n",
       " 'the decoder searches for the best translation given a set of models h m e i 1  s k 1  f j 1 by maximizing the loglinear feature score och and ney 2004  e i 1  arg max ie i 1 m m1the loglinear approach to phrasebased translation och and ney 2004 directly models the predictive translation distribution pef  w  1 zf exp w e f 1 where e is the target string',\n",
       " 'the training set is used to train the phrasebased translation model and language model for moses koehn et al 2007 we used the moses toolkit koehn et al 2007 to train standard phrasebased systems with default configurations',\n",
       " 'the usages from the ukwac are tokenised and lemmatised using treetagger schmid 1994  as provided by the corpus3 all english data are pos tagged and lemmatised using the treetagger schmid 1994',\n",
       " 'to quantify the redundancy of structures we partof speech tagger the english gigaword corpus graff and cieri 2003 the trigram target language model is trained from the xinhua portion of english gigaword corpus graff and cieri 2003',\n",
       " 'baseline word alignments were obtained by running giza in both directions and symmetrizing using the growdiagfinaland heuristic och and ney 2003 translation models were trained over the bilingual data that was automatically wordaligned using giza och and ney 2003 in both directions  and the diaggrowfinal heuristic was used to ldc200',\n",
       " 'the results displayed in table 3 are obtained with the smo classifier trained using the weka library hall et al 2009 on our downloaded semeval 2013 development and training corpora 7595 tweetsthe core model is a decision tree classifier trained on the qalb parallel training data using weka hall et al 2009',\n",
       " 'we trained a number of frenchenglish smt systems using the moses toolkit koehn et al 2007 in its default settingwe built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " 'the performance of our algorithm is compared with the disambiguation accuracy obtained with a variation of the lesk algorithm 3 lesk 1986  which selects the meaning of an openclass word by findinote that word sense disambiguation is performed using the lesk algorithm lesk 1986',\n",
       " 'we use the scikit implementation of svm pedregosa et al 2011 we use a wellknown scikitlearn pedregosa et al 2011 implementation of random forests and svms in python as well as a very effective gradient boosting trees implementation from the xgboost libr',\n",
       " 'for example turian et al 2010 showed that the optimal dimensionality for word embeddings is task specific turian et al 2010 applied word embeddings to chunking and named entity recognition ner',\n",
       " 'ease uses nltk bird et al 2009 for pos tagging and stemming a spell for spell checking and wordnet fellbaum 1998 to get the synonyms we used the brill tagger provided by nltk for our pos tagging bird et al 2009',\n",
       " 'all our translation systems are based on moses koehn et al 2007 and standard components for training and tuning the modelsall our systems are contrasted with a standard phrasebased system built with moses koehn et al 2007',\n",
       " 'the training set is used to train the phrasebased translation model and language model for moses koehn et al 2007 we use the moses software package 5 to train a pbmt model koehn et al 2007',\n",
       " 'additionally we train phrasebased machine translation models with our alignments using the popular moses toolkit koehn et al 2007 for training the translation model and for decoding we used the moses toolkit koehn et al 2007',\n",
       " 'we thank gbor recski has research institute for linguistics for performing the pca on the operators of the socher et al 2013 cvgwe employed the stateoftheart sentiment annotator by socher et al 2013 for this purpose',\n",
       " 'it has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the information required to assert a relationship between them bunescu and mooney 2005 our work is also related to bunescu and mooney 2005  where the similarity between the words on the path connecting two entities in the dependency graph is used to devise a kernel function',\n",
       " 'these algorithms were used to participate in the the expression level task subtask a and message level task subtask b of the semeval2014 task 9 sentiment analysis in twitter rosenthal et al the system that obtained the best performance in the sentiment analysis at message level task of semeval 2013 nakov et al 2013 and 2014 rosenthal et al 2014 mined twitter to build big sent',\n",
       " 'phrasebased smt system a standard non factored phrasebased smt system was built using the open source moses toolkit koehn et al 2007 with parameters set similar to those of neubig 2011smt translations a phrasebased smt system built using the moses toolkit koehn et al 2007 and the whole spanishenglish dataset except the sentences in the test set was used to translated the',\n",
       " 'for the theory of cho  chai 2000 to be complete we have proposed a new type of marker and the adjunct lp constraint in conjunction with their argument lp constraintgiven the lp constraints in 4 7 and 8 we can provide a simpler explanation to various scrambled sentences including the alleged counterexamples to the analysis of cho  chai 2000',\n",
       " 'all linguistic annotations needed for features pos chunks 7  parses are from stanford corenlp manning et al 2014 part of speech tagging and named entity recognition for comment plausibility features are carried out using stanford corenlp manning et al 2014',\n",
       " 'algorithm 5 shows a passiveaggressive algorithm for the structured output crammer et al 2006 to build a parser we use a structured classifier to approximate the oracle and apply the passiveaggressive pa algorithm crammer et al 2006 for parameter estimation',\n",
       " 'the task is part of the semantic evaluation 2012 workshop agirre et al 2012 semantic textual similarity sts is the task of judging the similarity of a pair of sentences on a scale from 1 to 5 agirre et al 2012',\n",
       " 'we used the same annotation guidelines as zaidan et al 2007 we also use a0 to compare against the masking svm method and svm baseline of zaidan et al 2007',\n",
       " 'the stanford parser 1 marneffe et al 2006 was used to produce all dependency parsesthe same kind of process was applied to the penn treebank using the stanford conversion system to produce dependency annotations de marneffe et al 2006',\n",
       " '1 after tokenization  we lemmatize and stem tweets and remove stopwords from each tweet using the nltk toolkit bird et al 2009 for reuters we segmented and tokenized the data using nltk bird et al 2009',\n",
       " 'machine translation system settings we used a phrasebased statistical machine translation model as implemented in the moses toolkit koehn et al 2007 for machine translationwe use the moses phrasebased translation system koehn et al 2007 to implement our models',\n",
       " 'the phrase table is extracted from a bilingual text aligned on the word level using eg giza  och and ney 2003 the corpus is first wordaligned using a word alignment heuristic och and ney 2003',\n",
       " 'this architecture is very similar to the framework of uima ferrucci and lally 2004 3 dkpro is a collection of software components for natural language processing based on the apache uima framework ferrucci and lally 2004',\n",
       " 'besides using sentistrength we use the lexicon approach proposed by hu and liu 2004 in order to construct the lexical prior knowledge matrix u 0  we use the sentiment lexicon generated by hu and liu 2004',\n",
       " 'run1 we firstly use the stanford corenlp toolkit 3 manning et al 2014 to split each token for the sentence pairs in the evaluation datawe use the stanford corenlp caseless tagger for partofspeech tagging manning et al 2014',\n",
       " 'by setting n inw and enwn1 for all nodes the generalized kernel can be converted to the kernel proposed in collins and duffy 2001 the proof is similar to the proof for the tree kernel in collins and duffy 2001',\n",
       " '2 we tested the difference in performance for statistical significance using an approximate randomization procedure yeh 2000 with 10000 iterations we calculate statistical significance of performance differences using stratified shuffling yeh 2000',\n",
       " 'run1 we firstly use the stanford corenlp toolkit 3 manning et al 2014 to split each token for the sentence pairs in the evaluation datawe use the stanford corenlp caseless tagger for partofspeech tagging manning et al 2014',\n",
       " 'all linguistic annotations needed for features pos chunks 7  parses are from stanford corenlp manning et al 2014 part of speech tagging and named entity recognition for comment plausibility features are carried out using stanford corenlp manning et al 2014',\n",
       " 'in erkan and radev 2004  the concept of graph based centrality was used to rank a set of sentences in producing generic multidocument summariesit was also the model used to rank sentences in erkan and radev 2004',\n",
       " 'significance was tested using a paired bootstrap koehn 2004 with 1000 samples p002statistical significance is tested on the bleu metric using paired bootstrap resampling koehn 2004 with n  1000 and p  005',\n",
       " 'for seed and test paradigms we used verbal inflectional paradigms from the celex morphological database baayen et al 1995 to validate this measure we computed the cosine similarity between words and their morphological parents from the celex2 database baayen et al 1995',\n",
       " 'we used kenlm heafield 2011 to create 3gram language models with kneserney smoothing on the target side of the bilingual training corpora1 we used the kenlm toolkit heafield 2011 to build all language models used in this work ie both for data selection and for the mt systems used for extrinsic evaluation',\n",
       " 'we built a 5gram language model on the english side of qcatrain using kenlm heafield 2011 we used kenlm heafield 2011 to create 3gram language models with kneserney smoothing on the target side of the bilingual training corpora',\n",
       " 'an estimate of the likelihood of a verb taking a event subject was computed over the annotated english gigaword v5 corpus napoles et al 2012 in the news column  we show the statistics of a subset of annotated english gigaword napoles et al 2012',\n",
       " 'we use the web 1t 5gram corpus brants and franz 2006 to compute the language model score for a sentencesince the googleapi is not available any more we use the web 1t 5gram corpus brants and franz 2006 to extract the google distance feature',\n",
       " 'machine translation system settings we used a phrasebased statistical machine translation model as implemented in the moses toolkit koehn et al 2007 for machine translationthe moses smt toolkit koehn et al 2007 provides a complete statistical translation system distributed under the lgpl license',\n",
       " 'machine translation system settings we used a phrasebased statistical machine translation model as implemented in the moses toolkit koehn et al 2007 for machine translationwe trained a model using moses toolkit koehn et al 2007 on the training data as our baseline system',\n",
       " 'a chunk is a minimal  nonrecursive structure consisting of correlated groups of words bharati et al 2006 the chunk label tagset is a coarser version of anncorra tagset bharati et al 2006',\n",
       " 'mc30 a subset of rg65 dataset with 30 word pairs miller and charles 1991 we use a set of 30 word pairs from a study carried out by miller and charles 1991',\n",
       " 'we obtained newspeg judgments using the brat annotation tool stenetorp et al 2012 from two annotators 3 all annotations were done using the brat rapid annotation tool stenetorp et al 2012',\n",
       " 'we transformed the parse trees in ontonotes into syntactic dependencies using stanford corenlp manning et al 2014 we use the stanford corenlp caseless tagger for partofspeech tagging manning et al 2014',\n",
       " 'gradient clipping heuristic to prevent the exploding gradient problem graves 2013 for the exploding gradient problem numerical stability can be achieved by clipping the gradients graves 2013',\n",
       " 'we obtained newspeg judgments using the brat annotation tool stenetorp et al 2012 from two annotators 3 the annotations were made using the brat rapid annotation tool stenetorp et al 2012',\n",
       " 'we then describe in more detail a modern chinese corpus the penn chinese treebank xue et al 2005 for chinese we use the penn chinese treebank version 51 ctb xue et al 2005',\n",
       " 'in order to assess statistical significance of the obtained results we use the paired bootstrap resampling method koehn 2004 which estimates the probability pvalue that a measured difference for statistical significance testing we use a paired bootstrap resampling method proposed in koehn 2004',\n",
       " 'statistical significance in bleu score difference was measured by using paired bootstrap resampling koehn 2004 significance was tested using a paired bootstrap koehn 2004 with 1000 samples p002',\n",
       " 'the article system builds on the elements of the system described in rozovskaya and roth 2010c the preposition classifier uses a combined system building on work described in  and rozovskaya and roth 2010b',\n",
       " 'for calculating the required frequencies we use the web1t corpus 6 brants and franz 2006 as a practical approximation we use bigram counts from the web 1t corpus brants and franz 2006',\n",
       " 'phrase pairs were extracted from symmetrized word alignments and distortions generated by giza och and ney 2003 using the combination of heuristics growdiagfinaland and msdbidirectionalword alignments were created using giza och and ney 2003',\n",
       " 'we ran all of our experiments in weka hall et al 2009 using logistic regressionweka hall et al 2009 which contains the implementation of all three algorithms was used in our study',\n",
       " 'both corpora were extracted from the open parallel corpus opus tiedemann 2012 the parallel data were taken from opus tiedemann 2012  which provides sentencealigned corpora with annotation',\n",
       " 'we ran all of our experiments in weka hall et al 2009 using logistic regressionwe conducted experiments using multinomial naive bayes classifier implemented in the weka toolkit hall et al 2009',\n",
       " 'the spanishenglish s2e training corpus was drawn from the europarl collection koehn 2005 the experiments focus on translation from german to english using the europarl data koehn 2005',\n",
       " 'type system extends the type system that is built into the uima 6 framework ferrucci and lally 2004 this architecture is very similar to the framework of uima ferrucci and lally 2004',\n",
       " 'for other languages we use the corpora made available for the conllx shared task buchholz and marsi 2006 we use the conllx buchholz and marsi 2006 distribution data from seven different languages arabic bulgarian dutch portuguese slovene spanish and swedish',\n",
       " 'table 5 compares our reordering model with a reimplementation of the reordering model proposed in tromble and eisner 2009 we note that our model outperforms the model proposed in tromble and eisner 2009 in all cases',\n",
       " 'table 5 compares our reordering model with a reimplementation of the reordering model proposed in tromble and eisner 2009 we note that our model outperforms the model proposed in tromble and eisner 2009 in all cases',\n",
       " 'these results contradict those given in zelenko et al 2003  where the sparse kernel achieves 23 better f1 performance than the contiguous kernelzelenko et al 2003 have shown the contiguous kernel to be computable in omn and the sparse kernel in omn 3  where m and n are the number of children in trees t 1 and t 2 respectively',\n",
       " 'for example ontonotes hovy et al 2006  a largescale annotation project chose this optionto this end a recent largescale annotation effort called the ontonotes project hovy et al 2006 was started',\n",
       " 'as for ej translation we use the stanford parser de marneffe et al 2006 to obtain english abstraction treeswe use stanford parser de marneffe et al 2006 to obtain parse trees and dependency relations',\n",
       " 'for both english and german we used the partof speech tagger treetagger schmid 1994 to obtain postagsonly for german data did we used the treetagger schmid 1994 tokenizer',\n",
       " 'we train our model on a subset of the wackypedia en 6 corpus baroni et al 2009 we built a knowledge base v 2 r 1 using the frwac corpus baroni et al 2009',\n",
       " 'the questions are translated using a phrasebased system built using moses koehn et al 2007 the mo setthe first two baselines are standard systems using pbmt or hiero trained using moses koehn et al 2007',\n",
       " 'the questions are translated using a phrasebased system built using moses koehn et al 2007 the mo setthe decoder is built on top of an opensource phrasebased smt decoder moses koehn et al 2007',\n",
       " 'we measure statistical significance using 95 confidence intervals computed with paired bootstrap resampling koehn 2004 significance was tested using a paired bootstrap koehn 2004 with 1000 samples p002',\n",
       " 'as for ej translation we use the stanford parser de marneffe et al 2006 to obtain english abstraction treeswe use stanford parser de marneffe et al 2006 to obtain parse trees and dependency relations',\n",
       " 'words were downcased and lemmatized using the wordnet lemmatizer in the nltk 2 toolkit bird et al 2009 for reuters we segmented and tokenized the data using nltk bird et al 2009',\n",
       " 'english sentences are parsed into dependency structures by stanford parser marneffe et al 2006 the grammatical relations are all the collapsed dependencies produced by the stanford dependency parser marneffe et al 2006',\n",
       " 'both of our systems were based on the moses decoder koehn et al 2007 the decoder is built on top of an opensource phrasebased smt decoder moses koehn et al 2007',\n",
       " 'we also used anew bradley and lang 1999 for bootstrapping the affective lexicon expansion processfinally the anew lexicon bradley and lang 1999 is used for selecting the initial set of seed words of 1',\n",
       " 'translation at the discomt 2015 workshop hardmeier et al 2015 the nlp group of the idiap research institute participated in both subtasks of the discomt 2015 shared task pronounfocused translation and pronoun prediction hardmeier et al 2015',\n",
       " 'establishing and maintaining common ground is a complicated process even for human interlocutors clark 1996 an example of such a pragmatic factor is common ground clark 1996',\n",
       " 'sentiwordnet score senti we used the senti wordnet baccianella et al 2010 lexical resource to assign scores for each word based on three sentiments ie positive negative and objective respecwe then have assigned a sentiment score using the sentiwordnet baccianella et al 2010 lexical resource to each word in the set of retrieved snippets',\n",
       " 'europarl 2 koehn 2005  it is a corpus of parallel texts in 11 languages from the proceedings of the european parliament we used a subset of the data provided for the second workshop on statistical machine translation 2  which consists mainly of texts from the europarl corpus koehn 2005',\n",
       " 'we also carried out a chunkreordering pbsmt experiment where the chunks are reordered based on the final alignments obtained by 1pass experiment of holmqvist et al 2012 holmqvist et al 2012 presented a method where source text is reordered to replicate the target word order based on word alignment',\n",
       " 'wsi is generally considered as an unsupervised clustering task under the distributional hypothesis harris 1954 that the word meaning is reflected by the set of contexts in which it appearsdistributional models of meaning follow the distributional hypothesis harris 1954  which states that two words that occur in similar contexts have similar meanings',\n",
       " 'the data used for the experiments described in this paper comes predominantly from bible translations  wikipedia and the europarl corpus of european parliamentary proceedings koehn 2005  europarl 2 koehn 2005  it is a corpus of parallel texts in 11 languages from the proceedings of the european parliament',\n",
       " 'for the language model we used the kenlm toolkit heafield 2011 to create a 5gram language model on the target side of the europarl corpus v7 with approximately 54m tokens with kneserney smoolanguage model for all 3scfg systems we use a 4gram kneserney smoothed language model trained using the kenlm toolkit heafield 2011',\n",
       " 'first used by blitzer et al 2007  the mds dataset contains 4 different types of product reviews taken from amazoncom including books dvds electronics and kitchen appliances with 1000 positive to evaluate da for sentiment classification we use the amazon product reviews collected by blitzer et al 2007  for four different product categories books b dvds d electronic items e and',\n",
       " 'the most famous example would probably be the europarl corpus koehn 2005 for this purpose we use the europarl corpus koehn 2005',\n",
       " 'for language modeling we trained a separate 5gram kneserney smoothed lm model on the target ie english side of the training bitext using kenlm heafield 2011 in order to evaluate the fluency of each system we train 5gram language models for each language using kenlm heafield 2011',\n",
       " 'they used the webbased annotation tool brat stenetorp et al 2012 for the annotation the annotations were made using the brat rapid annotation tool stenetorp et al 2012',\n",
       " 'the classification was conducted using different scikitlearn algorithms pedregosa et al 2011 the svm models were trained using the scikitlearn toolkit 4 pedregosa et al 2011',\n",
       " 'the europarl corpus koehn 2005 is built from the proceedings of the european parliamentwe used the english side of the europarl corpus koehn 2005',\n",
       " 'for language modeling we trained a separate 5gram kneserney smoothed lm model on the target ie english side of the training bitext using kenlm heafield 2011 the language model is a 5gram kenlm heafield 2011 model trained using lmplz with modified kneserney smoothing and no pruning',\n",
       " 'the spanishenglish s2e training corpus was drawn from the europarl collection koehn 2005 we used the english side of the europarl corpus koehn 2005',\n",
       " 'we use the stanford corenlp caseless tagger for partofspeech tagging manning et al 2014 we also lemmatized all words using stanford corenlp manning et al 2014',\n",
       " 'all of our models are trained using nematus sennrich et al 2017 we trained our basic neural machine translation systems labeled base in table 3 with nematus sennrich et al 2017',\n",
       " 'we use giza och and ney 2003 with its default parameters to produce phrase alignmentswe used the giza software och and ney 2003 to do the word alignments',\n",
       " 'we are working with standard tools as dissect dinu et al 2013 the matrix is weighted with ppmi as implemented in dissect dinu et al 2013',\n",
       " 'they are based on distributional hypothesis which works under the assumption that similar words occur in similar contexts harris 1954 corpusbased vsms follow the standard distributional hypothesis which states that words appearing in the same contexts tend to have similar meaning harris 1954',\n",
       " 'its segmentation model is a classbased hidden markov model hmm model zhang et al 2003 for chinese a segmentation model zhang et al 2003 is used for detecting word boundaries',\n",
       " 'the edit distance kernel was trained with libsvm chang and lin 2011 as our learner we use libsvm with a linear kernel chang and lin 2011',\n",
       " 'the corpora are first tokenized and lowercased using the moses scripts then lemmatized and tagged by partofspeech pos using the treetagger schmid 1994 the corpus was converted from xml to raw text various string normalization operations were then applied and the corpus was lemmatized using treetagger schmid 1994',\n",
       " 'for example ontonotes hovy et al 2006  a largescale annotation project chose this optionfor example the ontonotes hovy et al 2006 project opted for this approach',\n",
       " 'we use the opensource moses toolkit koehn et al 2007 to build a standard phrasebased smt system which extracts up to 8 words phrases in the moses phrase tablethe moses toolkit koehn et al 2007 is used to train a phrase based smt system with the parallel data previously introduced',\n",
       " 'in our experiments we use the liblinear package fan et al 2008 to solve the primal problem with l 2 regularization and l 2 lossspecifically we use the liblinear svm package fan et al 2008 as it is wellsuited to text classification tasks with large numbers of features and texts',\n",
       " 'the spanishenglish s2e training corpus was drawn from the europarl collection koehn 2005 for this purpose we use the europarl corpus koehn 2005',\n",
       " 'the icsi meeting corpus janin et al 2003 is a corpus of text transcripts of research meetingsthe icsi meeting corpus the icsi meeting corpus janin et al 2003 is 75 transcribed meetings',\n",
       " 'we use rouge lin 2004 for evaluating the content of summarieswe use rouge metric lin 2004 to evaluate generated timelines against reference summaries',\n",
       " 'the measure selected is the normalised pearson correlation agirre et al 2012 the task is part of the semantic evaluation 2012 workshop agirre et al 2012',\n",
       " 'the main corpora we use are europarl koehn 2005 and the canadian hansardfor this purpose we use the europarl corpus koehn 2005',\n",
       " 'we exploit this monolingual data for training as described in sennrich et al 2016a we also used automatically backtranslated indomain monolingual data sennrich et al 2016a',\n",
       " 'we use the stanford corenlp caseless tagger for partofspeech tagging manning et al 2014 we also lemmatized all words using stanford corenlp manning et al 2014',\n",
       " 'this is a corpusbased metric relying on the distributional hypothesis of meaning suggesting that similarity of context implies similarity of meaning z harris 1954 corpusbased meaning representations rely on the distributional hypothesis which assumes that words occurring in a similar set of contexts are also similar in meaning harris 1954',\n",
       " 'similarly turian et al 2010 evaluated three different word representations on ner and chunking and concluded that unsupervised word representations improved ner and chunkingturian et al 2010  evaluate different techniques for inducing word representations and detail significant improvements for supervised ner and chunking systems when also incorporating word embedding',\n",
       " '331 reference system we compare a number of analogical devices to the stateoftheart statistical translation engine moses koehn et al 2007 we then use the phrase extraction utility in the moses statistical machine translation system koehn et al 2007 to extract a phrase table which operates over characters',\n",
       " 'for training svm classifiers we used libsvm package chang and lin 2001 we used libsvm to implement our own svm for regression chang and lin 2001',\n",
       " 'the word alignment was trained using giza och and ney 2003 with the configuration growdiagfinalandthe word alignment was obtained by running giza och and ney 2003',\n",
       " 'the corpus is first wordaligned using a word alignment heuristic och and ney 2003 the word alignment was obtained by running giza och and ney 2003',\n",
       " 'we perform bootstrap resampling with bounds estimation as described in koehn 2004 we used bootstrap resampling for testing statistical significance koehn 2004',\n",
       " 'we perform bootstrap resampling with bounds estimation as described by koehn 2004 we used bootstrap resampling for testing statistical significance koehn 2004',\n",
       " 'markov logic networks mln richardson and domingos 2006 is adopted for learning and predicationmarkov logic networks mln richardson and domingos 2006 are one of the statistical relational learning frameworks',\n",
       " 'with the training script of the moses toolkit koehn et al 2007 we used the moses toolkit koehn et al 2007 with its default settings',\n",
       " 'we train for 15 epochs using minibatch stochastic gradient descent the adadelta update rule zeiler 2012  and early stoppingwe initialize parameters uniformly using the range 005 005 for layer parameters and 001 001 for embeddings and train the model using stochastic gradient descent sgd with learning rates',\n",
       " 'this confirms the finding of liu et al 2012 that language model and lexicalized reordering models only have modest effects on translation retrievalmore recently liu et al 2012 have proposed a new translation retrieval architecture that depends only on monolingual corpora',\n",
       " 'for this purpose we use the europarl corpus koehn 2005 we evaluate our method by means of the europarl corpus koehn 2005',\n",
       " 'we exploit this monolingual data for training as described in sennrich et al 2016a we also used automatically backtranslated indomain monolingual data sennrich et al 2016a',\n",
       " 'our decoder is a stack decoder similar to koehn et al 2003 phrase extraction was performed following koehn et al 2003',\n",
       " 'for the english spanish and frenchenglish systems we used parallel training data from the europarl and news commentary corpora as well as the ted corpus cettolo et al 2012 in our lowresource condition we trained an smt system using only training data from the ted corpus cettolo et al 2012',\n",
       " 'the word alignment was trained using giza och and ney 2003 with the configuration growdiagfinalandword alignment was done with giza och and ney 2003 for both systems',\n",
       " 'rhetorical structure theory rst mann and thompson 1988 represents the coherence structure of a text by a labeled tree called discourse tree dt as shown in figure 1 the closest area to our work consists of investigations of discourse relations in the context of rhetorical structure theory mann and thompson 1988',\n",
       " 'the dataset used for the experiments reported in this paper has been prepared by reyes et al 2013 the other groups have been used already for example by carvalho et al 2009 or reyes et al 2013 yet our implementation is different in most of the cases',\n",
       " 'a good data source for this is the europarl corpus koehn 2005 for this purpose we use the europarl corpus koehn 2005',\n",
       " 'the most famous example would probably be the europarl corpus koehn 2005 we used the english side of the europarl corpus koehn 2005',\n",
       " 'previously socher et al 2011  used a recursive autoencoder to similarly obtain a vector representation of each sentence again combining other lexical similarity features to improve the resultsfor the msrp task socher et al 2011 used a recursive neural network to model each sentence  recursively computing the representation for the sentence from the representations of its constituents',\n",
       " 'the first source is the conll 2003 shared task date tjong kim sang and de meulder 2003 and the second source is the 2004 nist automatic content extraction weischedel 2004 aidayago is derived from the conll2003 shared task tjong kim sang and de meulder 2003',\n",
       " 'we used the moses toolkit koehn et al 2007 to build a phrase based machine translation system with a traditional 5gram lm trained on the target side of our bitextfor this purpose we use the moses toolkit to build a phrasebased statistical mt system koehn et al 2007  with training data from the translation task of the wmt 2013 workshop bojar et al 2',\n",
       " 'we therefore propose an alternative method based on correlation matrices computed from the bleu performance measure papineni et al 2001 we measure translation quality via the bleu score papineni et al 2001',\n",
       " 'we used the implementation of the scikitlearn 2 module pedregosa et al 2011 we used the linear svm implementation with default parameters and random forest with 50 trees both available at scikitlearn pedregosa et al 2011',\n",
       " 'finally the conll2003 shared task tjong kim sang and de meulder 2003 is an figure 1  search results for the queries michael jackson and michael jackson footballeraidayago is derived from the conll2003 shared task tjong kim sang and de meulder 2003',\n",
       " 'further we sentencesplit tokenized and lemmatized the text in wikipedia and gigaword using stanford corenlp toolkit 360 manning et al 2014 we also lemmatized all words using stanford corenlp manning et al 2014',\n",
       " 'the corpora are first tokenized and lowercased using the moses scripts then lemmatized and tagged by partofspeech pos using the treetagger schmid 1994 all novels were lemmatized and postagged using treetagger schmid 1994',\n",
       " 'we use adadelta zeiler 2012 to update the parameters during trainingwe also use adadelta zeiler 2012 to optimize the learning of  which is a effective method to train the neural networks',\n",
       " 'the first source is the conll 2003 shared task date tjong kim sang and de meulder 2003 and the second source is the 2004 nist automatic content extraction weischedel 2004 aidayago is derived from the conll2003 shared task tjong kim sang and de meulder 2003',\n",
       " 'we used a 2009 snapshot of wikipedia 2 which was pos tagged and lemmatized using the treetagger schmid 1994 all novels were lemmatized and postagged using treetagger schmid 1994',\n",
       " 'recently hovy et al 2013 utilized word embeddings by collobert et al 2011 for capturing coherence and contextual features for supervised metaphor detectionhovy et al 2013 applied tree kernels to metaphor detection',\n",
       " 'finally the conll2003 shared task tjong kim sang and de meulder 2003 is an figure 1  search results for the queries michael jackson and michael jackson footballeraidayago is derived from the conll2003 shared task tjong kim sang and de meulder 2003',\n",
       " 'we run our experiments on europarl koehn 2005  a multilingual parallel corpus extracted from the proceedings of the european parliamentwe used the english side of the europarl corpus koehn 2005',\n",
       " 'we applied bootstrap resampling koehn 2004 to measure statistical significance  p  005 of our models compared to a baselinewe used bootstrap resampling for testing statistical significance koehn 2004',\n",
       " 'we used the moses toolkit koehn et al 2007 with its default settingsthe moses toolkit koehn et al 2007 is used to train a phrase based smt system with the parallel data previously introduced',\n",
       " 'the statistical significance test is also carried out with paired bootstrap resampling method with p  0001 intervals koehn 2004 we used bootstrap resampling for testing statistical significance koehn 2004',\n",
       " 'this model is motivated by vector space model salton et al 1975 one of the bestknown methods of representing a document for similarity computation between documents is the vector space model salton et al 1975',\n",
       " 'the data was segmented into basenp parts and nonbasenp parts in a similar fashion as the data used by ramshaw and marcus 1995 we have used the basenp data presented in ramshaw and marcus 1995 2',\n",
       " 'the njuparser is based on the stateofthe art mat parser mcdonald 2006 the second algorithm denoted glotr is the chuliu edmonds algorithm for maximal spanning tree implemented in the mstparser mcdonald 2006',\n",
       " 'one is a 3gram language model built using kenlm heafield 2011 and trained over a modified version of the annotated corpus in which every it is concatenated with its type eg it eventtraining and querying of a modified kneserney smoothed 5 gram language model are done on the english side of the training data using kenlm heafield 2011 7',\n",
       " 'the word alignment was obtained by running giza och and ney 2003 baseline word alignments were obtained by running giza in both directions and symmetrizing using the growdiagfinaland heuristic och and ney 2003',\n",
       " 'translations for english words in the lexical sample are extracted from a semiautomatic word alignment of sentences from the europarl parallel corpus koehn 2005 in order to improve the robustness of the word alignments the documents were concatenated into a single file together with englishfrench parallel data from the europarl corpus koehn 2005',\n",
       " 'all the language models lm used in our experiments are 5grams modified kneserney smoothed lms trained using kenlm heafield et al 2013 the language models are estimated using the kenlm toolkit heafield et al 2013 with modified kneserney smoothing',\n",
       " 'in all experiments we use the svm classifier with sequential minimal optimization smo implementation available in the weka package hall et al 2009 for all experiments we use a decisiontree classifier as implemented in weka hall et al 2009 toolkit',\n",
       " 'we use the stanford parser to generate a dg for each sentence de marneffe et al 2006 we used the lexicalized dependency parser in the stanford statistical natural language parser ver203 de marneffe et al 2006 to obtain parses for the data sets',\n",
       " 'preprocessing we tokenized the english side of all bitexts for language modeling using the standard tokenizer of the moses toolkit koehn et al 2007 we built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " 'we use the moses software package 5 to train a pbmt model koehn et al 2007 we use moses 7 koehn et al 2007 to implement this model by setting the source and target sides to be kana sequences',\n",
       " 'indomain smt we used the parallel corpus section 3 to train an enpt phrasebased smt system using the moses toolkit koehn et al 2007 for training the translation model and for decoding we used the moses toolkit koehn et al 2007',\n",
       " 'indomain smt we used the parallel corpus section 3 to train an enpt phrasebased smt system using the moses toolkit koehn et al 2007 the baseline systems are built with the opensource phrasebased smt toolkit moses koehn et al 2007',\n",
       " 'the first competitive learning based system is described in soon et al 2001 to solve coreference we used a variation of the closest antecedent approach described in soon et al 2001',\n",
       " 'indomain smt we used the parallel corpus section 3 to train an enpt phrasebased smt system using the moses toolkit koehn et al 2007 we built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " 'we used predicted collapsed stanford dependencies de marneffe et al 2006 to extract arguments of the verbs and used only a subset of dependents of a verbwe extract syntactic dependencies using stanford parser de marneffe et al 2006 and use its collapsed dependency format',\n",
       " 'we select as a generalpurpose corpus europarl v7 koehn 2005  with 197m parallel sentenceswe run our experiments on europarl koehn 2005  a multilingual parallel corpus which is described in detail in section 31',\n",
       " 'we used the lexicalized dependency parser in the stanford statistical natural language parser ver203 de marneffe et al 2006 to obtain parses for the data setsin our experiments  we used the stanford parser de marneffe et al 2006 to create dependency parses',\n",
       " 'the result of the operation is equivalent to weighted composition of the lexicon and the weighted intersection of the rules as defined in allauzen et al 2007 the openfst library is used to perform all of the wfst operations allauzen et al 2007',\n",
       " 'the distributional hypothesis of meaning harris 1954 is a widelyused approach for estimating term similaritythis is a corpusbased metric relying on the distributional hypothesis of meaning suggesting that similarity of context implies similarity of meaning z harris 1954',\n",
       " 'preprocessing we tokenized the english side of all bitexts for language modeling using the standard tokenizer of the moses toolkit koehn et al 2007 we build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " 'one is from turian et al 2010  the dimension of word embedding is 50for brown the features are the prefix features extracted from word clusters in the same way as turian et al 2010',\n",
       " 'the thresholds were thoroughly selected depending on our analysis for the wordnet hierarchy and semantic similarity measures pedersen et al 2004 for a pair of words wordnet provides a series of measures of the semantic similarity pedersen et al 2004',\n",
       " 'indomain smt we used the parallel corpus section 3 to train an enpt phrasebased smt system using the moses toolkit koehn et al 2007 our baseline is a phrasebased mt system trained using the moses toolkit koehn et al 2007',\n",
       " 'the statistical significance test is also carried out with paired bootstrap resampling method with p  0001 intervals koehn 2004 we measure significance of results using bootstrap resampling at p  005 koehn 2004',\n",
       " 'we use the moses software package 5 to train a pbmt model koehn et al 2007 331 reference system we compare a number of analogical devices to the stateoftheart statistical translation engine moses koehn et al 2007',\n",
       " 'we used a 2009 snapshot of wikipedia 2 which was pos tagged and lemmatized using the treetagger schmid 1994 3 all english data are pos tagged and lemmatised using the treetagger schmid 1994',\n",
       " 'the text was preprocessed using wp2txt 6 to remove markup and then tokenized with the stanford tokenizer manning et al 2014 in addition the data was tokenized lemmatized and parsed using stanford corenlp manning et al 2014',\n",
       " 'we run our experiments on europarl koehn 2005  a multilingual parallel corpus which is described in detail in section 31europarl koehn 2005 is a multilingual parallel corpus extracted from the proceedings of the european parliament',\n",
       " 'we use scikitlearn pedregosa et al 2011  the machine learning library for python for implementing the different approacheslogistic regression implemented in python with the scikitlearn machine learning library pedregosa et al 2011  is used for all classification models',\n",
       " 'the way they were added is similar to incorporating the negation effect described by pang et al 2002 in this paper we use the subjectivity corpus by pang et al 2002',\n",
       " 'we used the lexicalized dependency parser in the stanford statistical natural language parser ver203 de marneffe et al 2006 to obtain parses for the data setsin our experiments  we used the stanford parser de marneffe et al 2006 to create dependency parses',\n",
       " 'we modified the implementation of the swell java package 4 of dhillon et al 2015 we also compare our word embeddings with the eigen word embeddings of dhillon et al 2015  without any prior knowl edge',\n",
       " 'the stanford dependency parser de marneffe et al 2006 is used for extracting features from the dependency parse treeswe used the lexicalized dependency parser in the stanford statistical natural language parser ver203 de marneffe et al 2006 to obtain parses for the data sets',\n",
       " 'the first work on this topic was done back in the eighties church 1988 the dutch version was tagged automatically using a tagger inspired on the english tagger described in church 1988',\n",
       " 'the openfst library is used to perform all of the wfst operations allauzen et al 2007 the decoder is implemented with weighted finite state transducers wfsts using standard operations available in the openfst libraries allauzen et al 2007',\n",
       " 'for our experiments we used translated movie subtitles from the opus corpus tiedemann 2009b for chineseenglish experiments we used the openoffice3 oo3 parallel corpus tiedemann 2009  which is oo3 computer office productivity software documentation',\n",
       " 'the moses15 result is obtained by applying the smt toolkit moses koehn et al 2007 over letter strings with 15character context windowsthe baseline systems are built with the opensource phrasebased smt toolkit moses koehn et al 2007',\n",
       " 'the openfst library is used to perform all of the wfst operations allauzen et al 2007 the decoder is implemented with weighted finite state transducers wfsts using standard operations available in the openfst libraries allauzen et al 2007',\n",
       " 'we select as a generalpurpose corpus europarl v7 koehn 2005  with 197m parallel sentenceswe run our experiments on europarl koehn 2005  a multilingual parallel corpus extracted from the proceedings of the european parliament',\n",
       " 'we also compare our word embeddings with the eigen word embeddings of dhillon et al 2015  without any prior knowl edgedhillon et al 2015 used cca to derive word embeddings through the following procedure',\n",
       " 'the relationship between language and sentiment is an active area of investigation pang and lee 2008 the rapid growth of usergenerated content much of which is sentimentladen has fueled an interest in sentiment analysis pang and lee 2008 liu 2010',\n",
       " 'support vector machines svm are one of the binary classifiers based on maximum margin strategy introduced by vapnik vapnik 1995 these classifiers are based on a discriminative model support vector machine svm 6 vapnik 1995',\n",
       " 'in barankov and tamchyna 2014 we experimented with targeted paraphrasing using the freely available smt system moses koehn et al 2007 we build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " 'to learn the parameters of the model we minimize the crossentropy loss as the training objective using the adam optimization algorithm kingma and ba 2014 for optimization we employed the adam 6 httpnlpstanfordedusoftwarecorenlpshtml algorithm kingma and ba 2014 to update parameters',\n",
       " 'for training the translation model and for decoding we used the moses toolkit koehn et al 2007 was used for word alignment and phrase translation probabilities were estimated from them by the moses system koehn et al 2007',\n",
       " 'the text was preprocessed using wp2txt 6 to remove markup and then tokenized with the stanford tokenizer manning et al 2014 in addition the data was tokenized lemmatized and parsed using stanford corenlp manning et al 2014',\n",
       " 'the task reported on here is to produce propbank kingsbury and palmer 2002 labels given the features provided for the conll2005 closed task carreras and m arquez 2005a standard for predicate argument annotation is provided in the propbank project kingsbury and palmer 2002',\n",
       " 'for both systems the used training data is from the 4th version of the europarl corpus koehn 2005 and the news commentary corpusthe source of bilingual data used in the experiments is the europarl collection koehn 2005',\n",
       " 'we use the stanford parser to generate a dg for each sentence de marneffe et al 2006 we used the lexicalized dependency parser in the stanford statistical natural language parser ver203 de marneffe et al 2006 to obtain parses for the data sets',\n",
       " 'we used predicted collapsed stanford dependencies de marneffe et al 2006 to extract arguments of the verbs and used only a subset of dependents of a verbwe extract syntactic dependencies using stanford parser de marneffe et al 2006 and use its collapsed dependency format',\n",
       " 'we tokenize english data and segment chinese data using the stanford corenlp toolkit manning et al 2014 2 further we sentencesplit tokenized and lemmatized the text in wikipedia and gigaword using stanford corenlp toolkit 360 manning et al 2014',\n",
       " 'the evaluation emphasis in multidocument summarization has been on evaluating content not readability  using manual  as well as automatic lin and hovy 2003 methodsrouge lin and hovy 2003 has been adopted as a standard evaluation metric in various summarization tasks',\n",
       " 'we conducted baseline experiments for phrase based machine translation using the moses toolkit koehn et al 2007 all experiments were carried out using the opensource smt toolkit moses koehn et al 2007  in its standard nonmonotonic configuration',\n",
       " 'the similarity function is here the smoothed partial tree kernel sptk proposed in croce et al 2011 we directly apply the smoothed partial treekernel sptk proposed in croce et al 2011  to estimate the similarity among a specific tree representation',\n",
       " 'we tokenize english data and segment chinese data using the stanford corenlp toolkit manning et al 2014 2 further we sentencesplit tokenized and lemmatized the text in wikipedia and gigaword using stanford corenlp toolkit 360 manning et al 2014',\n",
       " 'the stanford dependency parser de marneffe et al 2006 is used for extracting features from the dependency parse treeswe used the lexicalized dependency parser in the stanford statistical natural language parser ver203 de marneffe et al 2006 to obtain parses for the data sets',\n",
       " 'to demonstrate the effect of the proposed method we use the stateoftheart phrasebased system and hierarchical phrasebased system implemented in moses koehn et al 2007 we build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " 'the baseline systems are built with the opensource phrasebased smt toolkit moses koehn et al 2007 the moses toolkit koehn et al 2007 is used to train a phrase based smt system with the parallel data previously introduced',\n",
       " '331 reference system we compare a number of analogical devices to the stateoftheart statistical translation engine moses koehn et al 2007 we build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " 'preprocessing we tokenized the english side of all bitexts for language modeling using the standard tokenizer of the moses toolkit koehn et al 2007 we tokenize and frequentcase the data with the standard scripts from the moses toolkit koehn et al 2007',\n",
       " 'finally it is also noticeable that the percentage of compounds detected in the training set is similar to the one reported by baroni et al 2002 and referenced to in section 2baroni et al 2002  report that 47 of the vocabulary types in the apa corpus 2 were compounds',\n",
       " 'as a learning algorithm we adopt a ranking svm joachims 2002  which is an instance of preference learningto train a mentionpair classifier we use the svm learning algorithm from the svm light package joachims 2002  converting all multivalued features into an equivalent set of binaryvalued featur',\n",
       " 'brain images are quite noisy so we used the methodology from mitchell et al 2008 to select the most stable brain image features for each of the 18 participantsfollowing the evaluation paradigm of mitchell et al 2008  linear models trained on corpusbased features are used to predict the pattern of eeg activity for unseen concepts',\n",
       " 'the training set is used to train the phrasebased translation model and language model for moses koehn et al 2007 this system is the moses decoder koehn et al 2007 with a translation and a language model trained with no additional resources other than the official data provided by the shared task organizers',\n",
       " 'results on englishfrench englishromanian and czech english alignment show that our model significantly outperforms fast align a popular loglinear reparameterization of ibm model 2 dyer et al 2013 our alignment model is based on a simple variant of ibm model 2 where the alignment distribution is only controlled by two parameters  and p 0 dyer et al 2013',\n",
       " 'the work presented in berger et al 1996 that is based on the a  concept however introduces word reordering restrictions in order to reduce the overall search spacethe original reordering constraint in berger et al 1996 is shown to be a special case of a more general restriction scheme in which the word reordering constraints are expressed in terms of simple',\n",
       " 'our second method is based on the recurrent neural network language model rnnlm approach to learning word embeddings of mikolov et al 2013a and mikolov et al 2013b  using the word2vec packagemikolov et al 2013a proposed a faster skipgram model word2vec 5 which tries to maximize classification of a word based on another word in the same sentence',\n",
       " 'the susanne corpus is a modified and condensed version of brown corpus francis and kucera 1979 in order to avoid the errors introduced by tagger the susanne corpus is used as the training and testhe corpus consists of a subset of the brown corpus 700000 words with more than 200000 senseannotated francis and kucera 1979  and it has been partofspeechtagged and sensetagged',\n",
       " 'the corpora are tokenised and truecased using scripts from the moses toolkit koehn et al 2007 all these features are inherited from moses koehn et al 2007',\n",
       " 'english annotations were all produced using the stanford core nlp toolkit manning et al 2014 we also lemmatized all words using stanford corenlp manning et al 2014',\n",
       " 'the parameters are estimated by gibbs sampling using the mallet implementation mccallum 2002 10 we used the pltm implementation in mallet mccallum 2002',\n",
       " 'the metaclassifier is a linear svm fan et al 2008 implemented with kelpliblinear fan et al 2008  a library for large svm linear classification is used for implementation',\n",
       " 'we use adadelta zeiler 2012 to update the parameters during trainingwe use a minibatch stochastic gradient descent sgd with the adadelta update rule zeiler 2012  apply random initialization within 001 001 for w f j  and initialize the remaining parameter',\n",
       " 'we train a support vector machine svm cortes and vapnik 1995 on the tweets provided for trainingspecifically we use support vector machine svm cortes and vapnik 1995 for this purpose',\n",
       " 'parameter tuning is carried out using z mert zaidan 2009 feature weights based on bleu are then tuned using z mert zaidan 2009',\n",
       " 'brin identifies the use of patterns in the discovery of relations on the web brin 1998 brin proposed the bootstrapping method for relation discovery brin 1998',\n",
       " 'we adopt the setting of socher et al 2012 neural networks are first used in this task in socher et al 2012',\n",
       " 'the publicly available tool giza was used to align the letters och and ney 2003 we used the giza software och and ney 2003 to do the word alignments',\n",
       " 'crfsuite implementation okazaki 2007 we trained a crf tagger using crfsuite 1 okazaki 2007',\n",
       " 'the hmm classifier used in our experiments follows the algorithm described in bikel et al 1999 the hmm classifier used in the experiments in section 4 follows the system description in bikel et al 1999  and it performs sequence classification by assigning each word either one of the named',\n",
       " 'we computed 4gram lms with modified kneserney smoothing kneser and ney 1995 the language model is a 5gram with interpolation and kneserney smoothing kneser and ney 1995',\n",
       " 'socher et al 2011 come closest to our target problemwe plan to adapt ideas from socher et al 2011 for this task',\n",
       " 'we use ridge regression rr with l2norm regularization and support vector regression svr with an rbf kernel from scikitlearn pedregosa et al 2011 we use the support vector machines implementation in the scikitlearn toolkit pedregosa et al 2011 to perform regression svr on each feature set with either rbf kernels and parameters optimise',\n",
       " 'as for the former hereafter it is referred to synpth we continue to use a dependency version of the pruning algorithm of xue and palmer 2004 for argument a dependency version of the pruning algorithm in xue and palmer 2004 is used to find in an iterative way the current syntactic head and its siblings in a parse tree in a constitue',\n",
       " '15 the significance tests were performed using the bootstrap resampling method koehn 2004 we used bootstrap resampling for testing statistical significance koehn 2004',\n",
       " 'in this work we focus on learning with support vector machines svms vapnik 1995 specifically we use support vector machine svm cortes and vapnik 1995 for this purpose',\n",
       " 'we used treetagger schmid 1994 to obtain a lemmatag pair for each russian wordwe used the stuttgart treetagger schmid 1994 to lemmatise constituent heads',\n",
       " 'to account for this constraint  include information from latent semantic analysis deerwester et al 1990 earlier models made use of latent semantic analysis lsa deerwester et al 1990',\n",
       " 'the system was trained on the english and danish part of the europarl corpus version 3 koehn 2005 in principle classifiers trained on pdtb data can be applied directly to label connectives over the english side of the europarl corpus koehn 2005 used for training and testing smt',\n",
       " 'we use linear svms from liblinear and svms with rbf kernel from libsvm chang and lin 2011 the edit distance kernel was trained with libsvm chang and lin 2011',\n",
       " 'in current phrasebased statistical machine translation systems such as moses 1 koehn et al 2007  the translation model is defined in terms of phrase pairs biphrases extracted from a bilingualit is a standard phrasebased machine translation model koehn et al 2007',\n",
       " 'we train a support vector machine svm cortes and vapnik 1995 on the tweets provided for trainingspecifically we use support vector machine svm cortes and vapnik 1995 for this purpose',\n",
       " 'the word alignment was obtained by running giza och and ney 2003 the word alignment is created by giza och and ney 2003  the intersection symmetrization is used',\n",
       " 'for training svm classifiers we used libsvm package chang and lin 2001 the svm implementation used was libsvm chang and lin 2001  and default parameters were used radial basis function kernel cost parameter of 1 and a gamma value of the inverse of the number of d',\n",
       " 'we use the standard alignment tool giza och and ney 2003 to word align the parallel datawe used giza och and ney 2003 to align the words in the corpus',\n",
       " 'we use the standard alignment tool giza och and ney 2003 to word align the parallel datawe used the giza software och and ney 2003 to do the word alignments',\n",
       " 'in the penn discourse treebank 20 prasad et al 2008  the sense is called chosen alternativein contrast the set of discourse markers in our work is fixed for english  we use 61 markers annotated in the penn discourse treebank 20 prasad et al 2008  for german we use 155 oneword tr',\n",
       " 'weka hall et al 2009 was used to apply learning methods to extracted featuresthe svm implementation of weka hall et al 2009 was used to build the clte model',\n",
       " 'we used the moses toolkit koehn et al 2007 with its default settingsthis was done with a specific tool provided with the moses toolkit koehn et al 2007',\n",
       " 'we use the standard alignment tool giza och and ney 2003 to word align the parallel datawe used the giza software och and ney 2003 to do the word alignments',\n",
       " 'we used the giza software och and ney 2003 to do the word alignmentswe performed word alignment using giza och and ney 2003 with the default growdiagfinaland alignment symmetrization method',\n",
       " 'the language model used is the 5gram corpus from google books michel et al 2011 we used google books ngrams michel et al 2011 as the general corpus',\n",
       " 'english annotations were all produced using the stanford core nlp toolkit manning et al 2014 we also lemmatized all words using stanford corenlp manning et al 2014',\n",
       " 'for example turian et al 2010 compared brown clusters cw embeddings and hlbl embeddings in ner and chunking taskswe evaluate cw word embeddings with 25 50 and 100 dimensions as well as hlbl word embeddings with 50 and 100 dimensions that are introduced in turian et al 2010 and can be downloaded here 4',\n",
       " 'distributional representations encode an expression by its environment assuming the context dependent nature of meaning according to which one shall know a word by the company it keeps firth 1957 this is often called distributional semantics where a word or a name in our case is known by the company it keeps firth 1957',\n",
       " 'li et al 2013  use crowdsourcing to build plot graphsour neural network is similar to that of li et al 2013',\n",
       " 'we use the standard alignment tool giza och and ney 2003 to word align the parallel datawe used giza och and ney 2003 to align the words in the corpus',\n",
       " 'the formalism that is used to represent the semantics in the delphin grammars is minimal recursion semantics mrs copestake et al 2005 meaning representation and composition in the erg builds on the formal framework of minimal recursion semantics mrs copestake et al 2005',\n",
       " 'for french hungarian polish and swedish we used europarl corpus 1 koehn 2005 for this purpose we use the europarl corpus koehn 2005',\n",
       " 'the english text was tokenized using the word tokenize routine from nltk bird et al 2009 the stopwords are taken from the stopword list provided by the nltk toolkit bird et al 2009',\n",
       " 'we use the moses decoder koehn et al 2007 as a representative smt decoder inside the system described belowwe build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " 'specifically the sentence compression dataset clarke and lapata 2008 referred as cl08 is used for subtree deletion model training arc one idea is to apply it to the language model based compression method clarke and lapata 2008',\n",
       " 'use the stanford parser de marneffe et al 2006 to extract a set of dependencies from each commentwe extract syntactic dependencies using stanford parser de marneffe et al 2006 and use its collapsed dependency format',\n",
       " 'we conducted baseline experiments for phrasebased machine translation using the moses toolkit koehn et al 2007 the corpora are tokenised and truecased using scripts from the moses toolkit koehn et al 2007',\n",
       " 'in practice the decoder has to employ beam search to make it tractable koehn 2004 to make the exponential algorithm practical beam search is the standard approximate search method koehn 2004',\n",
       " 'the source of bilingual data used in the experiments is the europarl collection koehn 2005 the europarl data set consists of 707 sentences of the german part of the europarl corpus koehn 2005',\n",
       " 'the tectogrammatical annotation layer is based on the functional generative description theory sgall et al 1986 our approach is based on the czech linguistic tradition represented mainly in sgall et al 1986',\n",
       " 'in this paper we use the subjectivity corpus by pang et al 2002 these classifiers have been used in related work by pang et al 2002',\n",
       " 'we conducted baseline experiments for phrasebased machine translation using the moses toolkit koehn et al 2007 we build a baseline error correction system using the moses smt system koehn et al 2007',\n",
       " 'all our systems are contrasted with a standard phrasebased system built with moses koehn et al 2007 we used the moses toolkit koehn et al 2007 to train standard phrasebased systems with default configurations',\n",
       " 'a common approach to computing similarity is to count the number of common words lesk 1986 this sense similarity measure is inspired by the definition of the lesk algorithm lesk 1986',\n",
       " 'we conducted baseline experiments for phrasebased machine translation using the moses toolkit koehn et al 2007 the baseline systems are built with the opensource phrasebased smt toolkit moses koehn et al 2007',\n",
       " 'the stanford dependency parser de marneffe et al 2006 is used for extracting features from the dependency parse treesin our experiments  we used the stanford parser de marneffe et al 2006 to create dependency parses',\n",
       " 'we tokenize and frequentcase the data with the standard scripts from the moses toolkit koehn et al 2007 we implement mert and mira 1  and directly use mira 2 from moses koehn et al 2007',\n",
       " 'we conducted baseline experiments for phrasebased machine translation using the moses toolkit koehn et al 2007 we tokenize and frequentcase the data with the standard scripts from the moses toolkit koehn et al 2007',\n",
       " 'we conducted baseline experiments for phrasebased machine translation using the moses toolkit koehn et al 2007 our baseline is a phrasebased mt system trained using the moses toolkit koehn et al 2007',\n",
       " 'for training the translation model and for decoding we used the moses toolkit koehn et al 2007 the corpora are tokenised and truecased using scripts from the moses toolkit koehn et al 2007',\n",
       " 'we tokenize and frequentcase the data with the standard scripts from the moses toolkit koehn et al 2007 we built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " 'we conducted baseline experiments for phrasebased machine translation using the moses toolkit koehn et al 2007 we build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " 'we use the moses software package 5 to train a pbmt model koehn et al 2007 we build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " 'the corpora are tokenised and truecased using scripts from the moses toolkit koehn et al 2007 we built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " 'in contrast mcclosky et al 2006 focus on large seeds and exploit a rerankingparsermcclosky et al 2006  use selftraining in combination with a pcfg parser and reranking',\n",
       " 'we use logistic regression with l2 regularization implemented using the scikitlearn toolkit pedregosa et al 2011 bullet logistic regressionlr we use logistic regression with l2 loss penalty and c1 pedregosa et al 2011',\n",
       " 'since the phrase table contains lemmas the wikipedia corpus was lemmatised using the treetagger schmid 1994 3 all english data are pos tagged and lemmatised using the treetagger schmid 1994',\n",
       " 'we use logistic regression with l2 regularization implemented using the scikitlearn toolkit pedregosa et al 2011 we use the bernoulli naive bayes classifier in scikit with the default settings pedregosa et al 2011',\n",
       " 'the corpora are tokenised and truecased using scripts from the moses toolkit koehn et al 2007 both systems are based on the moses smt toolkit koehn et al 2007 and constructed as follows',\n",
       " 'for training the translation model and for decoding we used the moses toolkit koehn et al 2007 we tokenize and frequentcase the data with the standard scripts from the moses toolkit koehn et al 2007',\n",
       " 'we use the moses software package 5 to train a pbmt model koehn et al 2007 we used the moses toolkit koehn et al 2007 to train standard phrasebased systems with default configurations',\n",
       " 'we use the moses software package 5 to train a pbmt model koehn et al 2007 we built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " 'europarl koehn 2005 is a multilingual parallel corpus extracted from the proceedings of the european parliamentwe extract our paraphrase grammar from the frenchenglish portion of the europarl corpus version 5 koehn 2005',\n",
       " 'for all syntactic parsers we used the basic stanford dependency representation de marneffe et al 2006 in our experiments  we used the stanford parser de marneffe et al 2006 to create dependency parses',\n",
       " 'the stanford dependency parser de marneffe et al 2006 is used for extracting features from the dependency parse treesin our experiments  we used the stanford parser de marneffe et al 2006 to create dependency parses',\n",
       " 'we built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007 all our systems are contrasted with a standard phrasebased system built with moses koehn et al 2007',\n",
       " 'we use the crossentropy loss function and minibatch adagrad duchi et al 2011 to optimize parameters we use adagrad duchi et al 2011  with the initial learning rate set to   05',\n",
       " 'we use the crossentropy loss function and minibatch adagrad duchi et al 2011 to optimize parameters we train the concept identification stage using infinite ramp loss 1 with adagrad duchi et al 2011',\n",
       " 'we use the adagrad method duchi et al 2011 to automatically update the learning rate for each parameterwe use the crossentropy loss function and minibatch adagrad duchi et al 2011 to optimize parameters',\n",
       " 'socher et al 2011  explored using recursive autoencoders raes and dynamic pooling for paraphrase detectionsocher et al 2011  use recursive autoencoders for sentiment analysis on the sentence level',\n",
       " '2 translation model moses is a stateoftheart toolkit for phrasebased smt systems koehn et al 2007 we build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " '2 translation model moses is a stateoftheart toolkit for phrasebased smt systems koehn et al 2007 we build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " '2 translation model moses is a stateoftheart toolkit for phrasebased smt systems koehn et al 2007 our baseline is a phrasebased mt system trained using the moses toolkit koehn et al 2007',\n",
       " 'for training the translation model and for decoding we used the moses toolkit koehn et al 2007 we built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " 'all our systems are contrasted with a standard phrasebased system built with moses koehn et al 2007 we build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " 'for all syntactic parsers we used the basic stanford dependency representation de marneffe et al 2006 in our experiments  we used the stanford parser de marneffe et al 2006 to create dependency parses',\n",
       " 'can be evaluated by maximising the pseudolikelihood on a training corpus malouf 2002 the parameters can be efficiently estimated from a treebank as shown by malouf 2002',\n",
       " 'the baseline systems are built with the opensource phrasebased smt toolkit moses koehn et al 2007 we build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " '2 translation model moses is a stateoftheart toolkit for phrasebased smt systems koehn et al 2007 all our systems are contrasted with a standard phrasebased system built with moses koehn et al 2007',\n",
       " 'in this approach we used the nersuite software based on the crfsuite implementation okazaki 2007 nersuite is a ner system that is built on top of the crfsuite okazaki 2007',\n",
       " '2 translation model moses is a stateoftheart toolkit for phrasebased smt systems koehn et al 2007 we built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " 'these efforts focused exclusively on the meronymy relation as used in wordnet miller et al 1990 experts can manually specify the attributes of entities as in the wordnet project miller et al 1990',\n",
       " 'special forms of relatedness are represented in the lexical entries of the wordnet lexical database miller et al 1990 experts can manually specify the attributes of entities as in the wordnet project miller et al 1990',\n",
       " 'we train our model on a subset of the wackypedia en 6 corpus baroni et al 2009 we used the publicly available wacky corpus baroni et al 2009',\n",
       " 'since the first shared task on recognising textual entailment rte dagan et al 2005 was organised in 2005 much research has been done on how one can detect entailment between natural language although there had been research on reasoning expressed in natural language the pascal recognising textual entailment rte challenge dagan et al 2005 spurred wide interest in the problem',\n",
       " 'we used kbest batch mira cherry and foster 2012 for tuningwe tune the systems using kbest batch mira cherry and foster 2012',\n",
       " 'we used the mallet toolkit mccallum 2002 for learning maximum entropy models with gaussian priors for all our experimentswe use the mallet package mccallum 2002 for experiments',\n",
       " 'we use the implementation provided by crfsuite 7 okazaki 2007 for both training and classification taskswe use crfsuite okazaki 2007 as an implementation of crf',\n",
       " 'glish source with target french by using giza och and ney 2003 word alignments were created using giza och and ney 2003',\n",
       " 'we used rougen lin 2004 for evaluation of summarieswe expect this restriction is more consistent with the rouge evaluation metric used for summarization lin 2004',\n",
       " '1 with 2 regularization using adagrad duchi et al 2011 we adopt online learning updating parameters using adagrad duchi et al 2011',\n",
       " 'we used weka hall et al 2009 for all our classification experimentsweka hall et al 2009 which contains the implementation of all three algorithms was used in our study',\n",
       " 'pos tagging was performed with treetagger schmid 1994 treetagger is a statistical decision treebased pos tagger schmid 1994',\n",
       " 'word alignments on the parallel corpus are performed using giza och and ney 2003 with the growdiagfinal refinementthe parallel corpus is wordaligned using giza och and ney 2003',\n",
       " 'we used the implementation of the scikitlearn 2 module pedregosa et al 2011 classification uses the scikitlearn python package pedregosa et al 2011',\n",
       " 'the lexical sample data was parsed using the clark and curran ccg parser clark and curran 2004 it builds on the cc ccg parser clark and curran 2004',\n",
       " 'for postagging we used the stanford postagger toutanova and manning 2000 next we replace all nouns with their pos tag we use the stanford pos tagger toutanova and manning 2000',\n",
       " 'we used the english side of the europarl corpus koehn 2005 all the data come from europarl koehn 2005',\n",
       " 'this data is part of the nucle corpus dahlmeier et al 2013 the training data released by the task organizers comes from the nucle corpus dahlmeier et al 2013  which contains essays written by learners of english as a foreign language and is corrected by',\n",
       " 'the training set is used to train the phrasebased translation model and language model for moses koehn et al 2007 conducted using the moses phrasebased decoder koehn et al 2007',\n",
       " 'we used the word alignment produced by giza och and ney 2000 out of an ibm model 2och is the hmm alignment model of och and ney 2000',\n",
       " 'for this purpose we use the europarl corpus koehn 2005 all the data come from europarl koehn 2005',\n",
       " 'word alignment is performed using giza och and ney 2003 glish source with target french by using giza och and ney 2003',\n",
       " 'word alignment was done with giza och and ney 2003 for both systemsword alignment is performed with giza och and ney 2003',\n",
       " 'the evaluation results were provided by the organizers using their evaluation script in python dahlmeier and ng 2012 the models were evaluated using m2 scorer dahlmeier and ng 2012',\n",
       " 'in future work we can therefore incorporate unsupervised methods of nsw classification and expansion along the lines of the automatic dictionary construction method presented by han et al 2012 han et al 2012 introduced a dictionary based method and an automatic normalisationdictionary construction method',\n",
       " 'glish source with target french by using giza och and ney 2003 word alignment is done using giza och and ney 2003',\n",
       " 'we conducted statistical significance tests for bleu between our best domainadapted system the baseline and the three thirdparty systems using paired bootstrap resampling koehn 2004 with 1000 we used bootstrap resampling for testing statistical significance koehn 2004',\n",
       " 'the word alignment was obtained by running giza och and ney 2003 word alignment is performed with giza och and ney 2003',\n",
       " 'word alignment was done with giza och and ney 2003 for both systemsword alignment is performed using giza och and ney 2003',\n",
       " 'for the theory of cho  chai 2000 to be complete we have proposed a new type of marker and the adjunct lp constraint in conjunction with their argument lp constraintin section 3 we present a solution to the alleged counterexample of cho  chai 2000 and explain some representative scrambled sentences in korean under our analysis',\n",
       " 'in order to compare our method to a well understood phrase baseline we present a method that tracts phrases by harvesting the viterbi path from an hmm alignment model vogel et al 1996 in our experiments we investigated both weak and a strong initialisations the former based on word alignments from ibm model 1 and the latter on alignments from an hmm model vogel et al 1996',\n",
       " 'we examine the quality of translations to english from chinese and arabic using humantargeted translation edit rates hter snover et al 2006  which roughly captures the minimal number of editsto calculate the number of changes we used a modified translation edit rate ter which measures the number of edits needed to transform one sentence into another snover et al 2006',\n",
       " 'integer linear programming ilp has recently been applied to inference in sequential conditional random fields roth and yih 2004  this has allowed the use of truly global constraints during infesecond to avoid the error propagation problem inherent in the pipeline approach we perform joint inference over the outputs of the aci and ri classifiers in an integer linear programming ilp frame',\n",
       " 'for that purpose we use the word analogy task proposed by mikolov et al 2013a  which measures the accuracy on answering questions like what is the word that is similar to small in the same sensmikolov et al 2013a proposed a faster skipgram model word2vec 5 which tries to maximize classification of a word based on another word in the same sentence',\n",
       " 'the corpus is first wordaligned using a word alignment heuristic och and ney 2003 a core component of every pbsmt system is the phrase table which contains bilingual phrase pairs extracted from a bilingual corpus after word alignment och and ney 2003',\n",
       " 'for ranking we use the svm rank ranker joachims 2006  which learns a sparse weight vector that minimizes the number of swapped pairs in the training setwe use the svm rank implementation joachims 2006 of ranking svm in this paper',\n",
       " 'parameters are updated using adagrad duchi et al 2011 with a learning rate of 01all models were trained using adagrad duchi et al 2011 with an initial base learning rate of 01 which we exponentially decayed with a decade of 15 million steps',\n",
       " 'the tagger we use is tnt brants 2000  a hidden markov trigram tagger which was trained on the spoken dutch corpus cgn internal release 6hcrc is tagged with tnt brants 2000  trained on the full ptb',\n",
       " 'sentiment score of the last post of the observation period we trained an l2 regularized logistic regression from liblinear fan et al 2008 using the data collected from the depression forumwe trained the classifiers using the liblinear implementation fan et al 2008 of logistic regres sion',\n",
       " 'for german english we also have a system based on moses koehn et al 2007 for reference we also show the mt performance of the phrase based stringtotree and treetostring systems which are based on the opensource gizamoses pipeline koehn et al 2007',\n",
       " 'we use the universal pos tagset upos of petrov et al 2012 for example petrov et al 2012  build supervised pos taggers for 22 languages using the tnt tagger brants 2000  with an average accuracy of 952',\n",
       " 'we also considered an ensemble of our approach and that of berant and liang 2014 we chose a threshold such that our approach predicts 50 of the time when sq a is above its value and the other 50 of the time we use the prediction of berant and liang 2014 instead',\n",
       " 'one way to solve this problem is to use a kernel function that is tailored for particular nlp applications such as the tree kernel collins and duffy 2001 for statistical parsingthe proof is similar to the proof for the tree kernel in collins and duffy 2001',\n",
       " 'we use adam kingma and ba 2015 for optimisation with initial learning rate of 0001we train with the adam optimizer kingma and ba 2015  a learning rate of 00001 batch size of 50 and dropout with probability 02 applied to the hidden layer',\n",
       " 'we build our pbsmt systems in a standard way using the moses system  kenlm for language modelling heafield 2011  and standard lexical reordering model we use kenlm 3 heafield 2011 for computing the target language model score',\n",
       " 'the starting point for our model is the skipgram with negative sampling sgns objective of mikolov et al 2013b our model is an extension of the contextual bag of words cbow model of mikolov et al 2013b  a method for learning vector representations of words based on their distributional contexts',\n",
       " 'we introduce a new anaphoricity detection model as the second neural model using a long short term memory lstm network hochreiter and schmidhuber 1997 we generated embeddings by training a characterlevel longshort term memory lstm network hochreiter and schmidhuber 1997  using it as an encoder on the turns at talk from our corpus',\n",
       " 'the realisation ranking component is an svm ranking model implemented with svmrank a support vector machinebased learning tool joachims 2006 the advantage of an svm rank model is that it is very fast and it has been shown to be accurate for a variety of ranking problems joachims 2006',\n",
       " 'wsi is generally considered as an unsupervised clustering task under the distributional hypothesis harris 1954 that the word meaning is reflected by the set of contexts in which it appearsaccording to the distributional hypothesis of meaning harris 1954  words that occur in similar contexts tend to be semantically similar',\n",
       " 'we run our experiments on europarl koehn 2005  a multilingual parallel corpus which is described in detail in section 31our experiments were carried out on the europarl koehn 2005 corpus which is a corpus widely used in smt and that has been used in several mt evaluation campaigns',\n",
       " 'for the language model we used the kenlm toolkit heafield 2011 to create a 5gram language model on the target side of the europarl corpus v7 with approximately 54m tokens with kneserney smoothe language model is a 5gram kenlm heafield 2011 model trained using lmplz with modified kneserney smoothing and no pruning',\n",
       " 'the second collection is constituted by the genia corpus kim et al 2003 3  which contains 2000 abstracts from medline a total of 18546 sentences genia kim et al 2003 is a collection of 2000 research abstracts selected from the search results of medline database using keywords mesh terms human blood cells and transcription factors',\n",
       " 'we introduce a new anaphoricity detection model as the second neural model using a long short term memory lstm network hochreiter and schmidhuber 1997 we generated embeddings by training a characterlevel longshort term memory lstm network hochreiter and schmidhuber 1997  using it as an encoder on the turns at talk from our corpus',\n",
       " 'the sentence aligned parallel data is first wordaligned using giza in both sourcetarget and targetsource directions followed by the application of traditional symmetrisation heuristics och and ney 2003 baseline word alignments were obtained by running giza in both directions and symmetrizing using the growdiagfinaland heuristic och and ney 2003',\n",
       " 'all the language models lm used in our experiments are 5grams modified kneserney smoothed lms trained using kenlm heafield et al 2013 for the language model we used all monolingual datasets and the french parts of the parallel datasets and trained a 5gram language model with modified kneserney smoothing using kenlm heafield et al 2013',\n",
       " 'they are based on distributional hypothesis which works under the assumption that similar words occur in similar contexts harris 1954 many researchers have considered generating paraphrases by mining the web guided by the distributional hypothesis which states that words occurring in similar contexts tend to have similar meanings harris 1954',\n",
       " 'finally we consider the europarl corpus v7 koehn 2005  given it is widely used in the mt community for spanish englishour experiments were carried out on the europarl koehn 2005 corpus which is a corpus widely used in smt and that has been used in several mt evaluation campaigns',\n",
       " 'for the contextual check we use the google web 1t 5gram corpus brants and franz 2006 which contains counts for ngrams from unigrams through to fivegrams obtained from over 1 trillion word tokeorgwikifogindex 8 for word frequency we use the unigrams data from the google web1t collection brants and franz 2006',\n",
       " 'by imposing constraints on the possible word reorderings similar to that described in berger et al 1996  the dpbased approach becomes more effective when the constraints are applied the number the first is a reimplementation of the stackbased decoder described in berger et al 1996',\n",
       " 'surdeanu et al 2012 propose a twolayer multiinstance multilabel miml framework to capture the dependencies among relationssurdeanu et al 2012 proposed a novel approach to multiinstance multilabel learning for relation extraction  which jointly modeled all the sentences in texts and all labels in knowledge bases for',\n",
       " 'we have theoretically suggested that based on cho  chai 2000  our theory can be a complete theory of scrambling phenomenon by providing the new type marker and the adjunct lp constraintthen we revise the two lp constraints of cho  chai 2000 and add the adjunct lp constraint',\n",
       " 'for the contextual check we use the google web 1t 5gram corpus brants and franz 2006 which contains counts for ngrams from unigrams through to fivegrams obtained from over 1 trillion word tokewe used the unigram counts from the web 1t 5gram corpus brants and franz 2006 to determine the frequency of use of each candidate synonym',\n",
       " 'rouge lin 2004 is a set of evaluation metrics used for automatic summarizationwe expect this restriction is more consistent with the rouge evaluation metric used for summarization lin 2004',\n",
       " 'here we review the parameters of the standard phrasebased translation model koehn et al 2007 the training set is used to train the phrasebased translation model and language model for moses koehn et al 2007',\n",
       " 'the spanishenglish s2e training corpus was drawn from the europarl collection koehn 2005 the systems for the english  spanish translation tasks were trained on the sentencealigned europarl corpus koehn 2005',\n",
       " 'we convert the trees in both treebanks from constituencies to labeled dependencies see figure 1 using the stanford converter which produces 46 types of labeled dependencies 1 de marneffe et al 2006 we use the stanford parser with stanford dependencies de marneffe et al 2006',\n",
       " 'we convert the trees in both treebanks from constituencies to labeled dependencies see figure 1 using the stanford converter which produces 46 types of labeled dependencies 1 de marneffe et al we use the stanford parser with stanford dependencies de marneffe et al 2006',\n",
       " 'we use kenlm 3 heafield 2011 for computing the target language model scorewe use a 5gram lm trained on the gigaword corpus and use kenlm heafield 2011 for lm scoring during decoding',\n",
       " 'the most famous example would probably be the europarl corpus koehn 2005 the system was trained on the english and danish part of the europarl corpus version 3 koehn 2005',\n",
       " 'we use the stanford parser with stanford dependencies de marneffe et al 2006 we convert the trees in both treebanks from constituencies to labeled dependencies see figure 1 using the stanford converter which produces 46 types of labeled dependencies 1 de marneffe et al 2006',\n",
       " 'additionally we train phrasebased machine translation models with our alignments using the popular moses toolkit koehn et al 2007 here we review the parameters of the standard phrasebased translation model koehn et al 2007',\n",
       " 'the training set is used to train the phrasebased translation model and language model for moses koehn et al 2007 the moses decoder koehn et al 2007 was used for generating lattices and nbest lists',\n",
       " 'the morpho syntactically annotated corpus we used is a variant of the french treebank or ftb abeille et al 2003 this corresponds to a new version of the french treebank abeille et al 2003',\n",
       " 'there has been a large amount of work on sentiment analysis at various levels of granularity pang and lee 2008 for a detailed survey of the field of sentiment analysis see pang and lee 2008',\n",
       " 'this task setup is further described in the task description paper rosenthal et al 2014 a complete description of the training and test datasets can be found at the task description paper rosenthal et al 2014',\n",
       " 'the baseline will be created by the moses smt toolkit koehn et al 2007 18 our baseline is the smt toolkit moses koehn et al 2007 run over letter strings rather than word strings',\n",
       " 'discourse structure in summarization rhetorical structure theory rst mann and thompson 1988 represents the discourse in a document in the form of a tree figure 1 causal relations are among discourse relations defined by rhetorical structure theory mann and thompson 1988',\n",
       " 'it builds on the cc ccg parser clark and curran 2004 it receives ccg derivations from the cc parser clark and curran 2004  and maps lexical categories and combinatory rules into semantic descriptions expressed by lambda calculus',\n",
       " 'we use boxer bos et al 2004 to parse natural language into a logical formin transforming natural language text to logical form we build on the software package boxer bos et al 2004',\n",
       " 'the training set is used to train the phrasebased translation model and language model for moses koehn et al 2007 we use the moses phrasebased translation system koehn et al 2007 to implement our models',\n",
       " 'we perform bootstrap resampling with bounds estimation as described in koehn 2004 we also report 95 confidence intervals ci measured on 1000 iterations of bootstrap resampling with replacement koehn 2004',\n",
       " 'the major part of data comes from current and upcoming full releases of the europarl data set koehn 2005 the europarl corpus koehn 2005 is built from the proceedings of the european parliament',\n",
       " 'for italian we use the word2vec to train word embeddings on the europarl italian corpus koehn 2005 2 7 httpopennlpapacheorg 523 we use the europarl corpus koehn 2005 as testing data',\n",
       " 'the gate pluginbased architecture cunningham et al 2002 is the basis for the platform environment gate cunningham et al 2002a is an architecture  a framework and a development environment for human language technology modules and applications',\n",
       " 'we trained a 5gram language model on the xinhua section of the english gigaword corpus 306 million words using the srilm toolkit stolcke 2002 with the modified kneserney smoothing chen and goodman 1996 both language models use modified kneserney smoothing chen and goodman 1996',\n",
       " 'we used giza och and ney 2003 along with the growing heuristics to wordalign the training cor puswe automatically wordaligned the german part to each of the others with the giza tool och and ney 2003',\n",
       " 'our previous mlnbased approach for joint disambiguation and clustering of concepts fahrni and strube 2012 joint disambiguation and clustering of mentions improves the disambiguation results for both the scopeignorant fahrni and strube 2012 and the scopeaware approach',\n",
       " 'pang et al 2002 compare the performance of three commonly used machine learning models naive bayes maximum entropy and svmpang et al 2002  use machine learning methods nb svm and maxent to detect sentiments on movie reviews',\n",
       " 'jans et al 2012 focused solely on the narrative cloze test as an end goalwe refer to the system of jans et al 2012 as the single protagonist system',\n",
       " 'we applied the naive bayes probabilistic supervised learning algorithm from the weka machine learning library hall et al 2009 naive bayes and decision tree models were built with the weka hall et al 2009 package for decision trees we used the j48 implementation',\n",
       " 'we therefore seek to allow quick incremental updates of the rm within moses koehn et al 2007 we build a state of the art phrasebased smt system using moses koehn et al 2007',\n",
       " 'we use the moses software package 5 to train a pbmt model koehn et al 2007 we compare the model against the moses phrasebased translation system koehn et al 2007  applied to phoneme sequences',\n",
       " 'it has a much longer average sentence length than penn chinese treebank pctb 33 compare to 28 xue et al 2005 we then describe in more detail a modern chinese corpus the penn chinese treebank xue et al 2005',\n",
       " 'the baseline systems are built with the opensource phrasebased smt toolkit moses koehn et al 2007 the first two baselines are standard systems using pbmt or hiero trained using moses koehn et al 2007',\n",
       " 'the corpora are tokenised and truecased using scripts from the moses toolkit koehn et al 2007 the first two baselines are standard systems using pbmt or hiero trained using moses koehn et al 2007',\n",
       " 'the dropout rate was set to 05 and the model was trained via adagrad duchi et al 2011 we use the crossentropy loss function and minibatch adagrad duchi et al 2011 to optimize parameters',\n",
       " 'the dropout rate was set to 05 and the model was trained via adagrad duchi et al 2011 we use adagrad duchi et al 2011  with the initial learning rate set to   05',\n",
       " 'we built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007 we compare the model against the moses phrasebased translation system koehn et al 2007  applied to phoneme sequences',\n",
       " 'they had shown that the penn discourse treebank pdtb style prasad et al 2008 discourse relations are usefulthe penn discourse treebank pdtb prasad et al 2008 includes several relation types that are relevant to causation primarily cause and reason',\n",
       " 'they had shown that the penn discourse treebank pdtb style prasad et al 2008 discourse relations are usefulpitler and nenkova 2008 used discourse relations of the penn discourse treebank prasad et al 2008 as a feature',\n",
       " 'mcdonald et al 2005 present a technique for training discriminative models for dependency parsing the details of parsing model were presented in mcdonald et al 2005 and mcdonald and pereira 2006',\n",
       " 'we transformed the parse trees in ontonotes into syntactic dependencies using stanford corenlp manning et al 2014 in addition the data was tokenized lemmatized and parsed using stanford corenlp manning et al 2014',\n",
       " '2 translation model moses is a stateoftheart toolkit for phrasebased smt systems koehn et al 2007 the decoder is built on top of an opensource phrasebased smt decoder moses koehn et al 2007',\n",
       " 'for the ontonotes data sets same speaker lee et al 2013 is the only feature for resolving pronounsthe mention detection of the stanford coreference system lee et al 2013 is used for the ontonotes data sets',\n",
       " 'it was one of the best parsers in the conll shared task 2009 haji et al 2009 ctb6 is used as the chinese data set in the conll 2009 shared task haji et al 2009',\n",
       " 'we transformed the parse trees in ontonotes into syntactic dependencies using stanford corenlp manning et al 2014 in addition the data was tokenized lemmatized and parsed using stanford corenlp manning et al 2014',\n",
       " 'results are reported on the test data using f1 computed with the conll scorer dahlmeier and ng 2012 we report f1 performance scored using the official scorer from the shared task dahlmeier and ng 2012',\n",
       " 'the perplexity achieved by the 6gram nn lm in the spanish news2009 set was 281 versus 145 obtained with the standard 6gram language model with interpolation and kneserney smoothing kneser and nethe results of this experiment appear in table 2 which further compares these models with the following baselines 1 vanila rnnlstm and 2 a 6gram lm with kneserney smoothing 3 kneser and ney',\n",
       " 'we address the qe problem as a regression task by building svm models with an epsilon regressor and a radial basis function kernel using the libsvm toolkit chang and lin 2011 we use libsvm chang and lin 2011 as the svm tool',\n",
       " 'finally it is also noticeable that the percentage of compounds detected in the training set is similar to the one reported by baroni et al 2002 and referenced to in section 2baroni et al 2002 also pointed out that the small percentage of compounds detected at token level 7 suggested that many of them are productively formed hapax legomena or very rare words',\n",
       " 'facts such as these are difficult to account for in a purely semantic theory of ellipsis resolution such as the one proposed in dalrymple et al 1991 the most influential of the semantic approaches to ellpsis has been the higherorder unification approach proposed in dalrymple et al 1991',\n",
       " 'all these reordering models are tested using moses koehn et al 2007  except that the neural model needs an additional hypergraph reranking procedure section 33we refer to that model as moses enes100k  because it was trained using the moses toolkit koehn et al 2007',\n",
       " 'bullet raesubj socher et al 2011 proposed to use recursive autoencoders for sentencelevel predication of sentiment label distributionswe follow the formulation of vector composition proposed by socher et al 2011  except that we do not stack autoencoders for recursion',\n",
       " 'we train a ridge regression model scikitlearn pedregosa et al 2011  for each assignment and test using annotations from the rest as training examples our system is a linear model estimated using ridge regression as implemented in the scikitlearn toolkit pedregosa et al 2011',\n",
       " 'we follow the protocols in collobert et al 2011  using the concatenation of neighboring embeddings as input to a multilayer neural modelwe use srl collobert et al 2011 to determine the agent and patient arguments of an event mention',\n",
       " 'this is equivalent to an svm with the compound cluster features as in koo et al 2008 the sparsity of lexical features can also be tackled by the use of distributional word clusters as pioneered by koo et al 2008',\n",
       " 'all experiments are conducted using the moses phrasebased smt system koehn et al 2007 with a maximum phrase length of 8for the comparisons of translation quality the models are trained up using a phrasebased translation system koehn et al 2007 that used the above listed models to align the data',\n",
       " 'we use the moses toolkit koehn et al 2007 to create a statistical phrasebased machine translation model built on the best preprocessed data as described abovewe refer to that model as moses enes100k  because it was trained using the moses toolkit koehn et al 2007',\n",
       " 'all experiments were on english part of speech pos tagging on the postagged wall street journal text in the penn treebank ptb version 3 marcus et al 1994 these techniques were evaluated in experiments on the penn treebank marcus et al 1994 with the widecoverage hpsg parser developed by',\n",
       " 'we use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic och and ney 2004 to all 5gramswe use the intersection of direct and reverse giza och and ney 2004 alignments as a heuristic rule to find words reliably aligned to each other',\n",
       " 'we estimated a hierarchical mt model for the train partition with the standard configuration of the moses toolkit koehn et al 2007 we also compare with the standard phrasebased system of moses koehn et al 2007  with standard settings except for the ttable limit which we set to 100',\n",
       " 'a small amount of labeled data is used to map the induced topics to realworld senses for a description of the method see agirre and soroa 2007 thus inducing a number of clusters similar to the number of senses is not a requirement for good results agirre and soroa 2007a',\n",
       " 'we use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic och and ney 2004 to all 5gramswe use the intersection of direct and reverse giza och and ney 2004 alignments as a heuristic rule to find words reliably aligned to each other',\n",
       " '1 for a reference standard text we used the written portion of the british national corpus bnc burnard 2000 the evaluation corpus is a subset of an ungrammatical version of the british national corpus bnc a 100 million word balanced corpus of british english burnard 2000',\n",
       " 'we worked with the europarl corpus koehn 2005 in order to have a parallel comparative corpus for italian and spanishtranslations for english words in the lexical sample are extracted from a semiautomatic word alignment of sentences from the europarl parallel corpus koehn 2005',\n",
       " 'the organization from semeval2013 task 2 sentiment analysis in twitter provided three datasets for the task wilson et al 2013 we participated in both subtask a and b of semeval2013 task 2 sentiment analysis in twitter wilson et al 2013 with an adaptation of our existing system',\n",
       " 'pending a planned full evaluation using the moses system koehn et al 2007 as a benchmark we tested the mt method outlined above on two simple tasksto test our method we conducted two lowresource translation experiments using the phrasebased mt system moses koehn et al 2007',\n",
       " 'the entity transition features are then used to train a support vector machine ranker joachims 2002 to rank the source documents higher than the permutationswe use the support vector machine svm rank algorithm joachims 2002 to predict a rank order for each list of comments',\n",
       " 'our second method is based on the recurrent neural network language model rnnlm approach to learning word embeddings of mikolov et al 2013a and mikolov et al 2013b  using the word2vec packagein all of the above tasks we compare the neural word embeddings of mikolov et al 2013a with two vector spaces both based on cooccurrence counts and produced by standard distributional techniques',\n",
       " 'we apply the hidden markov model hmm viterbi 1967 and the forwardbackward algorithm james 1995 to obtain the contextdependent probabilitieswe use viterbi algorithm viterbi 1967 to decode the character sentence',\n",
       " 'we report bleu papineni et al 2001 of translation system output measured against the original english querieswe measure translation quality via the bleu score papineni et al 2001',\n",
       " 'it is a standard phrasebased machine translation model koehn et al 2007 all our systems are contrasted with a standard phrasebased system built with moses koehn et al 2007',\n",
       " 'finally we used moses toolkit as phrasebased reference koehn et al 2007 the baseline systems are built with the opensource phrasebased smt toolkit moses koehn et al 2007',\n",
       " 'concept similarity is computed using the edgebased path similarity pedersen et al 2004 the degree of similarity between two similar words is identified using wordnet pedersen et al 2004',\n",
       " 'finally we used moses toolkit as phrasebased reference koehn et al 2007 we built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " 'it is a standard phrasebased machine translation model koehn et al 2007 we built phrasebased machine translation systems using the open software toolkit moses koehn et al 2007',\n",
       " '2 translation model moses is a stateoftheart toolkit for phrasebased smt systems koehn et al 2007 finally we used moses toolkit as phrasebased reference koehn et al 2007',\n",
       " 'otherwise it is measured by wordnet similarity package pedersen et al 2004 the degree of similarity between two similar words is identified using wordnet pedersen et al 2004',\n",
       " 'we use 5gram language models with kneserney discounting heafield et al 2013 the language models are estimated using the kenlm toolkit heafield et al 2013 with modified kneserney smoothing',\n",
       " 'we adopt online learning updating parameters using adagrad duchi et al 2011 we use the crossentropy loss function and minibatch adagrad duchi et al 2011 to optimize parameters',\n",
       " 'we used the implementation of the scikitlearn 2 module pedregosa et al 2011 we used svm implementations from scikitlearn pedregosa et al 2011 and experimented with a number of classifiers',\n",
       " 'we adopt online learning updating parameters using adagrad duchi et al 2011 we train the concept identification stage using infinite ramp loss 1 with adagrad duchi et al 2011',\n",
       " 'finally we used moses toolkit as phrasebased reference koehn et al 2007 we used the moses toolkit koehn et al 2007 to train standard phrasebased systems with default configurations',\n",
       " 'europarl koehn 2005 is a multilingual parallel corpus extracted from the proceedings of the european parliamentwe used the english side of the europarl corpus koehn 2005',\n",
       " 'we leave the thirdorder models koo and collins 2010 for a future study koo10 stands for the model 1 in koo and collins 2010 which is a thirdorder model',\n",
       " 'we used the implementation of the scikitlearn 2 module pedregosa et al 2011 specifically we used the standard gradient boosting regressor in the scikitlearn toolkit 4 pedregosa et al 2011',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['recently', 'there', 'has', 'been', 'a', 'successful', 'attempt', 'to', 'harmonize', 'the', 'linguistic', 'principles', 'behind', 'the', 'coding', 'systems', 'msd', 'and', 'kr', 'farkas', 'et', 'al', '2010', 'recently', 'there', 'has', 'been', 'a', 'successful', 'attempt', 'to', 'harmonize', 'the', 'coding', 'systems', 'msd', 'and', 'kr', 'farkas', 'et', 'al', '2010'], ['the', 'first', 'one', 'is', 'the', 'ws353', 'dataset', 'finkelstein', 'et', 'al', '2001', 'containing', '353', 'pairs', 'of', 'english', 'words', 'that', 'have', 'been', 'assigned', 'similarity', 'ratings', 'by', 'humansthe', 'first', 'one', 'is', 'the', 'ws353', 'dataset', 'finkelstein', 'et', 'al', '2001', '', 'which', 'contains', '353', 'pairs', 'of', 'english', 'words', 'that', 'have', 'been', 'assigned', 'similarity', 'ratings', 'by', 'humans'], ['abstract', 'meaning', 'representation', 'amr', 'banarescu', 'et', 'al', '2013', 'is', 'a', 'semantic', 'formalism', 'where', 'the', 'meaning', 'of', 'a', 'sentence', 'is', 'encoded', 'as', 'a', 'rooted', 'directed', 'graphabstract', 'meaning', 'representation', 'amr', 'banarescu', 'et', 'al', '2013', 'is', 'a', 'semantic', 'formalism', 'encoding', 'the', 'meaning', 'of', 'a', 'sentence', 'as', 'a', 'rooted', 'directed', 'graph'], ['we', 'perform', 'bootstrap', 'resampling', 'with', 'bounds', 'estimation', 'as', 'described', 'in', 'koehn', '2004', 'we', 'perform', 'bootstrap', 'resampling', 'with', 'bounds', 'estimation', 'as', 'described', 'by', 'koehn', '2004'], ['it', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'hpsg', 'english', 'resource', '', 'grammar', 'erg', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfgit', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'the', 'hpsg', 'english', 'resource', 'grammar', '', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfg'], ['we', 'use', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parse', 'trees', 'and', 'dependency', 'relationswe', 'first', 'use', 'a', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'parse', 'each', 'sentence', 'and', 'extract', 'the', 'set', 'of', 'dependency', 'relations', 'associated', 'with', 'the', 'sentence'], ['rooth', 'et', 'al', '1999', 'propose', 'an', 'expectationmaximization', 'em', 'clustering', 'algorithm', 'for', 'selectional', 'preference', 'acquisition', 'based', 'on', 'a', 'probabilistic', 'latent', 'variable', 'modelalternatively', 'rooth', 'et', 'al', '1999', '', 'propose', 'an', 'embased', 'clustering', 'smooth', 'for', 'sp'], ['the', 'levenshtein', 'distance', 'levenshtein', '1966', 'between', 'two', 'strings', 'is', 'defined', 'as', 'the', 'minimum', 'number', 'of', 'editing', 'operations', 'substitutions', 'deletions', 'and', 'insertionsthe', 'levenshtein', 'distance', 'gives', 'an', 'indication', 'of', 'the', 'similarity', 'between', 'two', 'strings', 'levenshtein', '1966'], ['we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'create', 'a', 'statistical', 'phrasebased', 'machine', 'translation', 'model', 'built', 'on', 'the', 'best', 'preprocessed', 'data', 'as', 'described', 'abovewe', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'perform', 'bootstrap', 'resampling', 'with', 'bounds', 'estimation', 'as', 'described', 'in', 'koehn', '2004', 'we', 'perform', 'bootstrap', 'resampling', 'with', 'bounds', 'estimation', 'as', 'described', 'by', 'koehn', '2004'], ['it', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'hpsg', 'english', 'resource', '', 'grammar', 'erg', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfgit', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'the', 'hpsg', 'english', 'resource', 'grammar', '', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfg'], ['it', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'the', 'english', 'hpsg', 'grammar', 'erg', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfgit', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'the', 'hpsg', 'english', 'resource', 'grammar', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfg'], ['it', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'hpsg', 'english', 'grammar', 'erg', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfgit', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'the', 'hpsg', 'english', 'resource', 'grammar', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfg'], ['it', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'the', 'english', 'hpsg', 'grammar', 'erg', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfgit', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'the', 'hpsg', 'english', 'resource', 'grammar', '', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfg'], ['we', 'build', 'upon', 'our', 'previous', 'markov', 'logic', 'based', 'approach', 'for', 'joint', 'concept', 'disambiguation', 'and', 'clustering', 'fahrni', 'and', 'strube', '2012', 'we', 'build', 'upon', 'our', 'previous', 'approach', 'for', 'joint', 'concept', 'disambiguation', 'and', 'clustering', 'fahrni', 'and', 'strube', '2012'], ['details', 'about', 'svm', 'and', 'kfd', 'can', 'be', 'found', 'in', 'taylor', 'and', 'cristianini', '2004', 'details', 'about', 'svm', 'and', 'krr', 'can', 'be', 'found', 'in', 'taylor', 'and', 'cristianini', '2004'], ['we', 'learn', 'the', 'parameters', 'using', 'a', 'quasinewton', 'procedure', 'with', 'l', '1', 'lasso', 'regularization', 'andrew', 'and', 'gao', '2007', 'we', 'learn', 'the', 'parameters', '', 'using', 'a', 'quasinewton', 'qn', 'procedure', 'with', 'l', '1', 'lasso', 'regularization', 'andrew', 'and', 'gao', '2007'], ['we', 'use', 'the', 'scfg', 'decoder', 'cdec', 'dyer', 'et', 'al', '2010', '4', 'and', 'build', 'grammars', 'using', 'its', 'implementation', 'of', 'the', 'suffix', 'array', 'extraction', 'method', 'described', 'in', 'lopez', '2007for', 'direct', 'translation', 'we', 'use', 'the', 'scfg', 'decoder', 'cdec', 'dyer', 'et', 'al', '2010', '4', 'and', 'build', 'grammars', 'using', 'its', 'implementation', 'of', 'the', 'suffix', 'array', 'extraction', 'method', 'described', 'in', 'lopez', '2007'], ['this', 'is', 'known', 'as', 'the', 'distributional', 'hypothesis', 'harris', '1968', 'this', 'is', 'known', 'as', 'the', 'distributional', 'hypothesis', 'in', 'linguistics', 'harris', '1968'], ['all', 'our', 'models', '', 'as', 'well', 'as', 'the', 'parser', 'described', 'in', 'henderson', '2003', '', 'are', 'run', 'only', 'oncethe', 'models', '', 'as', 'well', 'as', 'the', 'parser', 'described', 'in', 'henderson', '2003', '', 'are', 'run', 'only', 'once'], ['for', 'strings', 'many', 'such', 'kernel', 'functions', 'exist', 'with', 'various', 'applications', 'in', 'computational', 'biology', 'and', 'computational', 'linguistics', 'taylor', 'and', 'cristianini', '2004', 'for', 'strings', 'a', 'lot', 'of', 'such', 'kernel', 'functions', 'exist', 'with', 'many', 'applications', 'in', 'computational', 'biology', 'and', 'computational', 'linguistics', 'taylor', 'and', 'cristianini', '2004'], ['the', 'estimation', 'of', 'the', 'semantically', 'smoothed', 'partial', 'tree', 'kernel', 'sptk', 'is', 'made', 'available', 'by', 'an', 'extended', 'version', 'of', 'svmlighttk', 'software', '5', 'moschitti', '2006', 'the', 'estimation', 'of', 'the', 'semantically', 'smoothed', 'partial', 'tree', 'kernel', 'sptk', 'is', 'made', 'available', 'by', 'an', 'extended', 'version', 'of', 'svmlighttk', 'software', '7', 'moschitti', '2006'], ['we', 'train', 'with', 'the', 'adam', 'optimizer', 'kingma', 'and', 'ba', '2015', '', 'a', 'learning', 'rate', 'of', '00001', 'batch', 'size', 'of', '50', 'and', 'dropout', 'with', 'probability', '02', 'applied', 'to', 'the', 'hidden', 'layerwe', 'train', 'with', 'the', 'adam', 'optimizer', 'kingma', 'and', 'ba', '2015', '', 'a', 'learning', 'rate', 'of', '00001', 'batch', 'size', 'of', '50', 'and', 'dropout', 'with', 'probability', '02', 'applied', 'to', 'the', 'hidden', 'layer'], ['in', 'our', 'experimental', 'study', 'we', 'use', 'the', 'freely', 'available', 'implementations', 'in', 'weka', 'witten', 'and', 'frank', '2005', 'in', 'our', 'experimental', 'study', 'we', 'use', 'the', 'freely', 'available', 'implementation', 'of', 'svm', 'in', 'weka', 'witten', 'and', 'frank', '2005'], ['more', 'recently', 'carpineto', 'and', 'romano', '2010', 'showed', 'that', 'the', 'characteristics', 'of', 'the', 'outputs', 'returned', 'by', 'src', 'algorithms', 'suggest', 'the', 'adoption', 'of', 'a', 'meta', 'clustering', 'approachoptimsrc', 'carpineto', 'and', 'romano', '2010', 'showed', 'that', 'the', 'characteristics', 'of', 'the', 'outputs', 'returned', 'by', 'prc', 'algorithms', 'suggest', 'the', 'adoption', 'of', 'a', 'meta', 'clustering', 'approach'], ['they', 'are', 'based', 'on', 'the', 'distributional', 'hypothesis', 'harris', '1968', 'and', 'by', 'looking', 'at', 'a', 'set', 'of', 'event', 'expressions', 'whose', 'argument', 'fillers', 'have', 'a', 'similar', 'distribution', 'they', 'try', 'to', 'recognize', 'synonymous', 'eventthese', 'methods', 'rely', 'on', 'the', 'distributional', 'hypothesis', 'harris', '1968', '', 'and', 'by', 'looking', 'at', 'a', 'set', 'of', 'event', 'expressions', 'whose', 'argument', 'fillers', 'have', 'a', 'similar', 'distribution', 'try', 'to', 'recognize', 'synonymous', 'event'], ['word', 'alignment', 'is', 'performed', 'using', 'giza', 'och', 'and', 'ney', '2003', 'word', 'alignment', 'is', 'done', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['word', 'alignment', 'is', 'performed', 'using', 'giza', 'och', 'and', 'ney', '2003', 'word', 'alignment', 'is', 'done', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'used', 'mallet', 'software', 'mccallum', '2002', 'for', 'crf', 'experimentswe', 'used', 'mallet', 'software', 'mccallum', '2002', 'for', 'sscrf', 'experiments'], ['the', 'thesaurus', 'consists', 'of', 'a', 'hierarchy', 'of', '2710', 'semantic', 'classes', 'defined', 'for', 'over', '264312', 'nouns', 'with', 'a', 'maximum', 'depth', 'of', 'twelve', 'ikehara', 'et', 'al', '1997', 'the', 'ontology', 'goitaikei', 'consists', 'of', 'a', 'hierarchy', 'of', '2710', 'semantic', 'classes', 'defined', 'for', 'over', '264312', 'nouns', 'with', 'a', 'maximum', 'depth', 'of', '12', 'ikehara', 'et', 'al', '1997'], ['the', 'detailed', 'discussion', 'is', 'provided', 'in', 'the', 'longer', 'version', 'of', 'the', 'paper', 'kim', 'et', 'al', '2013', 'a', 'detailed', 'discussion', 'on', 'the', 'results', 'is', 'provided', 'in', 'the', 'longer', 'version', 'of', 'the', 'paper', 'kim', 'et', 'al', '2013'], ['the', 'first', 'one', 'is', 'the', 'ws', '353', '3', 'dataset', 'finkelstein', 'et', 'al', '2001', 'containing', '353', 'pairs', 'of', 'english', 'words', 'that', 'have', 'been', 'assigned', 'similarity', 'ratings', 'by', 'humansthe', 'first', 'one', 'is', 'the', 'ws353', 'dataset', 'finkelstein', 'et', 'al', '2001', '', 'which', 'contains', '353', 'pairs', 'of', 'english', 'words', 'that', 'have', 'been', 'assigned', 'similarity', 'ratings', 'by', 'humans'], ['a', 'framework', 'for', 'human', 'error', 'analysis', 'and', 'error', 'classification', 'has', 'been', 'proposed', 'in', 'vilar', 'et', 'al', '2006', '', 'but', 'like', 'human', 'evaluation', '', 'this', 'is', 'also', 'a', 'time', 'consuming', 'taska', 'framework', 'for', 'human', 'error', 'analysis', 'has', 'been', 'proposed', 'in', 'vilar', 'et', 'al', '2006', '', 'but', 'as', 'every', 'human', 'evaluation', 'this', 'is', 'also', 'a', 'time', 'consuming', 'task'], ['maxent', 'classifier', 'is', 'a', 'good', 'example', 'of', 'this', 'group', 'mani', 'et', 'al', '2006', 'maxent', 'classifier', 'is', 'an', 'example', 'of', 'this', 'group', 'mani', 'et', 'al', '2006'], ['among', 'these', 'media', 'blog', 'is', 'one', 'of', 'the', 'communicative', 'and', 'informative', 'repository', 'of', 'text', 'based', 'emotional', 'contents', 'in', 'the', 'web', '20', 'lin', 'et', 'al', '2007', 'blog', 'is', 'one', 'of', 'the', 'crucial', 'communicative', 'and', 'informative', 'repository', 'of', 'text', 'based', 'emotional', 'contents', 'in', 'the', 'web', '20', 'lin', 'et', 'al', '2007'], ['for', 'the', 'gold', 'preprocessing', 'and', 'all', '5k', 'settings', 'we', 'refer', 'the', 'reader', 'to', 'the', 'shared', 'task', 'overview', 'paper', 'seddah', 'et', 'al', '2013'], ['for', 'preprocessing', 'we', 'used', 'mada', 'morphological', 'analysis', 'and', 'disambiguation', 'for', 'arabic', 'habash', 'et', 'al', '2009', 'which', 'is', 'one', 'of', 'the', 'most', 'accurate', 'arabic', 'preprocessing', 'toolkitsfor', 'this', 'purpose', 'we', 'use', 'mada', 'morphological', 'analysis', 'and', 'disambiguation', 'for', 'arabic', 'habash', 'et', 'al', '2009', 'which', 'is', 'one', 'of', 'the', 'most', 'accurate', 'arabic', 'preprocessing', 'toolkits'], ['we', 'trained', 'a', '5gram', 'language', 'model', 'on', 'the', 'xinhua', 'section', 'of', 'the', 'english', 'gigaword', 'corpus', '306', 'million', 'words', 'using', 'the', 'srilm', 'toolkit', 'stolcke', '2002', 'with', 'the', 'modified', 'kneserney', 'smoothingour', '5gram', 'language', 'model', 'was', 'trained', 'on', 'the', 'xinhua', 'section', 'of', 'the', 'english', 'gigaword', 'corpus', '306', 'million', 'words', 'using', 'the', 'srilm', 'toolkit', 'stolcke', '2002', '', 'with', 'modified', 'kneserney', 'smoothing'], ['to', 'determine', 'semantic', 'type', 'and', 'subtype', 'we', 'train', 'two', 'svm', 'multiclass', 'classifiers', 'using', 'svm', 'multiclass', 'tsochantaridis', 'et', 'al', '2004', 'to', 'determine', 'semantic', 'types', 'and', 'subtypes', 'we', 'train', 'two', 'svm', 'multiclass', 'classifiers', 'using', 'svm', 'multiclass', 'tsochantaridis', 'et', 'al', '2004'], ['we', 'use', 'the', 'scikit', 'implementation', 'of', 'random', 'forest', 'pedregosa', 'et', 'al', '2011', 'we', 'use', 'the', 'scikit', 'implementation', 'of', 'svm', 'pedregosa', 'et', 'al', '2011'], ['each', 'term', 'in', 'the', 'input', 'text', 'will', 'be', 'represented', 'by', 'its', 'stem', 'and', 'pos', 'tag', 'in', 'the', 'following', 'format', 'stempos', 'using', 'buckwalter', 'transliteration', 'buckwalter', '2002', 'each', 'term', 'in', 'the', 'input', 'text', 'is', 'represented', 'by', 'its', 'stem', 'and', 'pos', 'tag', 'using', 'buckwalter', 'transliteration', 'buckwalter', '2002'], ['an', 'algorithm', 'the', 'kuhnmunkres', 'method', 'kuhn', '1955', '', 'can', 'find', 'solutions', 'to', 'the', 'optimum', 'assignment', 'problem', 'in', 'polynomial', 'timean', 'algorithm', 'the', 'kuhnmunkres', 'method', 'kuhn', '1955', '', 'has', 'been', 'proposed', 'that', 'can', 'find', 'a', 'solution', 'to', 'the', 'optimum', 'assignment', 'problem', 'in', 'polynomial', 'time'], ['we', 'use', 'collapsed', 'gibbs', 'sampling', 'griffiths', 'and', 'steyvers', '2004', 'to', 'infer', 'the', 'parameters', 'of', 'the', 'model', 'and', 'the', 'latent', 'violent', 'categories', 'and', 'topics', 'assignments', 'for', 'tweets', 'given', 'observed', 'data', 'd', 'gibbswe', 'use', 'collapsed', 'gibbs', 'sampling', 'griffiths', 'and', 'steyvers', '2004', 'to', 'infer', 'the', 'parameters', 'of', 'the', 'model', '', 'given', 'observed', 'data', 'd', 'gibbs', 'sampling', 'is', 'a', 'markov', 'chain', 'monte', 'carlo', 'method', 'which', 'allows', 'us', 'repeat'], ['we', 'use', 'the', 'stanford', 'dependency', 'parser', 'marneffe', 'et', 'al', '2006', 'we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006'], ['filter', 'weights', 'are', 'initialized', 'using', 'glorotbengio', 'strategy', 'glorot', 'and', 'bengio', '2010', 'weights', 'are', 'initialized', 'using', 'glorotbengio', 'strategy', 'glorot', 'and', 'bengio', '2010'], ['automatic', 'sentence', 'alignment', 'of', 'the', 'training', 'data', 'was', 'provided', 'by', 'ulrich', 'german', 'and', 'the', 'hand', 'alignments', 'of', 'the', 'words', 'in', 'the', 'test', 'data', 'were', 'created', 'by', 'franz', 'och', 'and', 'hermann', 'ney', 'och', 'and', 'ney', '2003', 'automatic', 'sentence', 'alignment', 'of', 'the', 'training', 'data', 'was', 'provided', 'by', 'ulrich', 'german', 'and', 'the', 'hand', 'alignments', 'of', 'the', 'words', 'in', 'the', 'trial', 'and', 'test', 'data', 'were', 'created', 'by', 'franz', 'och', 'and', 'hermann', 'ney', 'och', 'and', 'ney', '2003'], ['we', 'use', 'the', 'adagrad', 'optimizer', 'duchi', 'et', 'al', '2011', '', 'with', 'initial', 'learning', 'rate', 'set', 'to', '01we', 'use', 'adagrad', 'duchi', 'et', 'al', '2011', '', 'with', 'the', 'initial', 'learning', 'rate', 'set', 'to', '05'], ['then', 'we', 'did', 'word', 'alignment', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'default', 'growdiagfinaland', 'alignment', 'symmetrization', 'methodwe', 'performed', 'word', 'alignment', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'default', 'growdiagfinaland', 'alignment', 'symmetrization', 'method'], ['for', 'example', 'dirt', 'lin', 'and', 'pantel', '2001', 'aims', 'to', 'discover', 'different', 'representations', 'of', 'the', 'same', 'semantic', 'relation', 'ie', 'similar', 'dependency', 'pathsfor', 'example', 'dirt', 'lin', 'and', 'pantel', '2001', 'aims', 'to', 'discover', 'different', 'representations', 'of', 'the', 'same', 'semantic', 'relation', 'using', 'distributional', 'similarity', 'of', 'dependency', 'paths'], ['the', 'annotation', 'was', 'performed', 'manually', 'using', 'the', 'brat', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012', 'the', 'annotation', 'was', 'performed', 'using', 'the', 'brat', '2', 'tool', 'stenetorp', 'et', 'al', '2012'], ['system', 'proposed', 'by', 'li', 'et', 'al', '2006', '', 'uses', 'a', 'semanticvector', 'approach', 'to', 'measure', 'sentence', 'similaritya', 'similar', 'semantic', 'similarity', 'measure', 'proposed', 'by', 'li', 'et', 'al', '2006', '', 'uses', 'a', 'semanticvector', 'approach', 'to', 'measure', 'sentence', 'similarity'], ['this', 'corpus', 'contains', 'around', '11000', 'nps', 'annotated', 'for', 'information', 'status', 'including', '663', 'bridging', 'nps', 'and', 'their', 'antecedents', 'in', '50', 'texts', 'taken', 'from', 'the', 'wsj', 'portion', 'of', 'the', 'ontonotes', 'corpus', '', 'weischedel', 'et', 'al', '2011it', 'consists', 'of', '50', 'texts', 'taken', 'from', 'the', 'wsj', 'portion', 'of', 'the', 'ontonotes', 'corpus', 'weischedel', 'et', 'al', '2011', 'with', 'almost', '11000', 'nps', 'annotated', 'for', 'information', 'status', 'including', '663', 'bridging', 'nps', 'and', 'their', 'antecedent'], ['all', 'modules', 'take', 'as', 'input', 'the', 'corpus', 'documents', 'preprocessed', 'with', 'a', 'partofspeech', 'tagger', '4', 'and', 'shallow', 'parser', '5', 'punyakanok', 'and', 'roth', '2001', 'all', 'components', 'take', 'as', 'input', 'the', 'corpus', 'documents', 'preprocessed', 'with', 'a', 'partofspeech', 'tagger', '2', 'and', 'shallow', 'parser', '3', 'punyakanok', 'and', 'roth', '2001'], ['from', 'the', 'pioneering', 'work', 'of', 'rapp', '1995', '', 'contextual', 'similarity', 'has', 'been', 'used', 'for', 'ble', 'for', 'a', 'long', 'timefrom', 'the', 'pioneering', 'work', 'of', 'rapp', '1995', '', 'ble', 'from', 'comparable', 'corpora', 'has', 'been', 'studied', 'for', 'a', 'long', 'time'], ['europarl', 'koehn', '2005', 'is', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'run', 'our', 'experiments', 'on', 'europarl', 'koehn', '2005', '', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['tesla', 'translation', 'evaluation', 'of', 'sentences', 'with', 'linearprogrammingbased', 'analysis', 'was', 'first', 'proposed', 'in', 'liu', 'et', 'al', '2010', 'teslaf', 'was', 'called', 'tesla', 'in', 'liu', 'et', 'al', '2010'], ['for', 'building', 'the', 'baseline', 'smt', 'system', 'we', 'used', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', '', 'in', 'its', 'standard', 'setupfor', 'building', 'our', 'smt', 'systems', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'was', 'used', 'in', 'its', 'standard', 'setup'], ['rhetorical', 'structure', 'theory', 'rst', 'mann', 'and', 'thompson', '1988', '', 'one', 'of', 'the', 'most', 'influential', 'theories', 'of', 'discourse', 'represents', 'texts', 'by', 'labeled', 'hierarchical', 'structures', 'called', 'discourse', 'trees', 'dtsrhetorical', 'structure', 'theory', 'rst', 'mann', 'and', 'thompson', '1988', '', 'one', 'of', 'the', 'most', 'influential', 'theories', 'of', 'discourse', 'posits', 'a', 'tree', 'representation', 'of', 'a', 'discourse', 'known', 'as', 'a', 'discourse', 'tree', 'dt'], ['in', 'addition', 'the', 'fixdiscount', 'method', 'in', 'foster', 'et', 'al', '2006', 'for', 'phrase', 'table', 'smoothing', 'is', 'also', 'usedin', 'addition', 'the', 'fixdiscount', 'method', 'foster', 'et', 'al', '2006', 'for', 'phrase', 'table', 'smoothing', 'was', 'also', 'used'], ['memorybased', 'language', 'processing', 'daelemans', 'and', 'van', 'den', 'bosch', '2005', 'is', 'based', 'on', 'the', 'idea', 'that', 'nlp', 'problems', 'can', 'be', 'solved', 'by', 'reuse', 'of', 'solved', 'examples', 'of', 'the', 'problem', 'stored', 'in', 'memorymemorybased', 'language', 'processing', 'daelemans', 'and', 'van', 'den', 'bosch', '2005', 'is', 'based', 'on', 'the', 'idea', 'that', 'nlp', 'problems', 'can', 'be', 'solved', 'by', 'reuse', 'of', 'solved', 'examples', 'of', 'the', 'problem', 'in', 'memory'], ['we', 'calculate', 'statistical', 'significance', 'of', 'performance', 'differences', 'using', 'stratified', 'shuffling', 'yeh', '2000', 'we', 'tested', 'the', 'significance', 'of', 'differences', 'using', 'stratified', 'shuffling', 'yeh', '2000'], ['for', 'instance', 'machine', 'translation', 'mt', 'systems', 'can', 'benefit', 'from', 'training', 'on', 'sentences', 'extracted', 'from', 'parallel', 'or', 'comparable', 'documents', 'retrieved', 'from', 'the', 'web', 'munteanu', 'and', 'marcu', '2005', 'in', 'addition', 'machine', 'translation', 'mt', 'systems', 'can', 'be', 'improved', 'by', 'training', 'on', 'sentences', 'extracted', 'from', 'parallel', 'or', 'comparable', 'documents', 'mined', 'from', 'the', 'web', 'munteanu', 'and', 'marcu', '2005'], ['1', 'with', '2', 'regularization', 'using', 'adagrad', 'duchi', 'et', 'al', '2011', '1', 'regularization', 'using', 'adagrad', 'duchi', 'et', 'al', '2011'], ['why', 'does', 'the', 'lr', 'model', 'outperform', 'berkeley', '13', 'the', 'muc', 'vilain', 'et', 'al', '1995', 'score', 'is', 'the', 'minimum', 'number', 'of', 'links', 'between', 'mentions', 'to', 'be', 'inserted', 'or', 'deleted', 'when', 'mapping', 'the', 'output', 'to', 'a', 'gold', 'standardthe', 'muc', 'score', 'vilain', 'et', 'al', '1995', 'counts', 'the', 'minimum', 'number', 'of', 'links', 'between', 'mentions', 'to', 'be', 'inserted', 'or', 'deleted', 'when', 'mapping', 'a', 'system', 'response', 'to', 'a', 'gold', 'standard', 'key', 'set'], ['in', 'the', 'context', 'of', 'this', 'paper', 'we', 'will', 'be', 'focusing', 'on', 'the', 'subset', 'tree', 'sst', 'kernel', 'described', 'in', 'collins', 'and', 'duffy', '2002', '', 'which', 'relies', 'on', 'a', 'fragment', 'definition', 'that', 'does', 'not', 'allow', 'to', 'break', 'productionwe', 'will', 'focus', 'on', 'the', 'syntactic', 'tree', 'kernel', 'described', 'in', 'collins', 'and', 'duffy', '2002', '', 'which', 'relies', 'on', 'a', 'fragment', 'definition', 'that', 'does', 'not', 'allow', 'to', 'break', 'production', 'rules'], ['europarl', 'koehn', '2005', 'is', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentthe', 'europarl', 'corpus', 'koehn', '2005', 'is', 'built', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['we', 'have', 'used', 'foma', 'a', 'free', 'software', 'tool', 'to', 'specify', 'finitestate', 'automata', 'and', 'transducers', 'hulden', '2009', 'the', 'module', 'was', 'implemented', 'using', 'foma', 'a', 'free', 'software', 'tool', 'to', 'specify', 'finitestate', 'automata', 'and', 'transducers', 'hulden', '2009'], ['we', 'used', 'the', 'same', 'test', 'set', 'used', 'in', 'li', 'et', 'al', '2004', 'for', 'our', 'testing', '5', 'we', 'used', 'the', 'same', 'test', 'data', 'as', 'in', 'li', 'et', 'al', '2004'], ['the', 'penn', 'discourse', 'treebank', 'pdtb', 'prasad', 'et', 'al', '2008', 'is', 'a', 'large', 'corpus', 'annotated', 'with', 'discourse', 'relations', 'covering', 'the', 'wall', 'street', 'journal', 'part', 'of', 'the', 'penn', 'treebankpenn', 'discourse', 'treebank', 'the', 'penn', 'discourse', 'treebank', 'pdtb', 'is', 'a', 'corpus', 'of', 'wall', 'street', 'journal', 'articles', 'annotated', 'with', 'discourse', 'relations', 'prasad', 'et', 'al', '2008'], ['we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'a', 'gb', 'implementation', 'of', 'the', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011'], ['since', 'the', 'commonly', 'used', 'word', 'similarity', 'datasets', 'contain', 'a', 'small', 'number', 'of', 'word', 'pairs', 'we', 'also', 'use', 'the', 'men', 'dataset', 'bruni', 'et', 'al', '2012', 'of', '3000', 'word', 'pairs', 'sampled', 'from', 'words', 'that', 'occur', 'at', 'least', '700', 'times', 'in', 'a', 'large', 'web', 'corpusthe', 'second', 'is', 'the', 'men', 'dataset', 'bruni', 'et', 'al', '2012', 'of', '3000', 'words', 'pairs', 'sampled', 'from', 'words', 'that', 'occur', 'at', 'least', '700', 'times', 'in', 'a', 'large', 'web', 'corpus'], ['the', 'training', 'data', 'of', 'the', 'shared', 'task', 'is', 'the', 'nucle', 'corpus', 'dahlmeier', 'et', 'al', '2013', '', 'which', 'contains', 'essays', 'written', 'by', 'learners', 'of', 'english', 'the', 'training', 'data', 'released', 'by', 'the', 'task', 'organizers', 'comes', 'from', 'the', 'nucle', 'corpus', 'dahlmeier', 'et', 'al', '2013', '', 'which', 'contains', 'essays', 'written', 'by', 'learners', 'of', 'english', 'as', 'a', 'foreign', 'language'], ['all', 'system', 'implementation', 'was', 'done', 'using', 'python', 'and', 'the', 'opensource', 'machine', 'learning', 'toolkit', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'all', 'of', 'the', 'machine', 'learning', 'was', 'done', 'using', 'scikitlearn', 'pedregosa', 'et', 'al', '2011'], ['it', 'has', 'been', 'shown', 'that', 'a', 'diverse', 'set', 'of', 'predictions', 'can', 'be', 'used', 'to', 'help', 'improve', 'decoder', 'accuracy', 'for', 'various', 'problems', 'in', 'nlp', 'henderson', 'and', 'brill', '1999', 'it', 'has', 'been', 'long', 'identified', 'in', 'nlp', 'that', 'a', 'diverse', 'set', 'of', 'solutions', 'from', 'a', 'decoder', 'can', 'be', 'reranked', 'or', 'recombined', 'in', 'order', 'to', 'improve', 'the', 'accuracy', 'in', 'various', 'problems', 'henderson', 'and', 'brill', '1999'], ['in', 'the', '2013', 'system', 'we', 'had', 'used', 'sentistrength', 'lexicon', 'thelwall', 'et', 'al', '2010', 'in', 'our', 'system', 'we', 'used', 'the', 'sentiment', 'lexicon', 'provided', 'by', 'sentistrength', 'thelwall', 'et', 'al', '2010'], ['finally', 'we', 'also', 'compare', 'the', 'quality', 'of', 'the', 'candidate', 'phrase', 'embeddings', 'with', 'word', 'embeddings', 'dhillon', 'et', 'al', '2011', 'by', 'adding', 'them', 'as', 'features', 'in', 'a', 'crf', 'based', 'sequence', 'taggerwe', 'also', 'compared', 'the', 'quality', 'of', 'the', 'candidate', 'phrase', 'embeddings', 'with', 'the', 'wordlevel', 'embeddings', 'by', 'adding', 'them', 'as', 'features', 'dhillon', 'et', 'al', '2011', 'along', 'with', 'the', 'baseline', 'features', 'in', 'the', 'crf', 'tagger'], ['these', 'methods', 'are', 'based', 'on', 'the', 'distributional', 'hypothesis', 'which', 'states', 'that', 'words', 'appearing', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meaning', 'harris', '1954', 'corpusbased', 'vsms', 'follow', 'the', 'standard', 'distributional', 'hypothesis', 'which', 'states', 'that', 'words', 'appearing', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meaning', 'harris', '1954'], ['all', 'annotations', 'were', 'done', 'using', 'the', 'brat', 'rapid', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012', 'the', 'annotations', 'were', 'made', 'using', 'the', 'brat', 'rapid', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012'], ['compared', 'to', 'wordnet', 'fellbaum', '1998', '', 'there', 'are', 'similarities', 'but', 'also', 'significant', 'differencescompared', 'to', 'wordnet', 'fellbaum', '1998', '', 'there', 'are', 'similarities', 'as', 'well', 'as', 'considerable', 'differences'], ['for', 'training', 'we', 'use', 'adam', 'kingma', 'and', 'ba', '2015', 'for', 'optimization', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0001we', 'use', 'adam', 'kingma', 'and', 'ba', '2015', 'for', 'optimization', 'with', 'initial', 'learning', 'rate', 'of', '0001'], ['for', 'our', 'classifier', 'we', 'use', 'svms', 'specifically', 'the', 'liblinear', 'svm', 'software', 'package', 'fan', 'et', 'al', '2008', '', 'which', 'is', 'wellsuited', 'to', 'text', 'classification', 'tasks', 'with', 'large', 'numbers', 'of', 'features', 'and', 'large', 'numberspecifically', 'we', 'use', 'the', 'liblinear', 'svm', 'package', 'fan', 'et', 'al', '2008', 'as', 'it', 'is', 'wellsuited', 'to', 'text', 'classification', 'tasks', 'with', 'large', 'numbers', 'of', 'features', 'and', 'texts'], ['the', 'first', 'two', 'experiments', 'concern', 'the', 'prediction', 'of', 'the', 'sentiment', 'of', 'movie', 'reviews', 'in', 'the', 'stanford', 'sentiment', 'treebank', 'socher', 'et', 'al', '2013the', 'first', 'two', 'experiments', 'involve', 'predicting', 'the', 'sentiment', 'of', 'movie', 'reviews', 'socher', 'et', 'al', '2013'], ['we', 'used', 'only', 'the', 'nonensembled', 'lefttoright', 'run', 'ie', 'no', 'righttoleft', 'rescoring', 'as', 'done', 'by', 'sennrich', 'et', 'al', '2016a', 'with', 'beam', 'size', 'of', '5', '5', 'taking', 'just', 'the', 'singlebest', 'outputwe', 'used', 'only', 'the', 'nonensembled', 'lefttoright', 'run', 'ie', 'no', 'righttoleft', 'rescoring', 'as', 'done', 'by', 'sennrich', 'et', 'al', '2016a', 'with', 'beam', 'size', 'of', '12', 'default', 'value'], ['we', 'used', 'only', 'the', 'nonensembled', 'lefttoright', 'run', 'sennrich', 'et', 'al', '2016a', 'with', 'beam', 'size', 'of', '5', '5', 'taking', 'just', 'the', 'singlebest', 'outputwe', 'used', 'only', 'the', 'nonensembled', 'lefttoright', 'run', 'sennrich', 'et', 'al', '2016a', 'with', 'beam', 'size', 'of', '12', 'default', 'value'], ['the', 'msd', 'morphological', 'coding', 'system', 'was', 'developed', 'for', 'a', 'bunch', 'of', 'languages', 'including', 'hungarian', 'erjavec', '2004', 'the', 'msd', 'morphological', 'coding', 'system', 'is', 'a', 'positional', 'coding', 'system', 'developed', 'for', 'several', 'languages', 'erjavec', '2004'], ['the', 'phrase', 'tables', 'were', 'generated', 'by', 'means', 'of', 'symmetrised', 'word', 'alignments', 'obtained', 'with', 'giza', 'och', 'and', 'ney', '2003', 'the', 'phrase', 'table', 'was', 'generated', 'employing', 'symmetrised', 'word', 'alignments', 'obtained', 'with', 'giza', 'och', 'and', 'ney', '2003'], ['word', 'alignment', 'is', 'performed', 'using', 'giza', 'och', 'and', 'ney', '2003', 'word', 'alignment', 'is', 'performed', 'with', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'experimented', 'with', 'several', 'levels', 'of', 'cluster', 'granularity', 'using', 'development', 'data', 'and', 'following', 'koo', 'et', 'al', '2008following', 'koo', 'et', 'al', '2008', '', 'we', 'also', 'experimented', 'with', 'using', 'two', 'sets', 'of', 'cluster', 'labels', 'with', 'different', 'levels', 'of', 'granularity'], ['raghavan', 'et', 'al', '2007', 'measure', 'the', 'benefit', 'from', 'feature', 'feedback', 'as', 'the', 'gain', 'in', 'the', 'learning', 'speed', 'with', 'feature', 'feedbackraghavan', 'et', 'al', '2007', '', 'evaluate', 'benefit', 'from', 'feature', 'feedback', 'in', 'terms', 'of', 'the', 'gain', 'in', 'learning', 'speed'], ['we', 'built', 'a', 'modified', 'kneserney', 'smoothed', '5gram', 'language', 'model', 'using', 'the', 'english', 'side', 'of', 'the', 'training', 'data', 'and', 'performed', 'querying', 'with', 'kenlm', 'heafield', '2011', 'training', 'and', 'querying', 'of', 'a', 'modified', 'kneserney', 'smoothed', '5', 'gram', 'language', 'model', 'are', 'done', 'on', 'the', 'english', 'side', 'of', 'the', 'training', 'data', 'using', 'kenlm', 'heafield', '2011'], ['a', 'formal', 'pacstyle', 'analysis', 'can', 'be', 'found', 'in', 'ando', 'and', 'zhang', '2004', 'the', 'formal', 'derivation', 'can', 'be', 'found', 'in', 'ando', 'and', 'zhang', '2004'], ['the', 'first', 'model', 'we', 'introduce', 'is', 'based', 'on', 'the', 'recurrent', 'neural', 'network', 'language', 'model', 'of', 'mikolov', 'et', 'al', '2010', 'the', 'model', 'as', 'described', 'thus', 'far', 'is', 'identical', 'to', 'the', 'recurrent', 'neural', 'network', 'language', 'model', 'rnnlm', 'of', 'mikolov', 'et', 'al', '2010'], ['the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'classifier', 'experiments', 'were', 'carried', 'out', 'using', 'the', 'svmlight', 'software', 'joachims', '1999', 'available', 'at', 'httpsvmlightjoachimsorg', 'with', 'a', 'polynomial', 'kernel', '2', 'degree3the', 'classifier', 'evaluations', 'were', 'carried', 'out', 'using', 'the', 'svmlight', 'software', 'joachims', '1999', 'available', 'at', 'httpsvmlightjoachimsorg', 'with', 'the', 'default', 'linear', 'kernel', 'for', 'the', 'standard', 'feature', 'evaluation'], ['the', 'training', 'data', 'of', 'the', 'shared', 'task', 'is', 'the', 'nucle', 'corpus', 'dahlmeier', 'et', 'al', '2013', '', 'which', 'contains', 'essays', 'written', 'by', 'learners', 'of', 'englishthe', 'training', 'data', 'for', 'the', 'task', 'is', 'from', 'the', 'nucle', 'corpus', 'dahlmeier', 'et', 'al', '2013', '', 'an', 'errortagged', 'collection', 'of', 'essays', 'written', 'by', 'nonnative', 'learners', 'of', 'english'], ['saldo', 'borin', 'et', 'al', '2013', 'is', 'the', 'largest', 'freely', 'available', 'lexical', 'resource', 'for', 'swedishsaldo', 'borin', 'et', 'al', '2013', 'is', 'the', 'most', 'comprehensive', 'open', 'lexical', 'resource', 'for', 'swedish'], ['for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['however', 'those', 'stringtotree', 'systems', 'run', 'slowly', 'in', 'cubic', 'time', 'huang', 'et', 'al', '2006', 'however', 'such', 'stringtotree', 'systems', 'run', 'slowly', 'in', 'cubic', 'time', 'huang', 'et', 'al', '2006'], ['the', '5gram', 'target', 'language', 'model', 'was', 'trained', 'using', 'kenlm', 'heafield', '2011', 'we', 'trained', 'an', 'english', '5gram', 'language', 'model', 'using', 'kenlm', 'heafield', '2011'], ['for', 'all', 'experiments', 'we', 'used', 'the', 'moses', 'smt', 'system', 'koehn', 'et', 'al', '2007', 'for', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'evaluate', 'our', 'method', 'on', 'the', 'following', 'data', 'sets', 'bullet', 'ontonotesdev', 'development', 'set', 'of', 'the', 'ontonotes', 'data', 'provided', 'by', 'the', 'conll2012', 'shared', 'task', 'pradhan', 'et', 'al', '2012', 'bullet', 'ontonotestest', 'test', 'set', 'of', 'the', 'ontonotes', 'data', 'provided', 'by', 'the', 'conll2012', 'shared', 'task', 'pradhan', 'et', 'al', '2012'], ['we', 'use', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'implement', 'our', 'modelswe', 'use', 'the', 'stateoftheart', 'phrasebased', 'machine', 'translation', 'system', 'moses', 'koehn', 'et', 'al', '2007', 'to', 'perform', 'our', 'machine', 'translation', 'experiments'], ['but', 'we', 'randomly', 'selected', '90', 'of', 'the', 'training', 'data', 'used', 'in', 'li', 'et', 'al', '2004', 'as', 'our', 'training', 'data', 'and', 'the', 'remainder', 'as', 'the', 'development', 'data', 'as', 'shown', 'in', 'table', '5', 'since', 'the', 'training', 'data', 'used', 'in', 'li', 'et', 'al', '2004', 'is', 'identical', 'as', 'the', 'union', 'of', 'our', 'training', 'and', 'development', 'data', 'we', 'denoted', 'it', 'as', 'traindev', 'in', 'table', '6'], ['the', 'bleu', 'score', 'measures', 'the', 'precision', 'of', 'ngrams', 'over', 'all', 'n', 'to', '4', 'in', 'our', 'case', 'with', 'respect', 'to', 'a', 'reference', 'translation', 'with', 'a', 'penalty', 'for', 'short', 'translations', 'papineni', 'et', 'al', '2001', 'bleu', 'score', 'this', 'score', 'measures', 'the', 'precision', 'of', 'unigrams', 'bigrams', 'trigrams', 'and', 'fourgrams', 'with', 'respect', 'to', 'a', 'whole', 'set', 'of', 'reference', 'translations', 'with', 'a', 'penalty', 'for', 'too', 'short', 'sentences', 'papineni', 'et', 'al', '2001'], ['the', 'training', 'data', 'of', 'the', 'shared', 'task', 'is', 'the', 'nucle', 'corpus', 'dahlmeier', 'et', 'al', '2013', '', 'which', 'contains', 'essays', 'written', 'by', 'learners', 'of', 'englishthe', 'training', 'data', 'provided', 'for', 'the', 'task', 'is', 'a', 'subset', 'of', 'the', 'nucle', 'v23', 'corpus', 'dahlmeier', 'et', 'al', '2013', '', 'which', 'comprises', 'essays', 'written', 'in', 'english', 'by', 'students', 'at', 'the', 'national', 'university', 'of', 'singapore'], ['test', 'data', 'was', 'drawn', 'from', 'the', 'open', 'american', 'national', 'corpus', 'ide', 'and', 'suderman', '2004', '', 'oanc', 'across', 'a', 'variety', 'of', 'genres', 'and', 'from', 'both', 'the', 'spoken', 'and', 'written', 'portions', 'of', 'the', 'corpuswe', 'selected', 'the', 'dataset', 'of', 'jurgens', 'and', 'klapaftis', '2013', 'which', 'was', 'drawn', 'from', 'the', 'open', 'american', 'national', 'corpus', 'oanc', 'ide', 'and', 'suderman', '2004', 'across', 'a', 'variety', 'of', 'genres', 'and', 'from', 'both', 'the', 'spoken', 'and', 'written', 'portions', 'of', 'the', 'corpus'], ['we', 'measure', 'statistical', 'significance', 'using', '95', 'confidence', 'intervals', 'computed', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'the', 'statistical', 'significance', 'tests', 'using', '95', 'confidence', 'interval', 'are', 'measured', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004'], ['we', 'use', 'the', 'english', 'portion', 'of', 'the', 'ace', '2005', 'relation', 'extraction', 'dataset', 'walker', 'et', 'al', '2006', 'we', 'evaluate', 'our', 'relation', 'extraction', 'system', 'on', 'the', 'english', 'portion', 'of', 'the', 'ace', '2005', 'corpus', 'walker', 'et', 'al', '2006'], ['this', 'data', 'was', 'collected', 'for', 'the', '2014', 'semeval', 'competition', 'marelli', 'et', 'al', '2014', 'and', 'consists', 'of', '9927', 'sentence', 'pairs', 'with', '4500', 'for', 'training', '500', 'as', 'a', 'development', 'setsentences', 'involving', 'compositional', 'knowledge', 'sick', 'is', 'from', 'task', '1', 'of', 'the', '2014', 'semeval', 'competition', 'marelli', 'et', 'al', '2014', 'and', 'consists', 'of', '9927', 'annotated', 'sentence', 'pairs', 'with', '4500', 'for', 'training', '500', 'as', 'a', 'development', 'set'], ['the', 'parsing', 'model', 'used', 'for', 'intrasentential', 'parsing', 'is', 'a', 'dynamic', 'conditional', 'random', 'field', 'dcrf', 'sutton', 'et', 'al', '2007', 'shown', 'in', 'figure', '7', 'our', 'novel', 'parsing', 'model', 'is', 'the', 'dynamic', 'conditional', 'random', 'field', 'dcrf', 'sutton', 'et', 'al', '2007', 'shown', 'in', 'figure', '2'], ['latent', 'dirichlet', 'allocation', 'lda', 'is', 'a', 'generative', 'model', 'which', 'considers', 'a', 'document', 'model', 'salton', '1989', 'as', 'a', 'mixture', 'probability', 'of', 'latent', 'topics', 'combination', 'of', 'latent', 'topics', 'ent', 'dirichlet', 'allocation', 'lda', 'is', 'a', 'generative', 'model', 'which', 'considers', 'a', 'document', 'model', 'seen', 'as', 'a', 'bag', 'of', 'words', 'salton', '1989', 'as', 'a', 'mixture', 'probability', 'of', 'latent', 'topics'], ['distributional', 'hypothesis', 'theory', 'harris', '1954', 'indicates', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'context', 'tend', 'to', 'have', 'similar', 'meaningsit', 'therefore', 'follows', 'the', 'distributional', 'hypothesis', 'harris', '1954', 'which', 'states', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meanings'], ['distributional', 'hypothesis', 'theory', 'harris', '1954', 'indicates', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'context', 'tend', 'to', 'have', 'similar', 'meaningsit', 'therefore', 'follows', 'the', 'distributional', 'hypothesis', 'harris', '1954', 'which', 'states', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meanings'], ['in', '2009', 'yefang', 'wang', 'wang', 'et', 'al', '2009', 'used', 'cascading', 'classifiers', 'on', 'manually', 'annotated', 'data', 'which', 'fetched', 'fscore', 'of', '0832in', '2009', 'yefang', 'wang', 'wang', 'et', 'al', '2009', 'used', 'cascading', 'classifiers', 'on', 'manually', 'annotated', 'data', 'and', 'achieved', 'around', '832', 'accuracy'], ['we', 'built', 'a', '5gram', 'language', 'model', 'on', 'the', 'english', 'side', 'of', 'qcatrain', 'using', 'kenlm', 'heafield', '2011', 'we', 'built', 'a', 'modified', 'kneserney', 'smoothed', '5gram', 'language', 'model', 'using', 'the', 'english', 'side', 'of', 'the', 'training', 'data', 'and', 'performed', 'querying', 'with', 'kenlm', 'heafield', '2011'], ['the', 'remaining', 'three', 'models', 'are', 'all', 'naive', 'bayes', 'classifiers', 'trained', 'on', 'the', 'google', 'web', '1t', '5gram', 'corpus', 'henceforth', 'google', 'corpus', '', 'brants', 'and', 'franz', '2006', 'the', 'other', 'models', 'are', 'trained', 'on', 'native', 'english', 'data', 'the', 'google', 'web', '1t', '5gram', 'corpus', 'henceforth', 'google', 'brants', 'and', 'franz', '2006', '', 'with', 'the', 'naive', 'bayes', 'nb', 'algorithm'], ['we', 'apply', 'bootstrapping', 'kozareva', 'et', 'al', '2008', 'on', 'the', 'word', 'graphs', 'by', 'manually', 'selecting', '10', 'seeds', 'for', 'concrete', 'and', 'abstract', 'words', 'see', 'table', '10', 'we', 'then', 'apply', 'bootstrapping', 'kozareva', 'et', 'al', '2008', 'on', 'the', 'noun', 'and', 'adjective', 'graphs', 'by', 'selecting', '10', 'seeds', 'for', 'visual', 'and', 'nonvisual', 'nouns', 'and', 'adjectives', 'see', 'table', '1'], ['these', 'methods', 'are', 'based', 'on', 'the', 'distributional', 'hypothesis', 'which', 'states', 'that', 'words', 'appearing', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meaning', 'harris', '1954', 'it', 'therefore', 'follows', 'the', 'distributional', 'hypothesis', 'harris', '1954', 'which', 'states', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meanings'], ['these', 'results', 'verify', 'the', 'benefit', 'of', 'using', 'ltag', 'based', 'features', 'and', 'confirm', 'the', 'hypothesis', 'that', 'ltag', 'based', 'features', 'provide', 'a', 'novel', 'set', 'of', 'abstract', 'features', 'that', 'complement', 'the', 'hand', 'selected', 'features', 'collins', '2000', 'our', 'hypothesis', 'is', 'that', 'the', 'ltag', 'based', 'features', 'provide', 'a', 'novel', 'set', 'of', 'abstract', 'features', 'that', 'complement', 'the', 'hand', 'selected', 'features', 'from', 'collins', '2000', 'and', 'the', 'ltag', 'based', 'features', 'will', 'help', 'improve'], ['rg65', 'rubenstein', 'and', 'goodenough', '1965', 'has', '65', 'word', 'pairsrg65', 'rubenstein', 'and', 'goodenough', '1965', 'is', 'set', 'of', '65', 'word', 'pairs'], ['the', 'significance', 'tests', 'were', 'performed', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004', 'statistical', 'significance', 'tests', 'are', 'performed', 'using', 'bootstrap', 'resampling', 'koehn', '2004'], ['the', 'default', 'phrasal', 'search', 'algorithm', 'is', 'cube', 'pruning', 'huang', 'and', 'chiang', '2007', 'the', 'search', 'is', 'typically', 'carried', 'out', 'using', 'the', 'cube', 'pruning', 'algorithm', 'huang', 'and', 'chiang', '2007'], ['indomain', 'data', 'is', 'mainly', 'used', 'to', 'solve', 'the', 'problem', 'of', 'data', 'sparseness', 'sun', 'and', 'xu', '2011', 'indomain', 'data', 'only', 'solves', 'the', 'problem', 'of', 'data', 'sparseness', 'sun', 'and', 'xu', '2011'], ['all', 'experiments', 'were', 'carried', 'out', 'using', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'all', 'the', 'experiments', 'are', 'carried', 'out', 'in', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['first', 'we', 'apply', 'heuristics', 'to', 'determine', 'number', 'and', 'gender', 'based', 'on', 'word', 'lists', 'wordnet', 'miller', '1990', 'and', 'partofspeech', 'tagswe', 'then', 'apply', 'heuristics', 'to', 'determine', 'number', 'and', 'gender', 'for', 'the', 'characters', 'based', 'on', 'word', 'lists', 'word', 'net', 'miller', '1990', 'and', 'pos', 'tags'], ['rank', 'svm', 'joachims', '2002', 'is', 'a', 'method', 'based', 'on', 'support', 'vector', 'machines', 'svms', 'for', 'which', 'we', 'use', 'only', 'linear', 'kernels', 'to', 'keep', 'complexity', 'lowfor', 'this', 'task', 'we', 'use', 'ranksvm', 'joachims', '2002', 'which', 'is', 'a', 'method', 'based', 'on', 'support', 'vector', 'machines', 'svms'], ['a', 'more', 'detailed', 'description', 'of', 'the', 'task', 'can', 'be', 'found', 'in', 'nakov', 'et', 'al', '2017', 'a', 'precise', 'description', 'of', 'the', 'corpus', 'and', 'metrics', 'can', 'be', 'found', 'in', 'task3', 'description', 'paper', 'nakov', 'et', 'al', '2017'], ['a', 'tree', 'kernel', 'function', 'is', 'a', 'convolution', 'kernel', 'haussler', '1999', 'defined', 'over', 'pairs', 'of', 'treestree', 'kernel', 'tk', 'functions', 'are', 'convolution', 'kernels', 'haussler', '1999', 'defined', 'over', 'pairs', 'of', 'trees'], ['we', 'build', 'upon', 'our', 'previous', 'approach', 'for', 'joint', 'concept', 'disambiguation', 'and', 'clustering', 'fahrni', 'and', 'strube', '2012', 'for', 'disambiguation', 'and', 'clustering', 'we', 'build', 'upon', 'our', 'previous', 'work', 'fahrni', 'and', 'strube', '2012'], ['rouge2', 'metric', 'lin', '2004', 'is', 'used', 'for', 'the', 'evaluation', 'we', 'used', 'the', 'rouge1', 'evaluation', 'metric', 'lin', '2004'], ['we', 'used', 'mallet', 'software', 'mccallum', '2002', 'for', 'crf', 'experimentswe', 'used', 'mallet', 'toolkit', 'mccallum', '2002', 'for', 'crf', 'implementation'], ['we', 'exploit', 'a', 'transitionbased', 'framework', 'with', 'global', 'learning', 'and', 'beamsearch', 'decoding', 'to', 'implement', 'the', 'joint', 'model', 'zhang', 'and', 'clark', '2011', 'our', 'joint', 'parsing', 'model', 'exploits', 'a', 'transitionbased', 'framework', 'with', 'global', 'learning', 'and', 'beamsearch', 'decoding', 'zhang', 'and', 'clark', '2011', '', 'extended', 'from', 'a', 'arcstandard', 'transitionbased', 'parsing', 'model'], ['the', 'annotation', 'was', 'performed', 'using', 'the', 'brat', '2', 'tool', 'stenetorp', 'et', 'al', '2012', 'the', 'annotations', 'were', 'made', 'using', 'the', 'brat', 'rapid', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012'], ['for', 'building', 'the', 'word', 'alignment', 'models', 'we', 'use', 'mgiza', 'gao', 'and', 'vogel', '2008', 'to', 'build', 'the', 'word', 'alignment', 'models', 'we', 'used', 'the', 'mgiza', 'package', 'gao', 'and', 'vogel', '2008'], ['the', 'reliability', 'of', 'the', 'annotation', 'was', 'evaluated', 'using', 'the', 'kappa', 'statistic', 'carletta', '1996', 'we', 'evaluated', 'annotation', 'reliability', 'by', 'using', 'the', 'kappa', 'statistic', 'carletta', '1996'], ['it', 'therefore', 'follows', 'the', 'distributional', 'hypothesis', 'harris', '1954', 'which', 'states', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meaningsdistributional', 'models', 'of', 'meaning', 'follow', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'which', 'states', 'that', 'two', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'have', 'similar', 'meanings'], ['this', 'dataset', 'is', 'composed', 'of', '35', 'triplets', 'of', 'sentences', 'from', 'the', 'eyetracking', 'experiment', 'experiment', '1', 'in', 'traxler', 'et', 'al', '2002', '', 'for', 'a', 'total', 'of', '105', 'sentencesthis', 'dataset', 'is', 'composed', 'of', '32', 'sentence', 'quadruplets', 'from', 'experiments', '2', 'eyetracking', 'and', '3', 'selfpaced', 'reading', 'in', 'traxler', 'et', 'al', '2002', '', 'for', 'a', 'total', 'of', '120', 'sentences'], ['the', 'significance', 'tests', 'were', 'performed', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004', 'a', 'statistical', 'significance', 'test', 'was', 'performed', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004'], ['we', 'use', 'the', 'standard', 'alignment', 'tool', 'giza', 'och', 'and', 'ney', '2003', 'to', 'word', 'align', 'the', 'parallel', 'datawe', 'use', 'the', 'giza', 'tool', 'och', 'and', 'ney', '2003', 'to', 'align', 'words', 'in', 'our', 'parallel', 'corpora'], ['we', 'use', 'the', 'standard', 'alignment', 'tool', 'giza', 'och', 'and', 'ney', '2003', 'to', 'word', 'align', 'the', 'parallel', 'datawe', 'use', 'the', 'giza', 'tool', 'och', 'and', 'ney', '2003', 'to', 'align', 'words', 'in', 'our', 'parallel', 'corpora'], ['the', 'kernels', 'are', 'combined', 'using', 'gaussian', 'process', 'regression', 'gpr', 'rasmussen', 'and', 'williams', '2006', 'gaussian', 'process', 'regression', 'gpr', 'rasmussen', 'and', 'williams', '2006'], ['to', 'overcome', 'this', 'agirre', 'et', 'al', '2009', 'used', 'mapreduce', 'infrastructure', 'with', '2000', 'cores', 'to', 'compute', 'pairwise', 'similarities', 'of', 'words', 'on', 'a', 'corpus', 'of', 'roughly', '16', 'terawordsin', 'another', 'work', 'a', 'corpus', 'of', 'roughly', '16', 'terawords', 'was', 'used', 'by', 'agirre', 'et', 'al', '2009', '', 'to', 'compute', 'pairwise', 'similarities', 'of', 'the', 'words', 'in', 'the', 'test', 'sets', 'using', 'the', 'mapreduce', 'infrastructure', 'on', '2000', 'cores'], ['we', 'measure', 'statistical', 'significance', 'using', '95', 'confidence', 'intervals', 'computed', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'we', 'calculated', 'significance', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004'], ['we', 'use', 'the', 'stanford', 'dependency', 'parser', 'chen', 'and', 'manning', '2014', 'at', 'this', 'stage', 'and', 'have', 'not', 'experimented', 'with', 'alternatives', 'in', 'this', 'work', 'we', 'use', 'the', 'stanford', 'neural', 'dependency', 'parser', 'chen', 'and', 'manning', '2014'], ['next', 'a', 'tweet', 'was', 'tokenized', 'and', 'fed', 'into', 'madamira', 'pasha', 'et', 'al', '2014', '', 'a', 'morphological', 'analysis', 'tool', 'for', 'arabic', 'textmadamira', 'pasha', 'et', 'al', '2014', 'is', 'a', 'morphological', 'analysis', 'and', 'disambiguation', 'tool', 'of', 'arabic'], ['yarowsky', '1995', 'has', 'proposed', 'a', 'bootstrapping', 'method', 'for', 'word', 'sense', 'disambiguation', 'yarowsky', '1995', 'proposed', 'such', 'a', 'method', 'for', 'word', 'sense', 'disambiguation', 'which', 'we', 'refer', 'to', 'as', 'monolingual', 'bootstrapping'], ['we', 'also', 'list', 'the', 'previous', 'stateoftheart', 'performance', 'from', 'a', 'conventional', 'smt', 'system', 'durrani', 'et', 'al', '2014', 'with', 'the', 'bleu', 'of', '370we', 'also', 'list', 'the', 'results', 'from', 'smt', 'model', 'durrani', 'et', 'al', '2014', 'as', 'a', 'comparison'], ['we', 'used', 'adam', 'kingma', 'and', 'ba', '2014', 'with', 'a', 'learning', 'rate', 'of', '00002we', 'use', 'the', 'adam', 'kingma', 'and', 'ba', '2014', 'algorithm', 'to', 'minimize', 'the', 'sum', 'of', 'the', 'loss', 'for', 'p', 'asv', 'and', 'p', 'af', '', 'with', 'a', 'learning', 'rate', 'of', '10'], ['distributional', 'semantics', 'is', 'based', 'on', 'the', 'idea', 'that', 'firth', '1957', 'in', 'other', 'words', 'the', 'meaning', 'of', 'a', 'word', 'is', 'related', 'to', 'the', 'contexts', 'it', 'appears', 'inword', 'cooccurence', 'statistics', 'you', 'shall', 'know', 'a', 'word', 'by', 'the', 'company', 'it', 'keeps', 'firth', '1957'], ['in', 'order', 'to', 'estimate', 'the', 'basic', 'lexical', 'similarity', 'function', 'employed', 'in', 'the', 'sum', 'ssc', 'and', 'sptk', 'operators', 'a', 'cooccurrence', 'word', 'space', 'is', 'acquired', 'through', 'the', 'distributional', 'analysis', 'of', 'the', 'ukwac', 'corpus', 'baroni', 'et', 'al', '2009', 'the', 'cooccurrence', 'word', 'space', 'is', 'acquired', 'through', 'the', 'distributional', 'analysis', 'of', 'the', 'ukwac', 'corpus', 'baroni', 'et', 'al', '2009'], ['we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'the', 'scikitlearn', 'toolkit', 'to', 'train', 'our', 'classifiers', 'pedregosa', 'et', 'al', '2011'], ['the', 'translation', 'model', 'was', 'trained', 'by', 'giza', 'och', 'and', 'ney', '2003', '', 'and', 'the', 'trigram', 'was', 'trained', 'by', 'the', 'cmucambridge', 'statistical', 'language', 'modeling', 'toolkit', 'v2', 'clarkson', 'and', 'rosenfeld', '1997', 'the', 'probability', 'pe', 'is', 'computed', 'using', 'a', 'simple', 'trigram', 'language', 'model', 'that', 'was', 'trained', 'using', 'the', 'cmu', 'language', 'modeling', 'toolkit', 'clarkson', 'and', 'rosenfeld', '1997'], ['this', 'is', 'a', 'generalization', 'of', 'the', 'operator', 'id', 'in', 'kaplan', 'and', 'kay', '1994', 'this', 'is', 'similar', 'to', 'the', 'operator', 'intro', 'in', 'kaplan', 'and', 'kay', '1994'], ['we', 'used', 'standard', 'classifiers', 'available', 'in', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'a', 'gb', 'implementation', 'of', 'the', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011'], ['we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011', 'for', 'the', 'linear', 'logistic', 'regression', 'implementation', 'we', 'used', 'scikitlearn', 'pedregosa', 'et', 'al', '2011'], ['for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'finally', 'we', 'used', 'moses', 'toolkit', 'as', 'phrasebased', 'reference', 'koehn', 'et', 'al', '2007'], ['4', 'word', 'alignments', 'are', 'created', 'by', 'aligning', 'the', 'data', 'in', 'both', 'directions', 'with', 'giza', '5', 'and', 'symmetrizing', 'the', 'two', 'trained', 'alignments', 'och', 'and', 'ney', '2003', 'baseline', 'word', 'alignments', 'were', 'obtained', 'by', 'running', 'giza', 'in', 'both', 'directions', 'and', 'symmetrizing', 'using', 'the', 'growdiagfinaland', 'heuristic', 'och', 'and', 'ney', '2003'], ['the', 'perplexity', 'achieved', 'by', 'the', '6', 'gram', 'nn', 'lm', 'in', 'the', 'spanish', 'newstest08', 'development', 'set', 'was', '116', 'versus', '94', 'obtained', 'with', 'a', 'standard', '6gram', 'language', 'model', 'with', 'interpolation', 'and', 'kneserney', 'smoothing', 'kneser', 'and', 'ney', '1995', 'the', 'language', 'model', 'is', 'a', '5gram', 'with', 'interpolation', 'and', 'kneserney', 'smoothing', 'kneser', 'and', 'ney', '1995'], ['we', 'used', 'standard', 'classifiers', 'available', 'in', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'the', 'scikitlearn', 'toolkit', 'to', 'train', 'our', 'classifiers', 'pedregosa', 'et', 'al', '2011'], ['statistical', 'machine', 'translation', 'is', 'typically', 'performed', 'using', 'phrasebased', 'systems', 'koehn', 'et', 'al', '2007', 'it', 'is', 'a', 'standard', 'phrasebased', 'machine', 'translation', 'model', 'koehn', 'et', 'al', '2007'], ['wordnet', 'miller', 'et', 'al', '1990', 'is', 'an', 'online', 'hierarchical', 'lexical', 'database', 'which', 'contains', 'semantic', 'information', 'about', 'english', 'wordsthe', 'wordnet', 'online', 'lexical', 'database', 'miller', 'et', 'al', '1990'], ['we', 'use', 'the', 'scikit', 'implementation', 'of', 'random', 'forest', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'a', 'gb', 'implementation', 'of', 'the', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011'], ['we', 'lemmatise', 'the', 'head', 'of', 'each', 'constituent', 'with', 'treetagger', 'schmid', '1994', 'we', 'used', 'the', 'stuttgart', 'treetagger', 'schmid', '1994', 'to', 'lemmatise', 'constituent', 'heads'], ['our', 'text', 'processing', 'uses', 'the', 'natural', 'language', 'toolkit', 'nltk', 'bird', 'et', 'al', '2009', 'partofspeech', 'tagging', 'was', 'accomplished', 'using', 'the', 'natural', 'language', 'toolkit', 'nltk', 'bird', 'et', 'al', '2009'], ['they', 'used', 'the', 'webbased', 'annotation', 'tool', 'brat', 'stenetorp', 'et', 'al', '2012', 'for', 'the', 'annotation', 'the', 'annotation', 'was', 'performed', 'manually', 'using', 'the', 'brat', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012'], ['our', 'system', 'participated', 'in', 'semeval2013', 'task', '2', 'sentiment', 'analysis', 'in', 'twitter', 'wilson', 'et', 'al', '2013', 'we', 'participated', 'in', 'both', 'subtask', 'a', 'and', 'b', 'of', 'semeval2013', 'task', '2', 'sentiment', 'analysis', 'in', 'twitter', 'wilson', 'et', 'al', '2013', 'with', 'an', 'adaptation', 'of', 'our', 'existing', 'system'], ['the', 'english', 'text', 'was', 'tokenized', 'using', 'the', 'word', 'tokenize', 'routine', 'from', 'nltk', 'bird', 'et', 'al', '2009', 'we', 'tokenise', 'the', 'text', 'using', 'the', 'default', 'tokeniser', 'from', 'nltk', 'bird', 'et', 'al', '2009'], ['the', 'webpages', 'were', 'parsed', 'using', 'the', 'stanford', 'corenlp', 'software', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['statistical', 'machine', 'translation', 'is', 'typically', 'performed', 'using', 'phrasebased', 'systems', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'english', 'side', 'was', 'tokenized', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'trained', 'an', 'english', '5gram', 'language', 'model', 'using', 'kenlm', 'heafield', '2011', 'we', 'built', 'a', 'trigram', 'language', 'model', 'with', 'kneserney', 'smoothing', 'using', 'kenlm', 'toolkit', 'heafield', '2011'], ['all', 'of', 'the', 'text', 'data', 'from', 'reddit', 'was', 'tokenized', 'using', 'the', 'nltk', 'tokenizer', 'bird', 'et', 'al', '2009', 'we', 'tokenise', 'the', 'text', 'using', 'the', 'default', 'tokeniser', 'from', 'nltk', 'bird', 'et', 'al', '2009'], ['our', 'machine', 'translation', 'systems', 'are', 'trained', 'using', 'moses', '3', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'build', 'upon', 'our', 'previous', 'approach', 'for', 'joint', 'concept', 'disambiguation', 'and', 'clustering', 'fahrni', 'and', 'strube', '2012', 'scopeignorant', 'disambig', 'our', 'previous', 'mlnbased', 'approach', 'for', 'concept', 'disambiguation', 'fahrni', 'and', 'strube', '2012'], ['we', 'used', 'adam', 'kingma', 'and', 'ba', '2014', 'with', 'a', 'learning', 'rate', 'of', '00002', 'we', 'use', 'adam', 'kingma', 'and', 'ba', '2014', 'with', 'a', 'learning', 'rate', 'of', '4e4', 'and', 'a', 'batch', 'size', 'of', '32'], ['for', 'all', 'experiments', 'we', 'used', 'the', 'moses', 'smt', 'system', 'koehn', 'et', 'al', '2007', 'for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'first', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'statistical', 'machine', 'translation'], ['for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'develop', 'translation', 'models', 'using', 'the', 'phrasebased', 'moses', 'koehn', 'et', 'al', '2007', 'smt', 'systemwe', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'tnt', 'brants', '2000', '', 'trained', 'on', 'the', 'negra', 'training', 'setwe', 'employed', 'the', 'tnt', 'tagger', 'brants', '2000', 'which', 'was', 'trained', 'on', 'the', 'spective', 'conll', 'training', 'data'], ['the', 'webpages', 'were', 'parsed', 'using', 'the', 'stanford', 'corenlp', 'software', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['we', 'develop', 'translation', 'models', 'using', 'the', 'phrasebased', 'moses', 'koehn', 'et', 'al', '2007', 'smt', 'systemwe', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'term', 'frequency', 'count', 'is', 'normalized', 'with', 'the', 'inverse', 'document', 'frequency', 'in', 'the', 'test', 'collection', 'salton', 'and', 'buckley', '1988', 'tfidf', 'is', 'a', 'standard', 'statistical', 'method', 'that', 'combines', 'the', 'frequency', 'of', 'a', 'term', 'in', 'a', 'particular', 'document', 'with', 'its', 'inverse', 'document', 'frequency', 'in', 'general', 'use', 'salton', 'and', 'buckley', '1988'], ['we', 'assessed', 'the', 'statistical', 'significance', 'of', 'differences', 'in', 'score', 'with', 'an', 'approximate', 'randomization', 'test', '8', 'noreen', '1989', '', 'indicating', 'a', 'significant', 'impact', 'in', 'bold', 'fontwe', 'assess', 'statistical', 'significance', 'of', 'the', 'difference', 'in', 'f', '1', 'score', 'for', 'two', 'approaches', 'via', 'an', 'approximate', 'randomization', 'test', 'noreen', '1989'], ['a', 'framework', 'for', 'human', 'error', 'analysis', 'and', 'error', 'classification', 'has', 'been', 'proposed', 'in', 'vilar', 'et', 'al', '2006', 'and', 'a', 'detailed', 'analysis', 'of', 'the', 'obtained', 'results', 'has', 'been', 'carried', 'outa', 'framework', 'for', 'human', 'error', 'analysis', 'has', 'been', 'proposed', 'in', 'vilar', 'et', 'al', '2006', '', 'but', 'as', 'every', 'human', 'evaluation', 'this', 'is', 'also', 'a', 'time', 'consuming', 'task'], ['for', 'example', 'chang', 'et', 'al', '2009', 'found', 'that', 'the', 'probability', 'of', 'heldout', 'documents', 'is', 'not', 'always', 'a', 'good', 'predictor', 'of', 'human', 'judgmentschang', 'et', 'al', '2009', 'stated', 'that', 'one', 'reason', 'is', 'that', 'the', 'objective', 'function', 'of', 'topic', 'models', 'does', 'not', 'always', 'correlate', 'well', 'with', 'human', 'judgments'], ['on', 'the', 'chinese', 'side', 'we', 'used', 'the', 'morphological', 'analyzer', 'described', 'in', 'kruengkrai', 'et', 'al', '2009', 'trained', 'on', 'the', 'training', 'data', 'of', 'ctb', 'tp', 'to', 'perform', 'word', 'segmentation', 'and', 'pos', 'tagging', 'and', 'used', 'the', 'firstthe', 'mma', 'system', 'kruengkrai', 'et', 'al', '2009', 'trained', 'on', 'the', 'training', 'data', 'was', 'used', 'to', 'perform', 'word', 'segmentation', 'and', 'tagging', 'and', 'the', 'baseline', 'parser', 'was', 'used', 'to', 'parse', 'the', 'sentences', 'in', 'the', 'gigaword', 'corpus'], ['conducted', 'using', 'the', 'moses', 'phrasebased', 'decoder', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['conducted', 'using', 'the', 'moses', 'phrasebased', 'decoder', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'conducted', 'using', 'the', 'moses', 'phrasebased', 'decoder', 'koehn', 'et', 'al', '2007'], ['then', 'the', 'processed', 'data', 'was', 'performed', 'for', 'tokenization', 'pos', 'tagging', 'parsing', 'stemming', 'and', 'lemmatization', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['we', 'applied', 'bootstrap', 'resampling', 'koehn', '2004', 'to', 'measure', 'statistical', 'significance', '', 'p', '', '005', 'of', 'our', 'models', 'compared', 'to', 'a', 'baselinewe', 'measure', 'significance', 'of', 'results', 'using', 'bootstrap', 'resampling', 'at', 'p', '', '005', 'koehn', '2004'], ['this', 'system', 'uses', 'the', 'attentional', 'encoderdecoder', 'architecture', 'described', 'by', 'bahdanau', 'et', 'al', '2015', '', 'building', 'on', 'work', 'by', 'sutskever', 'et', 'al', '2014', 'we', 'followed', 'the', 'encoderdecoder', 'architecture', 'with', 'attention', 'proposed', 'by', 'bahdanau', 'et', 'al', '2015'], ['the', 'language', 'model', 'is', 'a', '5gram', 'kenlm', 'heafield', '2011', 'model', 'trained', 'using', 'lmplz', 'with', 'modified', 'kneserney', 'smoothing', 'and', 'no', 'pruningwe', 'built', 'a', 'trigram', 'language', 'model', 'with', 'kneserney', 'smoothing', 'using', 'kenlm', 'toolkit', 'heafield', '2011'], ['weighted', 'finite', 'state', 'transducers', 'fsts', 'used', 'in', 'our', 'model', 'are', 'constructed', 'with', 'openfst', 'allauzen', 'et', 'al', '2007', 'the', 'decoder', 'is', 'implemented', 'with', 'weighted', 'finite', 'state', 'transducers', 'wfsts', 'using', 'standard', 'operations', 'available', 'in', 'the', 'openfst', 'libraries', 'allauzen', 'et', 'al', '2007'], ['weighted', 'finite', 'state', 'transducers', 'fsts', 'used', 'in', 'our', 'model', 'are', 'constructed', 'with', 'openfst', 'allauzen', 'et', 'al', '2007', 'the', 'decoder', 'is', 'implemented', 'with', 'weighted', 'finite', 'state', 'transducers', 'wfsts', 'using', 'standard', 'operations', 'available', 'in', 'the', 'openfst', 'libraries', 'allauzen', 'et', 'al', '2007'], ['then', 'the', 'processed', 'data', 'was', 'performed', 'for', 'tokenization', 'pos', 'tagging', 'parsing', 'stemming', 'and', 'lemmatization', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['it', 'is', 'a', 'modification', 'of', 'the', 'model', 'proposed', 'by', 'mintz', 'et', 'al', '2009', 'our', 'first', 'baseline', 'is', 'mi09', 'a', 'distantly', 'supervised', 'classifier', 'based', 'on', 'the', 'work', 'of', 'mintz', 'et', 'al', '2009'], ['we', 'use', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', '', 'the', 'machine', 'learning', 'library', 'for', 'python', 'for', 'implementing', 'the', 'different', 'approacheswe', 'used', 'the', 'scikitlearn', 'machine', 'learning', 'library', 'pedregosa', 'et', 'al', '2011', 'for', 'both', 'implementing', 'our', 'classification', 'models', 'and', 'performing', 'statistical', 'feature', 'selection'], ['we', 'use', 'the', 'universal', 'pos', 'tagset', 'upos', 'of', 'petrov', 'et', 'al', '2012', 'this', 'in', 'turn', 'relies', 'on', 'the', 'same', 'underlying', 'feature', 'model', 'typically', 'drawing', 'from', 'a', 'shared', 'partofspeech', 'pos', 'representation', 'such', 'as', 'the', 'universal', 'pos', 'tagset', 'of', 'petrov', 'et', 'al', '2012'], ['we', 'use', 'the', 'universal', 'pos', 'tagset', 'upos', 'of', 'petrov', 'et', 'al', '2012', 'this', 'in', 'turn', 'relies', 'on', 'the', 'same', 'underlying', 'feature', 'model', 'typically', 'drawing', 'from', 'a', 'shared', 'partofspeech', 'pos', 'representation', 'such', 'as', 'the', 'universal', 'pos', 'tagset', 'of', 'petrov', 'et', 'al', '2012'], ['the', 'crf', 'is', 'trained', 'using', 'decisions', 'from', 'the', 'following', 'underlying', 'components', 'bullet', 'madamira', 'is', 'a', 'publicly', 'available', 'tool', 'for', 'morphological', 'analysis', 'and', 'disambiguation', 'of', 'eda', 'and', 'msa', 'text', 'pasha', 'et', 'al', '2014madamira', 'pasha', 'et', 'al', '2014', 'is', 'a', 'morphological', 'analysis', 'and', 'disambiguation', 'tool', 'of', 'arabic'], ['for', 'language', 'modeling', 'we', 'trained', 'a', 'separate', '5gram', 'kneserney', 'smoothed', 'lm', 'model', 'on', 'the', 'target', 'ie', 'english', 'side', 'of', 'the', 'training', 'bitext', 'using', 'kenlm', 'heafield', '2011', 'we', 'built', 'a', 'modified', 'kneser', 'ney', 'smoothed', '5gram', 'language', 'model', 'using', 'the', 'english', 'side', 'of', 'the', 'training', 'data', 'and', 'performed', 'querying', 'with', 'kenlm', 'heafield', '2011', '7'], ['with', 'the', 'training', 'script', 'of', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'preprocessed', 'the', 'training', 'corpora', 'with', 'scripts', 'included', 'in', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['the', 'word', 'alignment', 'was', 'trained', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'configuration', 'growdiagfinaland', 'alignment', 'symmetrization', 'methodwe', 'performed', 'word', 'alignment', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'default', 'growdiagfinaland', 'alignment', 'symmetrization', 'method'], ['we', 'use', 'linear', 'svms', 'from', 'liblinear', 'and', 'svms', 'with', 'rbf', 'kernel', 'from', 'libsvm', 'chang', 'and', 'lin', '2011', 'as', 'our', 'learner', 'we', 'use', 'libsvm', 'with', 'a', 'linear', 'kernel', 'chang', 'and', 'lin', '2011'], ['we', 'specify', 'the', 'hierarchical', 'aligner', 'in', 'terms', 'of', 'a', 'deduction', 'system', 'shieber', 'et', 'al', '1995', 'we', 'specify', 'our', 'dynamic', 'programming', 'algorithm', 'as', 'a', 'deduction', 'system', 'shieber', 'et', 'al', '1995'], ['we', 'use', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parse', 'trees', 'and', 'dependency', 'relationswe', 'use', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'for', 'extracting', 'dependency', 'path', 'and', 'partofspeech', 'features'], ['in', 'this', 'work', 'we', 'use', 'the', 'stanford', 'neural', 'dependency', 'parser', 'chen', 'and', 'manning', '2014', 'in', 'order', 'to', 'detect', 'the', 'object', 'pronouns', 'we', 'employ', 'stanford', 'parser', 'chen', 'and', 'manning', '2014'], ['we', 'use', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parse', 'trees', 'and', 'dependency', 'relationswe', 'also', 'obtain', 'the', 'dependency', 'parse', 'of', 'the', 'sentences', 'using', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006'], ['the', 'learning', 'algorithm', 'used', 'in', 'our', 'coreference', 'engine', 'is', 'c45', 'quinlan', '1993', 'the', 'question', 'classifier', 'used', 'in', 'the', 'experiments', 'is', 'the', 'c45', 'decision', 'tree', 'classifier', 'quinlan', '1993'], ['we', 'use', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parse', 'trees', 'and', 'dependency', 'relationswe', 'use', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'for', 'extracting', 'dependency', 'path', 'and', 'partofspeech', 'features'], ['distributional', 'semantics', 'see', 'cohen', 'and', 'widdows', '2009', 'for', 'an', 'overview', 'is', 'based', 'on', 'the', 'observation', 'that', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'tend', 'to', 'be', 'semantically', 'related', 'harris', '1954', 'the', 'former', 'that', 'is', 'the', 'most', 'popular', 'relies', 'on', 'the', 'distributional', 'hypothesis', 'that', 'puts', 'forward', 'the', 'idea', 'that', 'words', 'with', 'similar', 'meaning', 'tend', 'to', 'occur', 'in', 'similar', 'contexts', 'harris', '1954'], ['their', 'work', 'is', 'part', 'of', 'the', 'stateoftheart', 'arabic', 'morphological', 'tagger', 'madamira', 'pasha', 'et', 'al', '2014', 'madamira', 'pasha', 'et', 'al', '2014', 'is', 'a', 'morphological', 'analysis', 'and', 'disambiguation', 'tool', 'of', 'arabic'], ['we', 'use', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parse', 'trees', 'and', 'dependency', 'relationswe', 'also', 'obtain', 'the', 'dependency', 'parse', 'of', 'the', 'sentences', 'using', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006'], ['phrasal', 'follows', 'the', 'loglinear', 'approach', 'to', 'phrasebased', 'translation', 'och', 'and', 'ney', '2004', 'in', 'which', 'the', 'decision', 'rule', 'has', 'the', 'familiar', 'linear', 'form', '', 'arg', 'max', 'e', 'w', 'e', 'f', '1the', 'loglinear', 'approach', 'to', 'phrasebased', 'translation', 'och', 'and', 'ney', '2004', 'directly', 'models', 'the', 'predictive', 'translation', 'distribution', 'pef', '', 'w', '', '1', 'zf', 'exp', 'w', 'e', 'f', '1', 'where', 'e', 'is', 'the', 'target', 'string'], ['training', 'data', 'are', 'based', 'on', 'a', 'concatenation', 'of', '18', 'postagged', 'english', 'corpora', '2', 'from', 'the', 'childes', 'database', 'macwhinney', '2000', 'both', 'cds', 'corpora', 'are', 'available', 'from', 'the', 'childes', 'database', 'macwhinney', '2000'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['5gram', 'language', 'models', 'of', 'turkish', 'and', 'english', 'were', 'trained', 'using', 'kenlm', 'heafield', '2011', 'we', 'built', 'a', '5gram', 'language', 'model', 'on', 'the', 'english', 'side', 'of', 'qcatrain', 'using', 'kenlm', 'heafield', '2011'], ['we', 'used', 'the', 'relation', 'classification', 'dataset', 'of', 'the', 'semeval', '2010', 'task', '8', 'hendrickx', 'et', 'al', '2010', 'we', 'evaluated', 'our', 'model', 'on', 'a', 'semantic', 'relation', 'classification', 'task', 'semeval', '2010', 'task', '8', 'hendrickx', 'et', 'al', '2010'], ['we', 'then', 'run', 'word', 'alignment', 'with', 'giza', 'och', 'and', 'ney', '2003', 'in', 'both', 'directions', 'with', 'the', 'default', 'parameters', 'used', 'in', 'moseswe', 'performed', 'word', 'alignment', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'default', 'growdiagfinaland', 'alignment', 'symmetrization', 'method'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'all', 'experiments', 'we', 'used', 'the', 'moses', 'smt', 'system', 'koehn', 'et', 'al', '2007'], ['the', 'significance', 'tests', 'were', 'performed', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004', 'the', 'statistical', 'significance', 'tests', 'using', '95', 'confidence', 'interval', 'are', 'measured', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004'], ['all', 'experiments', 'were', 'carried', 'out', 'using', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', '', 'in', 'its', 'standard', 'nonmonotonic', 'configuration', 'the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['these', 'sentences', 'have', 'then', 'be', 'fed', 'into', 'an', 'efficient', 'hpsg', 'parser', 'pet', 'callmeier', '2000', '', 'with', 'erg', 'loadedthe', 'sentences', 'are', 'fed', 'into', 'the', 'pet', 'hpsg', 'parser', 'callmeier', '2000', 'with', 'the', 'gg', 'loaded'], ['the', 'language', 'model', 'is', 'a', '5gram', 'kenlm', 'heafield', '2011', 'model', 'trained', 'using', 'lmplz', 'with', 'modified', 'kneserney', 'smoothing', 'and', 'no', 'pruningthe', '5gram', 'target', 'language', 'model', 'was', 'trained', 'using', 'kenlm', 'heafield', '2011'], ['cohen', 'et', 'al', '2012', '', 'present', 'a', 'spectral', 'algorithm', 'for', 'lpcfg', 'estimation', 'but', 'the', 'transformation', 'of', 'the', 'lpcfg', 'model', 'and', 'its', 'spectral', 'algorithm', 'to', 'rhmms', 'is', 'awkward', 'and', 'opaquefirst', 'we', 'present', 'an', 'algorithm', 'for', 'estimating', 'lpcfgs', 'akin', 'to', 'the', 'spectral', 'algorithm', 'of', 'cohen', 'et', 'al', '2012', '', 'but', 'simpler', 'to', 'understand', 'and', 'implement'], ['distributional', 'hypothesis', 'theory', 'harris', '1954', 'indicates', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'context', 'tend', 'to', 'have', 'similar', 'meaningssimilar', 'row', 'vectors', 'in', 't', 'indicate', 'similar', 'context', 'of', 'two', 'terms', 'in', 'the', 'domain', 'and', 'terms', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meanings', 'harris', '1954'], ['distributional', 'hypothesis', 'theory', 'harris', '1954', 'indicates', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'context', 'tend', 'to', 'have', 'similar', 'meaningssimilar', 'row', 'vectors', 'in', 't', 'indicate', 'similar', 'context', 'of', 'two', 'terms', 'in', 'the', 'domain', 'and', 'terms', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meanings', 'harris', '1954'], ['we', 'use', 'the', 'stanford', 'dependency', 'parser', 'marneffe', 'et', 'al', '2006', 'we', 'first', 'use', 'a', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'parse', 'each', 'sentence', 'and', 'extract', 'the', 'set', 'of', 'dependency', 'relations', 'associated', 'with', 'the', 'sentence'], ['we', 'also', 'replicated', 'the', 'experiment', 'of', 'holmqvist', 'et', 'al', '2012', 'on', 'this', 'datasetwe', 'also', 'carried', 'out', 'a', 'chunkreordering', 'pbsmt', 'experiment', 'where', 'the', 'chunks', 'are', 'reordered', 'based', 'on', 'the', 'final', 'alignments', 'obtained', 'by', '1pass', 'experiment', 'of', 'holmqvist', 'et', 'al', '2012'], ['we', 'develop', 'translation', 'models', 'using', 'the', 'phrasebased', 'moses', 'koehn', 'et', 'al', '2007', 'smt', 'systemconducted', 'using', 'the', 'moses', 'phrasebased', 'decoder', 'koehn', 'et', 'al', '2007'], ['pos', 'tagging', 'was', 'performed', 'with', 'treetagger', 'schmid', '1994', 'the', 'test', 'set', 'was', 'tagged', 'with', 'the', 'french', 'treetagger', 'schmid', '1994'], ['we', 'used', 'the', 'maltparser', 'nivre', 'et', 'al', '2007', 'for', 'parsing', 'experimentsfor', 'the', 'parsing', 'experimens', 'i', 'used', 'maltparser', 'nivre', 'et', 'al', '2007', '', 'version', '181'], ['classification', 'uses', 'the', 'scikitlearn', 'python', 'package', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'a', 'gb', 'implementation', 'of', 'the', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011'], ['we', 'use', 'the', 'stanford', 'dependency', 'parser', 'marneffe', 'et', 'al', '2006', 'these', 'features', 'were', 'obtained', 'using', 'the', 'stanford', 'parser', '2', 'marneffe', 'et', 'al', '2006'], ['we', 'used', 'adam', 'kingma', 'and', 'ba', '2014', 'with', 'a', 'learning', 'rate', 'of', '00002we', 'used', 'adam', 'as', 'the', 'optimizer', 'kingma', 'and', 'ba', '2014'], ['word', 'alignment', 'is', 'performed', 'using', 'giza', 'och', 'and', 'ney', '2003', 'this', 'is', 'done', 'using', 'ibm', 'model', '1', 'brown', 'et', 'al', '1993', 'and', 'giza', 'och', 'and', 'ney', '2003'], ['the', 'test', 'set', 'was', 'tagged', 'with', 'the', 'french', 'treetagger', 'schmid', '1994', 'for', 'the', 'french', 'side', 'the', 'treetagger', 'schmid', '1994', 'was', 'used'], ['for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'conducted', 'using', 'the', 'moses', 'phrasebased', 'decoder', 'koehn', 'et', 'al', '2007'], ['pos', 'tagging', 'was', 'performed', 'with', 'treetagger', 'schmid', '1994', 'pos', 'tagging', 'is', 'performed', 'using', 'the', 'ims', 'tree', 'tagger', 'schmid', '1994'], ['for', 'building', 'the', 'word', 'alignment', 'models', 'we', 'use', 'mgiza', 'gao', 'and', 'vogel', '2008', 'for', 'word', 'alignments', 'we', 'used', 'mgiza', 'gao', 'and', 'vogel', '2008'], ['the', 'polish', 'data', 'is', 'taken', 'from', 'the', 'europarl', 'corpus', 'koehn', '2005', 'all', 'the', 'data', 'come', 'from', 'europarl', 'koehn', '2005'], ['the', 'germantoenglish', 'corpus', 'is', 'europarl', 'v7', 'koehn', '2005', 'the', 'polish', 'data', 'is', 'taken', 'from', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['distributional', 'models', 'of', 'meaning', 'follow', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'which', 'states', 'that', 'two', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'have', 'similar', 'meaningsdistributional', 'similarity', 'relies', 'on', 'the', 'distributional', 'hypothesis', 'that', 'similar', 'terms', 'appear', 'in', 'similar', 'contexts', 'harris', '1954'], ['conducted', 'using', 'the', 'moses', 'phrasebased', 'decoder', 'koehn', 'et', 'al', '2007', 'the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['europarl', '2', 'koehn', '2005', '', 'it', 'is', 'a', 'corpus', 'of', 'parallel', 'texts', 'in', '11', 'languages', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament', 'in', 'our', 'mt', 'experiments', 'we', 'translate', 'french', 'into', 'spanish', 'and', 'use', 'the', 'following', 'corpora', 'to', 'learn', 'our', 'translation', 'systems', 'bullet', 'europarl', 'corpus', 'koehn', '2005', '', 'the', 'europarl', 'parallel', 'corpus', 'is', 'extract'], ['we', 'conducted', 'statistical', 'significance', 'tests', 'for', 'bleu', 'between', 'our', 'best', 'domainadapted', 'system', 'the', 'baseline', 'and', 'the', 'three', 'thirdparty', 'systems', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'with', '1000', 'statistical', 'significance', 'is', 'tested', 'on', 'the', 'bleu', 'metric', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'with', 'n', '', '1000', 'and', 'p', '', '005'], ['the', 'english', 'side', 'was', 'tokenized', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'will', 'be', 'created', 'by', 'the', 'moses', 'smt', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'develop', 'translation', 'models', 'using', 'the', 'phrasebased', 'moses', 'koehn', 'et', 'al', '2007', 'smt', 'systemwe', 'use', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'implement', 'our', 'models'], ['the', 'baseline', 'will', 'be', 'created', 'by', 'the', 'moses', 'smt', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['the', 'resulting', 'matrix', 'is', 'weighted', 'using', 'pointwise', 'mutual', 'information', 'church', 'and', 'hanks', '1990', 'a', 'popular', 'measure', 'of', 'this', 'association', 'is', 'pointwise', 'mutual', 'information', 'pmi', 'church', 'and', 'hanks', '1990'], ['the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'questions', 'are', 'translated', 'using', 'a', 'phrasebased', 'system', 'built', 'using', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'mo', 'set'], ['the', 'morphosyntactic', 'tagging', 'has', 'been', 'made', 'with', 'the', 'tree', 'tagger', 'schmid', '1994', 'pos', 'tagging', 'is', 'performed', 'using', 'the', 'ims', 'tree', 'tagger', 'schmid', '1994'], ['for', 'the', 'determination', 'of', 'pos', 'tags', 'we', 'use', 'the', 'stuttgart', 'treetagger', 'schmid', '1994', 'for', 'both', 'we', 'use', 'treetagger', 'schmid', '1994', 'with', 'language', 'dependent', 'models', 'ie', 'french', 'model', 'for', 'french', 'texts', 'english', 'for', 'english', 'texts'], ['the', 'polish', 'data', 'is', 'taken', 'from', 'the', 'europarl', 'corpus', 'koehn', '2005', 'the', 'europarl', 'corpus', 'koehn', '2005', 'is', 'built', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['all', 'annotations', 'were', 'done', 'using', 'the', 'brat', 'rapid', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012', 'the', 'annotation', 'was', 'performed', 'using', 'the', 'brat', '2', 'tool', 'stenetorp', 'et', 'al', '2012'], ['the', 'spanishenglish', 's2e', 'training', 'corpus', 'was', 'drawn', 'from', 'the', 'europarl', 'collection', 'koehn', '2005', 'the', 'polish', 'data', 'is', 'taken', 'from', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'used', 'tnt', 'brants', '2000', '', 'trained', 'on', 'the', 'negra', 'training', 'sethcrc', 'is', 'tagged', 'with', 'tnt', 'brants', '2000', '', 'trained', 'on', 'the', 'full', 'ptb'], ['for', 'all', 'experiments', 'we', 'used', 'the', 'moses', 'smt', 'system', 'koehn', 'et', 'al', '2007', 'for', 'germanenglish', 'we', 'also', 'have', 'a', 'system', 'based', 'on', 'moses', 'koehn', 'et', 'al', '2007'], ['two', 'basenp', 'data', 'sets', 'have', 'been', 'put', 'forward', 'by', 'ramshaw', 'and', 'marcus', '1995', 'an', 'alternative', 'representation', 'for', 'basenps', 'has', 'been', 'put', 'forward', 'by', 'ramshaw', 'and', 'marcus', '1995'], ['both', 'of', 'our', 'systems', 'were', 'based', 'on', 'the', 'moses', 'decoder', 'koehn', 'et', 'al', '2007', 'the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'will', 'be', 'created', 'by', 'the', 'moses', 'smt', 'toolkit', 'koehn', 'et', 'al', '2007'], ['corpusbased', 'meaning', 'representations', 'rely', 'on', 'the', 'distributional', 'hypothesis', 'which', 'assumes', 'that', 'words', 'occurring', 'in', 'a', 'similar', 'set', 'of', 'contexts', 'are', 'also', 'similar', 'in', 'meaning', 'harris', '1954', 'distributional', 'models', 'of', 'meaning', 'follow', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'which', 'states', 'that', 'two', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'have', 'similar', 'meanings'], ['recently', 'naim', 'et', 'al', '2014', '', 'proposed', 'an', 'unsupervised', 'learning', 'algorithm', 'for', 'automatically', 'aligning', 'sentences', 'in', 'a', 'document', 'with', 'corresponding', 'video', 'segmentsrecently', 'naim', 'et', 'al', '2014', 'proposed', 'a', 'fully', 'unsupervised', 'approach', 'for', 'aligning', 'wetlab', 'experiment', 'videos', 'with', 'associated', 'text', 'protocols', 'without', 'any', 'direct', 'supervision'], ['a', 'distributional', 'similarity', 'model', 'is', 'constructed', 'based', 'on', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'words', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'share', 'similar', 'meaningsdistributional', 'models', 'of', 'meaning', 'follow', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'which', 'states', 'that', 'two', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'have', 'similar', 'meanings'], ['this', 'paper', 'describes', 'the', 'details', 'of', 'our', 'system', 'that', 'participated', 'in', 'the', 'subtask', 'a', 'of', 'semeval2014', 'task', '9', 'sentiment', 'analysis', 'in', 'twitter', 'rosenthal', 'et', 'al', '2014', 'in', 'the', 'following', 'we', 'will', 'describe', 'the', 'system', 'with', 'which', 'we', 'participated', 'in', 'the', 'message', 'polarity', 'classification', 'subtask', 'of', 'sentiment', 'analysis', 'in', 'twitter', 'task', '9', 'of', 'semeval', '2014', 'rosenthal', 'et', 'al', '2014'], ['we', 'used', 'the', 'phrasebased', 'model', 'moses', 'koehn', 'et', 'al', '2007', 'for', 'the', 'experiments', 'with', 'all', 'the', 'standard', 'settings', 'including', 'a', 'lexicalized', 'reordering', 'model', 'and', 'a', '5gram', 'language', 'modelit', 'was', 'built', 'with', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'using', 'the', '14', 'standard', 'core', 'features', 'including', 'a', '5gram', 'language', 'model'], ['combinatory', 'categorial', 'grammar', 'ccg', 'is', 'a', 'linguistic', 'formalism', 'that', 'represents', 'both', 'the', 'syntax', 'and', 'semantics', 'of', 'language', 'steedman', '1996', 'combinatory', 'categorial', 'grammar', 'ccg', 'steedman', '1996', 'is', 'a', 'lexicalized', 'grammar', 'formalism', 'that', 'has', 'been', 'used', 'for', 'both', 'broad', 'coverage', 'syntactic', 'parsing', 'and', 'semantic', 'parsing'], ['we', 'used', 'the', 'random', 'forests', 'implementation', 'of', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011', 'with', '50', 'estimatorswe', 'used', 'the', 'linear', 'svm', 'implementation', 'with', 'default', 'parameters', 'and', 'random', 'forest', 'with', '50', 'trees', 'both', 'available', 'at', 'scikitlearn', 'pedregosa', 'et', 'al', '2011'], ['the', 'proposed', 'model', 'extends', 'the', 'lda', 'framework', 'of', 'blei', 'et', 'al', '2003', 'the', 'article', 'of', 'blei', 'et', 'al', '2003', 'compares', 'lda', 'with', 'plsi', 'and', 'mixture', 'unigram', 'models', 'using', 'the', 'perplexity', 'of', 'the', 'model'], ['we', 'used', 'the', 'training', 'section', 'of', 'the', 'dataset', 'from', 'gimpel', 'et', 'al', '2011', 'gimpel', 'et', 'al', '2011', '', 'provided', 'a', 'dataset', 'of', 'postagged', 'tweets', 'consisting', 'almost', 'entirely', 'of', 'tweets', 'sampled', 'from', 'one', 'particular', 'day', 'october', '27', '2010'], ['5gram', 'language', 'models', 'of', 'turkish', 'and', 'english', 'were', 'trained', 'using', 'kenlm', 'heafield', '2011', 'in', 'order', 'to', 'evaluate', 'the', 'fluency', 'of', 'each', 'system', 'we', 'train', '5gram', 'language', 'models', 'for', 'each', 'language', 'using', 'kenlm', 'heafield', '2011'], ['the', 'corpora', 'are', 'first', 'tokenized', 'and', 'lowercased', 'using', 'the', 'moses', 'scripts', 'then', 'lemmatized', 'and', 'tagged', 'by', 'partofspeech', 'pos', 'using', 'the', 'treetagger', 'schmid', '1994', 'the', 'corpus', 'was', 'then', 'automatically', 'tagged', 'with', 'partofspeech', 'information', 'using', 'treetagger', 'schmid', '1994'], ['the', 'statistical', 'significance', 'test', 'is', 'also', 'carried', 'out', 'with', 'paired', 'bootstrap', 'resampling', 'method', 'with', 'p', '', '0001', 'intervals', 'koehn', '2004', 'the', 'improvement', 'is', 'statistically', 'significant', 'according', 'to', 'paired', 'bootstrap', 'resampling', 'test', 'koehn', '2004'], ['the', 'realisation', 'ranking', 'component', 'is', 'an', 'svm', 'ranking', 'model', 'implemented', 'with', 'svmrank', 'a', 'support', 'vector', 'machinebased', 'learning', 'tool', 'joachims', '2006', 'the', 'learner', 'is', 'implemented', 'as', 'a', 'ranking', 'component', 'trained', 'with', 'svmrank', 'joachims', '2006'], ['the', 'statistical', 'significance', 'test', 'is', 'also', 'carried', 'out', 'with', 'paired', 'bootstrap', 'resampling', 'method', 'with', 'p', '', '0001', 'intervals', 'koehn', '2004', 'a', 'statistical', 'significance', 'test', 'was', 'performed', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004'], ['we', 'use', 'the', 'nltk', 'toolkit', 'loper', 'and', 'bird', '2002', 'to', 'extract', 'the', 'numerical', 'quantity', 'from', 'each', 'sentencewe', 'use', 'the', 'punkt', 'sentence', 'splitter', 'from', 'nltk', 'loper', 'and', 'bird', '2002', 'to', 'perform', 'both', 'sentence', 'and', 'word', 'segmentation', 'on', 'each', 'text', 'chunk'], ['we', 'mark', 'the', 'source', 'tokens', 'to', 'which', 'each', 'target', 'unk', 'symbol', 'is', 'most', 'aligned', 'with', 'the', 'method', 'of', 'luong', 'et', 'al', '2015', 'we', 'find', 'this', 'method', 'provides', 'an', 'additional', '15', 'bleu', 'points', 'which', 'is', 'consistent', 'with', 'the', 'conclusion', 'in', 'luong', 'et', 'al', '2015'], ['we', 'rely', 'on', 'the', 'hybrid', 'aligned', 'lexical', 'semantic', 'resource', 'proposed', 'by', 'faralli', 'et', 'al', '2016', 'to', 'perform', 'wsdin', 'particular', 'the', 'contribution', 'of', 'this', 'paper', 'is', 'a', 'new', 'unsupervised', 'knowledgebased', 'approach', 'to', 'wsd', 'based', 'on', 'the', 'hybrid', 'aligned', 'resource', 'har', 'introduced', 'by', 'faralli', 'et', 'al', '2016'], ['to', 'help', 'improve', 'the', 'information', 'extraction', 'tools', 'a', 'corpus', '', 'called', 'bioscope', 'has', 'been', 'annotated', 'for', 'speculation', 'negation', 'and', 'its', 'linguistic', 'scopes', 'in', 'biomedical', 'texts', 'szarvas', 'et', 'al', '2008', 'the', 'bioscope', 'corpus', 'is', 'a', 'manually', 'annotated', 'corpus', 'for', 'speculation', 'and', 'negation', 'keywords', 'token', 'level', 'and', 'their', 'linguistic', 'scopes', 'sentence', 'level', 'szarvas', 'et', 'al', '2008'], ['the', 'module', 'of', 'coreference', 'resolution', 'included', 'in', 'the', 'ixa', 'pipeline', 'is', 'loosely', 'based', 'on', 'the', 'stanford', 'multi', 'sieve', 'pass', 'system', 'lee', 'et', 'al', '2013', 'we', 'apply', 'the', 'stanford', 'coreference', 'resolution', 'system', 'lee', 'et', 'al', '2013'], ['we', 'run', 'our', 'experiments', 'on', 'europarl', 'koehn', '2005', '', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamenteuroparl', '2', 'koehn', '2005', '', 'it', 'is', 'a', 'corpus', 'of', 'parallel', 'texts', 'in', '11', 'languages', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['we', 'used', 'the', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'implementation', 'of', 'these', 'classifiers', 'with', 'their', 'default', 'parameter', 'settings', 'for', 'our', 'experimentswe', 'used', 'a', 'gb', 'implementation', 'of', 'the', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011'], ['we', 'used', 'the', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'implementation', 'of', 'these', 'classifiers', 'with', 'their', 'default', 'parameter', 'settings', 'for', 'our', 'experimentswe', 'used', 'the', 'scikitlearn', 'toolkit', 'to', 'train', 'our', 'classifiers', 'pedregosa', 'et', 'al', '2011'], ['automatic', 'multidocument', 'summarization', 'mds', 'aims', 'at', 'selecting', 'the', 'relevant', 'information', 'from', 'multiple', 'documents', 'on', 'the', 'same', 'topic', 'to', 'produce', 'a', 'summary', 'mani', '2001', 'automatic', 'text', 'summarization', 'aims', 'to', 'automatically', 'produce', 'a', 'short', 'and', 'wellorganized', 'summary', 'of', 'single', 'or', 'multiple', 'documents', 'mani', '2001'], ['additionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'the', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'implementation', 'of', 'these', 'classifiers', 'with', 'their', 'default', 'parameter', 'settings', 'for', 'our', 'experimentswe', 'used', 'the', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'implementation', 'of', 'svrs', 'and', 'the', 'skll', 'toolkit'], ['additionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'train', 'classifiers', 'for', 'each', 'of', 'the', 'above', 'feature', 'types', 'and', 'for', 'the', 'full', 'feature', 'set', 'on', 'the', 'training', 'set', 'of', 'each', 'corpus', 'using', 'the', 'default', 'configuration', 'of', 'the', 'naive', 'bayes', 'implementation', 'of', 'weka', '', 'hall', 'et', 'al', '2009', 'we', 'train', 'random', 'forest', 'classifiers', 'breiman', '2001', 'using', 'weka', 'hall', 'et', 'al', '2009', 'for', 'each', 'step', 'and', 'also', 'for', 'the', 'joint', 'model'], ['in', 'addition', 'the', 'corpus', 'was', 'lemmatised', 'using', 'the', 'treetagger', 'lemmatizer', 'schmid', '1994', '1', 'the', 'corpus', 'is', 'lemmatised', 'and', 'tagged', 'by', 'partofspeech', 'on', 'both', 'sides', 'using', 'the', 'treetagger', 'schmid', '1994'], ['they', 'are', 'based', 'on', 'distributional', 'hypothesis', 'which', 'works', 'under', 'the', 'assumption', 'that', 'similar', 'words', 'occur', 'in', 'similar', 'contexts', 'harris', '1954', 'distributional', 'models', 'of', 'meaning', 'follow', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'which', 'states', 'that', 'two', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'have', 'similar', 'meanings'], ['we', 'used', 'the', 'same', 'test', 'set', 'used', 'in', 'li', 'et', 'al', '2004', 'for', 'our', 'testing', '5', 'but', 'we', 'randomly', 'selected', '90', 'of', 'the', 'training', 'data', 'used', 'in', 'li', 'et', 'al', '2004', 'as', 'our', 'training', 'data', 'and', 'the', 'remainder', 'as', 'the', 'development', 'data', 'as', 'shown', 'in', 'table', '5'], ['we', 'develop', 'translation', 'models', 'using', 'the', 'phrasebased', 'moses', 'koehn', 'et', 'al', '2007', 'smt', 'systemadditionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'nonlocal', 'features', 'based', 'on', 'finkel', 'et', 'al', '2005', 'for', 'example', 'finkel', 'et', 'al', '2005', 'enabled', 'the', 'use', 'of', 'nonlocal', 'features', 'by', 'using', 'gibbs', 'sampling'], ['for', 'building', 'the', 'baseline', 'smt', 'system', 'we', 'used', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', '', 'in', 'its', 'standard', 'setupfor', 'all', 'experiments', 'we', 'used', 'the', 'moses', 'smt', 'system', 'koehn', 'et', 'al', '2007'], ['for', 'translation', 'we', 'use', 'moses', 'koehn', 'et', 'al', '2007', 'with', 'lexicalized', 'reordering', 'step', 'and', 'the', 'proposed', 'model', 'with', 'latent', 'derivations', 'laderfor', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['to', 'recognize', 'explicit', 'connectives', 'we', 'construct', 'a', 'list', 'of', 'existing', 'connectives', 'labeled', 'in', 'the', 'penn', 'discourse', 'treebank', 'prasad', 'et', 'al', '2008a', 'in', 'addition', 'we', 'show', 'that', 'the', 'latent', 'representation', 'coheres', 'well', 'with', 'the', 'characterization', 'of', 'discourse', 'connectives', 'in', 'the', 'penn', 'discourse', 'treebank', 'prasad', 'et', 'al', '2008'], ['to', 'construct', 'language', 'models', 'and', 'measure', 'perplexity', '', 'we', 'use', 'srilm', 'stolcke', '2002', 'with', 'interpolated', 'modified', 'kneserney', 'discounting', 'chen', 'and', 'goodman', '1996', 'and', 'with', 'a', 'fixed', 'vocabulary', 'both', 'language', 'models', 'use', 'modified', 'kneserney', 'smoothing', 'chen', 'and', 'goodman', '1996'], ['we', 'use', 'gibbs', 'sampling', 'to', 'estimate', 'the', 'distributions', 'of', 'n', 'and', 'm', '', 'integrating', 'out', 'the', 'multinomial', 'parameters', 'griffiths', 'and', 'steyvers', '2004', 'we', 'use', 'the', 'gibbs', 'sampling', 'based', 'lda', 'griffiths', 'and', 'steyvers', '2004'], ['arabizi', 'is', 'not', 'a', 'letterbased', 'transliteration', 'from', 'the', 'arabic', 'script', 'as', 'is', 'for', 'example', 'the', 'buckwalter', 'transliteration', 'buckwalter', '2004', '1', 'arabic', 'transliteration', 'is', 'presented', 'in', 'the', 'buckwalter', 'scheme', 'buckwalter', '2004'], ['to', 'construct', 'language', 'models', 'and', 'measure', 'perplexity', '', 'we', 'use', 'srilm', 'stolcke', '2002', 'with', 'interpolated', 'modified', 'kneserney', 'discounting', 'chen', 'and', 'goodman', '1996', 'and', 'with', 'a', 'fixed', 'vocabulary', 'both', 'language', 'models', 'use', 'modified', 'kneserney', 'smoothing', 'chen', 'and', 'goodman', '1996'], ['we', 'build', 'upon', 'our', 'previous', 'markov', 'logic', 'based', 'approach', 'for', 'joint', 'concept', 'disambiguation', 'and', 'clustering', 'fahrni', 'and', 'strube', '2012', 'scopeignorant', 'disambig', 'our', 'previous', 'mlnbased', 'approach', 'for', 'concept', 'disambiguation', 'fahrni', 'and', 'strube', '2012'], ['we', 'use', 'the', 'feedforward', 'neural', 'probabilistic', 'language', 'model', 'architecture', 'of', 'vaswani', 'et', 'al', '2013', '', 'as', 'shown', 'in', 'figure', '4', 'we', 'follow', 'the', 'neural', 'network', 'architecture', 'of', 'vaswani', 'et', 'al', '2013', '', 'using', 'two', 'hidden', 'layers', 'of', 'rectified', 'linear', 'units'], ['the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'trained', 'a', 'number', 'of', 'frenchenglish', 'smt', 'systems', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'in', 'its', 'default', 'setting'], ['for', 'building', 'the', 'baseline', 'smt', 'system', 'we', 'used', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', '', 'in', 'its', 'standard', 'setupfor', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'smt', 'translations', 'a', 'phrasebased', 'smt', 'system', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'and', 'the', 'whole', 'spanishenglish', 'dataset', 'except', 'the', 'sentences', 'in', 'the', 'test', 'set', 'was', 'used', 'to', 'translated', 'the'], ['following', 'resource', 'collection', 'and', 'construction', 'a', 'smt', 'model', 'for', 'englishbrazilianportuguese', 'was', 'trained', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'using', 'its', 'baseline', 'settingswe', 'trained', 'a', 'model', 'using', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'on', 'the', 'training', 'data', 'as', 'our', 'baseline', 'system'], ['one', 'semiautomatic', 'approach', 'to', 'evaluation', 'is', 'rouge', 'lin', 'and', 'hovy', '2003', '', 'which', 'is', 'primarily', 'based', 'on', 'ngram', 'cooccurrence', 'between', 'automatic', 'and', 'human', 'summariesthe', 'method', 'rouge', 'lin', 'and', 'hovy', '2003', '', 'is', 'based', 'on', 'ngram', 'overlap', 'between', 'the', 'systemproduced', 'and', 'ideal', 'summaries'], ['we', 'use', 'the', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'to', 'compute', 'the', 'language', 'model', 'score', 'for', 'a', 'sentencewe', 'used', 'the', 'unigram', 'counts', 'from', 'the', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'to', 'determine', 'the', 'frequency', 'of', 'use', 'of', 'each', 'candidate', 'synonym'], ['the', 'semeval2015', 'aspect', 'based', 'sentiment', 'analysis', 'task', 'is', 'a', 'continuation', 'of', 'semeval2014', 'task', '4', 'pontiki', 'et', 'al', '2014', 'this', 'paper', 'describes', 'the', 'approach', 'of', 'the', 'semantic', 'analyis', 'project', 'snap', 'to', 'task', '4', 'of', 'semeval', '2014', 'aspect', 'based', 'sentiment', 'analysis', 'pontiki', 'et', 'al', '2014'], ['the', '2009', 'bio', 'nlp', 'shared', 'task', 'kim', 'et', 'al', '2009', 'aimed', 'at', 'extracting', 'biologicalevents', 'where', 'one', 'of', 'the', 'event', 'types', 'was', 'gene', 'expressionthe', 'recent', 'bionlp', '2009', 'shared', 'task', 'bionlp09st', 'on', 'event', 'extraction', 'kim', 'et', 'al', '2009', 'focused', 'on', 'event', 'types', 'of', 'varying', 'complexity'], ['the', 'constituent', 'context', 'model', 'ccm', 'for', 'inducing', 'constituency', 'parses', 'klein', 'and', 'manning', '2002', 'was', 'the', 'first', 'unsupervised', 'approach', 'to', 'surpass', 'a', 'rightbranching', 'baseline', 'the', 'ccm', 'is', 'a', 'generative', 'model', 'for', 'the', 'unsupervised', 'induction', 'of', 'binary', 'constituency', 'parses', 'over', 'sequences', 'of', 'partofspeech', 'pos', 'tags', 'klein', 'and', 'manning', '2002'], ['the', 'ccm', 'is', 'a', 'generative', 'model', 'for', 'the', 'unsupervised', 'induction', 'of', 'binary', 'constituency', 'parses', 'over', 'sequences', 'of', 'partofspeech', 'pos', 'tags', 'klein', 'and', 'manning', '2002', 'the', 'idea', 'of', 'representing', 'a', 'constituent', 'by', 'its', 'yield', 'and', 'a', 'different', 'definition', 'of', 'context', 'is', 'used', 'by', 'the', 'ccm', 'unsupervised', 'parsing', 'model', 'klein', 'and', 'manning', '2002'], ['we', 'use', 'the', 'implementation', 'provided', 'by', 'tai', 'et', 'al', '2015', '', 'changing', 'only', 'the', 'dependency', 'parses', 'that', 'are', 'fed', 'to', 'their', 'modelwe', 'use', 'the', 'dependency', 'tree', 'long', 'shortterm', 'memory', 'network', 'treelstm', 'proposed', 'by', 'tai', 'et', 'al', '2015', '', 'simply', 'replacing', 'their', 'default', 'dependency', 'parser', 'with', 'our', 'version', 'that', 'maps', 'unseen', 'words'], ['europarl', 'koehn', '2005', 'is', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'use', 'the', 'french', 'english', 'parallel', 'corpus', 'approximately', '12', 'million', 'sentences', 'from', 'the', 'corpus', 'of', 'european', 'parliamentary', 'proceedings', 'koehn', '2005', 'as', 'the', 'data', 'on', 'which', 'pivoting', 'is', 'performed', 'to', 'text'], ['the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'is', 'used', 'for', 'extracting', 'features', 'from', 'the', 'dependency', 'parse', 'treeswe', 'used', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'parse', 'the', 'sentences', 'that', 'contain', 'a', 'candidate', 'speculation', 'keyword', 'and', 'extracted', 'the', 'following', 'features', 'from', 'the', 'dependency', 'parse', 'trees'], ['the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'is', 'used', 'for', 'extracting', 'features', 'from', 'the', 'dependency', 'parse', 'treeswe', 'used', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'parse', 'the', 'sentences', 'that', 'contain', 'a', 'candidate', 'speculation', 'keyword', 'and', 'extracted', 'the', 'following', 'features', 'from', 'the', 'dependency', 'parse', 'trees'], ['the', 'major', 'part', 'of', 'data', 'comes', 'from', 'current', 'and', 'upcoming', 'full', 'releases', 'of', 'the', 'europarl', 'data', 'set', 'koehn', '2005', 'the', 'data', 'used', 'for', 'the', 'experiments', 'described', 'in', 'this', 'paper', 'comes', 'predominantly', 'from', 'bible', 'translations', '', 'wikipedia', 'and', 'the', 'europarl', 'corpus', 'of', 'european', 'parliamentary', 'proceedings', 'koehn', '2005'], ['we', 'compare', 'the', 'proposed', 'model', 'to', 'our', 'implementation', 'of', 'the', 'iobesbased', 'model', 'described', 'in', 'collobert', 'et', 'al', '2011', '', 'applied', 'to', 'mwe', 'tagging', 'the', 'model', 'architecture', 'shown', 'in', 'figure', '1', '', 'is', 'a', 'slight', 'variant', 'of', 'the', 'cnn', 'architecture', 'of', 'collobert', 'et', 'al', '2011'], ['it', 'is', 'a', 'phrasebased', 'system', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'and', 'trainedtuned', 'using', 'only', 'the', 'preprocessed', 'tokenised', 'lowercased', 'parallel', 'data', 'provided', 'for', 'the', 'shared', 'tasksmt', 'translations', 'a', 'phrasebased', 'smt', 'system', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'and', 'the', 'whole', 'spanishenglish', 'dataset', 'except', 'the', 'sentences', 'in', 'the', 'test', 'set', 'was', 'used', 'to', 'translated', 'the', 'shared', 'task'], ['in', 'this', 'section', 'we', 'first', 'discuss', 'the', 'hybrid', 'tree', 'model', 'of', 'lu', 'et', 'al', '2008', '', 'and', 'introduce', 'a', 'novel', 'extensionin', 'lu', 'et', 'al', '2008', '', 'the', 'mixgram', 'model', 'an', 'interpolation', 'between', 'the', 'unigram', 'model', 'and', 'the', 'bigram', 'model', 'was', 'also', 'considered', 'when', 'parsing', 'novel', 'sentences', 'which', 'yielded', 'a', 'better', 'performance'], ['the', 'europarl', 'corpus', 'koehn', '2005', 'is', 'built', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'use', 'the', 'french', 'english', 'parallel', 'corpus', 'approximately', '12', 'million', 'sentences', 'from', 'the', 'corpus', 'of', 'european', 'parliamentary', 'proceedings', 'koehn', '2005', 'as', 'the', 'data', 'on', 'which', 'pivoting', 'is', 'performed', 'to', 'text'], ['the', 'europarl', 'corpus', 'koehn', '2005', 'is', 'built', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'use', 'the', 'french', 'english', 'parallel', 'corpus', 'approximately', '12', 'million', 'sentences', 'from', 'the', 'corpus', 'of', 'european', 'parliamentary', 'proceedings', 'koehn', '2005', 'as', 'the', 'data', 'on', 'which', 'pivoting', 'is', 'performed', 'to', 'text'], ['we', 'use', 'adam', 'kingma', 'and', 'ba', '2015', 'for', 'optimisation', 'with', 'initial', 'learning', 'rate', 'of', '0001each', 'model', 'was', 'trained', 'during', '50', 'epochs', 'using', 'the', 'adaptive', 'variant', 'of', 'stochastic', 'gradient', 'descent', 'adam', 'kingma', 'and', 'ba', '2015', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0001'], ['we', 'use', 'the', 'opensource', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'build', 'a', 'standard', 'phrasebased', 'smt', 'system', 'which', 'extracts', 'up', 'to', '8', 'words', 'phrases', 'in', 'the', 'moses', 'phrase', 'tablewe', 'use', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'implement', 'our', 'models'], ['we', 'used', 'the', 'mkcls', 'tool', 'in', 'giza', 'och', 'and', 'ney', '2003', 'to', 'learn', 'the', 'word', 'classeswe', 'also', 'used', 'giza', 'word', 'alignment', 'tool', 'och', 'and', 'ney', '2003', 'on', 'the', 'same', 'files', 'and', 'collected', 'figures', 'pertaining', 'to', 'the', 'alignment', 'of', 'proper', 'names', 'in', 'hindi', 'and', 'english'], ['we', 'then', 'use', 'the', 'phrase', 'extraction', 'utility', 'in', 'the', 'moses', 'statistical', 'machine', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'extract', 'a', 'phrase', 'table', 'which', 'operates', 'over', 'characters', 'we', 'use', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'implement', 'our', 'models'], ['phrase', 'pairs', 'are', 'extracted', 'from', 'ibm4', 'alignments', 'obtained', 'with', 'giza', 'och', 'and', 'ney', '2003', 'phrase', 'pairs', 'were', 'extracted', 'from', 'symmetrized', 'word', 'alignments', 'and', 'distortions', 'generated', 'by', 'giza', 'och', 'and', 'ney', '2003', 'using', 'the', 'combination', 'of', 'heuristics', 'growdiagfinaland', 'and', 'msdbidirectiona'], ['specifically', 'we', 'build', 'off', 'the', 'bayesian', 'block', 'hmms', 'used', 'by', 'ritter', 'et', 'al', '2010', 'for', 'modeling', 'twitter', 'conversations', 'which', 'will', 'be', 'our', 'primary', 'baselineour', 'work', 'is', 'motivated', 'by', 'the', 'bayesian', 'hmm', 'approach', 'of', 'ritter', 'et', 'al', '2010', '', '', 'the', 'model', 'we', 'refer', 'to', 'as', 'the', 'block', 'hmm', 'bhmm', '', 'and', 'we', 'consider', 'this', 'our', 'primary', 'baseline'], ['like', 'cho', '', 'chai', '2000', '', 'our', 'analysis', 'also', 'provides', 'the', 'same', 'explanation', 'for', 'various', 'scrambled', 'sentences', 'such', 'as', 'the', 'double', 'accusative', 'construction', 'dac', 'in', 'koreangiven', 'the', 'lp', 'constraints', 'in', '4', '7', 'and', '8', 'we', 'can', 'provide', 'a', 'simpler', 'explanation', 'to', 'various', 'scrambled', 'sentences', 'including', 'the', 'alleged', 'counterexamples', 'to', 'the', 'analysis', 'of', 'cho', '', 'chai', '2000'], ['we', 'propose', 'a', 'relatively', 'simple', 'and', 'nuanced', 'unsupervised', 'model', 'inspired', 'by', 'the', 'monolingual', 'weighted', 'matrix', 'factorization', 'wmf', 'model', 'proposed', 'in', 'guo', 'and', 'diab', '2012', '', 'which', 'we', 'extend', 'to', 'the', 'crosslinas', 'mentioned', 'above', 'we', 'extend', 'the', 'wmf', 'model', 'proposed', 'in', 'guo', 'and', 'diab', '2012', 'to', 'bilingual', 'and', 'multilingual', 'settings', 'by', 'forcing', 'the', 'two', 'monolingual', 'components', 'to', 'use', 'a', 'shared', 'factor'], ['our', 'work', 'is', 'also', 'related', 'to', 'bunescu', 'and', 'mooney', '2005', '', 'where', 'the', 'similarity', 'between', 'the', 'words', 'on', 'the', 'path', 'connecting', 'two', 'entities', 'in', 'the', 'dependency', 'graph', 'is', 'used', 'to', 'devise', 'a', 'kernel', 'functionthe', 'dependency', 'path', 'is', 'the', 'shortest', 'path', 'between', 'the', 'two', 'entities', 'in', 'a', 'dependency', 'parse', 'graph', 'and', 'has', 'been', 'shown', 'to', 'be', 'important', 'for', 'relation', 'extraction', 'bunescu', 'and', 'mooney', '2005'], ['motivated', 'by', 'previous', 'work', 'we', 'include', 'a', 'frequency', 'count', 'of', '17', 'discourse', 'markers', 'which', 'were', 'found', 'to', 'be', 'the', 'most', 'common', 'across', 'the', 'argue', 'corpus', 'abbott', 'et', 'al', '2011', 'outside', 'of', 'the', 'rhetorical', 'features', 'the', 'discourse', 'markers', 'which', 'are', 'found', 'to', 'be', 'the', 'most', 'useful', 'in', 'our', 'experiments', 'agree', 'with', 'those', 'found', 'in', 'the', 'ar', 'gue', 'corpus', 'abbott', 'et', 'al', '2011'], ['the', 'highest', 'performance', 'levels', 'were', 'achieved', 'using', 'a', 'sequential', 'minimal', 'optimization', 'algorithm', 'for', 'training', 'a', 'support', 'vector', 'classifier', 'using', 'polynomial', 'kernels', 'platt', '1998', 'table', '8', 'to', 'table', '12', 'show', 'the', 'macroaverage', 'f', 'macroavg', 'scores', 'obtained', 'after', '10', 'crossvalidation', 'using', 'sequential', 'minimal', 'optimization', 'algorithm', 'j', 'platt', '1998', 'for', 'training', 'a', 'support', 'vector', 'mach'], ['in', 'this', 'rest', 'of', 'this', 'paper', 'we', 'discuss', 'related', 'work', 'the', 'methods', 'for', 'each', 'system', 'and', 'experiments', 'and', 'results', 'for', 'each', 'subtask', 'using', 'the', 'data', 'provided', 'by', 'semeval2014', 'task', '9', 'sentiment', 'analysis', 'in', 'twithis', 'paper', 'describes', 'the', 'details', 'of', 'our', 'system', 'that', 'participated', 'in', 'the', 'subtask', 'a', 'of', 'semeval2014', 'task', '9', 'sentiment', 'analysis', 'in', 'twitter', 'rosenthal', 'et', 'al', '2014'], ['in', 'principle', 'classifiers', 'trained', 'on', 'pdtb', 'data', 'can', 'be', 'applied', 'directly', 'to', 'label', 'connectives', 'over', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005', 'used', 'for', 'training', 'and', 'testing', 'smtwe', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['in', 'the', 'next', 'section', 'we', 'briefly', 'review', 'modeling', 'of', 'transition', 'probabilities', 'in', 'a', 'conventional', 'hmm', 'alignment', 'model', 'vogel', 'et', 'al', '1996', 'och', 'and', 'ney', '2000a', 'we', 'briefly', 'review', 'the', 'hmm', 'based', 'word', 'alignment', 'models', 'vogel', '1996', 'och', 'and', 'ney', '2000a'], ['this', 'result', 'is', 'statistically', 'significant', 'at', 'p', '', '005', 'according', 'to', 'bootstrap', 'resampling', 'test', 'koehn', '2004', 'the', 'improvement', 'is', 'statistically', 'significant', 'according', 'to', 'paired', 'bootstrap', 'resampling', 'test', 'koehn', '2004'], ['starting', 'with', 'textrank', 'mihalcea', 'and', 'tarau', '2004', '', 'graphbased', 'ranking', 'methods', 'are', 'becoming', 'the', 'most', 'widely', 'used', 'unsupervised', 'approach', 'for', 'keyphrase', 'extractionin', 'the', 'unsupervised', 'approach', 'graphbased', 'ranking', 'methods', 'are', 'stateoftheart', 'mihalcea', 'and', 'tarau', '2004'], ['because', 'of', 'our', 'experience', 'with', 'the', 'weka', 'package', 'hall', 'et', 'al', '2009', 'we', 'chose', 'this', 'tool', 'for', 'implementation', 'especially', 'we', 'use', 'the', 'weka', 'hall', 'et', 'al', '2009', 'implementation', 'of', 'the', 'simple', 'kmeans', 'for', 'our', 'experiments'], ['dropout', 'srivastava', 'et', 'al', '2014', 'is', 'implemented', 'with', 'a', 'dropout', 'rate', 'of', '02', 'to', 'prevent', 'the', 'model', 'from', 'overfittingdropout', 'srivastava', 'et', 'al', '2014', 'is', 'a', 'very', 'effective', 'regularization', 'technique', 'to', 'prevent', 'overfitting', 'of', 'a', 'network'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['as', 'mentioned', 'in', 'section', '3', 'we', 'obtained', 'dependencies', 'from', 'the', 'output', 'of', 'the', 'stanford', 'parser', 'de', 'marneffe', 'and', 'manning', '2008', 'we', 'make', 'use', 'of', 'dependency', 'parse', 'information', 'from', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'and', 'manning', '2008'], ['we', 'tokenize', 'and', 'truecase', 'all', 'of', 'the', 'corpora', 'using', 'code', 'released', 'with', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'preprocessed', 'the', 'training', 'corpora', 'with', 'scripts', 'included', 'in', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['to', 'obtain', 'these', 'we', 'use', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'and', 'the', 'forced', 'alignment', 'from', 'section', '39we', 'use', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'for', 'extracting', 'dependency', 'path', 'and', 'partofspeech', 'features'], ['the', 'nmt', 'models', 'are', 'trained', 'using', 'adam', 'optimizer', 'kingma', 'and', 'ba', '2014', 'with', 'an', 'initial', 'learning', 'rate', 'of', '00001all', 'parameters', 'are', 'learned', 'by', 'adam', 'optimizer', 'kingma', 'and', 'ba', '2014', 'with', 'the', 'learning', 'rate', '0001'], ['the', 'improved', 'alignments', 'gave', 'a', 'gain', 'of', 'table', '8', '', 'hierarchical', 'lexicalized', 'reordering', 'model', 'galley', 'and', 'manning', '2008', 'one', 'of', 'the', 'phrasebased', 'systems', 'moreover', 'utilizes', 'a', 'lexicalized', 'reordering', 'model', 'galley', 'and', 'manning', '2008'], ['the', 'statistical', 'significance', 'tests', 'using', '95', 'confidence', 'interval', 'are', 'measured', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'a', 'statistical', 'significance', 'test', 'was', 'performed', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004'], ['to', 'obtain', 'these', 'we', 'use', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'and', 'the', 'forced', 'alignment', 'from', 'section', '39we', 'use', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'for', 'extracting', 'dependency', 'path', 'and', 'partofspeech', 'features'], ['we', 'use', 'the', 'adagrad', 'algorithm', 'duchi', 'et', 'al', '2011', 'to', 'optimize', 'the', 'conditional', 'marginal', 'loglikelihood', 'of', 'the', 'datawe', 'use', 'online', 'learning', 'to', 'train', 'model', 'parameters', '', 'updating', 'the', 'parameters', 'using', 'the', 'adagrad', 'algorithm', 'duchi', 'et', 'al', '2011'], ['the', 'other', 'is', 'from', 'jeffrey', 'pennington', 'et', 'al', '2014', '', 'the', 'dimension', 'of', 'word', 'embedding', 'is', '100we', 'use', 'the', '100dimensional', 'glove', 'word', 'embeddings', 'from', 'jeffrey', 'pennington', 'et', 'al', '2014'], ['we', 'use', 'the', 'adagrad', 'optimizer', 'duchi', 'et', 'al', '2011', '', 'with', 'initial', 'learning', 'rate', 'set', 'to', '01we', 'use', 'online', 'learning', 'to', 'train', 'model', 'parameters', '', 'updating', 'the', 'parameters', 'using', 'the', 'adagrad', 'algorithm', 'duchi', 'et', 'al', '2011'], ['we', 'experiment', 'with', 'the', 'phrasebased', 'statistical', 'machine', 'translation', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'in', 'order', 'to', 'train', 'a', 'japanese', 'english', 'system', 'and', 'to', 'show', 'the', 'influence', 'of', 'the', 'expanded', 'parallelwe', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'build', 'a', 'phrase', 'based', 'machine', 'translation', 'system', 'with', 'a', 'traditional', '5gram', 'lm', 'trained', 'on', 'the', 'target', 'side', 'of', 'our', 'bitext'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'follow', 'the', 'definition', 'in', 'cohen', 'et', 'al', '2012', 'of', 'lpcfgswe', 'also', 'use', 'these', 'perturbation', 'schemes', 'to', 'create', 'multiple', 'models', 'for', 'the', 'algorithm', 'of', 'cohen', 'et', 'al', '2012'], ['the', 'language', 'model', 'is', 'a', '5gram', 'kenlm', 'heafield', '2011', 'model', 'trained', 'using', 'lmplz', 'with', 'modified', 'kneserney', 'smoothing', 'and', 'no', 'pruningwe', 'trained', 'an', 'english', '5gram', 'language', 'model', 'using', 'kenlm', 'heafield', '2011'], ['as', 'mentioned', 'in', 'section', '3', 'we', 'obtained', 'dependencies', 'from', 'the', 'output', 'of', 'the', 'stanford', 'parser', 'de', 'marneffe', 'and', 'manning', '2008', 'we', 'make', 'use', 'of', 'dependency', 'parse', 'information', 'from', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'and', 'manning', '2008'], ['also', 'we', 'evaluate', 'on', 'the', 'rte', 'part', 'of', 'the', 'sick', 'dataset', 'marelli', 'et', 'al', '2014', 'and', 'show', 'that', 'our', 'approach', 'leads', 'to', 'improvementsthe', 'second', 'is', 'the', 'rte', 'part', 'of', 'the', 'sick', 'dataset', 'marelli', 'et', 'al', '2014'], ['our', 'model', 'has', 'a', 'siamese', 'structure', 'bromley', 'et', 'al', '1993', 'with', 'two', 'subnetworks', 'each', 'processing', 'a', 'sentence', 'in', 'parallelmost', 'previous', 'work', 'use', 'sentence', 'modeling', 'with', 'a', 'siamese', 'structure', 'bromley', 'et', 'al', '1993'], ['the', 'dialogue', 'act', 'labelling', 'of', 'the', 'corpus', 'follows', 'the', 'date', 'tagging', 'scheme', 'walker', 'et', 'al', '2001', 'the', 'classes', 'used', 'to', 'train', 'the', 'date', 'tagger', 'are', 'derived', 'directly', 'from', 'the', 'date', 'tagging', 'scheme', 'walker', 'et', 'al', '2001c'], ['we', 'also', 'obtain', 'the', 'dependency', 'parse', 'of', 'the', 'sentences', 'using', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'examine', 'the', 'effect', 'of', 'normalization', 'on', 'dependency', 'parsing', 'we', 'employ', 'the', 'stanford', 'dependency', 'parser', '3', 'marneffe', 'et', 'al', '2006'], ['the', 'resulting', 'matrix', 'is', 'weighted', 'using', 'pointwise', 'mutual', 'information', 'church', 'and', 'hanks', '1990', 'we', 'construct', 'wordword', 'cooccurrence', 'matrix', 'x', 'every', 'element', 'in', 'the', 'matrix', 'is', 'the', 'pointwise', 'mutual', 'information', 'between', 'the', 'two', 'words', 'church', 'and', 'hanks', '1990'], ['we', 'extract', 'structured', 'facts', 'using', 'two', 'methods', 'clausie', 'del', 'corro', 'and', 'gemulla', '2013', 'and', 'sedona', 'detailed', 'later', 'in', 'sec', '4', 'also', 'see', 'fig', '1we', 'extract', 'facts', 'from', 'captions', 'using', 'clausie', 'del', 'corro', 'and', 'gemulla', '2013', 'and', 'our', 'proposed', 'sedonanlp', 'system'], ['dropout', 'srivastava', 'et', 'al', '2014', 'is', 'implemented', 'with', 'a', 'dropout', 'rate', 'of', '02', 'to', 'prevent', 'the', 'model', 'from', 'overfittingnext', 'the', 'output', 'of', 'the', 'maxpooling', 'layer', 'is', 'passed', 'to', 'a', 'dropout', 'layer', 'srivastava', 'et', 'al', '2014'], ['the', 'population', 'distribution', 'was', 'estimated', 'by', 'the', 'bootstrap', 'method', 'cohen', '1995', 'the', 'bootstrap', 'sampling', 'method', 'provides', 'a', 'way', 'for', 'artificially', 'establishing', 'a', 'sampling', 'distribution', 'for', 'a', 'statistic', 'when', 'the', 'distribution', 'is', 'not', 'known', 'cohen', '1995'], ['the', 'statistical', 'significance', 'test', 'is', 'also', 'carried', 'out', 'with', 'paired', 'bootstrap', 'resampling', 'method', 'with', 'p', '', '0001', 'intervals', 'koehn', '2004', 'the', 'significance', 'testing', 'is', 'performed', 'by', 'paired', 'bootstrap', 'resampling', 'koehn', '2004'], ['the', 'bootstrap', 'sampling', 'method', 'provides', 'a', 'way', 'for', 'artificially', 'establishing', 'a', 'sampling', 'distribution', 'for', 'a', 'statistic', 'when', 'the', 'distribution', 'is', 'not', 'known', 'cohen', '1995', 'the', 'population', 'distribution', 'was', 'estimated', 'by', 'the', 'bootstrap', 'method', 'cohen', '1995'], ['for', 'medical', 'we', 'use', 'the', 'biomedical', 'data', 'from', 'emea', 'tiedemann', '2009', 'for', 'frenchenglish', 'experiments', 'we', 'used', 'the', 'emea', 'parallel', 'corpus', 'tiedemann', '2009', '', 'which', 'are', 'medical', 'documents', 'from', 'the', 'european', 'medecines', 'agency'], ['the', 'mt', 'experiments', 'were', 'carried', 'out', 'using', 'the', 'standard', 'loglinear', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'translation', 'system', 'is', 'trained', 'using', 'the', 'well', 'known', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'the', 'theory', 'of', 'cho', '', 'chai', '2000', 'to', 'be', 'complete', 'we', 'have', 'proposed', 'a', 'new', 'type', 'of', 'marker', 'and', 'the', 'adjunct', 'lp', 'constraint', 'in', 'conjunction', 'with', 'their', 'argument', 'lp', 'constraintthen', 'we', 'revise', 'the', 'two', 'lp', 'constraints', 'of', 'cho', '', 'chai', '2000', 'and', 'add', 'the', 'adjunct', 'lp', 'constraint'], ['3', 'with', 'these', 'trees', 'fixed', 'the', 'partial', 'derivatives', 'with', 'respect', 'to', 'parameters', 'are', 'computed', 'via', 'the', 'backpropagation', 'through', 'structures', 'algorithm', 'goller', 'and', 'kuchler', '1996', 'derivatives', 'are', 'computed', 'efficiently', 'via', 'backpropagation', 'through', 'structure', 'goller', 'and', 'kuchler', '1996'], ['many', 'researchers', 'have', 'considered', 'generating', 'paraphrases', 'by', 'mining', 'the', 'web', 'guided', 'by', 'the', 'distributional', 'hypothesis', 'which', 'states', 'that', 'words', 'occurring', 'in', 'similar', 'contexts', 'tend', 'to', 'have', 'similar', 'meanings', 'harris', '1954', 'distributional', 'models', 'of', 'meaning', 'follow', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'which', 'states', 'that', 'two', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'have', 'similar', 'meanings'], ['we', 'used', 'weka', 'hall', 'et', 'al', '2009', 'for', 'all', 'our', 'classification', 'experimentswe', 'use', 'the', 'weka', 'toolkit', 'hall', 'et', 'al', '2009', 'for', 'our', 'supervised', 'learning', 'experiments'], ['bilingual', 'corpora', 'the', 'corpus', 'used', 'in', 'the', 'following', 'experiments', 'is', 'the', 'basic', 'travel', 'expression', 'corpus', 'takezawa', 'et', 'al', '2002', 'the', 'experiments', 'are', 'carried', 'out', 'on', 'a', 'subset', 'of', 'the', 'basic', 'travel', 'expression', 'corpus', 'btec', 'takezawa', 'et', 'al', '2002', '', 'as', 'it', 'is', 'used', 'for', 'the', 'supplied', 'data', 'track', 'condition', 'of', 'the', 'iwslt', 'evaluation', 'campaign'], ['first', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'statistical', 'machine', 'translationthe', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'is', 'used', 'for', 'phrase', 'extraction'], ['another', 'parallel', 'corpus', 'is', 'the', 'jrcacquis', 'multilingual', 'parallel', 'corpus', 'steinberger', 'et', 'al', '2006', 'corpus', 'steinberger', 'et', 'al', '2006'], ['the', 'parallel', 'corpus', 'is', 'wordaligned', 'using', 'giza', 'och', 'and', 'ney', '2003', 'this', 'is', 'done', 'using', 'ibm', 'model', '1', 'brown', 'et', 'al', '1993', 'and', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'thus', 'cast', 'msc', 'as', 'a', 'semantic', 'sentence', 'classification', 'task', 'in', 'a', 'cnn', 'architecture', 'adopting', 'the', 'onelayer', 'cnn', 'model', 'of', 'kim', '2014', 'a', 'variant', 'of', 'collobert', 'et', 'al', '2011', 'we', 'compare', 'the', 'proposed', 'model', 'to', 'our', 'implementation', 'of', 'the', 'iobesbased', 'model', 'described', 'in', 'collobert', 'et', 'al', '2011', '', 'applied', 'to', 'mwe', 'tagging'], ['all', 'these', 'features', 'are', 'inherited', 'from', 'moses', 'koehn', 'et', 'al', '2007', 'all', 'the', 'experiments', 'are', 'carried', 'out', 'in', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005', 'for', 'our', 'experiments', 'we', 'use', '40000', 'sentences', 'from', 'europarl', 'koehn', '2005', 'for', 'each', 'language', 'pair', 'following', 'the', 'basic', 'setup', 'of', 'tiedemann', '2014'], ['the', 'texts', 'were', 'first', 'automatically', 'segmented', 'and', 'tokenized', '10', 'and', 'then', 'they', 'were', 'partofspeech', 'tagged', 'by', 'tnt', 'tagger', 'brants', '2000', '', 'which', 'was', 'trained', 'on', 'the', 'respective', 'conll', 'training', 'data', 'the', 'filethe', 'data', 'was', 'tagged', 'using', 'tnt', 'brants', '2000', '', 'using', 'a', 'model', 'trained', 'on', 'the', 'wall', 'street', 'journal'], ['secondly', 'holmqvist', 'et', 'al', '2012', 'reordered', 'source', 'words', 'based', 'on', 'word', 'alignment', 'whereas', 'we', 'suggest', 'reordering', 'source', 'chunksholmqvist', 'et', 'al', '2012', 'presented', 'a', 'method', 'where', 'source', 'text', 'is', 'reordered', 'to', 'replicate', 'the', 'target', 'word', 'order', 'based', 'on', 'word', 'alignment'], ['for', 'language', 'modeling', 'we', 'use', 'the', 'english', 'gigaword', 'corpus', 'with', '5gram', 'lm', 'implemented', 'with', 'the', 'kenlm', 'toolkit', 'heafield', '2011', 'language', 'model', 'for', 'all', '3scfg', 'systems', 'we', 'use', 'a', '4gram', 'kneserney', 'smoothed', 'language', 'model', 'trained', 'using', 'the', 'kenlm', 'toolkit', 'heafield', '2011'], ['for', 'our', 'lda', 'implementations', 'we', 'use', 'mallet', 'mccallum', '2002', 'for', 'training', 'bilingual', 'topic', 'models', 'we', 'use', 'mallet', 'toolkit', 'mccallum', '2002'], ['we', 'use', 'the', 'standard', 'stanfordstyle', 'set', 'of', 'dependency', 'labels', 'de', 'marneffe', 'et', 'al', '2006', 'we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006'], ['next', 'we', 'evaluate', 'how', 'well', 'the', 'complexity', 'measures', 'proposed', 'in', '', 'raghavan', 'et', 'al', '2007', 'correlate', 'with', 'improvement', 'in', 'performance', 'and', 'improvement', 'in', 'learning', 'ratewe', 'also', 'compare', 'annotation', 'strategies', 'in', 'terms', 'of', 'the', 'learning', 'rate', 'similar', 'to', 'raghavan', 'et', 'al', '2007', '', 'except', 'that', 'we', 'estimate', 'and', 'compare', 'the', 'maximum', 'improvement', 'in', 'the', 'learning', 'rate'], ['syntax', 'in', 'epec', 'is', 'annotated', 'following', 'the', 'dependency', 'based', 'formalism', 'used', 'in', 'the', 'prague', 'dependency', 'treebank', 'which', 'was', 'also', 'used', 'in', 'the', 'german', 'negra', 'corpus', 'skut', 'et', 'al', '1997', 'for', 'the', 'german', 'experiments', 'we', 'used', 'the', 'negra', 'corpus', 'skut', 'et', 'al', '1997'], ['we', 'used', 'the', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'implementation', 'of', 'svrs', 'and', 'the', 'skll', 'toolkitwe', 'use', 'the', 'scikit', 'implementation', 'of', 'svm', 'pedregosa', 'et', 'al', '2011'], ['we', 'used', 'the', 'scikitlearn', 'toolkit', 'to', 'train', 'our', 'classifiers', 'pedregosa', 'et', 'al', '2011', 'for', 'indeplogistic', 'we', 'used', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'to', 'train', 'classifiers'], ['the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'trained', 'a', 'standard', 'moses', 'baseline', 'koehn', 'et', 'al', '2007', 'on', 'the', 'same', 'training', 'data', 'and', 'used', 'the', 'same', '4', 'gram', 'language', 'model', 'to', 'generate', 'responses'], ['these', 'features', 'were', 'obtained', 'using', 'the', 'stanford', 'parser', '2', 'marneffe', 'et', 'al', '2006', 'we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006'], ['we', 'built', 'a', '5gram', 'language', 'model', 'on', 'the', 'english', 'side', 'of', 'qcatrain', 'using', 'kenlm', 'heafield', '2011', 'one', 'is', 'a', '3gram', 'language', 'model', 'built', 'using', 'kenlm', 'heafield', '2011', 'and', 'trained', 'over', 'a', 'modified', 'version', 'of', 'the', 'annotated', 'corpus', 'in', 'which', 'every', 'it', 'is', 'concatenated', 'with', 'its', 'type', 'eg', 'it', 'event'], ['in', 'the', 'other', 'side', 'the', 'french', 'corpus', 'is', 'partofspeech', 'pos', 'tagged', 'by', 'using', 'treetagger', 'tool', 'schmid', '1994', 'for', 'annotating', 'text', 'with', 'partofspeech', 'and', 'lemma', 'information1', 'the', 'corpus', 'is', 'lemmatised', 'and', 'tagged', 'by', 'partofspeech', 'on', 'both', 'sides', 'using', 'the', 'treetagger', 'schmid', '1994'], ['the', 'comlex', 'syntax', 'dictionary', 'grishman', 'et', 'al', '1994', 'the', 'comlex', 'syntax', 'dictionary', 'grishman', 'et', 'al', '1994', 'brown', 'corpus', 'kuera', 'and', 'francis', '1967', 'has', 'been', 'used', 'as', 'a', 'reference', 'corpus', 'in', 'many', 'computational', 'applications'], ['we', 'lemmatise', 'the', 'head', 'of', 'each', 'constituent', 'with', 'treetagger', 'schmid', '1994', 'pos', 'tagging', 'was', 'performed', 'with', 'the', 'treetagger', 'schmid', '1994'], ['with', 'the', 'kenlm', 'toolkit', 'heafield', '2011', 'for', 'language', 'modeling', 'we', 'used', 'the', 'kenlm', 'toolkit', 'heafield', '2011', 'for', 'standard', 'ngram', 'modeling', 'with', 'an', 'ngram', 'length', 'of', '5'], ['we', 'briefly', 'review', 'the', 'hmm', 'based', 'word', 'alignment', 'models', 'vogel', '1996', 'och', 'and', 'ney', '2000', 'och', 'is', 'the', 'hmm', 'alignment', 'model', 'of', 'och', 'and', 'ney', '2000'], ['we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'marneffe', 'et', 'al', '2006', 'these', 'features', 'were', 'obtained', 'using', 'the', 'stanford', 'parser', '2', 'marneffe', 'et', 'al', '2006'], ['we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006', 'we', 'use', 'the', 'standard', 'stanfordstyle', 'set', 'of', 'dependency', 'labels', 'de', 'marneffe', 'et', 'al', '2006'], ['all', 'the', 'summaries', 'are', 'evaluated', 'using', 'rouge', 'lin', '2004', 'for', 'the', 'summarization', 'task', 'we', 'compare', 'results', 'using', 'rouge', 'lin', '2004'], ['the', 'reported', 'confidence', 'intervals', 'were', 'estimated', 'using', 'bootstrap', 'resampling', 'koehn', '2004', 'we', 'calculated', 'significance', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004'], ['we', 'implemented', 'charwnn', 'using', 'the', 'theano', 'library', 'bergstra', 'et', 'al', '2010', 'gradients', 'computed', 'using', 'the', 'automatic', 'differentiation', 'facilities', 'of', 'theano', 'bergstra', 'et', 'al', '2010', 'which', 'implements', 'a', 'generalized', 'bptt'], ['for', 'the', 'linear', 'logistic', 'regression', 'implementation', 'we', 'used', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'for', 'indeplogistic', 'we', 'used', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'to', 'train', 'classifiers'], ['the', 'tagger', 'we', 'use', 'is', 'tnt', 'brants', '2000', '', 'a', 'hidden', 'markov', 'trigram', 'tagger', 'which', 'was', 'trained', 'on', 'the', 'spoken', 'dutch', 'corpus', 'cgn', 'internal', 'release', '6the', 'data', 'was', 'tagged', 'using', 'tnt', 'brants', '2000', '', 'using', 'a', 'model', 'trained', 'on', 'the', 'wall', 'street', 'journal'], ['we', 'have', 'used', 'the', 'implementation', 'described', 'in', 'schapire', 'and', 'singer', '1999', 'with', 'decision', 'trees', 'of', 'depth', 'fixed', 'to', '31', 'regarding', 'the', 'learning', 'algorithm', 'we', 'used', 'generalized', 'adaboost', 'with', 'realvalued', 'weak', 'classifiers', 'which', 'constructs', 'an', 'ensemble', 'of', 'decision', 'trees', 'of', 'fixed', 'depth', 'schapire', 'and', 'singer', '1999'], ['we', 'built', 'a', '5gram', 'language', 'model', 'on', 'the', 'english', 'side', 'of', 'qcatrain', 'using', 'kenlm', 'heafield', '2011', 'we', 'build', 'our', 'pbsmt', 'systems', 'in', 'a', 'standard', 'way', 'using', 'the', 'moses', 'system', '', 'kenlm', 'for', 'language', 'modelling', 'heafield', '2011', '', 'and', 'standard', 'lexical', 'reordering', 'model'], ['delphin', 'minimal', 'recursion', 'semantics', 'dm', 'as', 'part', 'of', 'the', 'full', 'hpsg', 'sign', 'the', 'erg', 'also', 'makes', 'available', 'a', 'logicalform', 'representation', 'of', 'propositional', 'semantics', 'in', 'the', 'format', 'of', 'minimal', 'recursion', 'semantas', 'output', 'the', 'grammar', 'delivers', 'detailed', 'semantic', 'representations', 'in', 'the', 'form', 'of', 'minimal', 'recursion', 'semantics', 'copestake', 'et', 'al', '2005'], ['like', 'the', 'conll2006', 'shared', 'task', 'the', '2007', 'shared', 'task', 'focuses', 'on', 'dependency', 'parsing', 'and', 'aims', 'at', 'comparing', 'stateoftheart', 'machine', 'learning', 'algorithms', 'applied', 'to', 'this', 'task', 'nivre', 'et', 'al', '2007', 'one', 'of', 'the', 'first', 'venues', 'at', 'which', 'domain', 'adaptation', 'was', 'targeted', 'was', 'the', '2007', 'conll', 'shared', 'task', 'on', 'dependency', 'parsing', 'nivre', 'et', 'al', '2007'], ['like', 'cho', '', 'chai', '2000', '', 'our', 'analysis', 'also', 'provides', 'the', 'same', 'explanation', 'for', 'various', 'scrambled', 'sentences', 'such', 'as', 'the', 'double', 'accusative', 'construction', 'dac', 'in', 'koreanhowever', 'like', 'cho', '', 'chai', '2000', '', 'our', 'analysis', 'can', 'also', 'predict', 'that', 'the', 'scrambled', 'sentence', '19c', 'is', 'grammatical', 'whereas', '19b', 'is', 'ungrammatical'], ['the', 'phrase', 'table', 'is', 'extracted', 'from', 'a', 'bilingual', 'text', 'aligned', 'on', 'the', 'word', 'level', 'using', 'eg', 'giza', '', 'och', 'and', 'ney', '2003', 'a', 'core', 'component', 'of', 'every', 'pbsmt', 'system', 'is', 'the', 'phrase', 'table', 'which', 'contains', 'bilingual', 'phrase', 'pairs', 'extracted', 'from', 'a', 'bilingual', 'corpus', 'after', 'word', 'alignment', 'och', 'and', 'ney', '2003'], ['google', 'web', '1t', 'brants', 'and', 'franz', '2006', 'has', 'been', 'used', 'to', 'calculate', 'term', 'idf', 'which', 'is', 'used', 'as', 'a', 'measure', 'of', 'the', 'importance', 'of', 'the', 'termsthe', 'google', 'web', '1t', 'data', 'brants', 'and', 'franz', '2006', 'has', 'been', 'shown', 'to', 'give', 'a', 'higher', 'score', 'however', 'this', 'data', 'was', 'not', 'available', 'during', 'the', 'course', 'of', 'this', 'research'], ['we', 'first', 'use', 'a', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'parse', 'each', 'sentence', 'and', 'extract', 'the', 'set', 'of', 'dependency', 'relations', 'associated', 'with', 'the', 'sentence', 'a', 'complete', 'set', 'of', 'parser', 'tags', 'and', 'the', 'method', 'used', 'to', 'map', 'from', 'a', 'constituent', 'to', 'a', 'typed', 'dependency', 'grammar', 'can', 'be', 'found', 'in', 'de', 'marneffe', 'et', 'al', '2006'], ['we', 'first', 'use', 'a', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'parse', 'each', 'sentence', 'and', 'extract', 'the', 'set', 'of', 'dependency', 'relations', 'associated', 'with', 'the', 'sentence', 'a', 'complete', 'set', 'of', 'parser', 'tags', 'and', 'the', 'method', 'used', 'to', 'map', 'from', 'a', 'constituent', 'to', 'a', 'typed', 'dependency', 'grammar', 'can', 'be', 'found', 'in', 'de', 'marneffe', 'et', 'al', '2006'], ['these', 'analyses', 'provide', 'an', 'alternative', 'but', 'theoretically', 'more', 'reasonable', 'explanation', 'to', 'the', 'findings', 'of', 'liang', 'et', 'al', '2006', '', 'while', 'they', 'blame', 'unreasonable', 'gold', 'derivations', 'for', 'the', 'failure', 'of', 'standliang', 'et', 'al', '2006', 'observe', 'that', 'standard', 'update', 'performs', 'worse', 'than', 'local', 'update', 'which', 'they', 'attribute', 'to', 'the', 'fact', 'that', 'the', 'former', 'often', 'update', 'towards', 'a', 'gold', 'derivation', 'made', 'up', 'of', 'unreasonable', 'r'], ['we', 'experiment', 'with', 'the', 'phrasebased', 'statistical', 'machine', 'translation', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'in', 'order', 'to', 'train', 'a', 'japanese', 'english', 'system', 'and', 'to', 'show', 'the', 'influence', 'of', 'the', 'expanded', 'parallelwe', 'present', 'a', 'system', 'that', 'takes', 'a', 'general', 'moses', 'statistical', 'machine', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'and', 'adapts', 'it', 'to', 'the', 'biomedical', 'domain'], ['for', 'our', 'corpus', 'we', 'randomly', 'selected', 'documents', 'from', 'the', 'washington', 'section', 'of', 'the', 'new', 'york', 'times', 'corpus', 'sandhaus', '2008', 'from', 'the', 'year', '2007new', 'york', 'times', 'consists', 'of', '500', 'random', 'sentences', 'from', 'the', 'new', 'york', 'times', 'corpus', 'sandhaus', '2008', '', 'year', '2007', 'we', 'selected', 'only', 'sentences', 'that', 'contained', 'at', 'least', 'one', 'named', 'entity', 'according', 'to', 'the', 'stanf'], ['they', 'used', 'the', 'webbased', 'annotation', 'tool', 'brat', 'stenetorp', 'et', 'al', '2012', 'for', 'the', 'annotation', 'we', 'consider', 'the', 'following', 'types', 'of', 'implicit', 'referents', '', 'the', 'brat', 'tool', 'stenetorp', 'et', 'al', '2012', 'was', 'used', 'for', 'annotation'], ['all', 'corpora', 'were', 'taken', 'from', 'the', 'childes', 'database', 'macwhinney', '2000', 'the', 'evaluation', 'set', 'was', 'comprised', 'of', 'adult', 'utterances', 'from', 'the', 'brown', '1973', 'data', 'of', 'the', 'childes', 'database', 'macwhinney', '2000'], ['the', 'system', 'generated', 'tweets', 'were', 'evaluated', 'using', 'rouge', 'measures', 'lin', '2004', 'the', 'summaries', 'from', 'the', 'above', 'algorithm', 'for', 'the', 'qfmds', 'were', 'evaluated', 'based', 'on', 'rouge', 'metrics', 'lin', '2004'], ['the', 'toefl', 'synonym', 'selection', 'task', 'is', 'to', 'select', 'the', 'semantically', 'closest', 'word', 'to', 'a', 'target', 'from', 'a', 'list', 'of', 'four', 'candidates', 'landauer', 'and', 'dumais', '1997', 'the', 'toefl', 'synonym', 'dataset', 'landauer', 'and', 'dumais', '1997', 'consists', 'of', '80', 'question', 'words', 'for', 'each', 'of', 'which', '4', 'answer', 'words', 'are', 'given', 'and', 'the', 'task', 'is', 'to', 'select', 'the', 'answer', 'word', 'most', 'similar', 'to', 'the', 'question'], ['we', 'calculate', 'our', 'features', 'using', 'the', 'kenlm', 'toolkit', 'heafield', '2011', '1', 'we', 'used', 'the', 'kenlm', 'toolkit', 'heafield', '2011', 'to', 'build', 'all', 'language', 'models', 'used', 'in', 'this', 'work', 'ie', 'both', 'for', 'data', 'selection', 'and', 'for', 'the', 'mt', 'systems', 'used', 'for', 'extrinsic', 'evaluation'], ['we', 'use', 'the', 'scikit', 'implementation', 'of', 'random', 'forest', 'pedregosa', 'et', 'al', '2011', 'the', 'regressor', 'used', 'is', 'a', 'random', 'forest', 'regressor', 'in', 'the', 'implementation', 'provided', 'by', 'scikitlearn', 'pedregosa', 'et', 'al', '2011'], ['for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005', 'the', 'statistical', 'dictionary', 'for', 'this', 'task', 'was', 'extracted', 'from', 'the', 'englishfrench', 'europarl', '7', 'corpus', 'koehn', '2005'], ['the', 'statistical', 'significance', 'tests', 'using', '95', 'confidence', 'interval', 'are', 'measured', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'we', 'used', 'bootstrap', 'resampling', 'for', 'testing', 'statistical', 'significance', 'koehn', '2004'], ['we', 'then', 'run', 'word', 'alignment', 'with', 'giza', 'och', 'and', 'ney', '2003', 'in', 'both', 'directions', 'with', 'the', 'default', 'parameters', 'used', 'in', 'moseswe', 'used', 'the', 'giza', 'software', 'och', 'and', 'ney', '2003', 'to', 'do', 'the', 'word', 'alignments'], ['we', 'built', 'a', '5gram', 'language', 'model', 'on', 'the', 'english', 'side', 'of', 'qcatrain', 'using', 'kenlm', 'heafield', '2011', 'we', 'calculate', 'our', 'features', 'using', 'the', 'kenlm', 'toolkit', 'heafield', '2011'], ['we', 'used', 'giza', 'och', 'and', 'ney', '2003', 'to', 'align', 'the', 'words', 'in', 'the', 'corpuswe', 'then', 'run', 'word', 'alignment', 'with', 'giza', 'och', 'and', 'ney', '2003', 'in', 'both', 'directions', 'with', 'the', 'default', 'parameters', 'used', 'in', 'moses'], ['the', 'phrase', 'tables', 'were', 'generated', 'by', 'means', 'of', 'symmetrised', 'word', 'alignments', 'obtained', 'with', 'giza', 'och', 'and', 'ney', '2003', 'the', 'word', 'alignment', 'was', 'obtained', 'by', 'running', 'giza', 'och', 'and', 'ney', '2003'], ['zhu', 'et', 'al', '2013', 'applied', 'kalman', 'filter', 'model', 'to', 'learn', 'and', 'estimate', 'user', 'intentions', 'in', 'their', 'humancomputer', 'interactive', 'word', 'segmentation', 'frameworkin', 'the', 'scenario', 'of', 'humancomputer', 'interactive', 'chinese', 'word', 'segmentation', 'the', 'kalman', 'filter', 'approach', 'proposed', 'by', 'zhu', 'et', 'al', '2013', '', 'the', 'md', 'value', 'and', 'the', 'horizontal', 'axis', 'denotes', 'the', 'occurrence', 'of'], ['koo', 'et', 'al', '2008', '', 'have', 'proposed', 'to', 'use', 'word', 'clusters', 'as', 'features', 'to', 'improve', 'graphbased', 'statistical', 'dependency', 'parsing', 'for', 'english', 'and', 'czechin', 'order', 'to', 'reduce', 'the', 'amount', 'of', 'annotated', 'data', 'to', 'train', 'a', 'dependency', 'parser', 'koo', 'et', 'al', '2008', 'used', 'word', 'clusters', 'computed', 'from', 'unlabelled', 'data', 'as', 'features', 'for', 'training', 'a', 'parser'], ['more', 'recently', 'pasha', 'et', 'al', '2014', 'created', 'madamira', 'a', 'system', 'for', 'morphological', 'analysis', 'and', 'disambiguation', 'of', 'arabic', 'this', 'system', 'can', 'be', 'used', 'to', 'improve', 'the', 'accuracy', 'of', 'spelling', 'checking', 'system', 'esnext', 'the', 'files', 'are', 'processed', 'with', 'the', 'morphological', 'analysis', 'and', 'disambiguation', 'system', 'madamira', 'pasha', 'et', 'al', '2014', 'that', 'corrects', 'a', 'common', 'class', 'of', 'spelling', 'errors'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settingsfor', 'building', 'our', 'smt', 'systems', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'was', 'used', 'in', 'its', 'standard', 'setup'], ['bullet', 'naive', 'bayesnb', 'we', 'use', 'binomial', 'variant', 'with', 'laplace', 'smoothing', 'parameter', '', '1', 'pedregosa', 'et', 'al', '2011', 'we', 'use', 'the', 'bernoulli', 'naive', 'bayes', 'classifier', 'in', 'scikit', 'with', 'the', 'default', 'settings', 'pedregosa', 'et', 'al', '2011'], ['we', 'use', 'adagrad', 'duchi', 'et', 'al', '2011', '', 'with', 'the', 'initial', 'learning', 'rate', 'set', 'to', '', '', '05we', 'use', 'online', 'learning', 'to', 'train', 'model', 'parameters', '', 'updating', 'the', 'parameters', 'using', 'the', 'adagrad', 'algorithm', 'duchi', 'et', 'al', '2011'], ['the', 'mt', 'experiments', 'were', 'carried', 'out', 'using', 'the', 'standard', 'loglinear', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'adagrad', 'method', 'duchi', 'et', 'al', '2011', 'to', 'automatically', 'update', 'the', 'learning', 'rate', 'for', 'each', 'parameterwe', 'use', 'online', 'learning', 'to', 'train', 'model', 'parameters', '', 'updating', 'the', 'parameters', 'using', 'the', 'adagrad', 'algorithm', 'duchi', 'et', 'al', '2011'], ['among', 'the', 'existing', 'sensetagged', 'corpora', 'the', 'semcor', 'corpus', 'miller', 'et', 'al', '1994', 'is', 'one', 'of', 'the', 'most', 'widely', 'usedthe', 'semcor', 'corpus', 'miller', 'et', 'al', '1994', 'is', 'one', 'of', 'the', 'few', 'currently', 'available', 'manually', 'senseannotated', 'corpora', 'for', 'wsd'], ['we', 'use', 'the', 'liblinear', 'support', 'vector', 'machine', 'svm', 'chang', 'and', 'lin', '2011', 'classifier', 'for', 'training', 'and', 'run', '5fold', 'crossvalidation', 'for', 'evaluationa', 'linearkernel', 'support', 'vector', 'machine', 'chang', 'and', 'lin', '2011', 'classifier', 'is', 'trained', 'on', 'the', 'available', 'training', 'data'], ['we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'tokenize', 'and', 'truecase', 'all', 'of', 'the', 'corpora', 'using', 'code', 'released', 'with', 'moses', 'koehn', 'et', 'al', '2007', '1'], ['we', 'built', 'a', 'sourcetotarget', 'pbsmt', 'model', 'from', 'the', 'bilingual', 'domain', 'corpus', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'dictionaries', 'are', 'automatically', 'generated', 'via', 'word', 'alignment', 'using', 'giza', 'och', 'and', 'ney', '2000', 'on', 'parallel', 'corporawe', 'then', 'made', 'use', 'of', 'the', 'giza', 'software', 'och', 'and', 'ney', '2000', 'to', 'perform', 'word', 'alignment', 'on', 'the', 'parallel', 'corpora'], ['there', 'are', 'two', 'main', 'approaches', 'to', 'processing', 'nonstandard', 'data', 'normalization', 'and', 'domain', 'adaptation', 'eisenstein', '2013', 'there', 'are', 'two', 'approaches', 'proposed', 'in', 'the', 'literature', 'to', 'handle', 'this', 'problem', 'eisenstein', '2013'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'trained', 'a', 'model', 'using', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'on', 'the', 'training', 'data', 'as', 'our', 'baseline', 'system'], ['the', 'task', 'of', 'identifying', 'mentions', 'to', 'medical', 'concepts', 'in', 'free', 'text', 'and', 'mapping', 'these', 'mentions', 'to', 'a', 'knowledge', 'base', 'was', 'recently', 'proposed', 'in', 'shareclef', 'ehealth', 'evaluation', 'lab', '2013', 'suominen', 'et', 'al', '2013', 'we', 'found', 'the', 'largest', 'of', 'such', 'corpus', 'to', 'be', 'the', 'dataset', 'from', 'the', 'task', 'to', 'recognize', 'disorder', 'mentions', 'in', 'clinical', 'text', 'initially', 'organized', 'by', 'shareclef', 'ehealth', 'evaluation', 'lab', 'shel', 'in', '2013', '', 'suominen', 'et', 'al', '2013'], ['we', 'built', 'a', 'sourcetotarget', 'pbsmt', 'model', 'from', 'the', 'bilingual', 'domain', 'corpus', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007'], ['we', 'also', 'use', 'madatokan', 'habash', 'et', 'al', '2009', 'to', 'preprocess', 'and', 'tokenize', 'the', 'arabic', 'side', 'of', 'the', 'corpuswe', 'use', 'the', 'mada', 'package', 'habash', 'et', 'al', '2009', 'to', 'collect', 'the', 'stem', 'and', 'the', 'morphological', 'features', 'of', 'the', 'hypothesis', 'and', 'reference', 'translation'], ['both', 'training', 'and', 'testing', 'data', 'consist', 'of', 'pubmed', 'abstracts', 'extracted', 'from', 'the', 'genia', 'corpus', 'kim', 'et', 'al', '2008', 'the', 'data', 'provided', 'for', 'the', 'shared', 'task', 'is', 'prepared', 'from', 'the', 'genia', 'corpus', 'kim', 'et', 'al', '2008'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'built', 'a', 'sourcetotarget', 'pbsmt', 'model', 'from', 'the', 'bilingual', 'domain', 'corpus', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['to', 'extract', 'our', 'partofspeech', 'pos', 'features', 'we', 'first', 'tag', 'the', 'transcripts', 'using', 'the', 'nltk', 'pos', 'tagger', 'bird', 'et', 'al', '2009', 'we', 'used', 'the', 'brill', 'tagger', 'provided', 'by', 'nltk', 'for', 'our', 'pos', 'tagging', 'bird', 'et', 'al', '2009'], ['the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'tokenize', 'and', 'truecase', 'all', 'of', 'the', 'corpora', 'using', 'code', 'released', 'with', 'moses', 'koehn', 'et', 'al', '2007', '1'], ['we', 'trained', 'a', 'model', 'using', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'on', 'the', 'training', 'data', 'as', 'our', 'baseline', 'systemour', 'baseline', 'is', 'a', 'phrasebased', 'mt', 'system', 'trained', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'seed', 'and', 'test', 'paradigms', 'we', 'used', 'verbal', 'inflectional', 'paradigms', 'from', 'the', 'celex', 'morphological', 'database', 'baayen', 'et', 'al', '1995', 'for', 'english', 'we', 'used', 'the', 'celex', 'database', 'baayen', 'et', 'al', '1995', 'to', 'segment', 'english', 'words', 'into', 'morphemes'], ['then', 'we', 'did', 'word', 'alignment', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'default', 'growdiagfinaland', 'alignment', 'symmetrization', 'methodwe', 'then', 'run', 'word', 'alignment', 'with', 'giza', 'och', 'and', 'ney', '2003', 'in', 'both', 'directions', 'with', 'the', 'default', 'parameters', 'used', 'in', 'moses'], ['pang', 'et', 'al', '2002', 'have', 'reported', 'the', 'effectiveness', 'of', 'applying', 'machine', 'learning', 'techniques', 'to', 'the', 'pn', 'classificationpang', 'et', 'al', '2002', '', 'use', 'machine', 'learning', 'methods', 'nb', 'svm', 'and', 'maxent', 'to', 'detect', 'sentiments', 'on', 'movie', 'reviews'], ['we', 'train', 'the', 'concept', 'identification', 'stage', 'using', 'infinite', 'ramp', 'loss', '1', 'with', 'adagrad', 'duchi', 'et', 'al', '2011', 'we', 'use', 'online', 'learning', 'to', 'train', 'model', 'parameters', '', 'updating', 'the', 'parameters', 'using', 'the', 'adagrad', 'algorithm', 'duchi', 'et', 'al', '2011'], ['for', 'the', 'contextual', 'check', 'we', 'use', 'the', 'google', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'which', 'contains', 'counts', 'for', 'ngrams', 'from', 'unigrams', 'through', 'to', 'fivegrams', 'obtained', 'from', 'over', '1', 'trillion', 'word', 'tokethe', 'web1t', 'corpus', 'brants', 'and', 'franz', '2006', 'is', 'a', 'dataset', 'consisting', 'of', 'the', 'counts', 'for', 'ngrams', 'obtained', 'from', '1', 'trillion', '10', '12', 'words', 'of', 'english', 'web', 'text', 'subject', 'to', 'a', 'minimum', 'occurrence', 'threshold', '20'], ['we', 'built', 'a', 'sourcetotarget', 'pbsmt', 'model', 'from', 'the', 'bilingual', 'domain', 'corpus', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['bullet', 'naive', 'bayesnb', 'we', 'use', 'binomial', 'variant', 'with', 'laplace', 'smoothing', 'parameter', '', '1', 'pedregosa', 'et', 'al', '2011', 'bullet', 'logistic', 'regressionlr', 'we', 'use', 'logistic', 'regression', 'with', 'l2', 'loss', 'penalty', 'and', 'c1', 'pedregosa', 'et', 'al', '2011'], ['we', 'built', 'a', 'sourcetotarget', 'pbsmt', 'model', 'from', 'the', 'bilingual', 'domain', 'corpus', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'tokenize', 'and', 'frequentcase', 'the', 'data', 'with', 'the', 'standard', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['an', 'unpruned', 'modified', 'kneserneysmoothed', '4gram', 'language', 'model', 'is', 'estimated', 'using', 'the', 'kenlm', 'toolkit', 'heafield', 'et', 'al', '2013', 'the', 'language', 'model', 'is', 'a', 'standard', '5gram', 'model', 'estimated', 'from', 'the', 'monolingual', 'data', 'using', 'modified', 'kneserney', 'smoothing', 'without', 'pruning', 'applying', 'kenlm', 'tools', 'heafield', 'et', 'al', '2013'], ['for', 'native', 'data', 'several', 'teams', 'make', 'use', 'of', 'the', 'web', '1t', '5gram', 'corpus', 'henceforth', 'web1t', 'brants', 'and', 'franz', '2006', 'we', 'used', 'the', 'unigram', 'counts', 'from', 'the', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'to', 'determine', 'the', 'frequency', 'of', 'use', 'of', 'each', 'candidate', 'synonym'], ['we', 'measure', 'statistical', 'significance', 'using', '95', 'confidence', 'intervals', 'computed', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'the', 'column', 'pairci', 'shows', '95', 'confidence', 'intervals', 'relative', 'to', 'the', 'primary', 'system', 'using', 'the', 'paired', 'bootstrap', 'resampling', 'method', 'koehn', '2004'], ['1', 'the', 'models', 'are', 'constructed', 'using', 'c45', 'decision', 'tree', 'classifiers', 'as', 'implemented', 'within', 'weka', 'hall', 'et', 'al', '2009', '', 'with', 'default', 'parameter', 'settingsthe', 'weka', 'smo', 'implementation', 'of', 'svm', 'hall', 'et', 'al', '2009', 'was', 'used', 'as', 'classifier', 'with', 'default', 'parameter', 'settings'], ['it', 'therefore', 'follows', 'the', 'distributional', 'hypothesis', 'harris', '1954', 'which', 'states', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meaningsdistributional', 'similarity', 'relies', 'on', 'the', 'distributional', 'hypothesis', 'that', 'similar', 'terms', 'appear', 'in', 'similar', 'contexts', 'harris', '1954'], ['consider', 'the', 'two', 'examples', 'below', 'drawn', 'from', 'the', 'penn', 'discourse', 'treebank', 'pdtb', 'prasad', 'et', 'al', '2008', '', 'of', 'a', 'causal', 'and', 'a', 'contrast', 'relation', 'respectively', 'pitler', 'and', 'nenkova', '2008', 'used', 'discourse', 'relations', 'of', 'the', 'penn', 'discourse', 'treebank', 'prasad', 'et', 'al', '2008', 'as', 'a', 'feature'], ['ble', 'is', 'based', 'on', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'stating', 'that', 'words', 'with', 'similar', 'meaning', 'have', 'similar', 'distributions', 'across', 'languages', 'distributional', 'similarity', 'relies', 'on', 'the', 'distributional', 'hypothesis', 'that', 'similar', 'terms', 'appear', 'in', 'similar', 'contexts', 'harris', '1954'], ['we', 'use', 'the', 'bleu', 'score', 'as', 'primary', 'criterion', 'which', 'is', 'optimized', 'on', 'the', 'development', 'set', 'using', 'the', 'downhill', 'simplex', 'algorithm', 'press', 'et', 'al', '2002', 'the', 'optimization', 'is', 'done', 'using', 'the', 'downhill', 'simplex', 'algorithm', 'from', 'the', 'numerical', 'recipes', 'book', 'press', 'et', 'al', '2002'], ['the', 'relation', 'prediction', 'task', 'of', 'science', 'ie', 'is', 'challenging', 'and', 'quite', 'different', 'from', 'other', 'semantic', 'relation', 'prediction', 'task', 'like', 'semeval2010', 'task', '8', 'hendrickx', 'et', 'al', '2009', 'the', 'semeval2010', 'task', '8', 'dataset', 'is', 'a', 'widely', 'used', 'benchmark', 'for', 'relation', 'classification', 'hendrickx', 'et', 'al', '2009'], ['machine', 'translation', 'system', 'settings', 'we', 'used', 'a', 'phrasebased', 'statistical', 'machine', 'translation', 'model', 'as', 'implemented', 'in', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'machine', 'translationwe', 'compare', 'the', 'model', 'against', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', '', 'applied', 'to', 'phoneme', 'sequences'], ['we', 'use', 'the', 'mrs', 'analyses', 'that', 'are', 'produced', 'by', 'the', 'hpsg', 'english', 'resource', 'grammar', 'erg', 'flickinger', '2000', 'the', 'experiments', 'are', 'carried', 'out', 'on', 'a', 'broadcoverage', 'linguisticallyprecise', 'hpsg', 'grammar', 'for', 'english', 'the', 'lingo', 'english', 'resource', 'grammar', 'erg', 'copestake', 'and', 'flickinger', '2000'], ['in', 'each', 'case', 'the', 'improvement', 'of', 'ebmt', 'tm', '', 'smt', 'over', 'the', 'baseline', 'smt', 'is', 'statistically', 'significant', 'reliability', 'of', '98', 'using', 'bootstrap', 'resampling', 'koehn', '2004', 'in', 'all', 'cases', 'results', 'are', 'statistically', 'significant', '99', 'following', 'the', 'pair', 'bootstrap', 'resampling', 'koehn', '2004'], ['to', 'capture', 'typical', 'partof', 'bridging', 'see', 'example', '2', 'we', 'extract', 'a', 'list', 'of', '45', 'nouns', 'which', 'specify', 'building', 'parts', 'eg', 'room', 'or', 'roof', 'from', 'the', 'general', 'inquirer', 'lexicon', 'stone', 'et', 'al', '1966', 'we', 'extract', 'a', 'list', 'containing', 'around', '4000', 'relational', 'nouns', 'from', 'wordnet', 'and', 'a', 'list', 'containing', 'around', '500', 'nouns', 'that', 'specify', 'professional', 'roles', 'from', 'the', 'general', 'inquirer', 'lexicon', 'stone', 'et', 'al', '1966'], ['the', 'surfacesyntactic', 'representation', '', 'p', 'was', 'a', 'standard', 'firstorder', 'edge', 'factorization', 'using', 'the', 'same', 'features', 'as', 'mcdonald', 'et', 'al', '2005', 'the', 'number', 'of', 'features', 'extracted', 'from', 'the', 'pdt', 'training', 'set', 'was', '13', '450', '672', 'using', 'the', 'feature', 'set', 'outlined', 'by', 'mcdonald', 'et', 'al', '2005'], ['in', 'our', 'work', 'like', 'hernault', 'et', 'al', '2010', '', 'we', 'also', 'consider', 'the', 'discourse', 'segmentation', 'task', 'as', 'a', 'sequence', 'labeling', 'problemsimilar', 'to', 'the', 'work', 'of', 'hernault', 'et', 'al', '2010', '', 'our', 'base', 'model', 'uses', 'conditional', 'random', 'fields', '1', 'to', 'learn', 'a', 'sequence', 'labeling', 'model'], ['we', 'use', 'the', 'data', 'that', 'were', 'recorded', 'and', 'preprocessed', 'by', 'mitchell', 'et', 'al', '2008', '', 'available', 'for', 'download', 'in', 'their', 'supporting', 'online', 'material1', 'full', 'details', 'of', 'the', 'experimental', 'protocol', 'data', 'acquisition', 'and', 'preprocessing', 'can', 'be', 'found', 'in', 'mitchell', 'et', 'al', '2008', 'and', 'the', 'supporting', 'material'], ['we', 'trained', 'nonprojective', 'dependency', 'parsers', 'for', '6', 'languages', 'using', 'the', 'conllx', 'shared', 'task', 'datasets', 'buchholz', 'and', 'marsi', '2006', '', 'arabic', 'danish', 'dutch', 'japanese', 'slovene', 'and', 'spanishfor', 'nonprojective', 'parsing', 'experiments', 'four', 'languages', 'from', 'the', 'conll', 'x', 'shared', 'task', 'are', 'used', 'danish', 'dutch', 'slovene', 'and', 'swedish', 'buchholz', 'and', 'marsi', '2006'], ['our', 'lp', 'constraints', 'based', 'on', 'the', 'new', 'type', 'marker', 'are', 'compatible', 'with', 'those', 'of', 'cho', '', 'chai', '2000', 'on', 'the', 'one', 'hand', 'and', 'are', 'able', 'to', 'deal', 'with', 'the', 'scrambling', 'between', 'adjuncts', 'and', 'arguments', 'on', 'the', 'other', 'handfor', 'the', 'theory', 'of', 'cho', '', 'chai', '2000', 'to', 'be', 'complete', 'we', 'have', 'proposed', 'a', 'new', 'type', 'of', 'marker', 'and', 'the', 'adjunct', 'lp', 'constraint', 'in', 'conjunction', 'with', 'their', 'argument', 'lp', 'constraint'], ['the', 'wsj', 'grammar', 'covers', 'the', 'upenn', 'wall', 'street', 'journal', 'wsj', 'treebank', 'sentences', 'marcus', 'et', 'al', '1994', 'the', 'approach', 'presented', 'in', 'this', 'paper', 'is', 'a', 'first', 'attempt', 'to', 'scale', 'up', 'stochastic', 'parsing', 'systems', 'based', 'on', 'linguistically', 'finegrained', 'handcoded', 'grammars', 'to', 'the', 'upenn', 'wall', 'street', 'journal', 'henceforth', 'wsj'], ['statistically', 'significant', 'results', 'calculated', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'for', 'bleu', 'and', 'nist', 'are', 'indicated', 'with', 'symbols', 'p', '', '001', 'and', 'p', '', '005we', 'calculated', 'significance', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004'], ['in', 'testing', 'we', 'used', 'minimum', 'bayes', 'risk', 'decoding', 'kumar', 'and', 'byrne', '2004', '', 'cube', 'pruning', 'and', 'the', 'operation', 'sequence', 'model', 'durrani', 'et', 'al', '2011', 'selects', 'the', 'translation', 'with', 'minimum', 'bayes', 'risk', 'kumar', 'and', 'byrne', '2004'], ['selects', 'the', 'translation', 'with', 'minimum', 'bayes', 'risk', 'kumar', 'and', 'byrne', '2004', 'additionally', 'we', 'will', 'compare', 'two', 'decision', 'rules', 'the', 'common', 'maximum', 'aposteriori', 'map', 'decision', 'rule', 'and', 'the', 'minimum', 'bayes', 'risk', 'mbr', 'decision', 'rule', 'kumar', 'and', 'byrne', '2004'], ['we', 'use', 'a', 'prototypebased', 'selectional', 'preference', 'model', 'erk', '2007', 'we', 'build', 'on', 'a', 'recent', 'selectional', 'preference', 'model', 'erk', '2007', 'that', 'bases', 'its', 'generalisations', 'on', 'word', 'similarity', 'in', 'a', 'vector', 'space'], ['we', 'used', '10fold', 'crossvalidation', 'set', 'the', 'confidence', 'interval', 'to', '95', 'and', 'used', 'the', 'jackknifing', 'procedure', 'for', 'multiannotation', 'evaluation', 'lin', '2004', 'rouge2', 'metric', 'lin', '2004', 'is', 'used', 'for', 'the', 'evaluation'], ['the', 'brown', 'corpus', 'tagged', 'with', 'wordnet', 'senses', 'miller', 'et', 'al', '1993', 'the', 'senses', 'in', 'wordnet', 'are', 'ordered', 'according', 'to', 'the', 'frequency', 'data', 'in', 'the', 'manually', 'tagged', 'resource', 'sem', 'cor', 'miller', 'et', 'al', '1993'], ['as', 'an', 'implementation', 'we', 'use', 'svm', 'light', 'joachims', '1999', 'as', 'a', 'supervised', 'classifier', 'for', 'vsm', 'and', 'wiki', 'we', 'chose', 'support', 'vector', 'machines', 'using', 'svm', 'light', 'joachims', '1999'], ['for', 'translation', 'tables', '', 'the', 'moses', 'system', 'koehn', 'et', 'al', '2007', 'as', 'well', 'as', 'portage', 'offer', 'model', 'filtering', 'moses', 'offline', 'portage', 'offline', 'andor', 'at', 'load', 'timewe', 'compare', 'the', 'model', 'against', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', '', 'applied', 'to', 'phoneme', 'sequences'], ['the', 'particle', 'filter', 'of', 'canini', 'et', 'al', '2009', 'rejuvenates', 'over', 'independent', 'draws', 'from', 'the', 'history', 'by', 'storing', 'all', 'past', 'observations', 'and', 'statesthe', 'particle', 'filter', 'studied', 'empirically', 'by', 'canini', 'et', 'al', '2009', '', 'stored', 'the', 'entire', 'history', 'incurring', 'linear', 'storage', 'complexity', 'in', 'the', 'size', 'of', 'the', 'stream'], ['we', 'describe', 'an', 'approximation', 'to', 'the', 'bleu', 'score', 'papineni', 'et', 'al', '2001', 'that', 'will', 'satisfy', 'these', 'conditions', 'we', 'here', 'describe', 'a', 'linear', 'approximation', 'to', 'the', 'logbleu', 'score', 'papineni', 'et', 'al', '2001', 'which', 'allows', 'such', 'a', 'decomposition'], ['we', 'applied', 'the', 'naive', 'bayes', 'probabilistic', 'supervised', 'learning', 'algorithm', 'from', 'the', 'weka', 'machine', 'learning', 'library', 'hall', 'et', 'al', '2009', 'we', 'conducted', 'experiments', 'using', 'multinomial', 'naive', 'bayes', 'classifier', 'implemented', 'in', 'the', 'weka', 'toolkit', 'hall', 'et', 'al', '2009'], ['we', 'minimize', 'the', 'cross', 'entropy', 'loss', 'using', 'gradientbased', 'optimization', 'and', 'the', 'adam', 'update', 'rule', 'kingma', 'and', 'ba', '2014', 'the', 'network', 'is', 'trained', 'using', 'sgd', 'with', 'shuffled', 'minibatches', 'using', 'the', 'adam', 'update', 'rule', 'kingma', 'and', 'ba', '2014'], ['statistical', 'significance', 'in', 'bleu', 'score', 'difference', 'was', 'measured', 'by', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'statistical', 'significance', 'of', 'the', 'difference', 'between', 'systems', 'is', 'computed', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'p', '', '005', '1', '000', 'iterations'], ['we', 'measure', 'statistical', 'significance', 'using', '95', 'confidence', 'intervals', 'computed', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'for', 'all', 'results', 'we', 'computed', 'their', 'confidence', 'intervals', 'p', '', '005', 'by', 'means', 'of', 'bootstrap', 'resampling', 'koehn', '2004'], ['3', 'as', 'verbs', 'we', 'take', 'all', 'tags', 'that', 'map', 'to', 'v', 'in', 'the', 'universal', 'tag', 'mappings', 'from', 'petrov', 'et', 'al', '2012', 'in', 'the', 'pos', 'tag', 'level', 'we', 'basically', 'used', 'the', 'universal', 'tagset', 'proposed', 'by', 'petrov', 'et', 'al', '2012', '', 'in', 'mapping', 'original', 'tags', 'into', 'universal', 'ones'], ['we', 'use', 'the', 'mrs', 'analyses', 'that', 'are', 'produced', 'by', 'the', 'hpsg', 'english', 'resource', 'grammar', 'erg', 'flickinger', '2000', 'here', 'we', 'use', '1', 'the', 'link', 'grammar', 'parser', '8', 'and', '2', 'the', 'hpsg', 'english', 'resource', 'grammar', 'copestake', 'and', 'flickinger', '2000', 'and', 'pet', 'parser'], ['we', 'measure', 'statistical', 'significance', 'using', '95', 'confidence', 'intervals', 'computed', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'for', 'statistical', 'significance', 'testing', 'we', 'use', 'a', 'paired', 'bootstrap', 'resampling', 'method', 'proposed', 'in', 'koehn', '2004'], ['on', 'semantic', 'role', 'labeling', 'gildea', 'and', 'jurafsky', '2002', 'first', 'presented', 'a', 'system', 'based', 'on', 'a', 'statistical', 'classifier', 'which', 'is', 'trained', 'on', 'a', 'handannotated', 'corpora', 'framenetgildea', 'and', 'jurafsky', '2002', '', 'were', 'the', 'first', 'to', 'describe', 'a', 'statistical', 'system', 'trained', 'on', 'the', 'data', 'from', 'the', 'framenet', 'project', 'to', 'automatically', 'assign', 'semantic', 'roles'], ['we', 'apply', 'the', 'stochastic', 'gradient', 'descent', 'algorithm', 'with', 'minibatches', 'and', 'the', 'adadelta', 'update', 'rule', 'zeiler', '2012', 'we', 'use', 'a', 'minibatch', 'size', 'of', '100', 'and', 'use', 'adadelta', 'zeiler', '2012', 'as', 'the', 'gradient', 'update', 'method'], ['we', 'used', 'the', 'linear', 'svm', 'implementation', 'with', 'default', 'parameters', 'and', 'random', 'forest', 'with', '50', 'trees', 'both', 'available', 'at', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'we', 'use', 'the', 'scikit', 'implementation', 'of', 'svm', 'pedregosa', 'et', 'al', '2011'], ['we', 'test', 'the', 'statistical', 'significance', 'of', 'differences', 'between', 'various', 'mt', 'systems', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004', 'statistical', 'significance', 'tests', 'are', 'performed', 'using', 'bootstrap', 'resampling', 'koehn', '2004'], ['in', 'particular', 'the', 'parser', 'implements', 'the', 'arcstandard', 'parsing', 'algorithm', 'nivre', '2004', '1', 'the', 'parser', 'implements', 'the', 'arcstandard', 'algorithm', 'nivre', '2004', 'and', 'it', 'therefore', 'makes', 'use', 'of', 'a', 'stack', 'and', 'a', 'buffer'], ['we', 'use', 'adadelta', 'zeiler', '2012', 'to', 'update', 'the', 'parameters', 'during', 'trainingin', 'the', 'training', 'procedure', 'we', 'use', 'adadelta', 'zeiler', '2012', 'to', 'update', 'model', 'parameters', 'with', 'a', 'minibatch', 'size', '80'], ['we', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005', 'the', 'systems', 'for', 'the', 'english', '', 'spanish', 'translation', 'tasks', 'were', 'trained', 'on', 'the', 'sentencealigned', 'europarl', 'corpus', 'koehn', '2005'], ['ccg', 'is', 'a', 'lexicalized', 'theory', 'of', 'grammar', 'steedman', '2001', 'the', 'input', 'of', 'the', 'boxer', 'system', 'is', 'a', 'syntactic', 'analysis', 'in', 'the', 'form', 'of', 'a', 'derivation', 'of', 'combinatorial', 'categorial', 'grammar', 'ccg', 'steedman', '2001'], ['the', 'results', 'for', 'teslam', 'and', 'teslaf', 'have', 'previously', 'been', 'reported', 'in', 'liu', 'et', 'al', '2010', '3', 'tesla', 'translation', 'evaluation', 'of', 'sentences', 'with', 'linearprogrammingbased', 'analysis', 'was', 'first', 'proposed', 'in', 'liu', 'et', 'al', '2010'], ['the', 'semantic', 'representation', 'is', 'minimal', 'recursion', 'semantics', 'copestake', 'et', 'al', '2005', 'the', 'erg', 'produces', 'minimal', 'recursion', 'semantics', 'mrs', 'copestake', 'et', 'al', '2005', 'analyses', 'which', 'are', 'flat', 'structures', 'that', 'explicitly', 'encode', 'predicate', 'argument', 'relations', 'and', 'other', 'data'], ['germanet', 'gn', 'is', 'the', 'german', 'counterpart', 'to', 'wn', 'hamp', 'and', 'feldweg', '1997', 'future', 'work', 'could', 'be', 'to', 'extend', 'the', 'german', 'dataset', 'by', 'adding', 'additional', 'resources', 'to', 'the', 'llr', 'for', 'instance', 'germanet', 'hamp', 'and', 'feldweg', '1997'], ['baroni', 'et', 'al', '2002', '', 'report', 'that', '47', 'of', 'the', 'vocabulary', 'types', 'in', 'the', 'apa', 'corpus', '2', 'were', 'compounds', 'baroni', 'et', 'al', '2002', '', 'analyzed', 'the', '28', 'million', 'words', 'german', 'apa', 'news', 'corpus', 'and', 'discovered', 'that', 'compounds', 'account', 'for', '47', 'of', 'the', 'word', 'types', 'but', 'only', '7', 'of', 'the', 'overall', 'token', 'count', 'with', '83', 'of', 'compounds'], ['both', 'language', 'models', 'use', 'modified', 'kneserney', 'smoothing', 'chen', 'and', 'goodman', '1996', '5gram', 'language', 'models', 'are', 'trained', 'over', 'the', 'targetside', 'of', 'the', 'training', 'data', 'using', 'srilm', 'stolcke', '2002', 'with', 'modified', 'kneserney', 'discounting', 'chen', 'and', 'goodman', '1996'], ['we', 'used', 'mallet', 'mccallum', '2002', 'for', 'this', 'experiment', 'and', 'the', 'same', 'trainingtesting', 'partitions', 'used', 'in', 'the', 'experiment', 'reported', 'in', 'table', '3', '10', 'we', 'used', 'the', 'pltm', 'implementation', 'in', 'mallet', 'mccallum', '2002'], ['we', 'calculated', 'significance', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'the', 'statistical', 'significance', 'test', 'is', 'also', 'carried', 'out', 'with', 'paired', 'bootstrap', 'resampling', 'method', 'with', 'p', '', '0001', 'intervals', 'koehn', '2004'], ['additionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'finally', 'we', 'used', 'moses', 'toolkit', 'as', 'phrasebased', 'reference', 'koehn', 'et', 'al', '2007'], ['for', 'building', 'the', 'baseline', 'smt', 'system', 'we', 'used', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', '', 'in', 'its', 'standard', 'setupwe', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settings'], ['this', 'results', 'in', 'the', 'semantic', 'triple', 'shotmanbird', 'lemmatized', 'to', 'shootmanbird', 'using', 'the', 'stanford', 'corenlp', 'lemmatizer', 'manning', 'et', 'al', '2014', 'we', 'also', 'lemmatized', 'all', 'words', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['we', 'used', 'standard', 'classifiers', 'available', 'in', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011', '3', 'we', 'used', 'logistic', 'regression', 'though', 'linear', 'svm', 'showed', 'almost', 'the', 'same', 'results', 'for', 'the', 'final', 'reranking', 'the', 'implementation', 'was', 'taken', 'from', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011'], ['rhetorical', 'structure', 'theory', 'mann', 'and', 'thompson', '1988', 'belongs', 'to', 'the', 'first', 'sortthe', 'closest', 'area', 'to', 'our', 'work', 'consists', 'of', 'investigations', 'of', 'discourse', 'relations', 'in', 'the', 'context', 'of', 'rhetorical', 'structure', 'theory', 'mann', 'and', 'thompson', '1988'], ['both', 'language', 'models', 'use', 'modified', 'kneserney', 'smoothing', 'chen', 'and', 'goodman', '1996', '5gram', 'language', 'models', 'are', 'trained', 'over', 'the', 'targetside', 'of', 'the', 'training', 'data', 'using', 'srilm', 'stolcke', '2002', 'with', 'modified', 'kneserney', 'discounting', 'chen', 'and', 'goodman', '1996'], ['we', 'train', 'the', 'concept', 'identification', 'stage', 'using', 'infinite', 'ramp', 'loss', 'with', 'adagrad', 'duchi', 'et', 'al', '2011', '1', 'using', 'adagrad', 'duchi', 'et', 'al', '2011'], ['for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'all', 'our', 'translation', 'systems', 'are', 'based', 'on', 'moses', 'koehn', 'et', 'al', '2007', 'and', 'standard', 'components', 'for', 'training', 'and', 'tuning', 'the', 'models'], ['we', 'use', 'rouge', 'score', 'as', 'our', 'evaluation', 'metric', 'lin', '2004', 'with', 'standard', 'options', '8', 'we', 'expect', 'this', 'restriction', 'is', 'more', 'consistent', 'with', 'the', 'rouge', 'evaluation', 'metric', 'used', 'for', 'summarization', 'lin', '2004'], ['for', 'building', 'the', 'baseline', 'smt', 'system', 'we', 'used', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', '', 'in', 'its', 'standard', 'setupfor', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'europarl', 'parallel', 'corpus', 'koehn', '2005', 'as', 'the', 'bitext', 'from', 'which', 'to', 'extract', 'the', 'auto', 'matic', 'bilingual', 'lexiconswe', 'extract', 'our', 'paraphrase', 'grammar', 'from', 'the', 'frenchenglish', 'portion', 'of', 'the', 'europarl', 'corpus', 'version', '5', 'koehn', '2005'], ['we', 'use', 'ridge', 'regression', 'rr', 'with', 'l2norm', 'regularization', 'and', 'support', 'vector', 'regression', 'svr', 'with', 'an', 'rbf', 'kernel', 'from', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'bullet', 'logistic', 'regressionlr', 'we', 'use', 'logistic', 'regression', 'with', 'l2', 'loss', 'penalty', 'and', 'c1', 'pedregosa', 'et', 'al', '2011'], ['all', 'data', 'used', 'in', 'our', 'experiments', 'are', 'sentencesplit', 'lowercased', 'and', 'tokenize', 'using', 'the', 'corenlp', 'toolkit', 'manning', 'et', 'al', '2014', 'we', 'tokenize', 'english', 'data', 'and', 'segment', 'chinese', 'data', 'using', 'the', 'stanford', 'corenlp', 'toolkit', 'manning', 'et', 'al', '2014'], ['since', 'the', 'phrase', 'table', 'contains', 'lemmas', 'the', 'wikipedia', 'corpus', 'was', 'lemmatised', 'using', 'the', 'treetagger', 'schmid', '1994', '1', 'the', 'corpus', 'is', 'lemmatised', 'and', 'tagged', 'by', 'partofspeech', 'on', 'both', 'sides', 'using', 'the', 'treetagger', 'schmid', '1994'], ['for', 'this', 'purpose', 'we', 'used', 'the', 'logistic', 'regression', 'classifier', 'from', 'scikitlearn', 'with', 'l2', 'regularisation', 'pedregosa', 'et', 'al', '2011', '1', 'we', 'use', 'logistic', 'regression', 'with', 'l2', 'regularization', 'implemented', 'using', 'the', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011'], ['all', 'our', 'systems', 'are', 'contrasted', 'with', 'a', 'standard', 'phrasebased', 'system', 'built', 'with', 'moses', 'koehn', 'et', 'al', '2007', 'our', 'machine', 'translation', 'systems', 'this', 'year', 'are', 'a', 'departure', 'from', 'our', 'previous', 'moses', 'koehn', 'et', 'al', '2007', 'based', 'systems', 'from', 'wmt16'], ['we', 'test', 'our', 'metrics', 'in', 'the', 'setting', 'of', 'the', 'wmt', '2009', 'evaluation', 'task', 'callisonburch', 'et', 'al', '2009', 'the', 'compared', 'systems', 'are', 'evaluated', 'on', 'the', 'englishtogerman', '13', 'news', 'translation', 'task', 'of', 'wmt', '2009', 'callisonburch', 'et', 'al', '2009'], ['these', 'classifiers', 'have', 'been', 'used', 'in', 'related', 'work', 'by', 'pang', 'et', 'al', '2002', 'we', 'evaluated', 'our', 'method', 'with', 'movie', 'review', 'documents', 'that', 'were', 'used', 'in', 'pang', 'et', 'al', '2002'], ['one', 'is', 'from', 'turian', 'et', 'al', '2010', '', 'the', 'dimension', 'of', 'word', 'embedding', 'is', '50for', 'example', 'turian', 'et', 'al', '2010', '', 'showed', 'that', 'the', 'optimal', 'dimensionality', 'for', 'word', 'embeddings', 'is', 'taskspecific'], ['word', 'alignment', 'using', 'giza', 'toolkit', 'och', 'and', 'ney', '2000', '', 'the', 'default', 'configuration', 'as', 'available', 'in', 'training', 'scripts', 'for', 'mosesword', 'alignment', 'is', 'performed', 'by', 'giza', 'och', 'and', 'ney', '2000', 'in', 'both', 'directions', 'with', 'the', 'default', 'setting'], ['we', 'found', 'that', 'using', 'adagrad', 'duchi', 'et', 'al', '2011', 'to', 'update', 'the', 'parameters', 'is', 'very', 'effective', '1', 'using', 'adagrad', 'duchi', 'et', 'al', '2011'], ['the', 'penn', 'discourse', 'treebank', 'pdtb', 'prasad', 'et', 'al', '2008', 'is', 'a', 'large', 'corpus', 'annotated', 'with', 'discourse', 'relations', 'covering', 'the', 'wall', 'street', 'journal', 'part', 'of', 'the', 'penn', 'treebankone', 'of', 'the', 'most', 'important', 'resources', 'for', 'discourse', 'connectives', 'in', 'english', 'is', 'the', 'penn', 'discourse', 'treebank', 'prasad', 'et', 'al', '2008'], ['the', 'system', 'was', 'trained', 'on', 'the', 'english', 'and', 'danish', 'part', 'of', 'the', 'europarl', 'corpus', 'version', '3', 'koehn', '2005', 'the', 'europarl', 'data', 'set', 'consists', 'of', '707', 'sentences', 'of', 'the', 'german', 'part', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['for', 'example', 'turian', 'et', 'al', '2010', 'compared', 'brown', 'clusters', 'cw', 'embeddings', 'and', 'hlbl', 'embeddings', 'in', 'ner', 'and', 'chunking', 'tasksturian', 'et', 'al', '2010', 'applied', 'word', 'embeddings', 'to', 'chunking', 'and', 'named', 'entity', 'recognition', 'ner'], ['we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007', 'in', 'particular', 'we', 'use', 'moses', 'koehn', 'et', 'al', '2007', 'to', 'train', 'a', 'monolingual', 'phrasebased', 'mt', 'system', 'on', 'the', 'paralex', 'corpus'], ['past', 'experiences', 'on', 'this', 'system', 'have', 'shown', 'that', 'the', 'random', 'forest', 'breiman', '2001', 'outperforms', 'other', 'regression', 'algorithms', 'like', 'support', 'vector', 'regression', 'or', 'multilayer', 'perceptronour', 'submitted', 'system', 'for', 'the', 'second', 'task', 'is', 'based', 'on', 'random', 'forest', 'breiman', '2001'], ['we', 'trained', 'a', 'number', 'of', 'frenchenglish', 'smt', 'systems', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'in', 'its', 'default', 'settingwe', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'train', 'standard', 'phrasebased', 'systems', 'with', 'default', 'configurations'], ['bullet', 'zhang2015', 'zhang', 'et', 'al', '2015', 'proposed', 'to', 'use', 'shallow', 'convolutional', 'neural', 'networks', 'to', 'model', 'two', 'arguments', 'respectivelyzhang', 'et', 'al', '2015', 'explore', 'a', 'shallow', 'convolutional', 'neural', 'network', 'and', 'achieve', 'competitive', 'performance'], ['all', 'data', 'used', 'in', 'our', 'experiments', 'are', 'sentencesplit', 'lowercased', 'and', 'tokenize', 'using', 'the', 'corenlp', 'toolkit', 'manning', 'et', 'al', '2014', 'we', 'tokenize', 'english', 'data', 'and', 'segment', 'chinese', 'data', 'using', 'the', 'stanford', 'corenlp', 'toolkit', 'manning', 'et', 'al', '2014'], ['the', 'major', 'part', 'of', 'data', 'comes', 'from', 'current', 'and', 'upcoming', 'film', 'releases', 'of', 'the', 'europarl', 'dataset', 'koehn', '2005', 'the', 'source', 'of', 'bilingual', 'data', 'used', 'in', 'the', 'experiments', 'is', 'the', 'europarl', 'collection', 'koehn', '2005'], ['we', 'expect', 'this', 'restriction', 'is', 'more', 'consistent', 'with', 'the', 'rouge', 'evaluation', 'metric', 'used', 'for', 'summarization', 'lin', '2004', 'rouge', 'lin', '2004', 'is', 'the', 'fully', 'automatic', 'metric', 'commonly', 'used', 'to', 'evaluate', 'the', 'text', 'summarization', 'results'], ['additionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'train', 'standard', 'phrasebased', 'systems', 'with', 'default', 'configurations'], ['additionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'decoder', 'searches', 'for', 'the', 'best', 'translation', 'given', 'a', 'set', 'of', 'models', 'h', 'm', 'e', 'i', '1', '', 's', 'k', '1', '', 'f', 'j', '1', 'by', 'maximizing', 'the', 'loglinear', 'feature', 'score', 'och', 'and', 'ney', '2004', '', 'e', 'i', '1', '', 'arg', 'max', 'ie', 'i', '1', 'm', 'm1the', 'loglinear', 'approach', 'to', 'phrasebased', 'translation', 'och', 'and', 'ney', '2004', 'directly', 'models', 'the', 'predictive', 'translation', 'distribution', 'pef', '', 'w', '', '1', 'zf', 'exp', 'w', 'e', 'f', '1', 'where', 'e', 'is', 'the', 'target', 'string'], ['the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'train', 'standard', 'phrasebased', 'systems', 'with', 'default', 'configurations'], ['the', 'usages', 'from', 'the', 'ukwac', 'are', 'tokenised', 'and', 'lemmatised', 'using', 'treetagger', 'schmid', '1994', '', 'as', 'provided', 'by', 'the', 'corpus3', 'all', 'english', 'data', 'are', 'pos', 'tagged', 'and', 'lemmatised', 'using', 'the', 'treetagger', 'schmid', '1994'], ['to', 'quantify', 'the', 'redundancy', 'of', 'structures', 'we', 'partof', 'speech', 'tagger', 'the', 'english', 'gigaword', 'corpus', 'graff', 'and', 'cieri', '2003', 'the', 'trigram', 'target', 'language', 'model', 'is', 'trained', 'from', 'the', 'xinhua', 'portion', 'of', 'english', 'gigaword', 'corpus', 'graff', 'and', 'cieri', '2003'], ['baseline', 'word', 'alignments', 'were', 'obtained', 'by', 'running', 'giza', 'in', 'both', 'directions', 'and', 'symmetrizing', 'using', 'the', 'growdiagfinaland', 'heuristic', 'och', 'and', 'ney', '2003', 'translation', 'models', 'were', 'trained', 'over', 'the', 'bilingual', 'data', 'that', 'was', 'automatically', 'wordaligned', 'using', 'giza', 'och', 'and', 'ney', '2003', 'in', 'both', 'directions', '', 'and', 'the', 'diaggrowfinal', 'heuristic', 'was', 'used', 'to', 'ldc200'], ['the', 'results', 'displayed', 'in', 'table', '3', 'are', 'obtained', 'with', 'the', 'smo', 'classifier', 'trained', 'using', 'the', 'weka', 'library', 'hall', 'et', 'al', '2009', 'on', 'our', 'downloaded', 'semeval', '2013', 'development', 'and', 'training', 'corpora', '7595', 'tweetsthe', 'core', 'model', 'is', 'a', 'decision', 'tree', 'classifier', 'trained', 'on', 'the', 'qalb', 'parallel', 'training', 'data', 'using', 'weka', 'hall', 'et', 'al', '2009'], ['we', 'trained', 'a', 'number', 'of', 'frenchenglish', 'smt', 'systems', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'in', 'its', 'default', 'settingwe', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'performance', 'of', 'our', 'algorithm', 'is', 'compared', 'with', 'the', 'disambiguation', 'accuracy', 'obtained', 'with', 'a', 'variation', 'of', 'the', 'lesk', 'algorithm', '3', 'lesk', '1986', '', 'which', 'selects', 'the', 'meaning', 'of', 'an', 'openclass', 'word', 'by', 'findinote', 'that', 'word', 'sense', 'disambiguation', 'is', 'performed', 'using', 'the', 'lesk', 'algorithm', 'lesk', '1986'], ['we', 'use', 'the', 'scikit', 'implementation', 'of', 'svm', 'pedregosa', 'et', 'al', '2011', 'we', 'use', 'a', 'wellknown', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'implementation', 'of', 'random', 'forests', 'and', 'svms', 'in', 'python', 'as', 'well', 'as', 'a', 'very', 'effective', 'gradient', 'boosting', 'trees', 'implementation', 'from', 'the', 'xgboost', 'libr'], ['for', 'example', 'turian', 'et', 'al', '2010', 'showed', 'that', 'the', 'optimal', 'dimensionality', 'for', 'word', 'embeddings', 'is', 'task', 'specific', 'turian', 'et', 'al', '2010', 'applied', 'word', 'embeddings', 'to', 'chunking', 'and', 'named', 'entity', 'recognition', 'ner'], ['ease', 'uses', 'nltk', 'bird', 'et', 'al', '2009', 'for', 'pos', 'tagging', 'and', 'stemming', 'a', 'spell', 'for', 'spell', 'checking', 'and', 'wordnet', 'fellbaum', '1998', 'to', 'get', 'the', 'synonyms', 'we', 'used', 'the', 'brill', 'tagger', 'provided', 'by', 'nltk', 'for', 'our', 'pos', 'tagging', 'bird', 'et', 'al', '2009'], ['all', 'our', 'translation', 'systems', 'are', 'based', 'on', 'moses', 'koehn', 'et', 'al', '2007', 'and', 'standard', 'components', 'for', 'training', 'and', 'tuning', 'the', 'modelsall', 'our', 'systems', 'are', 'contrasted', 'with', 'a', 'standard', 'phrasebased', 'system', 'built', 'with', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007'], ['additionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'thank', 'gbor', 'recski', 'has', 'research', 'institute', 'for', 'linguistics', 'for', 'performing', 'the', 'pca', 'on', 'the', 'operators', 'of', 'the', 'socher', 'et', 'al', '2013', 'cvgwe', 'employed', 'the', 'stateoftheart', 'sentiment', 'annotator', 'by', 'socher', 'et', 'al', '2013', 'for', 'this', 'purpose'], ['it', 'has', 'been', 'shown', 'in', 'previous', 'work', 'on', 'relation', 'extraction', 'that', 'the', 'shortest', 'dependency', 'path', 'between', 'any', 'two', 'entities', 'captures', 'the', 'information', 'required', 'to', 'assert', 'a', 'relationship', 'between', 'them', 'bunescu', 'and', 'mooney', '2005', 'our', 'work', 'is', 'also', 'related', 'to', 'bunescu', 'and', 'mooney', '2005', '', 'where', 'the', 'similarity', 'between', 'the', 'words', 'on', 'the', 'path', 'connecting', 'two', 'entities', 'in', 'the', 'dependency', 'graph', 'is', 'used', 'to', 'devise', 'a', 'kernel', 'function'], ['these', 'algorithms', 'were', 'used', 'to', 'participate', 'in', 'the', 'the', 'expression', 'level', 'task', 'subtask', 'a', 'and', 'message', 'level', 'task', 'subtask', 'b', 'of', 'the', 'semeval2014', 'task', '9', 'sentiment', 'analysis', 'in', 'twitter', 'rosenthal', 'et', 'al', 'the', 'system', 'that', 'obtained', 'the', 'best', 'performance', 'in', 'the', 'sentiment', 'analysis', 'at', 'message', 'level', 'task', 'of', 'semeval', '2013', 'nakov', 'et', 'al', '2013', 'and', '2014', 'rosenthal', 'et', 'al', '2014', 'mined', 'twitter', 'to', 'build', 'big', 'sent'], ['phrasebased', 'smt', 'system', 'a', 'standard', 'non', 'factored', 'phrasebased', 'smt', 'system', 'was', 'built', 'using', 'the', 'open', 'source', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'parameters', 'set', 'similar', 'to', 'those', 'of', 'neubig', '2011smt', 'translations', 'a', 'phrasebased', 'smt', 'system', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'and', 'the', 'whole', 'spanishenglish', 'dataset', 'except', 'the', 'sentences', 'in', 'the', 'test', 'set', 'was', 'used', 'to', 'translated', 'the'], ['for', 'the', 'theory', 'of', 'cho', '', 'chai', '2000', 'to', 'be', 'complete', 'we', 'have', 'proposed', 'a', 'new', 'type', 'of', 'marker', 'and', 'the', 'adjunct', 'lp', 'constraint', 'in', 'conjunction', 'with', 'their', 'argument', 'lp', 'constraintgiven', 'the', 'lp', 'constraints', 'in', '4', '7', 'and', '8', 'we', 'can', 'provide', 'a', 'simpler', 'explanation', 'to', 'various', 'scrambled', 'sentences', 'including', 'the', 'alleged', 'counterexamples', 'to', 'the', 'analysis', 'of', 'cho', '', 'chai', '2000'], ['all', 'linguistic', 'annotations', 'needed', 'for', 'features', 'pos', 'chunks', '7', '', 'parses', 'are', 'from', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'part', 'of', 'speech', 'tagging', 'and', 'named', 'entity', 'recognition', 'for', 'comment', 'plausibility', 'features', 'are', 'carried', 'out', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['algorithm', '5', 'shows', 'a', 'passiveaggressive', 'algorithm', 'for', 'the', 'structured', 'output', 'crammer', 'et', 'al', '2006', 'to', 'build', 'a', 'parser', 'we', 'use', 'a', 'structured', 'classifier', 'to', 'approximate', 'the', 'oracle', 'and', 'apply', 'the', 'passiveaggressive', 'pa', 'algorithm', 'crammer', 'et', 'al', '2006', 'for', 'parameter', 'estimation'], ['the', 'task', 'is', 'part', 'of', 'the', 'semantic', 'evaluation', '2012', 'workshop', 'agirre', 'et', 'al', '2012', 'semantic', 'textual', 'similarity', 'sts', 'is', 'the', 'task', 'of', 'judging', 'the', 'similarity', 'of', 'a', 'pair', 'of', 'sentences', 'on', 'a', 'scale', 'from', '1', 'to', '5', 'agirre', 'et', 'al', '2012'], ['we', 'used', 'the', 'same', 'annotation', 'guidelines', 'as', 'zaidan', 'et', 'al', '2007', 'we', 'also', 'use', 'a0', 'to', 'compare', 'against', 'the', 'masking', 'svm', 'method', 'and', 'svm', 'baseline', 'of', 'zaidan', 'et', 'al', '2007'], ['the', 'stanford', 'parser', '1', 'marneffe', 'et', 'al', '2006', 'was', 'used', 'to', 'produce', 'all', 'dependency', 'parsesthe', 'same', 'kind', 'of', 'process', 'was', 'applied', 'to', 'the', 'penn', 'treebank', 'using', 'the', 'stanford', 'conversion', 'system', 'to', 'produce', 'dependency', 'annotations', 'de', 'marneffe', 'et', 'al', '2006'], ['1', 'after', 'tokenization', '', 'we', 'lemmatize', 'and', 'stem', 'tweets', 'and', 'remove', 'stopwords', 'from', 'each', 'tweet', 'using', 'the', 'nltk', 'toolkit', 'bird', 'et', 'al', '2009', 'for', 'reuters', 'we', 'segmented', 'and', 'tokenized', 'the', 'data', 'using', 'nltk', 'bird', 'et', 'al', '2009'], ['machine', 'translation', 'system', 'settings', 'we', 'used', 'a', 'phrasebased', 'statistical', 'machine', 'translation', 'model', 'as', 'implemented', 'in', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'machine', 'translationwe', 'use', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'implement', 'our', 'models'], ['the', 'phrase', 'table', 'is', 'extracted', 'from', 'a', 'bilingual', 'text', 'aligned', 'on', 'the', 'word', 'level', 'using', 'eg', 'giza', '', 'och', 'and', 'ney', '2003', 'the', 'corpus', 'is', 'first', 'wordaligned', 'using', 'a', 'word', 'alignment', 'heuristic', 'och', 'and', 'ney', '2003'], ['this', 'architecture', 'is', 'very', 'similar', 'to', 'the', 'framework', 'of', 'uima', 'ferrucci', 'and', 'lally', '2004', '3', 'dkpro', 'is', 'a', 'collection', 'of', 'software', 'components', 'for', 'natural', 'language', 'processing', 'based', 'on', 'the', 'apache', 'uima', 'framework', 'ferrucci', 'and', 'lally', '2004'], ['besides', 'using', 'sentistrength', 'we', 'use', 'the', 'lexicon', 'approach', 'proposed', 'by', 'hu', 'and', 'liu', '2004', 'in', 'order', 'to', 'construct', 'the', 'lexical', 'prior', 'knowledge', 'matrix', 'u', '0', '', 'we', 'use', 'the', 'sentiment', 'lexicon', 'generated', 'by', 'hu', 'and', 'liu', '2004'], ['run1', 'we', 'firstly', 'use', 'the', 'stanford', 'corenlp', 'toolkit', '3', 'manning', 'et', 'al', '2014', 'to', 'split', 'each', 'token', 'for', 'the', 'sentence', 'pairs', 'in', 'the', 'evaluation', 'datawe', 'use', 'the', 'stanford', 'corenlp', 'caseless', 'tagger', 'for', 'partofspeech', 'tagging', 'manning', 'et', 'al', '2014'], ['by', 'setting', 'n', 'inw', 'and', 'enwn1', 'for', 'all', 'nodes', 'the', 'generalized', 'kernel', 'can', 'be', 'converted', 'to', 'the', 'kernel', 'proposed', 'in', 'collins', 'and', 'duffy', '2001', 'the', 'proof', 'is', 'similar', 'to', 'the', 'proof', 'for', 'the', 'tree', 'kernel', 'in', 'collins', 'and', 'duffy', '2001'], ['2', 'we', 'tested', 'the', 'difference', 'in', 'performance', 'for', 'statistical', 'significance', 'using', 'an', 'approximate', 'randomization', 'procedure', 'yeh', '2000', 'with', '10000', 'iterations', 'we', 'calculate', 'statistical', 'significance', 'of', 'performance', 'differences', 'using', 'stratified', 'shuffling', 'yeh', '2000'], ['run1', 'we', 'firstly', 'use', 'the', 'stanford', 'corenlp', 'toolkit', '3', 'manning', 'et', 'al', '2014', 'to', 'split', 'each', 'token', 'for', 'the', 'sentence', 'pairs', 'in', 'the', 'evaluation', 'datawe', 'use', 'the', 'stanford', 'corenlp', 'caseless', 'tagger', 'for', 'partofspeech', 'tagging', 'manning', 'et', 'al', '2014'], ['all', 'linguistic', 'annotations', 'needed', 'for', 'features', 'pos', 'chunks', '7', '', 'parses', 'are', 'from', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'part', 'of', 'speech', 'tagging', 'and', 'named', 'entity', 'recognition', 'for', 'comment', 'plausibility', 'features', 'are', 'carried', 'out', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['in', 'erkan', 'and', 'radev', '2004', '', 'the', 'concept', 'of', 'graph', 'based', 'centrality', 'was', 'used', 'to', 'rank', 'a', 'set', 'of', 'sentences', 'in', 'producing', 'generic', 'multidocument', 'summariesit', 'was', 'also', 'the', 'model', 'used', 'to', 'rank', 'sentences', 'in', 'erkan', 'and', 'radev', '2004'], ['significance', 'was', 'tested', 'using', 'a', 'paired', 'bootstrap', 'koehn', '2004', 'with', '1000', 'samples', 'p002statistical', 'significance', 'is', 'tested', 'on', 'the', 'bleu', 'metric', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'with', 'n', '', '1000', 'and', 'p', '', '005'], ['for', 'seed', 'and', 'test', 'paradigms', 'we', 'used', 'verbal', 'inflectional', 'paradigms', 'from', 'the', 'celex', 'morphological', 'database', 'baayen', 'et', 'al', '1995', 'to', 'validate', 'this', 'measure', 'we', 'computed', 'the', 'cosine', 'similarity', 'between', 'words', 'and', 'their', 'morphological', 'parents', 'from', 'the', 'celex2', 'database', 'baayen', 'et', 'al', '1995'], ['we', 'used', 'kenlm', 'heafield', '2011', 'to', 'create', '3gram', 'language', 'models', 'with', 'kneserney', 'smoothing', 'on', 'the', 'target', 'side', 'of', 'the', 'bilingual', 'training', 'corpora1', 'we', 'used', 'the', 'kenlm', 'toolkit', 'heafield', '2011', 'to', 'build', 'all', 'language', 'models', 'used', 'in', 'this', 'work', 'ie', 'both', 'for', 'data', 'selection', 'and', 'for', 'the', 'mt', 'systems', 'used', 'for', 'extrinsic', 'evaluation'], ['we', 'built', 'a', '5gram', 'language', 'model', 'on', 'the', 'english', 'side', 'of', 'qcatrain', 'using', 'kenlm', 'heafield', '2011', 'we', 'used', 'kenlm', 'heafield', '2011', 'to', 'create', '3gram', 'language', 'models', 'with', 'kneserney', 'smoothing', 'on', 'the', 'target', 'side', 'of', 'the', 'bilingual', 'training', 'corpora'], ['an', 'estimate', 'of', 'the', 'likelihood', 'of', 'a', 'verb', 'taking', 'a', 'event', 'subject', 'was', 'computed', 'over', 'the', 'annotated', 'english', 'gigaword', 'v5', 'corpus', 'napoles', 'et', 'al', '2012', 'in', 'the', 'news', 'column', '', 'we', 'show', 'the', 'statistics', 'of', 'a', 'subset', 'of', 'annotated', 'english', 'gigaword', 'napoles', 'et', 'al', '2012'], ['we', 'use', 'the', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'to', 'compute', 'the', 'language', 'model', 'score', 'for', 'a', 'sentencesince', 'the', 'googleapi', 'is', 'not', 'available', 'any', 'more', 'we', 'use', 'the', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'to', 'extract', 'the', 'google', 'distance', 'feature'], ['machine', 'translation', 'system', 'settings', 'we', 'used', 'a', 'phrasebased', 'statistical', 'machine', 'translation', 'model', 'as', 'implemented', 'in', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'machine', 'translationthe', 'moses', 'smt', 'toolkit', 'koehn', 'et', 'al', '2007', 'provides', 'a', 'complete', 'statistical', 'translation', 'system', 'distributed', 'under', 'the', 'lgpl', 'license'], ['machine', 'translation', 'system', 'settings', 'we', 'used', 'a', 'phrasebased', 'statistical', 'machine', 'translation', 'model', 'as', 'implemented', 'in', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'machine', 'translationwe', 'trained', 'a', 'model', 'using', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'on', 'the', 'training', 'data', 'as', 'our', 'baseline', 'system'], ['a', 'chunk', 'is', 'a', 'minimal', '', 'nonrecursive', 'structure', 'consisting', 'of', 'correlated', 'groups', 'of', 'words', 'bharati', 'et', 'al', '2006', 'the', 'chunk', 'label', 'tagset', 'is', 'a', 'coarser', 'version', 'of', 'anncorra', 'tagset', 'bharati', 'et', 'al', '2006'], ['mc30', 'a', 'subset', 'of', 'rg65', 'dataset', 'with', '30', 'word', 'pairs', 'miller', 'and', 'charles', '1991', 'we', 'use', 'a', 'set', 'of', '30', 'word', 'pairs', 'from', 'a', 'study', 'carried', 'out', 'by', 'miller', 'and', 'charles', '1991'], ['we', 'obtained', 'newspeg', 'judgments', 'using', 'the', 'brat', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012', 'from', 'two', 'annotators', '3', 'all', 'annotations', 'were', 'done', 'using', 'the', 'brat', 'rapid', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012'], ['we', 'transformed', 'the', 'parse', 'trees', 'in', 'ontonotes', 'into', 'syntactic', 'dependencies', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'we', 'use', 'the', 'stanford', 'corenlp', 'caseless', 'tagger', 'for', 'partofspeech', 'tagging', 'manning', 'et', 'al', '2014'], ['gradient', 'clipping', 'heuristic', 'to', 'prevent', 'the', 'exploding', 'gradient', 'problem', 'graves', '2013', 'for', 'the', 'exploding', 'gradient', 'problem', 'numerical', 'stability', 'can', 'be', 'achieved', 'by', 'clipping', 'the', 'gradients', 'graves', '2013'], ['we', 'obtained', 'newspeg', 'judgments', 'using', 'the', 'brat', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012', 'from', 'two', 'annotators', '3', 'the', 'annotations', 'were', 'made', 'using', 'the', 'brat', 'rapid', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012'], ['we', 'then', 'describe', 'in', 'more', 'detail', 'a', 'modern', 'chinese', 'corpus', 'the', 'penn', 'chinese', 'treebank', 'xue', 'et', 'al', '2005', 'for', 'chinese', 'we', 'use', 'the', 'penn', 'chinese', 'treebank', 'version', '51', 'ctb', 'xue', 'et', 'al', '2005'], ['in', 'order', 'to', 'assess', 'statistical', 'significance', 'of', 'the', 'obtained', 'results', 'we', 'use', 'the', 'paired', 'bootstrap', 'resampling', 'method', 'koehn', '2004', 'which', 'estimates', 'the', 'probability', 'pvalue', 'that', 'a', 'measured', 'difference', 'for', 'statistical', 'significance', 'testing', 'we', 'use', 'a', 'paired', 'bootstrap', 'resampling', 'method', 'proposed', 'in', 'koehn', '2004'], ['statistical', 'significance', 'in', 'bleu', 'score', 'difference', 'was', 'measured', 'by', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'significance', 'was', 'tested', 'using', 'a', 'paired', 'bootstrap', 'koehn', '2004', 'with', '1000', 'samples', 'p002'], ['the', 'article', 'system', 'builds', 'on', 'the', 'elements', 'of', 'the', 'system', 'described', 'in', 'rozovskaya', 'and', 'roth', '2010c', 'the', 'preposition', 'classifier', 'uses', 'a', 'combined', 'system', 'building', 'on', 'work', 'described', 'in', '', 'and', 'rozovskaya', 'and', 'roth', '2010b'], ['for', 'calculating', 'the', 'required', 'frequencies', 'we', 'use', 'the', 'web1t', 'corpus', '6', 'brants', 'and', 'franz', '2006', 'as', 'a', 'practical', 'approximation', 'we', 'use', 'bigram', 'counts', 'from', 'the', 'web', '1t', 'corpus', 'brants', 'and', 'franz', '2006'], ['phrase', 'pairs', 'were', 'extracted', 'from', 'symmetrized', 'word', 'alignments', 'and', 'distortions', 'generated', 'by', 'giza', 'och', 'and', 'ney', '2003', 'using', 'the', 'combination', 'of', 'heuristics', 'growdiagfinaland', 'and', 'msdbidirectionalword', 'alignments', 'were', 'created', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'ran', 'all', 'of', 'our', 'experiments', 'in', 'weka', 'hall', 'et', 'al', '2009', 'using', 'logistic', 'regressionweka', 'hall', 'et', 'al', '2009', 'which', 'contains', 'the', 'implementation', 'of', 'all', 'three', 'algorithms', 'was', 'used', 'in', 'our', 'study'], ['both', 'corpora', 'were', 'extracted', 'from', 'the', 'open', 'parallel', 'corpus', 'opus', 'tiedemann', '2012', 'the', 'parallel', 'data', 'were', 'taken', 'from', 'opus', 'tiedemann', '2012', '', 'which', 'provides', 'sentencealigned', 'corpora', 'with', 'annotation'], ['we', 'ran', 'all', 'of', 'our', 'experiments', 'in', 'weka', 'hall', 'et', 'al', '2009', 'using', 'logistic', 'regressionwe', 'conducted', 'experiments', 'using', 'multinomial', 'naive', 'bayes', 'classifier', 'implemented', 'in', 'the', 'weka', 'toolkit', 'hall', 'et', 'al', '2009'], ['the', 'spanishenglish', 's2e', 'training', 'corpus', 'was', 'drawn', 'from', 'the', 'europarl', 'collection', 'koehn', '2005', 'the', 'experiments', 'focus', 'on', 'translation', 'from', 'german', 'to', 'english', 'using', 'the', 'europarl', 'data', 'koehn', '2005'], ['type', 'system', 'extends', 'the', 'type', 'system', 'that', 'is', 'built', 'into', 'the', 'uima', '6', 'framework', 'ferrucci', 'and', 'lally', '2004', 'this', 'architecture', 'is', 'very', 'similar', 'to', 'the', 'framework', 'of', 'uima', 'ferrucci', 'and', 'lally', '2004'], ['for', 'other', 'languages', 'we', 'use', 'the', 'corpora', 'made', 'available', 'for', 'the', 'conllx', 'shared', 'task', 'buchholz', 'and', 'marsi', '2006', 'we', 'use', 'the', 'conllx', 'buchholz', 'and', 'marsi', '2006', 'distribution', 'data', 'from', 'seven', 'different', 'languages', 'arabic', 'bulgarian', 'dutch', 'portuguese', 'slovene', 'spanish', 'and', 'swedish'], ['table', '5', 'compares', 'our', 'reordering', 'model', 'with', 'a', 'reimplementation', 'of', 'the', 'reordering', 'model', 'proposed', 'in', 'tromble', 'and', 'eisner', '2009', 'we', 'note', 'that', 'our', 'model', 'outperforms', 'the', 'model', 'proposed', 'in', 'tromble', 'and', 'eisner', '2009', 'in', 'all', 'cases'], ['table', '5', 'compares', 'our', 'reordering', 'model', 'with', 'a', 'reimplementation', 'of', 'the', 'reordering', 'model', 'proposed', 'in', 'tromble', 'and', 'eisner', '2009', 'we', 'note', 'that', 'our', 'model', 'outperforms', 'the', 'model', 'proposed', 'in', 'tromble', 'and', 'eisner', '2009', 'in', 'all', 'cases'], ['these', 'results', 'contradict', 'those', 'given', 'in', 'zelenko', 'et', 'al', '2003', '', 'where', 'the', 'sparse', 'kernel', 'achieves', '23', 'better', 'f1', 'performance', 'than', 'the', 'contiguous', 'kernelzelenko', 'et', 'al', '2003', 'have', 'shown', 'the', 'contiguous', 'kernel', 'to', 'be', 'computable', 'in', 'omn', 'and', 'the', 'sparse', 'kernel', 'in', 'omn', '3', '', 'where', 'm', 'and', 'n', 'are', 'the', 'number', 'of', 'children', 'in', 'trees', 't', '1', 'and', 't', '2', 'respectively'], ['for', 'example', 'ontonotes', 'hovy', 'et', 'al', '2006', '', 'a', 'largescale', 'annotation', 'project', 'chose', 'this', 'optionto', 'this', 'end', 'a', 'recent', 'largescale', 'annotation', 'effort', 'called', 'the', 'ontonotes', 'project', 'hovy', 'et', 'al', '2006', 'was', 'started'], ['as', 'for', 'ej', 'translation', 'we', 'use', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'english', 'abstraction', 'treeswe', 'use', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parse', 'trees', 'and', 'dependency', 'relations'], ['for', 'both', 'english', 'and', 'german', 'we', 'used', 'the', 'partof', 'speech', 'tagger', 'treetagger', 'schmid', '1994', 'to', 'obtain', 'postagsonly', 'for', 'german', 'data', 'did', 'we', 'used', 'the', 'treetagger', 'schmid', '1994', 'tokenizer'], ['we', 'train', 'our', 'model', 'on', 'a', 'subset', 'of', 'the', 'wackypedia', 'en', '6', 'corpus', 'baroni', 'et', 'al', '2009', 'we', 'built', 'a', 'knowledge', 'base', 'v', '2', 'r', '1', 'using', 'the', 'frwac', 'corpus', 'baroni', 'et', 'al', '2009'], ['the', 'questions', 'are', 'translated', 'using', 'a', 'phrasebased', 'system', 'built', 'using', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'mo', 'setthe', 'first', 'two', 'baselines', 'are', 'standard', 'systems', 'using', 'pbmt', 'or', 'hiero', 'trained', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'questions', 'are', 'translated', 'using', 'a', 'phrasebased', 'system', 'built', 'using', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'mo', 'setthe', 'decoder', 'is', 'built', 'on', 'top', 'of', 'an', 'opensource', 'phrasebased', 'smt', 'decoder', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'measure', 'statistical', 'significance', 'using', '95', 'confidence', 'intervals', 'computed', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'significance', 'was', 'tested', 'using', 'a', 'paired', 'bootstrap', 'koehn', '2004', 'with', '1000', 'samples', 'p002'], ['as', 'for', 'ej', 'translation', 'we', 'use', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'english', 'abstraction', 'treeswe', 'use', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parse', 'trees', 'and', 'dependency', 'relations'], ['words', 'were', 'downcased', 'and', 'lemmatized', 'using', 'the', 'wordnet', 'lemmatizer', 'in', 'the', 'nltk', '2', 'toolkit', 'bird', 'et', 'al', '2009', 'for', 'reuters', 'we', 'segmented', 'and', 'tokenized', 'the', 'data', 'using', 'nltk', 'bird', 'et', 'al', '2009'], ['english', 'sentences', 'are', 'parsed', 'into', 'dependency', 'structures', 'by', 'stanford', 'parser', 'marneffe', 'et', 'al', '2006', 'the', 'grammatical', 'relations', 'are', 'all', 'the', 'collapsed', 'dependencies', 'produced', 'by', 'the', 'stanford', 'dependency', 'parser', 'marneffe', 'et', 'al', '2006'], ['both', 'of', 'our', 'systems', 'were', 'based', 'on', 'the', 'moses', 'decoder', 'koehn', 'et', 'al', '2007', 'the', 'decoder', 'is', 'built', 'on', 'top', 'of', 'an', 'opensource', 'phrasebased', 'smt', 'decoder', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'also', 'used', 'anew', 'bradley', 'and', 'lang', '1999', 'for', 'bootstrapping', 'the', 'affective', 'lexicon', 'expansion', 'processfinally', 'the', 'anew', 'lexicon', 'bradley', 'and', 'lang', '1999', 'is', 'used', 'for', 'selecting', 'the', 'initial', 'set', 'of', 'seed', 'words', 'of', '1'], ['translation', 'at', 'the', 'discomt', '2015', 'workshop', 'hardmeier', 'et', 'al', '2015', 'the', 'nlp', 'group', 'of', 'the', 'idiap', 'research', 'institute', 'participated', 'in', 'both', 'subtasks', 'of', 'the', 'discomt', '2015', 'shared', 'task', 'pronounfocused', 'translation', 'and', 'pronoun', 'prediction', 'hardmeier', 'et', 'al', '2015'], ['establishing', 'and', 'maintaining', 'common', 'ground', 'is', 'a', 'complicated', 'process', 'even', 'for', 'human', 'interlocutors', 'clark', '1996', 'an', 'example', 'of', 'such', 'a', 'pragmatic', 'factor', 'is', 'common', 'ground', 'clark', '1996'], ['sentiwordnet', 'score', 'senti', 'we', 'used', 'the', 'senti', 'wordnet', 'baccianella', 'et', 'al', '2010', 'lexical', 'resource', 'to', 'assign', 'scores', 'for', 'each', 'word', 'based', 'on', 'three', 'sentiments', 'ie', 'positive', 'negative', 'and', 'objective', 'respecwe', 'then', 'have', 'assigned', 'a', 'sentiment', 'score', 'using', 'the', 'sentiwordnet', 'baccianella', 'et', 'al', '2010', 'lexical', 'resource', 'to', 'each', 'word', 'in', 'the', 'set', 'of', 'retrieved', 'snippets'], ['europarl', '2', 'koehn', '2005', '', 'it', 'is', 'a', 'corpus', 'of', 'parallel', 'texts', 'in', '11', 'languages', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament', 'we', 'used', 'a', 'subset', 'of', 'the', 'data', 'provided', 'for', 'the', 'second', 'workshop', 'on', 'statistical', 'machine', 'translation', '2', '', 'which', 'consists', 'mainly', 'of', 'texts', 'from', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'also', 'carried', 'out', 'a', 'chunkreordering', 'pbsmt', 'experiment', 'where', 'the', 'chunks', 'are', 'reordered', 'based', 'on', 'the', 'final', 'alignments', 'obtained', 'by', '1pass', 'experiment', 'of', 'holmqvist', 'et', 'al', '2012', 'holmqvist', 'et', 'al', '2012', 'presented', 'a', 'method', 'where', 'source', 'text', 'is', 'reordered', 'to', 'replicate', 'the', 'target', 'word', 'order', 'based', 'on', 'word', 'alignment'], ['wsi', 'is', 'generally', 'considered', 'as', 'an', 'unsupervised', 'clustering', 'task', 'under', 'the', 'distributional', 'hypothesis', 'harris', '1954', 'that', 'the', 'word', 'meaning', 'is', 'reflected', 'by', 'the', 'set', 'of', 'contexts', 'in', 'which', 'it', 'appearsdistributional', 'models', 'of', 'meaning', 'follow', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'which', 'states', 'that', 'two', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'have', 'similar', 'meanings'], ['the', 'data', 'used', 'for', 'the', 'experiments', 'described', 'in', 'this', 'paper', 'comes', 'predominantly', 'from', 'bible', 'translations', '', 'wikipedia', 'and', 'the', 'europarl', 'corpus', 'of', 'european', 'parliamentary', 'proceedings', 'koehn', '2005', '', 'europarl', '2', 'koehn', '2005', '', 'it', 'is', 'a', 'corpus', 'of', 'parallel', 'texts', 'in', '11', 'languages', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['for', 'the', 'language', 'model', 'we', 'used', 'the', 'kenlm', 'toolkit', 'heafield', '2011', 'to', 'create', 'a', '5gram', 'language', 'model', 'on', 'the', 'target', 'side', 'of', 'the', 'europarl', 'corpus', 'v7', 'with', 'approximately', '54m', 'tokens', 'with', 'kneserney', 'smoolanguage', 'model', 'for', 'all', '3scfg', 'systems', 'we', 'use', 'a', '4gram', 'kneserney', 'smoothed', 'language', 'model', 'trained', 'using', 'the', 'kenlm', 'toolkit', 'heafield', '2011'], ['first', 'used', 'by', 'blitzer', 'et', 'al', '2007', '', 'the', 'mds', 'dataset', 'contains', '4', 'different', 'types', 'of', 'product', 'reviews', 'taken', 'from', 'amazoncom', 'including', 'books', 'dvds', 'electronics', 'and', 'kitchen', 'appliances', 'with', '1000', 'positive', 'to', 'evaluate', 'da', 'for', 'sentiment', 'classification', 'we', 'use', 'the', 'amazon', 'product', 'reviews', 'collected', 'by', 'blitzer', 'et', 'al', '2007', '', 'for', 'four', 'different', 'product', 'categories', 'books', 'b', 'dvds', 'd', 'electronic', 'items', 'e', 'and'], ['the', 'most', 'famous', 'example', 'would', 'probably', 'be', 'the', 'europarl', 'corpus', 'koehn', '2005', 'for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['for', 'language', 'modeling', 'we', 'trained', 'a', 'separate', '5gram', 'kneserney', 'smoothed', 'lm', 'model', 'on', 'the', 'target', 'ie', 'english', 'side', 'of', 'the', 'training', 'bitext', 'using', 'kenlm', 'heafield', '2011', 'in', 'order', 'to', 'evaluate', 'the', 'fluency', 'of', 'each', 'system', 'we', 'train', '5gram', 'language', 'models', 'for', 'each', 'language', 'using', 'kenlm', 'heafield', '2011'], ['they', 'used', 'the', 'webbased', 'annotation', 'tool', 'brat', 'stenetorp', 'et', 'al', '2012', 'for', 'the', 'annotation', 'the', 'annotations', 'were', 'made', 'using', 'the', 'brat', 'rapid', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012'], ['the', 'classification', 'was', 'conducted', 'using', 'different', 'scikitlearn', 'algorithms', 'pedregosa', 'et', 'al', '2011', 'the', 'svm', 'models', 'were', 'trained', 'using', 'the', 'scikitlearn', 'toolkit', '4', 'pedregosa', 'et', 'al', '2011'], ['the', 'europarl', 'corpus', 'koehn', '2005', 'is', 'built', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['for', 'language', 'modeling', 'we', 'trained', 'a', 'separate', '5gram', 'kneserney', 'smoothed', 'lm', 'model', 'on', 'the', 'target', 'ie', 'english', 'side', 'of', 'the', 'training', 'bitext', 'using', 'kenlm', 'heafield', '2011', 'the', 'language', 'model', 'is', 'a', '5gram', 'kenlm', 'heafield', '2011', 'model', 'trained', 'using', 'lmplz', 'with', 'modified', 'kneserney', 'smoothing', 'and', 'no', 'pruning'], ['the', 'spanishenglish', 's2e', 'training', 'corpus', 'was', 'drawn', 'from', 'the', 'europarl', 'collection', 'koehn', '2005', 'we', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'use', 'the', 'stanford', 'corenlp', 'caseless', 'tagger', 'for', 'partofspeech', 'tagging', 'manning', 'et', 'al', '2014', 'we', 'also', 'lemmatized', 'all', 'words', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['all', 'of', 'our', 'models', 'are', 'trained', 'using', 'nematus', 'sennrich', 'et', 'al', '2017', 'we', 'trained', 'our', 'basic', 'neural', 'machine', 'translation', 'systems', 'labeled', 'base', 'in', 'table', '3', 'with', 'nematus', 'sennrich', 'et', 'al', '2017'], ['we', 'use', 'giza', 'och', 'and', 'ney', '2003', 'with', 'its', 'default', 'parameters', 'to', 'produce', 'phrase', 'alignmentswe', 'used', 'the', 'giza', 'software', 'och', 'and', 'ney', '2003', 'to', 'do', 'the', 'word', 'alignments'], ['we', 'are', 'working', 'with', 'standard', 'tools', 'as', 'dissect', 'dinu', 'et', 'al', '2013', 'the', 'matrix', 'is', 'weighted', 'with', 'ppmi', 'as', 'implemented', 'in', 'dissect', 'dinu', 'et', 'al', '2013'], ['they', 'are', 'based', 'on', 'distributional', 'hypothesis', 'which', 'works', 'under', 'the', 'assumption', 'that', 'similar', 'words', 'occur', 'in', 'similar', 'contexts', 'harris', '1954', 'corpusbased', 'vsms', 'follow', 'the', 'standard', 'distributional', 'hypothesis', 'which', 'states', 'that', 'words', 'appearing', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meaning', 'harris', '1954'], ['its', 'segmentation', 'model', 'is', 'a', 'classbased', 'hidden', 'markov', 'model', 'hmm', 'model', 'zhang', 'et', 'al', '2003', 'for', 'chinese', 'a', 'segmentation', 'model', 'zhang', 'et', 'al', '2003', 'is', 'used', 'for', 'detecting', 'word', 'boundaries'], ['the', 'edit', 'distance', 'kernel', 'was', 'trained', 'with', 'libsvm', 'chang', 'and', 'lin', '2011', 'as', 'our', 'learner', 'we', 'use', 'libsvm', 'with', 'a', 'linear', 'kernel', 'chang', 'and', 'lin', '2011'], ['the', 'corpora', 'are', 'first', 'tokenized', 'and', 'lowercased', 'using', 'the', 'moses', 'scripts', 'then', 'lemmatized', 'and', 'tagged', 'by', 'partofspeech', 'pos', 'using', 'the', 'treetagger', 'schmid', '1994', 'the', 'corpus', 'was', 'converted', 'from', 'xml', 'to', 'raw', 'text', 'various', 'string', 'normalization', 'operations', 'were', 'then', 'applied', 'and', 'the', 'corpus', 'was', 'lemmatized', 'using', 'treetagger', 'schmid', '1994'], ['for', 'example', 'ontonotes', 'hovy', 'et', 'al', '2006', '', 'a', 'largescale', 'annotation', 'project', 'chose', 'this', 'optionfor', 'example', 'the', 'ontonotes', 'hovy', 'et', 'al', '2006', 'project', 'opted', 'for', 'this', 'approach'], ['we', 'use', 'the', 'opensource', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'build', 'a', 'standard', 'phrasebased', 'smt', 'system', 'which', 'extracts', 'up', 'to', '8', 'words', 'phrases', 'in', 'the', 'moses', 'phrase', 'tablethe', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'is', 'used', 'to', 'train', 'a', 'phrase', 'based', 'smt', 'system', 'with', 'the', 'parallel', 'data', 'previously', 'introduced'], ['in', 'our', 'experiments', 'we', 'use', 'the', 'liblinear', 'package', 'fan', 'et', 'al', '2008', 'to', 'solve', 'the', 'primal', 'problem', 'with', 'l', '2', 'regularization', 'and', 'l', '2', 'lossspecifically', 'we', 'use', 'the', 'liblinear', 'svm', 'package', 'fan', 'et', 'al', '2008', 'as', 'it', 'is', 'wellsuited', 'to', 'text', 'classification', 'tasks', 'with', 'large', 'numbers', 'of', 'features', 'and', 'texts'], ['the', 'spanishenglish', 's2e', 'training', 'corpus', 'was', 'drawn', 'from', 'the', 'europarl', 'collection', 'koehn', '2005', 'for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['the', 'icsi', 'meeting', 'corpus', 'janin', 'et', 'al', '2003', 'is', 'a', 'corpus', 'of', 'text', 'transcripts', 'of', 'research', 'meetingsthe', 'icsi', 'meeting', 'corpus', 'the', 'icsi', 'meeting', 'corpus', 'janin', 'et', 'al', '2003', 'is', '75', 'transcribed', 'meetings'], ['we', 'use', 'rouge', 'lin', '2004', 'for', 'evaluating', 'the', 'content', 'of', 'summarieswe', 'use', 'rouge', 'metric', 'lin', '2004', 'to', 'evaluate', 'generated', 'timelines', 'against', 'reference', 'summaries'], ['the', 'measure', 'selected', 'is', 'the', 'normalised', 'pearson', 'correlation', 'agirre', 'et', 'al', '2012', 'the', 'task', 'is', 'part', 'of', 'the', 'semantic', 'evaluation', '2012', 'workshop', 'agirre', 'et', 'al', '2012'], ['the', 'main', 'corpora', 'we', 'use', 'are', 'europarl', 'koehn', '2005', 'and', 'the', 'canadian', 'hansardfor', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'exploit', 'this', 'monolingual', 'data', 'for', 'training', 'as', 'described', 'in', 'sennrich', 'et', 'al', '2016a', 'we', 'also', 'used', 'automatically', 'backtranslated', 'indomain', 'monolingual', 'data', 'sennrich', 'et', 'al', '2016a'], ['we', 'use', 'the', 'stanford', 'corenlp', 'caseless', 'tagger', 'for', 'partofspeech', 'tagging', 'manning', 'et', 'al', '2014', 'we', 'also', 'lemmatized', 'all', 'words', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['this', 'is', 'a', 'corpusbased', 'metric', 'relying', 'on', 'the', 'distributional', 'hypothesis', 'of', 'meaning', 'suggesting', 'that', 'similarity', 'of', 'context', 'implies', 'similarity', 'of', 'meaning', 'z', 'harris', '1954', 'corpusbased', 'meaning', 'representations', 'rely', 'on', 'the', 'distributional', 'hypothesis', 'which', 'assumes', 'that', 'words', 'occurring', 'in', 'a', 'similar', 'set', 'of', 'contexts', 'are', 'also', 'similar', 'in', 'meaning', 'harris', '1954'], ['similarly', 'turian', 'et', 'al', '2010', 'evaluated', 'three', 'different', 'word', 'representations', 'on', 'ner', 'and', 'chunking', 'and', 'concluded', 'that', 'unsupervised', 'word', 'representations', 'improved', 'ner', 'and', 'chunkingturian', 'et', 'al', '2010', '', 'evaluate', 'different', 'techniques', 'for', 'inducing', 'word', 'representations', 'and', 'detail', 'significant', 'improvements', 'for', 'supervised', 'ner', 'and', 'chunking', 'systems', 'when', 'also', 'incorporating', 'word', 'embedding'], ['331', 'reference', 'system', 'we', 'compare', 'a', 'number', 'of', 'analogical', 'devices', 'to', 'the', 'stateoftheart', 'statistical', 'translation', 'engine', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'then', 'use', 'the', 'phrase', 'extraction', 'utility', 'in', 'the', 'moses', 'statistical', 'machine', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'extract', 'a', 'phrase', 'table', 'which', 'operates', 'over', 'characters'], ['for', 'training', 'svm', 'classifiers', 'we', 'used', 'libsvm', 'package', 'chang', 'and', 'lin', '2001', 'we', 'used', 'libsvm', 'to', 'implement', 'our', 'own', 'svm', 'for', 'regression', 'chang', 'and', 'lin', '2001'], ['the', 'word', 'alignment', 'was', 'trained', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'configuration', 'growdiagfinalandthe', 'word', 'alignment', 'was', 'obtained', 'by', 'running', 'giza', 'och', 'and', 'ney', '2003'], ['the', 'corpus', 'is', 'first', 'wordaligned', 'using', 'a', 'word', 'alignment', 'heuristic', 'och', 'and', 'ney', '2003', 'the', 'word', 'alignment', 'was', 'obtained', 'by', 'running', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'perform', 'bootstrap', 'resampling', 'with', 'bounds', 'estimation', 'as', 'described', 'in', 'koehn', '2004', 'we', 'used', 'bootstrap', 'resampling', 'for', 'testing', 'statistical', 'significance', 'koehn', '2004'], ['we', 'perform', 'bootstrap', 'resampling', 'with', 'bounds', 'estimation', 'as', 'described', 'by', 'koehn', '2004', 'we', 'used', 'bootstrap', 'resampling', 'for', 'testing', 'statistical', 'significance', 'koehn', '2004'], ['markov', 'logic', 'networks', 'mln', 'richardson', 'and', 'domingos', '2006', 'is', 'adopted', 'for', 'learning', 'and', 'predicationmarkov', 'logic', 'networks', 'mln', 'richardson', 'and', 'domingos', '2006', 'are', 'one', 'of', 'the', 'statistical', 'relational', 'learning', 'frameworks'], ['with', 'the', 'training', 'script', 'of', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settings'], ['we', 'train', 'for', '15', 'epochs', 'using', 'minibatch', 'stochastic', 'gradient', 'descent', 'the', 'adadelta', 'update', 'rule', 'zeiler', '2012', '', 'and', 'early', 'stoppingwe', 'initialize', 'parameters', 'uniformly', 'using', 'the', 'range', '005', '005', 'for', 'layer', 'parameters', 'and', '001', '001', 'for', 'embeddings', 'and', 'train', 'the', 'model', 'using', 'stochastic', 'gradient', 'descent', 'sgd', 'with', 'learning', 'rates'], ['this', 'confirms', 'the', 'finding', 'of', 'liu', 'et', 'al', '2012', 'that', 'language', 'model', 'and', 'lexicalized', 'reordering', 'models', 'only', 'have', 'modest', 'effects', 'on', 'translation', 'retrievalmore', 'recently', 'liu', 'et', 'al', '2012', 'have', 'proposed', 'a', 'new', 'translation', 'retrieval', 'architecture', 'that', 'depends', 'only', 'on', 'monolingual', 'corpora'], ['for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005', 'we', 'evaluate', 'our', 'method', 'by', 'means', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'exploit', 'this', 'monolingual', 'data', 'for', 'training', 'as', 'described', 'in', 'sennrich', 'et', 'al', '2016a', 'we', 'also', 'used', 'automatically', 'backtranslated', 'indomain', 'monolingual', 'data', 'sennrich', 'et', 'al', '2016a'], ['our', 'decoder', 'is', 'a', 'stack', 'decoder', 'similar', 'to', 'koehn', 'et', 'al', '2003', 'phrase', 'extraction', 'was', 'performed', 'following', 'koehn', 'et', 'al', '2003'], ['for', 'the', 'english', 'spanish', 'and', 'frenchenglish', 'systems', 'we', 'used', 'parallel', 'training', 'data', 'from', 'the', 'europarl', 'and', 'news', 'commentary', 'corpora', 'as', 'well', 'as', 'the', 'ted', 'corpus', 'cettolo', 'et', 'al', '2012', 'in', 'our', 'lowresource', 'condition', 'we', 'trained', 'an', 'smt', 'system', 'using', 'only', 'training', 'data', 'from', 'the', 'ted', 'corpus', 'cettolo', 'et', 'al', '2012'], ['the', 'word', 'alignment', 'was', 'trained', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'configuration', 'growdiagfinalandword', 'alignment', 'was', 'done', 'with', 'giza', 'och', 'and', 'ney', '2003', 'for', 'both', 'systems'], ['rhetorical', 'structure', 'theory', 'rst', 'mann', 'and', 'thompson', '1988', 'represents', 'the', 'coherence', 'structure', 'of', 'a', 'text', 'by', 'a', 'labeled', 'tree', 'called', 'discourse', 'tree', 'dt', 'as', 'shown', 'in', 'figure', '1', 'the', 'closest', 'area', 'to', 'our', 'work', 'consists', 'of', 'investigations', 'of', 'discourse', 'relations', 'in', 'the', 'context', 'of', 'rhetorical', 'structure', 'theory', 'mann', 'and', 'thompson', '1988'], ['the', 'dataset', 'used', 'for', 'the', 'experiments', 'reported', 'in', 'this', 'paper', 'has', 'been', 'prepared', 'by', 'reyes', 'et', 'al', '2013', 'the', 'other', 'groups', 'have', 'been', 'used', 'already', 'for', 'example', 'by', 'carvalho', 'et', 'al', '2009', 'or', 'reyes', 'et', 'al', '2013', 'yet', 'our', 'implementation', 'is', 'different', 'in', 'most', 'of', 'the', 'cases'], ['a', 'good', 'data', 'source', 'for', 'this', 'is', 'the', 'europarl', 'corpus', 'koehn', '2005', 'for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['the', 'most', 'famous', 'example', 'would', 'probably', 'be', 'the', 'europarl', 'corpus', 'koehn', '2005', 'we', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['previously', 'socher', 'et', 'al', '2011', '', 'used', 'a', 'recursive', 'autoencoder', 'to', 'similarly', 'obtain', 'a', 'vector', 'representation', 'of', 'each', 'sentence', 'again', 'combining', 'other', 'lexical', 'similarity', 'features', 'to', 'improve', 'the', 'resultsfor', 'the', 'msrp', 'task', 'socher', 'et', 'al', '2011', 'used', 'a', 'recursive', 'neural', 'network', 'to', 'model', 'each', 'sentence', '', 'recursively', 'computing', 'the', 'representation', 'for', 'the', 'sentence', 'from', 'the', 'representations', 'of', 'its', 'constituents'], ['the', 'first', 'source', 'is', 'the', 'conll', '2003', 'shared', 'task', 'date', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003', 'and', 'the', 'second', 'source', 'is', 'the', '2004', 'nist', 'automatic', 'content', 'extraction', 'weischedel', '2004', 'aidayago', 'is', 'derived', 'from', 'the', 'conll2003', 'shared', 'task', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'build', 'a', 'phrase', 'based', 'machine', 'translation', 'system', 'with', 'a', 'traditional', '5gram', 'lm', 'trained', 'on', 'the', 'target', 'side', 'of', 'our', 'bitextfor', 'this', 'purpose', 'we', 'use', 'the', 'moses', 'toolkit', 'to', 'build', 'a', 'phrasebased', 'statistical', 'mt', 'system', 'koehn', 'et', 'al', '2007', '', 'with', 'training', 'data', 'from', 'the', 'translation', 'task', 'of', 'the', 'wmt', '2013', 'workshop', 'bojar', 'et', 'al', '2'], ['we', 'therefore', 'propose', 'an', 'alternative', 'method', 'based', 'on', 'correlation', 'matrices', 'computed', 'from', 'the', 'bleu', 'performance', 'measure', 'papineni', 'et', 'al', '2001', 'we', 'measure', 'translation', 'quality', 'via', 'the', 'bleu', 'score', 'papineni', 'et', 'al', '2001'], ['we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'the', 'linear', 'svm', 'implementation', 'with', 'default', 'parameters', 'and', 'random', 'forest', 'with', '50', 'trees', 'both', 'available', 'at', 'scikitlearn', 'pedregosa', 'et', 'al', '2011'], ['finally', 'the', 'conll2003', 'shared', 'task', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003', 'is', 'an', 'figure', '1', '', 'search', 'results', 'for', 'the', 'queries', 'michael', 'jackson', 'and', 'michael', 'jackson', 'footballeraidayago', 'is', 'derived', 'from', 'the', 'conll2003', 'shared', 'task', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003'], ['further', 'we', 'sentencesplit', 'tokenized', 'and', 'lemmatized', 'the', 'text', 'in', 'wikipedia', 'and', 'gigaword', 'using', 'stanford', 'corenlp', 'toolkit', '360', 'manning', 'et', 'al', '2014', 'we', 'also', 'lemmatized', 'all', 'words', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['the', 'corpora', 'are', 'first', 'tokenized', 'and', 'lowercased', 'using', 'the', 'moses', 'scripts', 'then', 'lemmatized', 'and', 'tagged', 'by', 'partofspeech', 'pos', 'using', 'the', 'treetagger', 'schmid', '1994', 'all', 'novels', 'were', 'lemmatized', 'and', 'postagged', 'using', 'treetagger', 'schmid', '1994'], ['we', 'use', 'adadelta', 'zeiler', '2012', 'to', 'update', 'the', 'parameters', 'during', 'trainingwe', 'also', 'use', 'adadelta', 'zeiler', '2012', 'to', 'optimize', 'the', 'learning', 'of', '', 'which', 'is', 'a', 'effective', 'method', 'to', 'train', 'the', 'neural', 'networks'], ['the', 'first', 'source', 'is', 'the', 'conll', '2003', 'shared', 'task', 'date', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003', 'and', 'the', 'second', 'source', 'is', 'the', '2004', 'nist', 'automatic', 'content', 'extraction', 'weischedel', '2004', 'aidayago', 'is', 'derived', 'from', 'the', 'conll2003', 'shared', 'task', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003'], ['we', 'used', 'a', '2009', 'snapshot', 'of', 'wikipedia', '2', 'which', 'was', 'pos', 'tagged', 'and', 'lemmatized', 'using', 'the', 'treetagger', 'schmid', '1994', 'all', 'novels', 'were', 'lemmatized', 'and', 'postagged', 'using', 'treetagger', 'schmid', '1994'], ['recently', 'hovy', 'et', 'al', '2013', 'utilized', 'word', 'embeddings', 'by', 'collobert', 'et', 'al', '2011', 'for', 'capturing', 'coherence', 'and', 'contextual', 'features', 'for', 'supervised', 'metaphor', 'detectionhovy', 'et', 'al', '2013', 'applied', 'tree', 'kernels', 'to', 'metaphor', 'detection'], ['finally', 'the', 'conll2003', 'shared', 'task', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003', 'is', 'an', 'figure', '1', '', 'search', 'results', 'for', 'the', 'queries', 'michael', 'jackson', 'and', 'michael', 'jackson', 'footballeraidayago', 'is', 'derived', 'from', 'the', 'conll2003', 'shared', 'task', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003'], ['we', 'run', 'our', 'experiments', 'on', 'europarl', 'koehn', '2005', '', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'applied', 'bootstrap', 'resampling', 'koehn', '2004', 'to', 'measure', 'statistical', 'significance', '', 'p', '', '005', 'of', 'our', 'models', 'compared', 'to', 'a', 'baselinewe', 'used', 'bootstrap', 'resampling', 'for', 'testing', 'statistical', 'significance', 'koehn', '2004'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settingsthe', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'is', 'used', 'to', 'train', 'a', 'phrase', 'based', 'smt', 'system', 'with', 'the', 'parallel', 'data', 'previously', 'introduced'], ['the', 'statistical', 'significance', 'test', 'is', 'also', 'carried', 'out', 'with', 'paired', 'bootstrap', 'resampling', 'method', 'with', 'p', '', '0001', 'intervals', 'koehn', '2004', 'we', 'used', 'bootstrap', 'resampling', 'for', 'testing', 'statistical', 'significance', 'koehn', '2004'], ['this', 'model', 'is', 'motivated', 'by', 'vector', 'space', 'model', 'salton', 'et', 'al', '1975', 'one', 'of', 'the', 'bestknown', 'methods', 'of', 'representing', 'a', 'document', 'for', 'similarity', 'computation', 'between', 'documents', 'is', 'the', 'vector', 'space', 'model', 'salton', 'et', 'al', '1975'], ['the', 'data', 'was', 'segmented', 'into', 'basenp', 'parts', 'and', 'nonbasenp', 'parts', 'in', 'a', 'similar', 'fashion', 'as', 'the', 'data', 'used', 'by', 'ramshaw', 'and', 'marcus', '1995', 'we', 'have', 'used', 'the', 'basenp', 'data', 'presented', 'in', 'ramshaw', 'and', 'marcus', '1995', '2'], ['the', 'njuparser', 'is', 'based', 'on', 'the', 'stateofthe', 'art', 'mat', 'parser', 'mcdonald', '2006', 'the', 'second', 'algorithm', 'denoted', 'glotr', 'is', 'the', 'chuliu', 'edmonds', 'algorithm', 'for', 'maximal', 'spanning', 'tree', 'implemented', 'in', 'the', 'mstparser', 'mcdonald', '2006'], ['one', 'is', 'a', '3gram', 'language', 'model', 'built', 'using', 'kenlm', 'heafield', '2011', 'and', 'trained', 'over', 'a', 'modified', 'version', 'of', 'the', 'annotated', 'corpus', 'in', 'which', 'every', 'it', 'is', 'concatenated', 'with', 'its', 'type', 'eg', 'it', 'eventtraining', 'and', 'querying', 'of', 'a', 'modified', 'kneserney', 'smoothed', '5', 'gram', 'language', 'model', 'are', 'done', 'on', 'the', 'english', 'side', 'of', 'the', 'training', 'data', 'using', 'kenlm', 'heafield', '2011', '7'], ['the', 'word', 'alignment', 'was', 'obtained', 'by', 'running', 'giza', 'och', 'and', 'ney', '2003', 'baseline', 'word', 'alignments', 'were', 'obtained', 'by', 'running', 'giza', 'in', 'both', 'directions', 'and', 'symmetrizing', 'using', 'the', 'growdiagfinaland', 'heuristic', 'och', 'and', 'ney', '2003'], ['translations', 'for', 'english', 'words', 'in', 'the', 'lexical', 'sample', 'are', 'extracted', 'from', 'a', 'semiautomatic', 'word', 'alignment', 'of', 'sentences', 'from', 'the', 'europarl', 'parallel', 'corpus', 'koehn', '2005', 'in', 'order', 'to', 'improve', 'the', 'robustness', 'of', 'the', 'word', 'alignments', 'the', 'documents', 'were', 'concatenated', 'into', 'a', 'single', 'file', 'together', 'with', 'englishfrench', 'parallel', 'data', 'from', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['all', 'the', 'language', 'models', 'lm', 'used', 'in', 'our', 'experiments', 'are', '5grams', 'modified', 'kneserney', 'smoothed', 'lms', 'trained', 'using', 'kenlm', 'heafield', 'et', 'al', '2013', 'the', 'language', 'models', 'are', 'estimated', 'using', 'the', 'kenlm', 'toolkit', 'heafield', 'et', 'al', '2013', 'with', 'modified', 'kneserney', 'smoothing'], ['in', 'all', 'experiments', 'we', 'use', 'the', 'svm', 'classifier', 'with', 'sequential', 'minimal', 'optimization', 'smo', 'implementation', 'available', 'in', 'the', 'weka', 'package', 'hall', 'et', 'al', '2009', 'for', 'all', 'experiments', 'we', 'use', 'a', 'decisiontree', 'classifier', 'as', 'implemented', 'in', 'weka', 'hall', 'et', 'al', '2009', 'toolkit'], ['we', 'use', 'the', 'stanford', 'parser', 'to', 'generate', 'a', 'dg', 'for', 'each', 'sentence', 'de', 'marneffe', 'et', 'al', '2006', 'we', 'used', 'the', 'lexicalized', 'dependency', 'parser', 'in', 'the', 'stanford', 'statistical', 'natural', 'language', 'parser', 'ver203', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parses', 'for', 'the', 'data', 'sets'], ['preprocessing', 'we', 'tokenized', 'the', 'english', 'side', 'of', 'all', 'bitexts', 'for', 'language', 'modeling', 'using', 'the', 'standard', 'tokenizer', 'of', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007', 'we', 'use', 'moses', '7', 'koehn', 'et', 'al', '2007', 'to', 'implement', 'this', 'model', 'by', 'setting', 'the', 'source', 'and', 'target', 'sides', 'to', 'be', 'kana', 'sequences'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'first', 'competitive', 'learning', 'based', 'system', 'is', 'described', 'in', 'soon', 'et', 'al', '2001', 'to', 'solve', 'coreference', 'we', 'used', 'a', 'variation', 'of', 'the', 'closest', 'antecedent', 'approach', 'described', 'in', 'soon', 'et', 'al', '2001'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'predicted', 'collapsed', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'extract', 'arguments', 'of', 'the', 'verbs', 'and', 'used', 'only', 'a', 'subset', 'of', 'dependents', 'of', 'a', 'verbwe', 'extract', 'syntactic', 'dependencies', 'using', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'and', 'use', 'its', 'collapsed', 'dependency', 'format'], ['we', 'select', 'as', 'a', 'generalpurpose', 'corpus', 'europarl', 'v7', 'koehn', '2005', '', 'with', '197m', 'parallel', 'sentenceswe', 'run', 'our', 'experiments', 'on', 'europarl', 'koehn', '2005', '', 'a', 'multilingual', 'parallel', 'corpus', 'which', 'is', 'described', 'in', 'detail', 'in', 'section', '31'], ['we', 'used', 'the', 'lexicalized', 'dependency', 'parser', 'in', 'the', 'stanford', 'statistical', 'natural', 'language', 'parser', 'ver203', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parses', 'for', 'the', 'data', 'setsin', 'our', 'experiments', '', 'we', 'used', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'create', 'dependency', 'parses'], ['the', 'result', 'of', 'the', 'operation', 'is', 'equivalent', 'to', 'weighted', 'composition', 'of', 'the', 'lexicon', 'and', 'the', 'weighted', 'intersection', 'of', 'the', 'rules', 'as', 'defined', 'in', 'allauzen', 'et', 'al', '2007', 'the', 'openfst', 'library', 'is', 'used', 'to', 'perform', 'all', 'of', 'the', 'wfst', 'operations', 'allauzen', 'et', 'al', '2007'], ['the', 'distributional', 'hypothesis', 'of', 'meaning', 'harris', '1954', 'is', 'a', 'widelyused', 'approach', 'for', 'estimating', 'term', 'similaritythis', 'is', 'a', 'corpusbased', 'metric', 'relying', 'on', 'the', 'distributional', 'hypothesis', 'of', 'meaning', 'suggesting', 'that', 'similarity', 'of', 'context', 'implies', 'similarity', 'of', 'meaning', 'z', 'harris', '1954'], ['preprocessing', 'we', 'tokenized', 'the', 'english', 'side', 'of', 'all', 'bitexts', 'for', 'language', 'modeling', 'using', 'the', 'standard', 'tokenizer', 'of', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['one', 'is', 'from', 'turian', 'et', 'al', '2010', '', 'the', 'dimension', 'of', 'word', 'embedding', 'is', '50for', 'brown', 'the', 'features', 'are', 'the', 'prefix', 'features', 'extracted', 'from', 'word', 'clusters', 'in', 'the', 'same', 'way', 'as', 'turian', 'et', 'al', '2010'], ['the', 'thresholds', 'were', 'thoroughly', 'selected', 'depending', 'on', 'our', 'analysis', 'for', 'the', 'wordnet', 'hierarchy', 'and', 'semantic', 'similarity', 'measures', 'pedersen', 'et', 'al', '2004', 'for', 'a', 'pair', 'of', 'words', 'wordnet', 'provides', 'a', 'series', 'of', 'measures', 'of', 'the', 'semantic', 'similarity', 'pedersen', 'et', 'al', '2004'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'our', 'baseline', 'is', 'a', 'phrasebased', 'mt', 'system', 'trained', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['the', 'statistical', 'significance', 'test', 'is', 'also', 'carried', 'out', 'with', 'paired', 'bootstrap', 'resampling', 'method', 'with', 'p', '', '0001', 'intervals', 'koehn', '2004', 'we', 'measure', 'significance', 'of', 'results', 'using', 'bootstrap', 'resampling', 'at', 'p', '', '005', 'koehn', '2004'], ['we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007', '331', 'reference', 'system', 'we', 'compare', 'a', 'number', 'of', 'analogical', 'devices', 'to', 'the', 'stateoftheart', 'statistical', 'translation', 'engine', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'a', '2009', 'snapshot', 'of', 'wikipedia', '2', 'which', 'was', 'pos', 'tagged', 'and', 'lemmatized', 'using', 'the', 'treetagger', 'schmid', '1994', '3', 'all', 'english', 'data', 'are', 'pos', 'tagged', 'and', 'lemmatised', 'using', 'the', 'treetagger', 'schmid', '1994'], ['the', 'text', 'was', 'preprocessed', 'using', 'wp2txt', '6', 'to', 'remove', 'markup', 'and', 'then', 'tokenized', 'with', 'the', 'stanford', 'tokenizer', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['we', 'run', 'our', 'experiments', 'on', 'europarl', 'koehn', '2005', '', 'a', 'multilingual', 'parallel', 'corpus', 'which', 'is', 'described', 'in', 'detail', 'in', 'section', '31europarl', 'koehn', '2005', 'is', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['we', 'use', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', '', 'the', 'machine', 'learning', 'library', 'for', 'python', 'for', 'implementing', 'the', 'different', 'approacheslogistic', 'regression', 'implemented', 'in', 'python', 'with', 'the', 'scikitlearn', 'machine', 'learning', 'library', 'pedregosa', 'et', 'al', '2011', '', 'is', 'used', 'for', 'all', 'classification', 'models'], ['the', 'way', 'they', 'were', 'added', 'is', 'similar', 'to', 'incorporating', 'the', 'negation', 'effect', 'described', 'by', 'pang', 'et', 'al', '2002', 'in', 'this', 'paper', 'we', 'use', 'the', 'subjectivity', 'corpus', 'by', 'pang', 'et', 'al', '2002'], ['we', 'used', 'the', 'lexicalized', 'dependency', 'parser', 'in', 'the', 'stanford', 'statistical', 'natural', 'language', 'parser', 'ver203', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parses', 'for', 'the', 'data', 'setsin', 'our', 'experiments', '', 'we', 'used', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'create', 'dependency', 'parses'], ['we', 'modified', 'the', 'implementation', 'of', 'the', 'swell', 'java', 'package', '4', 'of', 'dhillon', 'et', 'al', '2015', 'we', 'also', 'compare', 'our', 'word', 'embeddings', 'with', 'the', 'eigen', 'word', 'embeddings', 'of', 'dhillon', 'et', 'al', '2015', '', 'without', 'any', 'prior', 'knowl', 'edge'], ['the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'is', 'used', 'for', 'extracting', 'features', 'from', 'the', 'dependency', 'parse', 'treeswe', 'used', 'the', 'lexicalized', 'dependency', 'parser', 'in', 'the', 'stanford', 'statistical', 'natural', 'language', 'parser', 'ver203', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parses', 'for', 'the', 'data', 'sets'], ['the', 'first', 'work', 'on', 'this', 'topic', 'was', 'done', 'back', 'in', 'the', 'eighties', 'church', '1988', 'the', 'dutch', 'version', 'was', 'tagged', 'automatically', 'using', 'a', 'tagger', 'inspired', 'on', 'the', 'english', 'tagger', 'described', 'in', 'church', '1988'], ['the', 'openfst', 'library', 'is', 'used', 'to', 'perform', 'all', 'of', 'the', 'wfst', 'operations', 'allauzen', 'et', 'al', '2007', 'the', 'decoder', 'is', 'implemented', 'with', 'weighted', 'finite', 'state', 'transducers', 'wfsts', 'using', 'standard', 'operations', 'available', 'in', 'the', 'openfst', 'libraries', 'allauzen', 'et', 'al', '2007'], ['for', 'our', 'experiments', 'we', 'used', 'translated', 'movie', 'subtitles', 'from', 'the', 'opus', 'corpus', 'tiedemann', '2009b', 'for', 'chineseenglish', 'experiments', 'we', 'used', 'the', 'openoffice3', 'oo3', 'parallel', 'corpus', 'tiedemann', '2009', '', 'which', 'is', 'oo3', 'computer', 'office', 'productivity', 'software', 'documentation'], ['the', 'moses15', 'result', 'is', 'obtained', 'by', 'applying', 'the', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'over', 'letter', 'strings', 'with', '15character', 'context', 'windowsthe', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'openfst', 'library', 'is', 'used', 'to', 'perform', 'all', 'of', 'the', 'wfst', 'operations', 'allauzen', 'et', 'al', '2007', 'the', 'decoder', 'is', 'implemented', 'with', 'weighted', 'finite', 'state', 'transducers', 'wfsts', 'using', 'standard', 'operations', 'available', 'in', 'the', 'openfst', 'libraries', 'allauzen', 'et', 'al', '2007'], ['we', 'select', 'as', 'a', 'generalpurpose', 'corpus', 'europarl', 'v7', 'koehn', '2005', '', 'with', '197m', 'parallel', 'sentenceswe', 'run', 'our', 'experiments', 'on', 'europarl', 'koehn', '2005', '', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['we', 'also', 'compare', 'our', 'word', 'embeddings', 'with', 'the', 'eigen', 'word', 'embeddings', 'of', 'dhillon', 'et', 'al', '2015', '', 'without', 'any', 'prior', 'knowl', 'edgedhillon', 'et', 'al', '2015', 'used', 'cca', 'to', 'derive', 'word', 'embeddings', 'through', 'the', 'following', 'procedure'], ['the', 'relationship', 'between', 'language', 'and', 'sentiment', 'is', 'an', 'active', 'area', 'of', 'investigation', 'pang', 'and', 'lee', '2008', 'the', 'rapid', 'growth', 'of', 'usergenerated', 'content', 'much', 'of', 'which', 'is', 'sentimentladen', 'has', 'fueled', 'an', 'interest', 'in', 'sentiment', 'analysis', 'pang', 'and', 'lee', '2008', 'liu', '2010'], ['support', 'vector', 'machines', 'svm', 'are', 'one', 'of', 'the', 'binary', 'classifiers', 'based', 'on', 'maximum', 'margin', 'strategy', 'introduced', 'by', 'vapnik', 'vapnik', '1995', 'these', 'classifiers', 'are', 'based', 'on', 'a', 'discriminative', 'model', 'support', 'vector', 'machine', 'svm', '6', 'vapnik', '1995'], ['in', 'barankov', 'and', 'tamchyna', '2014', 'we', 'experimented', 'with', 'targeted', 'paraphrasing', 'using', 'the', 'freely', 'available', 'smt', 'system', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['to', 'learn', 'the', 'parameters', 'of', 'the', 'model', 'we', 'minimize', 'the', 'crossentropy', 'loss', 'as', 'the', 'training', 'objective', 'using', 'the', 'adam', 'optimization', 'algorithm', 'kingma', 'and', 'ba', '2014', 'for', 'optimization', 'we', 'employed', 'the', 'adam', '6', 'httpnlpstanfordedusoftwarecorenlpshtml', 'algorithm', 'kingma', 'and', 'ba', '2014', 'to', 'update', 'parameters'], ['for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'was', 'used', 'for', 'word', 'alignment', 'and', 'phrase', 'translation', 'probabilities', 'were', 'estimated', 'from', 'them', 'by', 'the', 'moses', 'system', 'koehn', 'et', 'al', '2007'], ['the', 'text', 'was', 'preprocessed', 'using', 'wp2txt', '6', 'to', 'remove', 'markup', 'and', 'then', 'tokenized', 'with', 'the', 'stanford', 'tokenizer', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['the', 'task', 'reported', 'on', 'here', 'is', 'to', 'produce', 'propbank', 'kingsbury', 'and', 'palmer', '2002', 'labels', 'given', 'the', 'features', 'provided', 'for', 'the', 'conll2005', 'closed', 'task', 'carreras', 'and', 'm', 'arquez', '2005a', 'standard', 'for', 'predicate', 'argument', 'annotation', 'is', 'provided', 'in', 'the', 'propbank', 'project', 'kingsbury', 'and', 'palmer', '2002'], ['for', 'both', 'systems', 'the', 'used', 'training', 'data', 'is', 'from', 'the', '4th', 'version', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005', 'and', 'the', 'news', 'commentary', 'corpusthe', 'source', 'of', 'bilingual', 'data', 'used', 'in', 'the', 'experiments', 'is', 'the', 'europarl', 'collection', 'koehn', '2005'], ['we', 'use', 'the', 'stanford', 'parser', 'to', 'generate', 'a', 'dg', 'for', 'each', 'sentence', 'de', 'marneffe', 'et', 'al', '2006', 'we', 'used', 'the', 'lexicalized', 'dependency', 'parser', 'in', 'the', 'stanford', 'statistical', 'natural', 'language', 'parser', 'ver203', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parses', 'for', 'the', 'data', 'sets'], ['we', 'used', 'predicted', 'collapsed', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'extract', 'arguments', 'of', 'the', 'verbs', 'and', 'used', 'only', 'a', 'subset', 'of', 'dependents', 'of', 'a', 'verbwe', 'extract', 'syntactic', 'dependencies', 'using', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'and', 'use', 'its', 'collapsed', 'dependency', 'format'], ['we', 'tokenize', 'english', 'data', 'and', 'segment', 'chinese', 'data', 'using', 'the', 'stanford', 'corenlp', 'toolkit', 'manning', 'et', 'al', '2014', '2', 'further', 'we', 'sentencesplit', 'tokenized', 'and', 'lemmatized', 'the', 'text', 'in', 'wikipedia', 'and', 'gigaword', 'using', 'stanford', 'corenlp', 'toolkit', '360', 'manning', 'et', 'al', '2014'], ['the', 'evaluation', 'emphasis', 'in', 'multidocument', 'summarization', 'has', 'been', 'on', 'evaluating', 'content', 'not', 'readability', '', 'using', 'manual', '', 'as', 'well', 'as', 'automatic', 'lin', 'and', 'hovy', '2003', 'methodsrouge', 'lin', 'and', 'hovy', '2003', 'has', 'been', 'adopted', 'as', 'a', 'standard', 'evaluation', 'metric', 'in', 'various', 'summarization', 'tasks'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrase', 'based', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'all', 'experiments', 'were', 'carried', 'out', 'using', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', '', 'in', 'its', 'standard', 'nonmonotonic', 'configuration'], ['the', 'similarity', 'function', 'is', 'here', 'the', 'smoothed', 'partial', 'tree', 'kernel', 'sptk', 'proposed', 'in', 'croce', 'et', 'al', '2011', 'we', 'directly', 'apply', 'the', 'smoothed', 'partial', 'treekernel', 'sptk', 'proposed', 'in', 'croce', 'et', 'al', '2011', '', 'to', 'estimate', 'the', 'similarity', 'among', 'a', 'specific', 'tree', 'representation'], ['we', 'tokenize', 'english', 'data', 'and', 'segment', 'chinese', 'data', 'using', 'the', 'stanford', 'corenlp', 'toolkit', 'manning', 'et', 'al', '2014', '2', 'further', 'we', 'sentencesplit', 'tokenized', 'and', 'lemmatized', 'the', 'text', 'in', 'wikipedia', 'and', 'gigaword', 'using', 'stanford', 'corenlp', 'toolkit', '360', 'manning', 'et', 'al', '2014'], ['the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'is', 'used', 'for', 'extracting', 'features', 'from', 'the', 'dependency', 'parse', 'treeswe', 'used', 'the', 'lexicalized', 'dependency', 'parser', 'in', 'the', 'stanford', 'statistical', 'natural', 'language', 'parser', 'ver203', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parses', 'for', 'the', 'data', 'sets'], ['to', 'demonstrate', 'the', 'effect', 'of', 'the', 'proposed', 'method', 'we', 'use', 'the', 'stateoftheart', 'phrasebased', 'system', 'and', 'hierarchical', 'phrasebased', 'system', 'implemented', 'in', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'is', 'used', 'to', 'train', 'a', 'phrase', 'based', 'smt', 'system', 'with', 'the', 'parallel', 'data', 'previously', 'introduced'], ['331', 'reference', 'system', 'we', 'compare', 'a', 'number', 'of', 'analogical', 'devices', 'to', 'the', 'stateoftheart', 'statistical', 'translation', 'engine', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['preprocessing', 'we', 'tokenized', 'the', 'english', 'side', 'of', 'all', 'bitexts', 'for', 'language', 'modeling', 'using', 'the', 'standard', 'tokenizer', 'of', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'tokenize', 'and', 'frequentcase', 'the', 'data', 'with', 'the', 'standard', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['finally', 'it', 'is', 'also', 'noticeable', 'that', 'the', 'percentage', 'of', 'compounds', 'detected', 'in', 'the', 'training', 'set', 'is', 'similar', 'to', 'the', 'one', 'reported', 'by', 'baroni', 'et', 'al', '2002', 'and', 'referenced', 'to', 'in', 'section', '2baroni', 'et', 'al', '2002', '', 'report', 'that', '47', 'of', 'the', 'vocabulary', 'types', 'in', 'the', 'apa', 'corpus', '2', 'were', 'compounds'], ['as', 'a', 'learning', 'algorithm', 'we', 'adopt', 'a', 'ranking', 'svm', 'joachims', '2002', '', 'which', 'is', 'an', 'instance', 'of', 'preference', 'learningto', 'train', 'a', 'mentionpair', 'classifier', 'we', 'use', 'the', 'svm', 'learning', 'algorithm', 'from', 'the', 'svm', 'light', 'package', 'joachims', '2002', '', 'converting', 'all', 'multivalued', 'features', 'into', 'an', 'equivalent', 'set', 'of', 'binaryvalued', 'featur'], ['brain', 'images', 'are', 'quite', 'noisy', 'so', 'we', 'used', 'the', 'methodology', 'from', 'mitchell', 'et', 'al', '2008', 'to', 'select', 'the', 'most', 'stable', 'brain', 'image', 'features', 'for', 'each', 'of', 'the', '18', 'participantsfollowing', 'the', 'evaluation', 'paradigm', 'of', 'mitchell', 'et', 'al', '2008', '', 'linear', 'models', 'trained', 'on', 'corpusbased', 'features', 'are', 'used', 'to', 'predict', 'the', 'pattern', 'of', 'eeg', 'activity', 'for', 'unseen', 'concepts'], ['the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007', 'this', 'system', 'is', 'the', 'moses', 'decoder', 'koehn', 'et', 'al', '2007', 'with', 'a', 'translation', 'and', 'a', 'language', 'model', 'trained', 'with', 'no', 'additional', 'resources', 'other', 'than', 'the', 'official', 'data', 'provided', 'by', 'the', 'shared', 'task', 'organizers'], ['results', 'on', 'englishfrench', 'englishromanian', 'and', 'czech', 'english', 'alignment', 'show', 'that', 'our', 'model', 'significantly', 'outperforms', 'fast', 'align', 'a', 'popular', 'loglinear', 'reparameterization', 'of', 'ibm', 'model', '2', 'dyer', 'et', 'al', '2013', 'our', 'alignment', 'model', 'is', 'based', 'on', 'a', 'simple', 'variant', 'of', 'ibm', 'model', '2', 'where', 'the', 'alignment', 'distribution', 'is', 'only', 'controlled', 'by', 'two', 'parameters', '', 'and', 'p', '0', 'dyer', 'et', 'al', '2013'], ['the', 'work', 'presented', 'in', 'berger', 'et', 'al', '1996', 'that', 'is', 'based', 'on', 'the', 'a', '', 'concept', 'however', 'introduces', 'word', 'reordering', 'restrictions', 'in', 'order', 'to', 'reduce', 'the', 'overall', 'search', 'spacethe', 'original', 'reordering', 'constraint', 'in', 'berger', 'et', 'al', '1996', 'is', 'shown', 'to', 'be', 'a', 'special', 'case', 'of', 'a', 'more', 'general', 'restriction', 'scheme', 'in', 'which', 'the', 'word', 'reordering', 'constraints', 'are', 'expressed', 'in', 'terms', 'of', 'simple'], ['our', 'second', 'method', 'is', 'based', 'on', 'the', 'recurrent', 'neural', 'network', 'language', 'model', 'rnnlm', 'approach', 'to', 'learning', 'word', 'embeddings', 'of', 'mikolov', 'et', 'al', '2013a', 'and', 'mikolov', 'et', 'al', '2013b', '', 'using', 'the', 'word2vec', 'packagemikolov', 'et', 'al', '2013a', 'proposed', 'a', 'faster', 'skipgram', 'model', 'word2vec', '5', 'which', 'tries', 'to', 'maximize', 'classification', 'of', 'a', 'word', 'based', 'on', 'another', 'word', 'in', 'the', 'same', 'sentence'], ['the', 'susanne', 'corpus', 'is', 'a', 'modified', 'and', 'condensed', 'version', 'of', 'brown', 'corpus', 'francis', 'and', 'kucera', '1979', 'in', 'order', 'to', 'avoid', 'the', 'errors', 'introduced', 'by', 'tagger', 'the', 'susanne', 'corpus', 'is', 'used', 'as', 'the', 'training', 'and', 'testhe', 'corpus', 'consists', 'of', 'a', 'subset', 'of', 'the', 'brown', 'corpus', '700000', 'words', 'with', 'more', 'than', '200000', 'senseannotated', 'francis', 'and', 'kucera', '1979', '', 'and', 'it', 'has', 'been', 'partofspeechtagged', 'and', 'sensetagged'], ['the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'all', 'these', 'features', 'are', 'inherited', 'from', 'moses', 'koehn', 'et', 'al', '2007'], ['english', 'annotations', 'were', 'all', 'produced', 'using', 'the', 'stanford', 'core', 'nlp', 'toolkit', 'manning', 'et', 'al', '2014', 'we', 'also', 'lemmatized', 'all', 'words', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['the', 'parameters', 'are', 'estimated', 'by', 'gibbs', 'sampling', 'using', 'the', 'mallet', 'implementation', 'mccallum', '2002', '10', 'we', 'used', 'the', 'pltm', 'implementation', 'in', 'mallet', 'mccallum', '2002'], ['the', 'metaclassifier', 'is', 'a', 'linear', 'svm', 'fan', 'et', 'al', '2008', 'implemented', 'with', 'kelpliblinear', 'fan', 'et', 'al', '2008', '', 'a', 'library', 'for', 'large', 'svm', 'linear', 'classification', 'is', 'used', 'for', 'implementation'], ['we', 'use', 'adadelta', 'zeiler', '2012', 'to', 'update', 'the', 'parameters', 'during', 'trainingwe', 'use', 'a', 'minibatch', 'stochastic', 'gradient', 'descent', 'sgd', 'with', 'the', 'adadelta', 'update', 'rule', 'zeiler', '2012', '', 'apply', 'random', 'initialization', 'within', '001', '001', 'for', 'w', 'f', 'j', '', 'and', 'initialize', 'the', 'remaining', 'parameter'], ['we', 'train', 'a', 'support', 'vector', 'machine', 'svm', 'cortes', 'and', 'vapnik', '1995', 'on', 'the', 'tweets', 'provided', 'for', 'trainingspecifically', 'we', 'use', 'support', 'vector', 'machine', 'svm', 'cortes', 'and', 'vapnik', '1995', 'for', 'this', 'purpose'], ['parameter', 'tuning', 'is', 'carried', 'out', 'using', 'z', 'mert', 'zaidan', '2009', 'feature', 'weights', 'based', 'on', 'bleu', 'are', 'then', 'tuned', 'using', 'z', 'mert', 'zaidan', '2009'], ['brin', 'identifies', 'the', 'use', 'of', 'patterns', 'in', 'the', 'discovery', 'of', 'relations', 'on', 'the', 'web', 'brin', '1998', 'brin', 'proposed', 'the', 'bootstrapping', 'method', 'for', 'relation', 'discovery', 'brin', '1998'], ['we', 'adopt', 'the', 'setting', 'of', 'socher', 'et', 'al', '2012', 'neural', 'networks', 'are', 'first', 'used', 'in', 'this', 'task', 'in', 'socher', 'et', 'al', '2012'], ['the', 'publicly', 'available', 'tool', 'giza', 'was', 'used', 'to', 'align', 'the', 'letters', 'och', 'and', 'ney', '2003', 'we', 'used', 'the', 'giza', 'software', 'och', 'and', 'ney', '2003', 'to', 'do', 'the', 'word', 'alignments'], ['crfsuite', 'implementation', 'okazaki', '2007', 'we', 'trained', 'a', 'crf', 'tagger', 'using', 'crfsuite', '1', 'okazaki', '2007'], ['the', 'hmm', 'classifier', 'used', 'in', 'our', 'experiments', 'follows', 'the', 'algorithm', 'described', 'in', 'bikel', 'et', 'al', '1999', 'the', 'hmm', 'classifier', 'used', 'in', 'the', 'experiments', 'in', 'section', '4', 'follows', 'the', 'system', 'description', 'in', 'bikel', 'et', 'al', '1999', '', 'and', 'it', 'performs', 'sequence', 'classification', 'by', 'assigning', 'each', 'word', 'either', 'one', 'of', 'the', 'named'], ['we', 'computed', '4gram', 'lms', 'with', 'modified', 'kneserney', 'smoothing', 'kneser', 'and', 'ney', '1995', 'the', 'language', 'model', 'is', 'a', '5gram', 'with', 'interpolation', 'and', 'kneserney', 'smoothing', 'kneser', 'and', 'ney', '1995'], ['socher', 'et', 'al', '2011', 'come', 'closest', 'to', 'our', 'target', 'problemwe', 'plan', 'to', 'adapt', 'ideas', 'from', 'socher', 'et', 'al', '2011', 'for', 'this', 'task'], ['we', 'use', 'ridge', 'regression', 'rr', 'with', 'l2norm', 'regularization', 'and', 'support', 'vector', 'regression', 'svr', 'with', 'an', 'rbf', 'kernel', 'from', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'we', 'use', 'the', 'support', 'vector', 'machines', 'implementation', 'in', 'the', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011', 'to', 'perform', 'regression', 'svr', 'on', 'each', 'feature', 'set', 'with', 'either', 'rbf', 'kernels', 'and', 'parameters', 'optimise'], ['as', 'for', 'the', 'former', 'hereafter', 'it', 'is', 'referred', 'to', 'synpth', 'we', 'continue', 'to', 'use', 'a', 'dependency', 'version', 'of', 'the', 'pruning', 'algorithm', 'of', 'xue', 'and', 'palmer', '2004', 'for', 'argument', 'a', 'dependency', 'version', 'of', 'the', 'pruning', 'algorithm', 'in', 'xue', 'and', 'palmer', '2004', 'is', 'used', 'to', 'find', 'in', 'an', 'iterative', 'way', 'the', 'current', 'syntactic', 'head', 'and', 'its', 'siblings', 'in', 'a', 'parse', 'tree', 'in', 'a', 'constitue'], ['15', 'the', 'significance', 'tests', 'were', 'performed', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004', 'we', 'used', 'bootstrap', 'resampling', 'for', 'testing', 'statistical', 'significance', 'koehn', '2004'], ['in', 'this', 'work', 'we', 'focus', 'on', 'learning', 'with', 'support', 'vector', 'machines', 'svms', 'vapnik', '1995', 'specifically', 'we', 'use', 'support', 'vector', 'machine', 'svm', 'cortes', 'and', 'vapnik', '1995', 'for', 'this', 'purpose'], ['we', 'used', 'treetagger', 'schmid', '1994', 'to', 'obtain', 'a', 'lemmatag', 'pair', 'for', 'each', 'russian', 'wordwe', 'used', 'the', 'stuttgart', 'treetagger', 'schmid', '1994', 'to', 'lemmatise', 'constituent', 'heads'], ['to', 'account', 'for', 'this', 'constraint', '', 'include', 'information', 'from', 'latent', 'semantic', 'analysis', 'deerwester', 'et', 'al', '1990', 'earlier', 'models', 'made', 'use', 'of', 'latent', 'semantic', 'analysis', 'lsa', 'deerwester', 'et', 'al', '1990'], ['the', 'system', 'was', 'trained', 'on', 'the', 'english', 'and', 'danish', 'part', 'of', 'the', 'europarl', 'corpus', 'version', '3', 'koehn', '2005', 'in', 'principle', 'classifiers', 'trained', 'on', 'pdtb', 'data', 'can', 'be', 'applied', 'directly', 'to', 'label', 'connectives', 'over', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005', 'used', 'for', 'training', 'and', 'testing', 'smt'], ['we', 'use', 'linear', 'svms', 'from', 'liblinear', 'and', 'svms', 'with', 'rbf', 'kernel', 'from', 'libsvm', 'chang', 'and', 'lin', '2011', 'the', 'edit', 'distance', 'kernel', 'was', 'trained', 'with', 'libsvm', 'chang', 'and', 'lin', '2011'], ['in', 'current', 'phrasebased', 'statistical', 'machine', 'translation', 'systems', 'such', 'as', 'moses', '1', 'koehn', 'et', 'al', '2007', '', 'the', 'translation', 'model', 'is', 'defined', 'in', 'terms', 'of', 'phrase', 'pairs', 'biphrases', 'extracted', 'from', 'a', 'bilingualit', 'is', 'a', 'standard', 'phrasebased', 'machine', 'translation', 'model', 'koehn', 'et', 'al', '2007'], ['we', 'train', 'a', 'support', 'vector', 'machine', 'svm', 'cortes', 'and', 'vapnik', '1995', 'on', 'the', 'tweets', 'provided', 'for', 'trainingspecifically', 'we', 'use', 'support', 'vector', 'machine', 'svm', 'cortes', 'and', 'vapnik', '1995', 'for', 'this', 'purpose'], ['the', 'word', 'alignment', 'was', 'obtained', 'by', 'running', 'giza', 'och', 'and', 'ney', '2003', 'the', 'word', 'alignment', 'is', 'created', 'by', 'giza', 'och', 'and', 'ney', '2003', '', 'the', 'intersection', 'symmetrization', 'is', 'used'], ['for', 'training', 'svm', 'classifiers', 'we', 'used', 'libsvm', 'package', 'chang', 'and', 'lin', '2001', 'the', 'svm', 'implementation', 'used', 'was', 'libsvm', 'chang', 'and', 'lin', '2001', '', 'and', 'default', 'parameters', 'were', 'used', 'radial', 'basis', 'function', 'kernel', 'cost', 'parameter', 'of', '1', 'and', 'a', 'gamma', 'value', 'of', 'the', 'inverse', 'of', 'the', 'number', 'of', 'd'], ['we', 'use', 'the', 'standard', 'alignment', 'tool', 'giza', 'och', 'and', 'ney', '2003', 'to', 'word', 'align', 'the', 'parallel', 'datawe', 'used', 'giza', 'och', 'and', 'ney', '2003', 'to', 'align', 'the', 'words', 'in', 'the', 'corpus'], ['we', 'use', 'the', 'standard', 'alignment', 'tool', 'giza', 'och', 'and', 'ney', '2003', 'to', 'word', 'align', 'the', 'parallel', 'datawe', 'used', 'the', 'giza', 'software', 'och', 'and', 'ney', '2003', 'to', 'do', 'the', 'word', 'alignments'], ['in', 'the', 'penn', 'discourse', 'treebank', '20', 'prasad', 'et', 'al', '2008', '', 'the', 'sense', 'is', 'called', 'chosen', 'alternativein', 'contrast', 'the', 'set', 'of', 'discourse', 'markers', 'in', 'our', 'work', 'is', 'fixed', 'for', 'english', '', 'we', 'use', '61', 'markers', 'annotated', 'in', 'the', 'penn', 'discourse', 'treebank', '20', 'prasad', 'et', 'al', '2008', '', 'for', 'german', 'we', 'use', '155', 'oneword', 'tr'], ['weka', 'hall', 'et', 'al', '2009', 'was', 'used', 'to', 'apply', 'learning', 'methods', 'to', 'extracted', 'featuresthe', 'svm', 'implementation', 'of', 'weka', 'hall', 'et', 'al', '2009', 'was', 'used', 'to', 'build', 'the', 'clte', 'model'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settingsthis', 'was', 'done', 'with', 'a', 'specific', 'tool', 'provided', 'with', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'standard', 'alignment', 'tool', 'giza', 'och', 'and', 'ney', '2003', 'to', 'word', 'align', 'the', 'parallel', 'datawe', 'used', 'the', 'giza', 'software', 'och', 'and', 'ney', '2003', 'to', 'do', 'the', 'word', 'alignments'], ['we', 'used', 'the', 'giza', 'software', 'och', 'and', 'ney', '2003', 'to', 'do', 'the', 'word', 'alignmentswe', 'performed', 'word', 'alignment', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'default', 'growdiagfinaland', 'alignment', 'symmetrization', 'method'], ['the', 'language', 'model', 'used', 'is', 'the', '5gram', 'corpus', 'from', 'google', 'books', 'michel', 'et', 'al', '2011', 'we', 'used', 'google', 'books', 'ngrams', 'michel', 'et', 'al', '2011', 'as', 'the', 'general', 'corpus'], ['english', 'annotations', 'were', 'all', 'produced', 'using', 'the', 'stanford', 'core', 'nlp', 'toolkit', 'manning', 'et', 'al', '2014', 'we', 'also', 'lemmatized', 'all', 'words', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['for', 'example', 'turian', 'et', 'al', '2010', 'compared', 'brown', 'clusters', 'cw', 'embeddings', 'and', 'hlbl', 'embeddings', 'in', 'ner', 'and', 'chunking', 'taskswe', 'evaluate', 'cw', 'word', 'embeddings', 'with', '25', '50', 'and', '100', 'dimensions', 'as', 'well', 'as', 'hlbl', 'word', 'embeddings', 'with', '50', 'and', '100', 'dimensions', 'that', 'are', 'introduced', 'in', 'turian', 'et', 'al', '2010', 'and', 'can', 'be', 'downloaded', 'here', '4'], ['distributional', 'representations', 'encode', 'an', 'expression', 'by', 'its', 'environment', 'assuming', 'the', 'context', 'dependent', 'nature', 'of', 'meaning', 'according', 'to', 'which', 'one', 'shall', 'know', 'a', 'word', 'by', 'the', 'company', 'it', 'keeps', 'firth', '1957', 'this', 'is', 'often', 'called', 'distributional', 'semantics', 'where', 'a', 'word', 'or', 'a', 'name', 'in', 'our', 'case', 'is', 'known', 'by', 'the', 'company', 'it', 'keeps', 'firth', '1957'], ['li', 'et', 'al', '2013', '', 'use', 'crowdsourcing', 'to', 'build', 'plot', 'graphsour', 'neural', 'network', 'is', 'similar', 'to', 'that', 'of', 'li', 'et', 'al', '2013'], ['we', 'use', 'the', 'standard', 'alignment', 'tool', 'giza', 'och', 'and', 'ney', '2003', 'to', 'word', 'align', 'the', 'parallel', 'datawe', 'used', 'giza', 'och', 'and', 'ney', '2003', 'to', 'align', 'the', 'words', 'in', 'the', 'corpus'], ['the', 'formalism', 'that', 'is', 'used', 'to', 'represent', 'the', 'semantics', 'in', 'the', 'delphin', 'grammars', 'is', 'minimal', 'recursion', 'semantics', 'mrs', 'copestake', 'et', 'al', '2005', 'meaning', 'representation', 'and', 'composition', 'in', 'the', 'erg', 'builds', 'on', 'the', 'formal', 'framework', 'of', 'minimal', 'recursion', 'semantics', 'mrs', 'copestake', 'et', 'al', '2005'], ['for', 'french', 'hungarian', 'polish', 'and', 'swedish', 'we', 'used', 'europarl', 'corpus', '1', 'koehn', '2005', 'for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['the', 'english', 'text', 'was', 'tokenized', 'using', 'the', 'word', 'tokenize', 'routine', 'from', 'nltk', 'bird', 'et', 'al', '2009', 'the', 'stopwords', 'are', 'taken', 'from', 'the', 'stopword', 'list', 'provided', 'by', 'the', 'nltk', 'toolkit', 'bird', 'et', 'al', '2009'], ['we', 'use', 'the', 'moses', 'decoder', 'koehn', 'et', 'al', '2007', 'as', 'a', 'representative', 'smt', 'decoder', 'inside', 'the', 'system', 'described', 'belowwe', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['specifically', 'the', 'sentence', 'compression', 'dataset', 'clarke', 'and', 'lapata', '2008', 'referred', 'as', 'cl08', 'is', 'used', 'for', 'subtree', 'deletion', 'model', 'training', 'arc', 'one', 'idea', 'is', 'to', 'apply', 'it', 'to', 'the', 'language', 'model', 'based', 'compression', 'method', 'clarke', 'and', 'lapata', '2008'], ['use', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'extract', 'a', 'set', 'of', 'dependencies', 'from', 'each', 'commentwe', 'extract', 'syntactic', 'dependencies', 'using', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'and', 'use', 'its', 'collapsed', 'dependency', 'format'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['in', 'practice', 'the', 'decoder', 'has', 'to', 'employ', 'beam', 'search', 'to', 'make', 'it', 'tractable', 'koehn', '2004', 'to', 'make', 'the', 'exponential', 'algorithm', 'practical', 'beam', 'search', 'is', 'the', 'standard', 'approximate', 'search', 'method', 'koehn', '2004'], ['the', 'source', 'of', 'bilingual', 'data', 'used', 'in', 'the', 'experiments', 'is', 'the', 'europarl', 'collection', 'koehn', '2005', 'the', 'europarl', 'data', 'set', 'consists', 'of', '707', 'sentences', 'of', 'the', 'german', 'part', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['the', 'tectogrammatical', 'annotation', 'layer', 'is', 'based', 'on', 'the', 'functional', 'generative', 'description', 'theory', 'sgall', 'et', 'al', '1986', 'our', 'approach', 'is', 'based', 'on', 'the', 'czech', 'linguistic', 'tradition', 'represented', 'mainly', 'in', 'sgall', 'et', 'al', '1986'], ['in', 'this', 'paper', 'we', 'use', 'the', 'subjectivity', 'corpus', 'by', 'pang', 'et', 'al', '2002', 'these', 'classifiers', 'have', 'been', 'used', 'in', 'related', 'work', 'by', 'pang', 'et', 'al', '2002'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'baseline', 'error', 'correction', 'system', 'using', 'the', 'moses', 'smt', 'system', 'koehn', 'et', 'al', '2007'], ['all', 'our', 'systems', 'are', 'contrasted', 'with', 'a', 'standard', 'phrasebased', 'system', 'built', 'with', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'train', 'standard', 'phrasebased', 'systems', 'with', 'default', 'configurations'], ['a', 'common', 'approach', 'to', 'computing', 'similarity', 'is', 'to', 'count', 'the', 'number', 'of', 'common', 'words', 'lesk', '1986', 'this', 'sense', 'similarity', 'measure', 'is', 'inspired', 'by', 'the', 'definition', 'of', 'the', 'lesk', 'algorithm', 'lesk', '1986'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'is', 'used', 'for', 'extracting', 'features', 'from', 'the', 'dependency', 'parse', 'treesin', 'our', 'experiments', '', 'we', 'used', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'create', 'dependency', 'parses'], ['we', 'tokenize', 'and', 'frequentcase', 'the', 'data', 'with', 'the', 'standard', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'implement', 'mert', 'and', 'mira', '1', '', 'and', 'directly', 'use', 'mira', '2', 'from', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'tokenize', 'and', 'frequentcase', 'the', 'data', 'with', 'the', 'standard', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'our', 'baseline', 'is', 'a', 'phrasebased', 'mt', 'system', 'trained', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'tokenize', 'and', 'frequentcase', 'the', 'data', 'with', 'the', 'standard', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['in', 'contrast', 'mcclosky', 'et', 'al', '2006', 'focus', 'on', 'large', 'seeds', 'and', 'exploit', 'a', 'rerankingparsermcclosky', 'et', 'al', '2006', '', 'use', 'selftraining', 'in', 'combination', 'with', 'a', 'pcfg', 'parser', 'and', 'reranking'], ['we', 'use', 'logistic', 'regression', 'with', 'l2', 'regularization', 'implemented', 'using', 'the', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011', 'bullet', 'logistic', 'regressionlr', 'we', 'use', 'logistic', 'regression', 'with', 'l2', 'loss', 'penalty', 'and', 'c1', 'pedregosa', 'et', 'al', '2011'], ['since', 'the', 'phrase', 'table', 'contains', 'lemmas', 'the', 'wikipedia', 'corpus', 'was', 'lemmatised', 'using', 'the', 'treetagger', 'schmid', '1994', '3', 'all', 'english', 'data', 'are', 'pos', 'tagged', 'and', 'lemmatised', 'using', 'the', 'treetagger', 'schmid', '1994'], ['we', 'use', 'logistic', 'regression', 'with', 'l2', 'regularization', 'implemented', 'using', 'the', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011', 'we', 'use', 'the', 'bernoulli', 'naive', 'bayes', 'classifier', 'in', 'scikit', 'with', 'the', 'default', 'settings', 'pedregosa', 'et', 'al', '2011'], ['the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'both', 'systems', 'are', 'based', 'on', 'the', 'moses', 'smt', 'toolkit', 'koehn', 'et', 'al', '2007', 'and', 'constructed', 'as', 'follows'], ['for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'tokenize', 'and', 'frequentcase', 'the', 'data', 'with', 'the', 'standard', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'train', 'standard', 'phrasebased', 'systems', 'with', 'default', 'configurations'], ['we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['europarl', 'koehn', '2005', 'is', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'extract', 'our', 'paraphrase', 'grammar', 'from', 'the', 'frenchenglish', 'portion', 'of', 'the', 'europarl', 'corpus', 'version', '5', 'koehn', '2005'], ['for', 'all', 'syntactic', 'parsers', 'we', 'used', 'the', 'basic', 'stanford', 'dependency', 'representation', 'de', 'marneffe', 'et', 'al', '2006', 'in', 'our', 'experiments', '', 'we', 'used', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'create', 'dependency', 'parses'], ['the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'is', 'used', 'for', 'extracting', 'features', 'from', 'the', 'dependency', 'parse', 'treesin', 'our', 'experiments', '', 'we', 'used', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'create', 'dependency', 'parses'], ['we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'all', 'our', 'systems', 'are', 'contrasted', 'with', 'a', 'standard', 'phrasebased', 'system', 'built', 'with', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'crossentropy', 'loss', 'function', 'and', 'minibatch', 'adagrad', 'duchi', 'et', 'al', '2011', 'to', 'optimize', 'parameters', 'we', 'use', 'adagrad', 'duchi', 'et', 'al', '2011', '', 'with', 'the', 'initial', 'learning', 'rate', 'set', 'to', '', '', '05'], ['we', 'use', 'the', 'crossentropy', 'loss', 'function', 'and', 'minibatch', 'adagrad', 'duchi', 'et', 'al', '2011', 'to', 'optimize', 'parameters', 'we', 'train', 'the', 'concept', 'identification', 'stage', 'using', 'infinite', 'ramp', 'loss', '1', 'with', 'adagrad', 'duchi', 'et', 'al', '2011'], ['we', 'use', 'the', 'adagrad', 'method', 'duchi', 'et', 'al', '2011', 'to', 'automatically', 'update', 'the', 'learning', 'rate', 'for', 'each', 'parameterwe', 'use', 'the', 'crossentropy', 'loss', 'function', 'and', 'minibatch', 'adagrad', 'duchi', 'et', 'al', '2011', 'to', 'optimize', 'parameters'], ['socher', 'et', 'al', '2011', '', 'explored', 'using', 'recursive', 'autoencoders', 'raes', 'and', 'dynamic', 'pooling', 'for', 'paraphrase', 'detectionsocher', 'et', 'al', '2011', '', 'use', 'recursive', 'autoencoders', 'for', 'sentiment', 'analysis', 'on', 'the', 'sentence', 'level'], ['2', 'translation', 'model', 'moses', 'is', 'a', 'stateoftheart', 'toolkit', 'for', 'phrasebased', 'smt', 'systems', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['2', 'translation', 'model', 'moses', 'is', 'a', 'stateoftheart', 'toolkit', 'for', 'phrasebased', 'smt', 'systems', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['2', 'translation', 'model', 'moses', 'is', 'a', 'stateoftheart', 'toolkit', 'for', 'phrasebased', 'smt', 'systems', 'koehn', 'et', 'al', '2007', 'our', 'baseline', 'is', 'a', 'phrasebased', 'mt', 'system', 'trained', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['all', 'our', 'systems', 'are', 'contrasted', 'with', 'a', 'standard', 'phrasebased', 'system', 'built', 'with', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['for', 'all', 'syntactic', 'parsers', 'we', 'used', 'the', 'basic', 'stanford', 'dependency', 'representation', 'de', 'marneffe', 'et', 'al', '2006', 'in', 'our', 'experiments', '', 'we', 'used', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'create', 'dependency', 'parses'], ['can', 'be', 'evaluated', 'by', 'maximising', 'the', 'pseudolikelihood', 'on', 'a', 'training', 'corpus', 'malouf', '2002', 'the', 'parameters', 'can', 'be', 'efficiently', 'estimated', 'from', 'a', 'treebank', 'as', 'shown', 'by', 'malouf', '2002'], ['the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['2', 'translation', 'model', 'moses', 'is', 'a', 'stateoftheart', 'toolkit', 'for', 'phrasebased', 'smt', 'systems', 'koehn', 'et', 'al', '2007', 'all', 'our', 'systems', 'are', 'contrasted', 'with', 'a', 'standard', 'phrasebased', 'system', 'built', 'with', 'moses', 'koehn', 'et', 'al', '2007'], ['in', 'this', 'approach', 'we', 'used', 'the', 'nersuite', 'software', 'based', 'on', 'the', 'crfsuite', 'implementation', 'okazaki', '2007', 'nersuite', 'is', 'a', 'ner', 'system', 'that', 'is', 'built', 'on', 'top', 'of', 'the', 'crfsuite', 'okazaki', '2007'], ['2', 'translation', 'model', 'moses', 'is', 'a', 'stateoftheart', 'toolkit', 'for', 'phrasebased', 'smt', 'systems', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['these', 'efforts', 'focused', 'exclusively', 'on', 'the', 'meronymy', 'relation', 'as', 'used', 'in', 'wordnet', 'miller', 'et', 'al', '1990', 'experts', 'can', 'manually', 'specify', 'the', 'attributes', 'of', 'entities', 'as', 'in', 'the', 'wordnet', 'project', 'miller', 'et', 'al', '1990'], ['special', 'forms', 'of', 'relatedness', 'are', 'represented', 'in', 'the', 'lexical', 'entries', 'of', 'the', 'wordnet', 'lexical', 'database', 'miller', 'et', 'al', '1990', 'experts', 'can', 'manually', 'specify', 'the', 'attributes', 'of', 'entities', 'as', 'in', 'the', 'wordnet', 'project', 'miller', 'et', 'al', '1990'], ['we', 'train', 'our', 'model', 'on', 'a', 'subset', 'of', 'the', 'wackypedia', 'en', '6', 'corpus', 'baroni', 'et', 'al', '2009', 'we', 'used', 'the', 'publicly', 'available', 'wacky', 'corpus', 'baroni', 'et', 'al', '2009'], ['since', 'the', 'first', 'shared', 'task', 'on', 'recognising', 'textual', 'entailment', 'rte', 'dagan', 'et', 'al', '2005', 'was', 'organised', 'in', '2005', 'much', 'research', 'has', 'been', 'done', 'on', 'how', 'one', 'can', 'detect', 'entailment', 'between', 'natural', 'language', 'although', 'there', 'had', 'been', 'research', 'on', 'reasoning', 'expressed', 'in', 'natural', 'language', 'the', 'pascal', 'recognising', 'textual', 'entailment', 'rte', 'challenge', 'dagan', 'et', 'al', '2005', 'spurred', 'wide', 'interest', 'in', 'the', 'problem'], ['we', 'used', 'kbest', 'batch', 'mira', 'cherry', 'and', 'foster', '2012', 'for', 'tuningwe', 'tune', 'the', 'systems', 'using', 'kbest', 'batch', 'mira', 'cherry', 'and', 'foster', '2012'], ['we', 'used', 'the', 'mallet', 'toolkit', 'mccallum', '2002', 'for', 'learning', 'maximum', 'entropy', 'models', 'with', 'gaussian', 'priors', 'for', 'all', 'our', 'experimentswe', 'use', 'the', 'mallet', 'package', 'mccallum', '2002', 'for', 'experiments'], ['we', 'use', 'the', 'implementation', 'provided', 'by', 'crfsuite', '7', 'okazaki', '2007', 'for', 'both', 'training', 'and', 'classification', 'taskswe', 'use', 'crfsuite', 'okazaki', '2007', 'as', 'an', 'implementation', 'of', 'crf'], ['glish', 'source', 'with', 'target', 'french', 'by', 'using', 'giza', 'och', 'and', 'ney', '2003', 'word', 'alignments', 'were', 'created', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'used', 'rougen', 'lin', '2004', 'for', 'evaluation', 'of', 'summarieswe', 'expect', 'this', 'restriction', 'is', 'more', 'consistent', 'with', 'the', 'rouge', 'evaluation', 'metric', 'used', 'for', 'summarization', 'lin', '2004'], ['1', 'with', '2', 'regularization', 'using', 'adagrad', 'duchi', 'et', 'al', '2011', 'we', 'adopt', 'online', 'learning', 'updating', 'parameters', 'using', 'adagrad', 'duchi', 'et', 'al', '2011'], ['we', 'used', 'weka', 'hall', 'et', 'al', '2009', 'for', 'all', 'our', 'classification', 'experimentsweka', 'hall', 'et', 'al', '2009', 'which', 'contains', 'the', 'implementation', 'of', 'all', 'three', 'algorithms', 'was', 'used', 'in', 'our', 'study'], ['pos', 'tagging', 'was', 'performed', 'with', 'treetagger', 'schmid', '1994', 'treetagger', 'is', 'a', 'statistical', 'decision', 'treebased', 'pos', 'tagger', 'schmid', '1994'], ['word', 'alignments', 'on', 'the', 'parallel', 'corpus', 'are', 'performed', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'growdiagfinal', 'refinementthe', 'parallel', 'corpus', 'is', 'wordaligned', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011', 'classification', 'uses', 'the', 'scikitlearn', 'python', 'package', 'pedregosa', 'et', 'al', '2011'], ['the', 'lexical', 'sample', 'data', 'was', 'parsed', 'using', 'the', 'clark', 'and', 'curran', 'ccg', 'parser', 'clark', 'and', 'curran', '2004', 'it', 'builds', 'on', 'the', 'cc', 'ccg', 'parser', 'clark', 'and', 'curran', '2004'], ['for', 'postagging', 'we', 'used', 'the', 'stanford', 'postagger', 'toutanova', 'and', 'manning', '2000', 'next', 'we', 'replace', 'all', 'nouns', 'with', 'their', 'pos', 'tag', 'we', 'use', 'the', 'stanford', 'pos', 'tagger', 'toutanova', 'and', 'manning', '2000'], ['we', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005', 'all', 'the', 'data', 'come', 'from', 'europarl', 'koehn', '2005'], ['this', 'data', 'is', 'part', 'of', 'the', 'nucle', 'corpus', 'dahlmeier', 'et', 'al', '2013', 'the', 'training', 'data', 'released', 'by', 'the', 'task', 'organizers', 'comes', 'from', 'the', 'nucle', 'corpus', 'dahlmeier', 'et', 'al', '2013', '', 'which', 'contains', 'essays', 'written', 'by', 'learners', 'of', 'english', 'as', 'a', 'foreign', 'language', 'and', 'is', 'corrected', 'by'], ['the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007', 'conducted', 'using', 'the', 'moses', 'phrasebased', 'decoder', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'the', 'word', 'alignment', 'produced', 'by', 'giza', 'och', 'and', 'ney', '2000', 'out', 'of', 'an', 'ibm', 'model', '2och', 'is', 'the', 'hmm', 'alignment', 'model', 'of', 'och', 'and', 'ney', '2000'], ['for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005', 'all', 'the', 'data', 'come', 'from', 'europarl', 'koehn', '2005'], ['word', 'alignment', 'is', 'performed', 'using', 'giza', 'och', 'and', 'ney', '2003', 'glish', 'source', 'with', 'target', 'french', 'by', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['word', 'alignment', 'was', 'done', 'with', 'giza', 'och', 'and', 'ney', '2003', 'for', 'both', 'systemsword', 'alignment', 'is', 'performed', 'with', 'giza', 'och', 'and', 'ney', '2003'], ['the', 'evaluation', 'results', 'were', 'provided', 'by', 'the', 'organizers', 'using', 'their', 'evaluation', 'script', 'in', 'python', 'dahlmeier', 'and', 'ng', '2012', 'the', 'models', 'were', 'evaluated', 'using', 'm2', 'scorer', 'dahlmeier', 'and', 'ng', '2012'], ['in', 'future', 'work', 'we', 'can', 'therefore', 'incorporate', 'unsupervised', 'methods', 'of', 'nsw', 'classification', 'and', 'expansion', 'along', 'the', 'lines', 'of', 'the', 'automatic', 'dictionary', 'construction', 'method', 'presented', 'by', 'han', 'et', 'al', '2012', 'han', 'et', 'al', '2012', 'introduced', 'a', 'dictionary', 'based', 'method', 'and', 'an', 'automatic', 'normalisationdictionary', 'construction', 'method'], ['glish', 'source', 'with', 'target', 'french', 'by', 'using', 'giza', 'och', 'and', 'ney', '2003', 'word', 'alignment', 'is', 'done', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'conducted', 'statistical', 'significance', 'tests', 'for', 'bleu', 'between', 'our', 'best', 'domainadapted', 'system', 'the', 'baseline', 'and', 'the', 'three', 'thirdparty', 'systems', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'with', '1000', 'we', 'used', 'bootstrap', 'resampling', 'for', 'testing', 'statistical', 'significance', 'koehn', '2004'], ['the', 'word', 'alignment', 'was', 'obtained', 'by', 'running', 'giza', 'och', 'and', 'ney', '2003', 'word', 'alignment', 'is', 'performed', 'with', 'giza', 'och', 'and', 'ney', '2003'], ['word', 'alignment', 'was', 'done', 'with', 'giza', 'och', 'and', 'ney', '2003', 'for', 'both', 'systemsword', 'alignment', 'is', 'performed', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['for', 'the', 'theory', 'of', 'cho', '', 'chai', '2000', 'to', 'be', 'complete', 'we', 'have', 'proposed', 'a', 'new', 'type', 'of', 'marker', 'and', 'the', 'adjunct', 'lp', 'constraint', 'in', 'conjunction', 'with', 'their', 'argument', 'lp', 'constraintin', 'section', '3', 'we', 'present', 'a', 'solution', 'to', 'the', 'alleged', 'counterexample', 'of', 'cho', '', 'chai', '2000', 'and', 'explain', 'some', 'representative', 'scrambled', 'sentences', 'in', 'korean', 'under', 'our', 'analysis'], ['in', 'order', 'to', 'compare', 'our', 'method', 'to', 'a', 'well', 'understood', 'phrase', 'baseline', 'we', 'present', 'a', 'method', 'that', 'tracts', 'phrases', 'by', 'harvesting', 'the', 'viterbi', 'path', 'from', 'an', 'hmm', 'alignment', 'model', 'vogel', 'et', 'al', '1996', 'in', 'our', 'experiments', 'we', 'investigated', 'both', 'weak', 'and', 'a', 'strong', 'initialisations', 'the', 'former', 'based', 'on', 'word', 'alignments', 'from', 'ibm', 'model', '1', 'and', 'the', 'latter', 'on', 'alignments', 'from', 'an', 'hmm', 'model', 'vogel', 'et', 'al', '1996'], ['we', 'examine', 'the', 'quality', 'of', 'translations', 'to', 'english', 'from', 'chinese', 'and', 'arabic', 'using', 'humantargeted', 'translation', 'edit', 'rates', 'hter', 'snover', 'et', 'al', '2006', '', 'which', 'roughly', 'captures', 'the', 'minimal', 'number', 'of', 'editsto', 'calculate', 'the', 'number', 'of', 'changes', 'we', 'used', 'a', 'modified', 'translation', 'edit', 'rate', 'ter', 'which', 'measures', 'the', 'number', 'of', 'edits', 'needed', 'to', 'transform', 'one', 'sentence', 'into', 'another', 'snover', 'et', 'al', '2006'], ['integer', 'linear', 'programming', 'ilp', 'has', 'recently', 'been', 'applied', 'to', 'inference', 'in', 'sequential', 'conditional', 'random', 'fields', 'roth', 'and', 'yih', '2004', '', 'this', 'has', 'allowed', 'the', 'use', 'of', 'truly', 'global', 'constraints', 'during', 'infesecond', 'to', 'avoid', 'the', 'error', 'propagation', 'problem', 'inherent', 'in', 'the', 'pipeline', 'approach', 'we', 'perform', 'joint', 'inference', 'over', 'the', 'outputs', 'of', 'the', 'aci', 'and', 'ri', 'classifiers', 'in', 'an', 'integer', 'linear', 'programming', 'ilp', 'frame'], ['for', 'that', 'purpose', 'we', 'use', 'the', 'word', 'analogy', 'task', 'proposed', 'by', 'mikolov', 'et', 'al', '2013a', '', 'which', 'measures', 'the', 'accuracy', 'on', 'answering', 'questions', 'like', 'what', 'is', 'the', 'word', 'that', 'is', 'similar', 'to', 'small', 'in', 'the', 'same', 'sensmikolov', 'et', 'al', '2013a', 'proposed', 'a', 'faster', 'skipgram', 'model', 'word2vec', '5', 'which', 'tries', 'to', 'maximize', 'classification', 'of', 'a', 'word', 'based', 'on', 'another', 'word', 'in', 'the', 'same', 'sentence'], ['the', 'corpus', 'is', 'first', 'wordaligned', 'using', 'a', 'word', 'alignment', 'heuristic', 'och', 'and', 'ney', '2003', 'a', 'core', 'component', 'of', 'every', 'pbsmt', 'system', 'is', 'the', 'phrase', 'table', 'which', 'contains', 'bilingual', 'phrase', 'pairs', 'extracted', 'from', 'a', 'bilingual', 'corpus', 'after', 'word', 'alignment', 'och', 'and', 'ney', '2003'], ['for', 'ranking', 'we', 'use', 'the', 'svm', 'rank', 'ranker', 'joachims', '2006', '', 'which', 'learns', 'a', 'sparse', 'weight', 'vector', 'that', 'minimizes', 'the', 'number', 'of', 'swapped', 'pairs', 'in', 'the', 'training', 'setwe', 'use', 'the', 'svm', 'rank', 'implementation', 'joachims', '2006', 'of', 'ranking', 'svm', 'in', 'this', 'paper'], ['parameters', 'are', 'updated', 'using', 'adagrad', 'duchi', 'et', 'al', '2011', 'with', 'a', 'learning', 'rate', 'of', '01all', 'models', 'were', 'trained', 'using', 'adagrad', 'duchi', 'et', 'al', '2011', 'with', 'an', 'initial', 'base', 'learning', 'rate', 'of', '01', 'which', 'we', 'exponentially', 'decayed', 'with', 'a', 'decade', 'of', '15', 'million', 'steps'], ['the', 'tagger', 'we', 'use', 'is', 'tnt', 'brants', '2000', '', 'a', 'hidden', 'markov', 'trigram', 'tagger', 'which', 'was', 'trained', 'on', 'the', 'spoken', 'dutch', 'corpus', 'cgn', 'internal', 'release', '6hcrc', 'is', 'tagged', 'with', 'tnt', 'brants', '2000', '', 'trained', 'on', 'the', 'full', 'ptb'], ['sentiment', 'score', 'of', 'the', 'last', 'post', 'of', 'the', 'observation', 'period', 'we', 'trained', 'an', 'l2', 'regularized', 'logistic', 'regression', 'from', 'liblinear', 'fan', 'et', 'al', '2008', 'using', 'the', 'data', 'collected', 'from', 'the', 'depression', 'forumwe', 'trained', 'the', 'classifiers', 'using', 'the', 'liblinear', 'implementation', 'fan', 'et', 'al', '2008', 'of', 'logistic', 'regres', 'sion'], ['for', 'german', 'english', 'we', 'also', 'have', 'a', 'system', 'based', 'on', 'moses', 'koehn', 'et', 'al', '2007', 'for', 'reference', 'we', 'also', 'show', 'the', 'mt', 'performance', 'of', 'the', 'phrase', 'based', 'stringtotree', 'and', 'treetostring', 'systems', 'which', 'are', 'based', 'on', 'the', 'opensource', 'gizamoses', 'pipeline', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'universal', 'pos', 'tagset', 'upos', 'of', 'petrov', 'et', 'al', '2012', 'for', 'example', 'petrov', 'et', 'al', '2012', '', 'build', 'supervised', 'pos', 'taggers', 'for', '22', 'languages', 'using', 'the', 'tnt', 'tagger', 'brants', '2000', '', 'with', 'an', 'average', 'accuracy', 'of', '952'], ['we', 'also', 'considered', 'an', 'ensemble', 'of', 'our', 'approach', 'and', 'that', 'of', 'berant', 'and', 'liang', '2014', 'we', 'chose', 'a', 'threshold', 'such', 'that', 'our', 'approach', 'predicts', '50', 'of', 'the', 'time', 'when', 'sq', 'a', 'is', 'above', 'its', 'value', 'and', 'the', 'other', '50', 'of', 'the', 'time', 'we', 'use', 'the', 'prediction', 'of', 'berant', 'and', 'liang', '2014', 'instead'], ['one', 'way', 'to', 'solve', 'this', 'problem', 'is', 'to', 'use', 'a', 'kernel', 'function', 'that', 'is', 'tailored', 'for', 'particular', 'nlp', 'applications', 'such', 'as', 'the', 'tree', 'kernel', 'collins', 'and', 'duffy', '2001', 'for', 'statistical', 'parsingthe', 'proof', 'is', 'similar', 'to', 'the', 'proof', 'for', 'the', 'tree', 'kernel', 'in', 'collins', 'and', 'duffy', '2001'], ['we', 'use', 'adam', 'kingma', 'and', 'ba', '2015', 'for', 'optimisation', 'with', 'initial', 'learning', 'rate', 'of', '0001we', 'train', 'with', 'the', 'adam', 'optimizer', 'kingma', 'and', 'ba', '2015', '', 'a', 'learning', 'rate', 'of', '00001', 'batch', 'size', 'of', '50', 'and', 'dropout', 'with', 'probability', '02', 'applied', 'to', 'the', 'hidden', 'layer'], ['we', 'build', 'our', 'pbsmt', 'systems', 'in', 'a', 'standard', 'way', 'using', 'the', 'moses', 'system', '', 'kenlm', 'for', 'language', 'modelling', 'heafield', '2011', '', 'and', 'standard', 'lexical', 'reordering', 'model', 'we', 'use', 'kenlm', '3', 'heafield', '2011', 'for', 'computing', 'the', 'target', 'language', 'model', 'score'], ['the', 'starting', 'point', 'for', 'our', 'model', 'is', 'the', 'skipgram', 'with', 'negative', 'sampling', 'sgns', 'objective', 'of', 'mikolov', 'et', 'al', '2013b', 'our', 'model', 'is', 'an', 'extension', 'of', 'the', 'contextual', 'bag', 'of', 'words', 'cbow', 'model', 'of', 'mikolov', 'et', 'al', '2013b', '', 'a', 'method', 'for', 'learning', 'vector', 'representations', 'of', 'words', 'based', 'on', 'their', 'distributional', 'contexts'], ['we', 'introduce', 'a', 'new', 'anaphoricity', 'detection', 'model', 'as', 'the', 'second', 'neural', 'model', 'using', 'a', 'long', 'short', 'term', 'memory', 'lstm', 'network', 'hochreiter', 'and', 'schmidhuber', '1997', 'we', 'generated', 'embeddings', 'by', 'training', 'a', 'characterlevel', 'longshort', 'term', 'memory', 'lstm', 'network', 'hochreiter', 'and', 'schmidhuber', '1997', '', 'using', 'it', 'as', 'an', 'encoder', 'on', 'the', 'turns', 'at', 'talk', 'from', 'our', 'corpus'], ['the', 'realisation', 'ranking', 'component', 'is', 'an', 'svm', 'ranking', 'model', 'implemented', 'with', 'svmrank', 'a', 'support', 'vector', 'machinebased', 'learning', 'tool', 'joachims', '2006', 'the', 'advantage', 'of', 'an', 'svm', 'rank', 'model', 'is', 'that', 'it', 'is', 'very', 'fast', 'and', 'it', 'has', 'been', 'shown', 'to', 'be', 'accurate', 'for', 'a', 'variety', 'of', 'ranking', 'problems', 'joachims', '2006'], ['wsi', 'is', 'generally', 'considered', 'as', 'an', 'unsupervised', 'clustering', 'task', 'under', 'the', 'distributional', 'hypothesis', 'harris', '1954', 'that', 'the', 'word', 'meaning', 'is', 'reflected', 'by', 'the', 'set', 'of', 'contexts', 'in', 'which', 'it', 'appearsaccording', 'to', 'the', 'distributional', 'hypothesis', 'of', 'meaning', 'harris', '1954', '', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'tend', 'to', 'be', 'semantically', 'similar'], ['we', 'run', 'our', 'experiments', 'on', 'europarl', 'koehn', '2005', '', 'a', 'multilingual', 'parallel', 'corpus', 'which', 'is', 'described', 'in', 'detail', 'in', 'section', '31our', 'experiments', 'were', 'carried', 'out', 'on', 'the', 'europarl', 'koehn', '2005', 'corpus', 'which', 'is', 'a', 'corpus', 'widely', 'used', 'in', 'smt', 'and', 'that', 'has', 'been', 'used', 'in', 'several', 'mt', 'evaluation', 'campaigns'], ['for', 'the', 'language', 'model', 'we', 'used', 'the', 'kenlm', 'toolkit', 'heafield', '2011', 'to', 'create', 'a', '5gram', 'language', 'model', 'on', 'the', 'target', 'side', 'of', 'the', 'europarl', 'corpus', 'v7', 'with', 'approximately', '54m', 'tokens', 'with', 'kneserney', 'smoothe', 'language', 'model', 'is', 'a', '5gram', 'kenlm', 'heafield', '2011', 'model', 'trained', 'using', 'lmplz', 'with', 'modified', 'kneserney', 'smoothing', 'and', 'no', 'pruning'], ['the', 'second', 'collection', 'is', 'constituted', 'by', 'the', 'genia', 'corpus', 'kim', 'et', 'al', '2003', '3', '', 'which', 'contains', '2000', 'abstracts', 'from', 'medline', 'a', 'total', 'of', '18546', 'sentences', 'genia', 'kim', 'et', 'al', '2003', 'is', 'a', 'collection', 'of', '2000', 'research', 'abstracts', 'selected', 'from', 'the', 'search', 'results', 'of', 'medline', 'database', 'using', 'keywords', 'mesh', 'terms', 'human', 'blood', 'cells', 'and', 'transcription', 'factors'], ['we', 'introduce', 'a', 'new', 'anaphoricity', 'detection', 'model', 'as', 'the', 'second', 'neural', 'model', 'using', 'a', 'long', 'short', 'term', 'memory', 'lstm', 'network', 'hochreiter', 'and', 'schmidhuber', '1997', 'we', 'generated', 'embeddings', 'by', 'training', 'a', 'characterlevel', 'longshort', 'term', 'memory', 'lstm', 'network', 'hochreiter', 'and', 'schmidhuber', '1997', '', 'using', 'it', 'as', 'an', 'encoder', 'on', 'the', 'turns', 'at', 'talk', 'from', 'our', 'corpus'], ['the', 'sentence', 'aligned', 'parallel', 'data', 'is', 'first', 'wordaligned', 'using', 'giza', 'in', 'both', 'sourcetarget', 'and', 'targetsource', 'directions', 'followed', 'by', 'the', 'application', 'of', 'traditional', 'symmetrisation', 'heuristics', 'och', 'and', 'ney', '2003', 'baseline', 'word', 'alignments', 'were', 'obtained', 'by', 'running', 'giza', 'in', 'both', 'directions', 'and', 'symmetrizing', 'using', 'the', 'growdiagfinaland', 'heuristic', 'och', 'and', 'ney', '2003'], ['all', 'the', 'language', 'models', 'lm', 'used', 'in', 'our', 'experiments', 'are', '5grams', 'modified', 'kneserney', 'smoothed', 'lms', 'trained', 'using', 'kenlm', 'heafield', 'et', 'al', '2013', 'for', 'the', 'language', 'model', 'we', 'used', 'all', 'monolingual', 'datasets', 'and', 'the', 'french', 'parts', 'of', 'the', 'parallel', 'datasets', 'and', 'trained', 'a', '5gram', 'language', 'model', 'with', 'modified', 'kneserney', 'smoothing', 'using', 'kenlm', 'heafield', 'et', 'al', '2013'], ['they', 'are', 'based', 'on', 'distributional', 'hypothesis', 'which', 'works', 'under', 'the', 'assumption', 'that', 'similar', 'words', 'occur', 'in', 'similar', 'contexts', 'harris', '1954', 'many', 'researchers', 'have', 'considered', 'generating', 'paraphrases', 'by', 'mining', 'the', 'web', 'guided', 'by', 'the', 'distributional', 'hypothesis', 'which', 'states', 'that', 'words', 'occurring', 'in', 'similar', 'contexts', 'tend', 'to', 'have', 'similar', 'meanings', 'harris', '1954'], ['finally', 'we', 'consider', 'the', 'europarl', 'corpus', 'v7', 'koehn', '2005', '', 'given', 'it', 'is', 'widely', 'used', 'in', 'the', 'mt', 'community', 'for', 'spanish', 'englishour', 'experiments', 'were', 'carried', 'out', 'on', 'the', 'europarl', 'koehn', '2005', 'corpus', 'which', 'is', 'a', 'corpus', 'widely', 'used', 'in', 'smt', 'and', 'that', 'has', 'been', 'used', 'in', 'several', 'mt', 'evaluation', 'campaigns'], ['for', 'the', 'contextual', 'check', 'we', 'use', 'the', 'google', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'which', 'contains', 'counts', 'for', 'ngrams', 'from', 'unigrams', 'through', 'to', 'fivegrams', 'obtained', 'from', 'over', '1', 'trillion', 'word', 'tokeorgwikifogindex', '8', 'for', 'word', 'frequency', 'we', 'use', 'the', 'unigrams', 'data', 'from', 'the', 'google', 'web1t', 'collection', 'brants', 'and', 'franz', '2006'], ['by', 'imposing', 'constraints', 'on', 'the', 'possible', 'word', 'reorderings', 'similar', 'to', 'that', 'described', 'in', 'berger', 'et', 'al', '1996', '', 'the', 'dpbased', 'approach', 'becomes', 'more', 'effective', 'when', 'the', 'constraints', 'are', 'applied', 'the', 'number', 'the', 'first', 'is', 'a', 'reimplementation', 'of', 'the', 'stackbased', 'decoder', 'described', 'in', 'berger', 'et', 'al', '1996'], ['surdeanu', 'et', 'al', '2012', 'propose', 'a', 'twolayer', 'multiinstance', 'multilabel', 'miml', 'framework', 'to', 'capture', 'the', 'dependencies', 'among', 'relationssurdeanu', 'et', 'al', '2012', 'proposed', 'a', 'novel', 'approach', 'to', 'multiinstance', 'multilabel', 'learning', 'for', 'relation', 'extraction', '', 'which', 'jointly', 'modeled', 'all', 'the', 'sentences', 'in', 'texts', 'and', 'all', 'labels', 'in', 'knowledge', 'bases', 'for'], ['we', 'have', 'theoretically', 'suggested', 'that', 'based', 'on', 'cho', '', 'chai', '2000', '', 'our', 'theory', 'can', 'be', 'a', 'complete', 'theory', 'of', 'scrambling', 'phenomenon', 'by', 'providing', 'the', 'new', 'type', 'marker', 'and', 'the', 'adjunct', 'lp', 'constraintthen', 'we', 'revise', 'the', 'two', 'lp', 'constraints', 'of', 'cho', '', 'chai', '2000', 'and', 'add', 'the', 'adjunct', 'lp', 'constraint'], ['for', 'the', 'contextual', 'check', 'we', 'use', 'the', 'google', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'which', 'contains', 'counts', 'for', 'ngrams', 'from', 'unigrams', 'through', 'to', 'fivegrams', 'obtained', 'from', 'over', '1', 'trillion', 'word', 'tokewe', 'used', 'the', 'unigram', 'counts', 'from', 'the', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'to', 'determine', 'the', 'frequency', 'of', 'use', 'of', 'each', 'candidate', 'synonym'], ['rouge', 'lin', '2004', 'is', 'a', 'set', 'of', 'evaluation', 'metrics', 'used', 'for', 'automatic', 'summarizationwe', 'expect', 'this', 'restriction', 'is', 'more', 'consistent', 'with', 'the', 'rouge', 'evaluation', 'metric', 'used', 'for', 'summarization', 'lin', '2004'], ['here', 'we', 'review', 'the', 'parameters', 'of', 'the', 'standard', 'phrasebased', 'translation', 'model', 'koehn', 'et', 'al', '2007', 'the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'spanishenglish', 's2e', 'training', 'corpus', 'was', 'drawn', 'from', 'the', 'europarl', 'collection', 'koehn', '2005', 'the', 'systems', 'for', 'the', 'english', '', 'spanish', 'translation', 'tasks', 'were', 'trained', 'on', 'the', 'sentencealigned', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'convert', 'the', 'trees', 'in', 'both', 'treebanks', 'from', 'constituencies', 'to', 'labeled', 'dependencies', 'see', 'figure', '1', 'using', 'the', 'stanford', 'converter', 'which', 'produces', '46', 'types', 'of', 'labeled', 'dependencies', '1', 'de', 'marneffe', 'et', 'al', '2006', 'we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006'], ['we', 'convert', 'the', 'trees', 'in', 'both', 'treebanks', 'from', 'constituencies', 'to', 'labeled', 'dependencies', 'see', 'figure', '1', 'using', 'the', 'stanford', 'converter', 'which', 'produces', '46', 'types', 'of', 'labeled', 'dependencies', '1', 'de', 'marneffe', 'et', 'al', 'we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006'], ['we', 'use', 'kenlm', '3', 'heafield', '2011', 'for', 'computing', 'the', 'target', 'language', 'model', 'scorewe', 'use', 'a', '5gram', 'lm', 'trained', 'on', 'the', 'gigaword', 'corpus', 'and', 'use', 'kenlm', 'heafield', '2011', 'for', 'lm', 'scoring', 'during', 'decoding'], ['the', 'most', 'famous', 'example', 'would', 'probably', 'be', 'the', 'europarl', 'corpus', 'koehn', '2005', 'the', 'system', 'was', 'trained', 'on', 'the', 'english', 'and', 'danish', 'part', 'of', 'the', 'europarl', 'corpus', 'version', '3', 'koehn', '2005'], ['we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006', 'we', 'convert', 'the', 'trees', 'in', 'both', 'treebanks', 'from', 'constituencies', 'to', 'labeled', 'dependencies', 'see', 'figure', '1', 'using', 'the', 'stanford', 'converter', 'which', 'produces', '46', 'types', 'of', 'labeled', 'dependencies', '1', 'de', 'marneffe', 'et', 'al', '2006'], ['additionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'here', 'we', 'review', 'the', 'parameters', 'of', 'the', 'standard', 'phrasebased', 'translation', 'model', 'koehn', 'et', 'al', '2007'], ['the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'moses', 'decoder', 'koehn', 'et', 'al', '2007', 'was', 'used', 'for', 'generating', 'lattices', 'and', 'nbest', 'lists'], ['the', 'morpho', 'syntactically', 'annotated', 'corpus', 'we', 'used', 'is', 'a', 'variant', 'of', 'the', 'french', 'treebank', 'or', 'ftb', 'abeille', 'et', 'al', '2003', 'this', 'corresponds', 'to', 'a', 'new', 'version', 'of', 'the', 'french', 'treebank', 'abeille', 'et', 'al', '2003'], ['there', 'has', 'been', 'a', 'large', 'amount', 'of', 'work', 'on', 'sentiment', 'analysis', 'at', 'various', 'levels', 'of', 'granularity', 'pang', 'and', 'lee', '2008', 'for', 'a', 'detailed', 'survey', 'of', 'the', 'field', 'of', 'sentiment', 'analysis', 'see', 'pang', 'and', 'lee', '2008'], ['this', 'task', 'setup', 'is', 'further', 'described', 'in', 'the', 'task', 'description', 'paper', 'rosenthal', 'et', 'al', '2014', 'a', 'complete', 'description', 'of', 'the', 'training', 'and', 'test', 'datasets', 'can', 'be', 'found', 'at', 'the', 'task', 'description', 'paper', 'rosenthal', 'et', 'al', '2014'], ['the', 'baseline', 'will', 'be', 'created', 'by', 'the', 'moses', 'smt', 'toolkit', 'koehn', 'et', 'al', '2007', '18', 'our', 'baseline', 'is', 'the', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'run', 'over', 'letter', 'strings', 'rather', 'than', 'word', 'strings'], ['discourse', 'structure', 'in', 'summarization', 'rhetorical', 'structure', 'theory', 'rst', 'mann', 'and', 'thompson', '1988', 'represents', 'the', 'discourse', 'in', 'a', 'document', 'in', 'the', 'form', 'of', 'a', 'tree', 'figure', '1', 'causal', 'relations', 'are', 'among', 'discourse', 'relations', 'defined', 'by', 'rhetorical', 'structure', 'theory', 'mann', 'and', 'thompson', '1988'], ['it', 'builds', 'on', 'the', 'cc', 'ccg', 'parser', 'clark', 'and', 'curran', '2004', 'it', 'receives', 'ccg', 'derivations', 'from', 'the', 'cc', 'parser', 'clark', 'and', 'curran', '2004', '', 'and', 'maps', 'lexical', 'categories', 'and', 'combinatory', 'rules', 'into', 'semantic', 'descriptions', 'expressed', 'by', 'lambda', 'calculus'], ['we', 'use', 'boxer', 'bos', 'et', 'al', '2004', 'to', 'parse', 'natural', 'language', 'into', 'a', 'logical', 'formin', 'transforming', 'natural', 'language', 'text', 'to', 'logical', 'form', 'we', 'build', 'on', 'the', 'software', 'package', 'boxer', 'bos', 'et', 'al', '2004'], ['the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'use', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'implement', 'our', 'models'], ['we', 'perform', 'bootstrap', 'resampling', 'with', 'bounds', 'estimation', 'as', 'described', 'in', 'koehn', '2004', 'we', 'also', 'report', '95', 'confidence', 'intervals', 'ci', 'measured', 'on', '1000', 'iterations', 'of', 'bootstrap', 'resampling', 'with', 'replacement', 'koehn', '2004'], ['the', 'major', 'part', 'of', 'data', 'comes', 'from', 'current', 'and', 'upcoming', 'full', 'releases', 'of', 'the', 'europarl', 'data', 'set', 'koehn', '2005', 'the', 'europarl', 'corpus', 'koehn', '2005', 'is', 'built', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['for', 'italian', 'we', 'use', 'the', 'word2vec', 'to', 'train', 'word', 'embeddings', 'on', 'the', 'europarl', 'italian', 'corpus', 'koehn', '2005', '2', '7', 'httpopennlpapacheorg', '523', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005', 'as', 'testing', 'data'], ['the', 'gate', 'pluginbased', 'architecture', 'cunningham', 'et', 'al', '2002', 'is', 'the', 'basis', 'for', 'the', 'platform', 'environment', 'gate', 'cunningham', 'et', 'al', '2002a', 'is', 'an', 'architecture', '', 'a', 'framework', 'and', 'a', 'development', 'environment', 'for', 'human', 'language', 'technology', 'modules', 'and', 'applications'], ['we', 'trained', 'a', '5gram', 'language', 'model', 'on', 'the', 'xinhua', 'section', 'of', 'the', 'english', 'gigaword', 'corpus', '306', 'million', 'words', 'using', 'the', 'srilm', 'toolkit', 'stolcke', '2002', 'with', 'the', 'modified', 'kneserney', 'smoothing', 'chen', 'and', 'goodman', '1996', 'both', 'language', 'models', 'use', 'modified', 'kneserney', 'smoothing', 'chen', 'and', 'goodman', '1996'], ['we', 'used', 'giza', 'och', 'and', 'ney', '2003', 'along', 'with', 'the', 'growing', 'heuristics', 'to', 'wordalign', 'the', 'training', 'cor', 'puswe', 'automatically', 'wordaligned', 'the', 'german', 'part', 'to', 'each', 'of', 'the', 'others', 'with', 'the', 'giza', 'tool', 'och', 'and', 'ney', '2003'], ['our', 'previous', 'mlnbased', 'approach', 'for', 'joint', 'disambiguation', 'and', 'clustering', 'of', 'concepts', 'fahrni', 'and', 'strube', '2012', 'joint', 'disambiguation', 'and', 'clustering', 'of', 'mentions', 'improves', 'the', 'disambiguation', 'results', 'for', 'both', 'the', 'scopeignorant', 'fahrni', 'and', 'strube', '2012', 'and', 'the', 'scopeaware', 'approach'], ['pang', 'et', 'al', '2002', 'compare', 'the', 'performance', 'of', 'three', 'commonly', 'used', 'machine', 'learning', 'models', 'naive', 'bayes', 'maximum', 'entropy', 'and', 'svmpang', 'et', 'al', '2002', '', 'use', 'machine', 'learning', 'methods', 'nb', 'svm', 'and', 'maxent', 'to', 'detect', 'sentiments', 'on', 'movie', 'reviews'], ['jans', 'et', 'al', '2012', 'focused', 'solely', 'on', 'the', 'narrative', 'cloze', 'test', 'as', 'an', 'end', 'goalwe', 'refer', 'to', 'the', 'system', 'of', 'jans', 'et', 'al', '2012', 'as', 'the', 'single', 'protagonist', 'system'], ['we', 'applied', 'the', 'naive', 'bayes', 'probabilistic', 'supervised', 'learning', 'algorithm', 'from', 'the', 'weka', 'machine', 'learning', 'library', 'hall', 'et', 'al', '2009', 'naive', 'bayes', 'and', 'decision', 'tree', 'models', 'were', 'built', 'with', 'the', 'weka', 'hall', 'et', 'al', '2009', 'package', 'for', 'decision', 'trees', 'we', 'used', 'the', 'j48', 'implementation'], ['we', 'therefore', 'seek', 'to', 'allow', 'quick', 'incremental', 'updates', 'of', 'the', 'rm', 'within', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007', 'we', 'compare', 'the', 'model', 'against', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', '', 'applied', 'to', 'phoneme', 'sequences'], ['it', 'has', 'a', 'much', 'longer', 'average', 'sentence', 'length', 'than', 'penn', 'chinese', 'treebank', 'pctb', '33', 'compare', 'to', '28', 'xue', 'et', 'al', '2005', 'we', 'then', 'describe', 'in', 'more', 'detail', 'a', 'modern', 'chinese', 'corpus', 'the', 'penn', 'chinese', 'treebank', 'xue', 'et', 'al', '2005'], ['the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'first', 'two', 'baselines', 'are', 'standard', 'systems', 'using', 'pbmt', 'or', 'hiero', 'trained', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'first', 'two', 'baselines', 'are', 'standard', 'systems', 'using', 'pbmt', 'or', 'hiero', 'trained', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'dropout', 'rate', 'was', 'set', 'to', '05', 'and', 'the', 'model', 'was', 'trained', 'via', 'adagrad', 'duchi', 'et', 'al', '2011', 'we', 'use', 'the', 'crossentropy', 'loss', 'function', 'and', 'minibatch', 'adagrad', 'duchi', 'et', 'al', '2011', 'to', 'optimize', 'parameters'], ['the', 'dropout', 'rate', 'was', 'set', 'to', '05', 'and', 'the', 'model', 'was', 'trained', 'via', 'adagrad', 'duchi', 'et', 'al', '2011', 'we', 'use', 'adagrad', 'duchi', 'et', 'al', '2011', '', 'with', 'the', 'initial', 'learning', 'rate', 'set', 'to', '', '', '05'], ['we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'compare', 'the', 'model', 'against', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', '', 'applied', 'to', 'phoneme', 'sequences'], ['they', 'had', 'shown', 'that', 'the', 'penn', 'discourse', 'treebank', 'pdtb', 'style', 'prasad', 'et', 'al', '2008', 'discourse', 'relations', 'are', 'usefulthe', 'penn', 'discourse', 'treebank', 'pdtb', 'prasad', 'et', 'al', '2008', 'includes', 'several', 'relation', 'types', 'that', 'are', 'relevant', 'to', 'causation', 'primarily', 'cause', 'and', 'reason'], ['they', 'had', 'shown', 'that', 'the', 'penn', 'discourse', 'treebank', 'pdtb', 'style', 'prasad', 'et', 'al', '2008', 'discourse', 'relations', 'are', 'usefulpitler', 'and', 'nenkova', '2008', 'used', 'discourse', 'relations', 'of', 'the', 'penn', 'discourse', 'treebank', 'prasad', 'et', 'al', '2008', 'as', 'a', 'feature'], ['mcdonald', 'et', 'al', '2005', 'present', 'a', 'technique', 'for', 'training', 'discriminative', 'models', 'for', 'dependency', 'parsing', 'the', 'details', 'of', 'parsing', 'model', 'were', 'presented', 'in', 'mcdonald', 'et', 'al', '2005', 'and', 'mcdonald', 'and', 'pereira', '2006'], ['we', 'transformed', 'the', 'parse', 'trees', 'in', 'ontonotes', 'into', 'syntactic', 'dependencies', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['2', 'translation', 'model', 'moses', 'is', 'a', 'stateoftheart', 'toolkit', 'for', 'phrasebased', 'smt', 'systems', 'koehn', 'et', 'al', '2007', 'the', 'decoder', 'is', 'built', 'on', 'top', 'of', 'an', 'opensource', 'phrasebased', 'smt', 'decoder', 'moses', 'koehn', 'et', 'al', '2007'], ['for', 'the', 'ontonotes', 'data', 'sets', 'same', 'speaker', 'lee', 'et', 'al', '2013', 'is', 'the', 'only', 'feature', 'for', 'resolving', 'pronounsthe', 'mention', 'detection', 'of', 'the', 'stanford', 'coreference', 'system', 'lee', 'et', 'al', '2013', 'is', 'used', 'for', 'the', 'ontonotes', 'data', 'sets'], ['it', 'was', 'one', 'of', 'the', 'best', 'parsers', 'in', 'the', 'conll', 'shared', 'task', '2009', 'haji', 'et', 'al', '2009', 'ctb6', 'is', 'used', 'as', 'the', 'chinese', 'data', 'set', 'in', 'the', 'conll', '2009', 'shared', 'task', 'haji', 'et', 'al', '2009'], ['we', 'transformed', 'the', 'parse', 'trees', 'in', 'ontonotes', 'into', 'syntactic', 'dependencies', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['results', 'are', 'reported', 'on', 'the', 'test', 'data', 'using', 'f1', 'computed', 'with', 'the', 'conll', 'scorer', 'dahlmeier', 'and', 'ng', '2012', 'we', 'report', 'f1', 'performance', 'scored', 'using', 'the', 'official', 'scorer', 'from', 'the', 'shared', 'task', 'dahlmeier', 'and', 'ng', '2012'], ['the', 'perplexity', 'achieved', 'by', 'the', '6gram', 'nn', 'lm', 'in', 'the', 'spanish', 'news2009', 'set', 'was', '281', 'versus', '145', 'obtained', 'with', 'the', 'standard', '6gram', 'language', 'model', 'with', 'interpolation', 'and', 'kneserney', 'smoothing', 'kneser', 'and', 'nethe', 'results', 'of', 'this', 'experiment', 'appear', 'in', 'table', '2', 'which', 'further', 'compares', 'these', 'models', 'with', 'the', 'following', 'baselines', '1', 'vanila', 'rnnlstm', 'and', '2', 'a', '6gram', 'lm', 'with', 'kneserney', 'smoothing', '3', 'kneser', 'and', 'ney'], ['we', 'address', 'the', 'qe', 'problem', 'as', 'a', 'regression', 'task', 'by', 'building', 'svm', 'models', 'with', 'an', 'epsilon', 'regressor', 'and', 'a', 'radial', 'basis', 'function', 'kernel', 'using', 'the', 'libsvm', 'toolkit', 'chang', 'and', 'lin', '2011', 'we', 'use', 'libsvm', 'chang', 'and', 'lin', '2011', 'as', 'the', 'svm', 'tool'], ['finally', 'it', 'is', 'also', 'noticeable', 'that', 'the', 'percentage', 'of', 'compounds', 'detected', 'in', 'the', 'training', 'set', 'is', 'similar', 'to', 'the', 'one', 'reported', 'by', 'baroni', 'et', 'al', '2002', 'and', 'referenced', 'to', 'in', 'section', '2baroni', 'et', 'al', '2002', 'also', 'pointed', 'out', 'that', 'the', 'small', 'percentage', 'of', 'compounds', 'detected', 'at', 'token', 'level', '7', 'suggested', 'that', 'many', 'of', 'them', 'are', 'productively', 'formed', 'hapax', 'legomena', 'or', 'very', 'rare', 'words'], ['facts', 'such', 'as', 'these', 'are', 'difficult', 'to', 'account', 'for', 'in', 'a', 'purely', 'semantic', 'theory', 'of', 'ellipsis', 'resolution', 'such', 'as', 'the', 'one', 'proposed', 'in', 'dalrymple', 'et', 'al', '1991', 'the', 'most', 'influential', 'of', 'the', 'semantic', 'approaches', 'to', 'ellpsis', 'has', 'been', 'the', 'higherorder', 'unification', 'approach', 'proposed', 'in', 'dalrymple', 'et', 'al', '1991'], ['all', 'these', 'reordering', 'models', 'are', 'tested', 'using', 'moses', 'koehn', 'et', 'al', '2007', '', 'except', 'that', 'the', 'neural', 'model', 'needs', 'an', 'additional', 'hypergraph', 'reranking', 'procedure', 'section', '33we', 'refer', 'to', 'that', 'model', 'as', 'moses', 'enes100k', '', 'because', 'it', 'was', 'trained', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['bullet', 'raesubj', 'socher', 'et', 'al', '2011', 'proposed', 'to', 'use', 'recursive', 'autoencoders', 'for', 'sentencelevel', 'predication', 'of', 'sentiment', 'label', 'distributionswe', 'follow', 'the', 'formulation', 'of', 'vector', 'composition', 'proposed', 'by', 'socher', 'et', 'al', '2011', '', 'except', 'that', 'we', 'do', 'not', 'stack', 'autoencoders', 'for', 'recursion'], ['we', 'train', 'a', 'ridge', 'regression', 'model', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', '', 'for', 'each', 'assignment', 'and', 'test', 'using', 'annotations', 'from', 'the', 'rest', 'as', 'training', 'examples', 'our', 'system', 'is', 'a', 'linear', 'model', 'estimated', 'using', 'ridge', 'regression', 'as', 'implemented', 'in', 'the', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011'], ['we', 'follow', 'the', 'protocols', 'in', 'collobert', 'et', 'al', '2011', '', 'using', 'the', 'concatenation', 'of', 'neighboring', 'embeddings', 'as', 'input', 'to', 'a', 'multilayer', 'neural', 'modelwe', 'use', 'srl', 'collobert', 'et', 'al', '2011', 'to', 'determine', 'the', 'agent', 'and', 'patient', 'arguments', 'of', 'an', 'event', 'mention'], ['this', 'is', 'equivalent', 'to', 'an', 'svm', 'with', 'the', 'compound', 'cluster', 'features', 'as', 'in', 'koo', 'et', 'al', '2008', 'the', 'sparsity', 'of', 'lexical', 'features', 'can', 'also', 'be', 'tackled', 'by', 'the', 'use', 'of', 'distributional', 'word', 'clusters', 'as', 'pioneered', 'by', 'koo', 'et', 'al', '2008'], ['all', 'experiments', 'are', 'conducted', 'using', 'the', 'moses', 'phrasebased', 'smt', 'system', 'koehn', 'et', 'al', '2007', 'with', 'a', 'maximum', 'phrase', 'length', 'of', '8for', 'the', 'comparisons', 'of', 'translation', 'quality', 'the', 'models', 'are', 'trained', 'up', 'using', 'a', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'that', 'used', 'the', 'above', 'listed', 'models', 'to', 'align', 'the', 'data'], ['we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'create', 'a', 'statistical', 'phrasebased', 'machine', 'translation', 'model', 'built', 'on', 'the', 'best', 'preprocessed', 'data', 'as', 'described', 'abovewe', 'refer', 'to', 'that', 'model', 'as', 'moses', 'enes100k', '', 'because', 'it', 'was', 'trained', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['all', 'experiments', 'were', 'on', 'english', 'part', 'of', 'speech', 'pos', 'tagging', 'on', 'the', 'postagged', 'wall', 'street', 'journal', 'text', 'in', 'the', 'penn', 'treebank', 'ptb', 'version', '3', 'marcus', 'et', 'al', '1994', 'these', 'techniques', 'were', 'evaluated', 'in', 'experiments', 'on', 'the', 'penn', 'treebank', 'marcus', 'et', 'al', '1994', 'with', 'the', 'widecoverage', 'hpsg', 'parser', 'developed', 'by'], ['we', 'use', 'the', 'word', 'alignments', 'to', 'construct', 'a', 'phrase', 'table', 'by', 'applying', 'the', 'consistent', 'phrase', 'pair', 'heuristic', 'och', 'and', 'ney', '2004', 'to', 'all', '5gramswe', 'use', 'the', 'intersection', 'of', 'direct', 'and', 'reverse', 'giza', 'och', 'and', 'ney', '2004', 'alignments', 'as', 'a', 'heuristic', 'rule', 'to', 'find', 'words', 'reliably', 'aligned', 'to', 'each', 'other'], ['we', 'estimated', 'a', 'hierarchical', 'mt', 'model', 'for', 'the', 'train', 'partition', 'with', 'the', 'standard', 'configuration', 'of', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'also', 'compare', 'with', 'the', 'standard', 'phrasebased', 'system', 'of', 'moses', 'koehn', 'et', 'al', '2007', '', 'with', 'standard', 'settings', 'except', 'for', 'the', 'ttable', 'limit', 'which', 'we', 'set', 'to', '100'], ['a', 'small', 'amount', 'of', 'labeled', 'data', 'is', 'used', 'to', 'map', 'the', 'induced', 'topics', 'to', 'realworld', 'senses', 'for', 'a', 'description', 'of', 'the', 'method', 'see', 'agirre', 'and', 'soroa', '2007', 'thus', 'inducing', 'a', 'number', 'of', 'clusters', 'similar', 'to', 'the', 'number', 'of', 'senses', 'is', 'not', 'a', 'requirement', 'for', 'good', 'results', 'agirre', 'and', 'soroa', '2007a'], ['we', 'use', 'the', 'word', 'alignments', 'to', 'construct', 'a', 'phrase', 'table', 'by', 'applying', 'the', 'consistent', 'phrase', 'pair', 'heuristic', 'och', 'and', 'ney', '2004', 'to', 'all', '5gramswe', 'use', 'the', 'intersection', 'of', 'direct', 'and', 'reverse', 'giza', 'och', 'and', 'ney', '2004', 'alignments', 'as', 'a', 'heuristic', 'rule', 'to', 'find', 'words', 'reliably', 'aligned', 'to', 'each', 'other'], ['1', 'for', 'a', 'reference', 'standard', 'text', 'we', 'used', 'the', 'written', 'portion', 'of', 'the', 'british', 'national', 'corpus', 'bnc', 'burnard', '2000', 'the', 'evaluation', 'corpus', 'is', 'a', 'subset', 'of', 'an', 'ungrammatical', 'version', 'of', 'the', 'british', 'national', 'corpus', 'bnc', 'a', '100', 'million', 'word', 'balanced', 'corpus', 'of', 'british', 'english', 'burnard', '2000'], ['we', 'worked', 'with', 'the', 'europarl', 'corpus', 'koehn', '2005', 'in', 'order', 'to', 'have', 'a', 'parallel', 'comparative', 'corpus', 'for', 'italian', 'and', 'spanishtranslations', 'for', 'english', 'words', 'in', 'the', 'lexical', 'sample', 'are', 'extracted', 'from', 'a', 'semiautomatic', 'word', 'alignment', 'of', 'sentences', 'from', 'the', 'europarl', 'parallel', 'corpus', 'koehn', '2005'], ['the', 'organization', 'from', 'semeval2013', 'task', '2', 'sentiment', 'analysis', 'in', 'twitter', 'provided', 'three', 'datasets', 'for', 'the', 'task', 'wilson', 'et', 'al', '2013', 'we', 'participated', 'in', 'both', 'subtask', 'a', 'and', 'b', 'of', 'semeval2013', 'task', '2', 'sentiment', 'analysis', 'in', 'twitter', 'wilson', 'et', 'al', '2013', 'with', 'an', 'adaptation', 'of', 'our', 'existing', 'system'], ['pending', 'a', 'planned', 'full', 'evaluation', 'using', 'the', 'moses', 'system', 'koehn', 'et', 'al', '2007', 'as', 'a', 'benchmark', 'we', 'tested', 'the', 'mt', 'method', 'outlined', 'above', 'on', 'two', 'simple', 'tasksto', 'test', 'our', 'method', 'we', 'conducted', 'two', 'lowresource', 'translation', 'experiments', 'using', 'the', 'phrasebased', 'mt', 'system', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'entity', 'transition', 'features', 'are', 'then', 'used', 'to', 'train', 'a', 'support', 'vector', 'machine', 'ranker', 'joachims', '2002', 'to', 'rank', 'the', 'source', 'documents', 'higher', 'than', 'the', 'permutationswe', 'use', 'the', 'support', 'vector', 'machine', 'svm', 'rank', 'algorithm', 'joachims', '2002', 'to', 'predict', 'a', 'rank', 'order', 'for', 'each', 'list', 'of', 'comments'], ['our', 'second', 'method', 'is', 'based', 'on', 'the', 'recurrent', 'neural', 'network', 'language', 'model', 'rnnlm', 'approach', 'to', 'learning', 'word', 'embeddings', 'of', 'mikolov', 'et', 'al', '2013a', 'and', 'mikolov', 'et', 'al', '2013b', '', 'using', 'the', 'word2vec', 'packagein', 'all', 'of', 'the', 'above', 'tasks', 'we', 'compare', 'the', 'neural', 'word', 'embeddings', 'of', 'mikolov', 'et', 'al', '2013a', 'with', 'two', 'vector', 'spaces', 'both', 'based', 'on', 'cooccurrence', 'counts', 'and', 'produced', 'by', 'standard', 'distributional', 'techniques'], ['we', 'apply', 'the', 'hidden', 'markov', 'model', 'hmm', 'viterbi', '1967', 'and', 'the', 'forwardbackward', 'algorithm', 'james', '1995', 'to', 'obtain', 'the', 'contextdependent', 'probabilitieswe', 'use', 'viterbi', 'algorithm', 'viterbi', '1967', 'to', 'decode', 'the', 'character', 'sentence'], ['we', 'report', 'bleu', 'papineni', 'et', 'al', '2001', 'of', 'translation', 'system', 'output', 'measured', 'against', 'the', 'original', 'english', 'querieswe', 'measure', 'translation', 'quality', 'via', 'the', 'bleu', 'score', 'papineni', 'et', 'al', '2001'], ['it', 'is', 'a', 'standard', 'phrasebased', 'machine', 'translation', 'model', 'koehn', 'et', 'al', '2007', 'all', 'our', 'systems', 'are', 'contrasted', 'with', 'a', 'standard', 'phrasebased', 'system', 'built', 'with', 'moses', 'koehn', 'et', 'al', '2007'], ['finally', 'we', 'used', 'moses', 'toolkit', 'as', 'phrasebased', 'reference', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['concept', 'similarity', 'is', 'computed', 'using', 'the', 'edgebased', 'path', 'similarity', 'pedersen', 'et', 'al', '2004', 'the', 'degree', 'of', 'similarity', 'between', 'two', 'similar', 'words', 'is', 'identified', 'using', 'wordnet', 'pedersen', 'et', 'al', '2004'], ['finally', 'we', 'used', 'moses', 'toolkit', 'as', 'phrasebased', 'reference', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['it', 'is', 'a', 'standard', 'phrasebased', 'machine', 'translation', 'model', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['2', 'translation', 'model', 'moses', 'is', 'a', 'stateoftheart', 'toolkit', 'for', 'phrasebased', 'smt', 'systems', 'koehn', 'et', 'al', '2007', 'finally', 'we', 'used', 'moses', 'toolkit', 'as', 'phrasebased', 'reference', 'koehn', 'et', 'al', '2007'], ['otherwise', 'it', 'is', 'measured', 'by', 'wordnet', 'similarity', 'package', 'pedersen', 'et', 'al', '2004', 'the', 'degree', 'of', 'similarity', 'between', 'two', 'similar', 'words', 'is', 'identified', 'using', 'wordnet', 'pedersen', 'et', 'al', '2004'], ['we', 'use', '5gram', 'language', 'models', 'with', 'kneserney', 'discounting', 'heafield', 'et', 'al', '2013', 'the', 'language', 'models', 'are', 'estimated', 'using', 'the', 'kenlm', 'toolkit', 'heafield', 'et', 'al', '2013', 'with', 'modified', 'kneserney', 'smoothing'], ['we', 'adopt', 'online', 'learning', 'updating', 'parameters', 'using', 'adagrad', 'duchi', 'et', 'al', '2011', 'we', 'use', 'the', 'crossentropy', 'loss', 'function', 'and', 'minibatch', 'adagrad', 'duchi', 'et', 'al', '2011', 'to', 'optimize', 'parameters'], ['we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'svm', 'implementations', 'from', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'and', 'experimented', 'with', 'a', 'number', 'of', 'classifiers'], ['we', 'adopt', 'online', 'learning', 'updating', 'parameters', 'using', 'adagrad', 'duchi', 'et', 'al', '2011', 'we', 'train', 'the', 'concept', 'identification', 'stage', 'using', 'infinite', 'ramp', 'loss', '1', 'with', 'adagrad', 'duchi', 'et', 'al', '2011'], ['finally', 'we', 'used', 'moses', 'toolkit', 'as', 'phrasebased', 'reference', 'koehn', 'et', 'al', '2007', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'train', 'standard', 'phrasebased', 'systems', 'with', 'default', 'configurations'], ['europarl', 'koehn', '2005', 'is', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'leave', 'the', 'thirdorder', 'models', 'koo', 'and', 'collins', '2010', 'for', 'a', 'future', 'study', 'koo10', 'stands', 'for', 'the', 'model', '1', 'in', 'koo', 'and', 'collins', '2010', 'which', 'is', 'a', 'thirdorder', 'model'], ['we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011', 'specifically', 'we', 'used', 'the', 'standard', 'gradient', 'boosting', 'regressor', 'in', 'the', 'scikitlearn', 'toolkit', '4', 'pedregosa', 'et', 'al', '2011'], ['for', 'english', 'and', 'french', 'a', 'model', 'was', 'trained', 'using', 'the', 'europarl', 'corpus', 'koehn', '2005', 'for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['empirically', 'we', 'show', 'that', 'our', 'model', 'beats', 'the', 'stateoftheart', 'systems', 'of', 'rush', 'et', 'al', '2015', '', 'on', 'multiple', 'data', 'setsvery', 'recently', 'rush', 'et', 'al', '2015', 'proposed', 'a', 'neural', 'attention', 'model', 'for', 'this', 'problem', 'using', 'a', 'new', 'data', 'set', 'for', 'training', 'and', 'showing', 'stateoftheart', 'performance', 'on', 'the', 'duc', 'tasks'], ['we', 'use', 'logistic', 'regression', 'with', 'l2', 'regularization', 'implemented', 'using', 'the', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011', 'we', 'use', 'the', 'scikit', 'implementation', 'of', 'random', 'forest', 'pedregosa', 'et', 'al', '2011'], ['we', 'use', 'the', 'new', 'york', 'times', 'annotated', 'corpus', 'sandhaus', '2008', 'for', 'all', 'our', 'experimentsof', 'the', 'remaining', 'twelve', 'eight', 'came', 'from', 'a', 'large', 'set', 'of', 'possible', 'stimuli', 'we', 'collected', 'from', 'the', 'new', 'york', 'times', 'annotated', 'corpus', 'nytac', 'sandhaus', '2008', 'for', 'use', 'in', 'later', 'phases', 'of', 'the', 'experiment', '', 'wh'], ['we', 'use', 'the', 'nmf', 'and', 'tfidf', 'implementations', 'provided', 'by', 'scikitlearn', 'version', '014', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settingswe', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'we', 'also', 'lemmatized', 'all', 'words', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settingsthe', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settingsfor', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settingsbullet', 'decoder', 'moses', 'koehn', 'et', 'al', '2007', 'with', 'default', 'options', '', 'except', 'for', 'the', 'distortion', 'limit', '620'], ['feature', 'selection', 'was', 'performed', 'using', 'chisquare', 'in', 'weka', 'hall', 'et', 'al', '2009', 'we', 'train', 'and', 'evaluate', 'the', 'classifiers', 'in', 'a', '10fold', 'crossvalidation', 'using', 'weka', 'hall', 'et', 'al', '2009'], ['we', 'use', 'the', 'support', 'vector', 'machines', 'implementation', 'in', 'the', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011', 'to', 'perform', 'regression', 'svr', 'on', 'each', 'feature', 'set', 'with', 'either', 'rbf', 'kernels', 'and', 'parameters', 'optimise'], ['recently', 'there', 'has', 'been', 'a', 'successful', 'attempt', 'to', 'harmonize', 'the', 'linguistic', 'principles', 'behind', 'the', 'coding', 'systems', 'msd', 'and', 'kr', 'farkas', 'et', 'al', '2010', 'recently', 'there', 'has', 'been', 'a', 'successful', 'attempt', 'to', 'harmonize', 'the', 'coding', 'systems', 'msd', 'and', 'kr', 'farkas', 'et', 'al', '2010'], ['the', 'first', 'one', 'is', 'the', 'ws353', 'dataset', 'finkelstein', 'et', 'al', '2001', 'containing', '353', 'pairs', 'of', 'english', 'words', 'that', 'have', 'been', 'assigned', 'similarity', 'ratings', 'by', 'humansthe', 'first', 'one', 'is', 'the', 'ws353', 'dataset', 'finkelstein', 'et', 'al', '2001', '', 'which', 'contains', '353', 'pairs', 'of', 'english', 'words', 'that', 'have', 'been', 'assigned', 'similarity', 'ratings', 'by', 'humans'], ['abstract', 'meaning', 'representation', 'amr', 'banarescu', 'et', 'al', '2013', 'is', 'a', 'semantic', 'formalism', 'where', 'the', 'meaning', 'of', 'a', 'sentence', 'is', 'encoded', 'as', 'a', 'rooted', 'directed', 'graphabstract', 'meaning', 'representation', 'amr', 'banarescu', 'et', 'al', '2013', 'is', 'a', 'semantic', 'formalism', 'encoding', 'the', 'meaning', 'of', 'a', 'sentence', 'as', 'a', 'rooted', 'directed', 'graph'], ['we', 'perform', 'bootstrap', 'resampling', 'with', 'bounds', 'estimation', 'as', 'described', 'in', 'koehn', '2004', 'we', 'perform', 'bootstrap', 'resampling', 'with', 'bounds', 'estimation', 'as', 'described', 'by', 'koehn', '2004'], ['it', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'hpsg', 'english', 'resource', '', 'grammar', 'erg', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfgit', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'the', 'hpsg', 'english', 'resource', 'grammar', '', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfg'], ['we', 'use', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parse', 'trees', 'and', 'dependency', 'relationswe', 'first', 'use', 'a', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'parse', 'each', 'sentence', 'and', 'extract', 'the', 'set', 'of', 'dependency', 'relations', 'associated', 'with', 'the', 'sentence'], ['rooth', 'et', 'al', '1999', 'propose', 'an', 'expectationmaximization', 'em', 'clustering', 'algorithm', 'for', 'selectional', 'preference', 'acquisition', 'based', 'on', 'a', 'probabilistic', 'latent', 'variable', 'modelalternatively', 'rooth', 'et', 'al', '1999', '', 'propose', 'an', 'embased', 'clustering', 'smooth', 'for', 'sp'], ['the', 'levenshtein', 'distance', 'levenshtein', '1966', 'between', 'two', 'strings', 'is', 'defined', 'as', 'the', 'minimum', 'number', 'of', 'editing', 'operations', 'substitutions', 'deletions', 'and', 'insertionsthe', 'levenshtein', 'distance', 'gives', 'an', 'indication', 'of', 'the', 'similarity', 'between', 'two', 'strings', 'levenshtein', '1966'], ['we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'create', 'a', 'statistical', 'phrasebased', 'machine', 'translation', 'model', 'built', 'on', 'the', 'best', 'preprocessed', 'data', 'as', 'described', 'abovewe', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'perform', 'bootstrap', 'resampling', 'with', 'bounds', 'estimation', 'as', 'described', 'in', 'koehn', '2004', 'we', 'perform', 'bootstrap', 'resampling', 'with', 'bounds', 'estimation', 'as', 'described', 'by', 'koehn', '2004'], ['it', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'hpsg', 'english', 'resource', '', 'grammar', 'erg', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfgit', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'the', 'hpsg', 'english', 'resource', 'grammar', '', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfg'], ['it', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'the', 'english', 'hpsg', 'grammar', 'erg', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfgit', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'the', 'hpsg', 'english', 'resource', 'grammar', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfg'], ['it', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'hpsg', 'english', 'grammar', 'erg', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfgit', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'the', 'hpsg', 'english', 'resource', 'grammar', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfg'], ['it', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'the', 'english', 'hpsg', 'grammar', 'erg', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfgit', 'is', 'used', 'to', 'support', 'semantic', 'analyses', 'in', 'the', 'hpsg', 'english', 'resource', 'grammar', '', 'copestake', 'and', 'flickinger', '2000', '', 'but', 'also', 'in', 'other', 'grammar', 'formalisms', 'like', 'lfg'], ['we', 'build', 'upon', 'our', 'previous', 'markov', 'logic', 'based', 'approach', 'for', 'joint', 'concept', 'disambiguation', 'and', 'clustering', 'fahrni', 'and', 'strube', '2012', 'we', 'build', 'upon', 'our', 'previous', 'approach', 'for', 'joint', 'concept', 'disambiguation', 'and', 'clustering', 'fahrni', 'and', 'strube', '2012'], ['details', 'about', 'svm', 'and', 'kfd', 'can', 'be', 'found', 'in', 'taylor', 'and', 'cristianini', '2004', 'details', 'about', 'svm', 'and', 'krr', 'can', 'be', 'found', 'in', 'taylor', 'and', 'cristianini', '2004'], ['we', 'learn', 'the', 'parameters', 'using', 'a', 'quasinewton', 'procedure', 'with', 'l', '1', 'lasso', 'regularization', 'andrew', 'and', 'gao', '2007', 'we', 'learn', 'the', 'parameters', '', 'using', 'a', 'quasinewton', 'qn', 'procedure', 'with', 'l', '1', 'lasso', 'regularization', 'andrew', 'and', 'gao', '2007'], ['we', 'use', 'the', 'scfg', 'decoder', 'cdec', 'dyer', 'et', 'al', '2010', '4', 'and', 'build', 'grammars', 'using', 'its', 'implementation', 'of', 'the', 'suffix', 'array', 'extraction', 'method', 'described', 'in', 'lopez', '2007for', 'direct', 'translation', 'we', 'use', 'the', 'scfg', 'decoder', 'cdec', 'dyer', 'et', 'al', '2010', '4', 'and', 'build', 'grammars', 'using', 'its', 'implementation', 'of', 'the', 'suffix', 'array', 'extraction', 'method', 'described', 'in', 'lopez', '2007'], ['this', 'is', 'known', 'as', 'the', 'distributional', 'hypothesis', 'harris', '1968', 'this', 'is', 'known', 'as', 'the', 'distributional', 'hypothesis', 'in', 'linguistics', 'harris', '1968'], ['all', 'our', 'models', '', 'as', 'well', 'as', 'the', 'parser', 'described', 'in', 'henderson', '2003', '', 'are', 'run', 'only', 'oncethe', 'models', '', 'as', 'well', 'as', 'the', 'parser', 'described', 'in', 'henderson', '2003', '', 'are', 'run', 'only', 'once'], ['for', 'strings', 'many', 'such', 'kernel', 'functions', 'exist', 'with', 'various', 'applications', 'in', 'computational', 'biology', 'and', 'computational', 'linguistics', 'taylor', 'and', 'cristianini', '2004', 'for', 'strings', 'a', 'lot', 'of', 'such', 'kernel', 'functions', 'exist', 'with', 'many', 'applications', 'in', 'computational', 'biology', 'and', 'computational', 'linguistics', 'taylor', 'and', 'cristianini', '2004'], ['the', 'estimation', 'of', 'the', 'semantically', 'smoothed', 'partial', 'tree', 'kernel', 'sptk', 'is', 'made', 'available', 'by', 'an', 'extended', 'version', 'of', 'svmlighttk', 'software', '5', 'moschitti', '2006', 'the', 'estimation', 'of', 'the', 'semantically', 'smoothed', 'partial', 'tree', 'kernel', 'sptk', 'is', 'made', 'available', 'by', 'an', 'extended', 'version', 'of', 'svmlighttk', 'software', '7', 'moschitti', '2006'], ['we', 'train', 'with', 'the', 'adam', 'optimizer', 'kingma', 'and', 'ba', '2015', '', 'a', 'learning', 'rate', 'of', '00001', 'batch', 'size', 'of', '50', 'and', 'dropout', 'with', 'probability', '02', 'applied', 'to', 'the', 'hidden', 'layerwe', 'train', 'with', 'the', 'adam', 'optimizer', 'kingma', 'and', 'ba', '2015', '', 'a', 'learning', 'rate', 'of', '00001', 'batch', 'size', 'of', '50', 'and', 'dropout', 'with', 'probability', '02', 'applied', 'to', 'the', 'hidden', 'layer'], ['in', 'our', 'experimental', 'study', 'we', 'use', 'the', 'freely', 'available', 'implementations', 'in', 'weka', 'witten', 'and', 'frank', '2005', 'in', 'our', 'experimental', 'study', 'we', 'use', 'the', 'freely', 'available', 'implementation', 'of', 'svm', 'in', 'weka', 'witten', 'and', 'frank', '2005'], ['more', 'recently', 'carpineto', 'and', 'romano', '2010', 'showed', 'that', 'the', 'characteristics', 'of', 'the', 'outputs', 'returned', 'by', 'src', 'algorithms', 'suggest', 'the', 'adoption', 'of', 'a', 'meta', 'clustering', 'approachoptimsrc', 'carpineto', 'and', 'romano', '2010', 'showed', 'that', 'the', 'characteristics', 'of', 'the', 'outputs', 'returned', 'by', 'prc', 'algorithms', 'suggest', 'the', 'adoption', 'of', 'a', 'meta', 'clustering', 'approach'], ['they', 'are', 'based', 'on', 'the', 'distributional', 'hypothesis', 'harris', '1968', 'and', 'by', 'looking', 'at', 'a', 'set', 'of', 'event', 'expressions', 'whose', 'argument', 'fillers', 'have', 'a', 'similar', 'distribution', 'they', 'try', 'to', 'recognize', 'synonymous', 'eventthese', 'methods', 'rely', 'on', 'the', 'distributional', 'hypothesis', 'harris', '1968', '', 'and', 'by', 'looking', 'at', 'a', 'set', 'of', 'event', 'expressions', 'whose', 'argument', 'fillers', 'have', 'a', 'similar', 'distribution', 'try', 'to', 'recognize', 'synonymous', 'event'], ['word', 'alignment', 'is', 'performed', 'using', 'giza', 'och', 'and', 'ney', '2003', 'word', 'alignment', 'is', 'done', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['word', 'alignment', 'is', 'performed', 'using', 'giza', 'och', 'and', 'ney', '2003', 'word', 'alignment', 'is', 'done', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'used', 'mallet', 'software', 'mccallum', '2002', 'for', 'crf', 'experimentswe', 'used', 'mallet', 'software', 'mccallum', '2002', 'for', 'sscrf', 'experiments'], ['the', 'thesaurus', 'consists', 'of', 'a', 'hierarchy', 'of', '2710', 'semantic', 'classes', 'defined', 'for', 'over', '264312', 'nouns', 'with', 'a', 'maximum', 'depth', 'of', 'twelve', 'ikehara', 'et', 'al', '1997', 'the', 'ontology', 'goitaikei', 'consists', 'of', 'a', 'hierarchy', 'of', '2710', 'semantic', 'classes', 'defined', 'for', 'over', '264312', 'nouns', 'with', 'a', 'maximum', 'depth', 'of', '12', 'ikehara', 'et', 'al', '1997'], ['the', 'detailed', 'discussion', 'is', 'provided', 'in', 'the', 'longer', 'version', 'of', 'the', 'paper', 'kim', 'et', 'al', '2013', 'a', 'detailed', 'discussion', 'on', 'the', 'results', 'is', 'provided', 'in', 'the', 'longer', 'version', 'of', 'the', 'paper', 'kim', 'et', 'al', '2013'], ['the', 'first', 'one', 'is', 'the', 'ws', '353', '3', 'dataset', 'finkelstein', 'et', 'al', '2001', 'containing', '353', 'pairs', 'of', 'english', 'words', 'that', 'have', 'been', 'assigned', 'similarity', 'ratings', 'by', 'humansthe', 'first', 'one', 'is', 'the', 'ws353', 'dataset', 'finkelstein', 'et', 'al', '2001', '', 'which', 'contains', '353', 'pairs', 'of', 'english', 'words', 'that', 'have', 'been', 'assigned', 'similarity', 'ratings', 'by', 'humans'], ['a', 'framework', 'for', 'human', 'error', 'analysis', 'and', 'error', 'classification', 'has', 'been', 'proposed', 'in', 'vilar', 'et', 'al', '2006', '', 'but', 'like', 'human', 'evaluation', '', 'this', 'is', 'also', 'a', 'time', 'consuming', 'taska', 'framework', 'for', 'human', 'error', 'analysis', 'has', 'been', 'proposed', 'in', 'vilar', 'et', 'al', '2006', '', 'but', 'as', 'every', 'human', 'evaluation', 'this', 'is', 'also', 'a', 'time', 'consuming', 'task'], ['maxent', 'classifier', 'is', 'a', 'good', 'example', 'of', 'this', 'group', 'mani', 'et', 'al', '2006', 'maxent', 'classifier', 'is', 'an', 'example', 'of', 'this', 'group', 'mani', 'et', 'al', '2006'], ['among', 'these', 'media', 'blog', 'is', 'one', 'of', 'the', 'communicative', 'and', 'informative', 'repository', 'of', 'text', 'based', 'emotional', 'contents', 'in', 'the', 'web', '20', 'lin', 'et', 'al', '2007', 'blog', 'is', 'one', 'of', 'the', 'crucial', 'communicative', 'and', 'informative', 'repository', 'of', 'text', 'based', 'emotional', 'contents', 'in', 'the', 'web', '20', 'lin', 'et', 'al', '2007'], ['for', 'preprocessing', 'we', 'used', 'mada', 'morphological', 'analysis', 'and', 'disambiguation', 'for', 'arabic', 'habash', 'et', 'al', '2009', 'which', 'is', 'one', 'of', 'the', 'most', 'accurate', 'arabic', 'preprocessing', 'toolkitsfor', 'this', 'purpose', 'we', 'use', 'mada', 'morphological', 'analysis', 'and', 'disambiguation', 'for', 'arabic', 'habash', 'et', 'al', '2009', 'which', 'is', 'one', 'of', 'the', 'most', 'accurate', 'arabic', 'preprocessing', 'toolkits'], ['we', 'trained', 'a', '5gram', 'language', 'model', 'on', 'the', 'xinhua', 'section', 'of', 'the', 'english', 'gigaword', 'corpus', '306', 'million', 'words', 'using', 'the', 'srilm', 'toolkit', 'stolcke', '2002', 'with', 'the', 'modified', 'kneserney', 'smoothingour', '5gram', 'language', 'model', 'was', 'trained', 'on', 'the', 'xinhua', 'section', 'of', 'the', 'english', 'gigaword', 'corpus', '306', 'million', 'words', 'using', 'the', 'srilm', 'toolkit', 'stolcke', '2002', '', 'with', 'modified', 'kneserney', 'smoothing'], ['to', 'determine', 'semantic', 'type', 'and', 'subtype', 'we', 'train', 'two', 'svm', 'multiclass', 'classifiers', 'using', 'svm', 'multiclass', 'tsochantaridis', 'et', 'al', '2004', 'to', 'determine', 'semantic', 'types', 'and', 'subtypes', 'we', 'train', 'two', 'svm', 'multiclass', 'classifiers', 'using', 'svm', 'multiclass', 'tsochantaridis', 'et', 'al', '2004'], ['we', 'use', 'the', 'scikit', 'implementation', 'of', 'random', 'forest', 'pedregosa', 'et', 'al', '2011', 'we', 'use', 'the', 'scikit', 'implementation', 'of', 'svm', 'pedregosa', 'et', 'al', '2011'], ['each', 'term', 'in', 'the', 'input', 'text', 'will', 'be', 'represented', 'by', 'its', 'stem', 'and', 'pos', 'tag', 'in', 'the', 'following', 'format', 'stempos', 'using', 'buckwalter', 'transliteration', 'buckwalter', '2002', 'each', 'term', 'in', 'the', 'input', 'text', 'is', 'represented', 'by', 'its', 'stem', 'and', 'pos', 'tag', 'using', 'buckwalter', 'transliteration', 'buckwalter', '2002'], ['an', 'algorithm', 'the', 'kuhnmunkres', 'method', 'kuhn', '1955', '', 'can', 'find', 'solutions', 'to', 'the', 'optimum', 'assignment', 'problem', 'in', 'polynomial', 'timean', 'algorithm', 'the', 'kuhnmunkres', 'method', 'kuhn', '1955', '', 'has', 'been', 'proposed', 'that', 'can', 'find', 'a', 'solution', 'to', 'the', 'optimum', 'assignment', 'problem', 'in', 'polynomial', 'time'], ['we', 'use', 'collapsed', 'gibbs', 'sampling', 'griffiths', 'and', 'steyvers', '2004', 'to', 'infer', 'the', 'parameters', 'of', 'the', 'model', 'and', 'the', 'latent', 'violent', 'categories', 'and', 'topics', 'assignments', 'for', 'tweets', 'given', 'observed', 'data', 'd', 'gibbswe', 'use', 'collapsed', 'gibbs', 'sampling', 'griffiths', 'and', 'steyvers', '2004', 'to', 'infer', 'the', 'parameters', 'of', 'the', 'model', '', 'given', 'observed', 'data', 'd', 'gibbs', 'sampling', 'is', 'a', 'markov', 'chain', 'monte', 'carlo', 'method', 'which', 'allows', 'us', 'repeat'], ['we', 'use', 'the', 'stanford', 'dependency', 'parser', 'marneffe', 'et', 'al', '2006', 'we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006'], ['filter', 'weights', 'are', 'initialized', 'using', 'glorotbengio', 'strategy', 'glorot', 'and', 'bengio', '2010', 'weights', 'are', 'initialized', 'using', 'glorotbengio', 'strategy', 'glorot', 'and', 'bengio', '2010'], ['automatic', 'sentence', 'alignment', 'of', 'the', 'training', 'data', 'was', 'provided', 'by', 'ulrich', 'german', 'and', 'the', 'hand', 'alignments', 'of', 'the', 'words', 'in', 'the', 'test', 'data', 'were', 'created', 'by', 'franz', 'och', 'and', 'hermann', 'ney', 'och', 'and', 'ney', '2003', 'automatic', 'sentence', 'alignment', 'of', 'the', 'training', 'data', 'was', 'provided', 'by', 'ulrich', 'german', 'and', 'the', 'hand', 'alignments', 'of', 'the', 'words', 'in', 'the', 'trial', 'and', 'test', 'data', 'were', 'created', 'by', 'franz', 'och', 'and', 'hermann', 'ney', 'och', 'and', 'ney', '2003'], ['we', 'use', 'the', 'adagrad', 'optimizer', 'duchi', 'et', 'al', '2011', '', 'with', 'initial', 'learning', 'rate', 'set', 'to', '01we', 'use', 'adagrad', 'duchi', 'et', 'al', '2011', '', 'with', 'the', 'initial', 'learning', 'rate', 'set', 'to', '05'], ['then', 'we', 'did', 'word', 'alignment', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'default', 'growdiagfinaland', 'alignment', 'symmetrization', 'methodwe', 'performed', 'word', 'alignment', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'default', 'growdiagfinaland', 'alignment', 'symmetrization', 'method'], ['for', 'example', 'dirt', 'lin', 'and', 'pantel', '2001', 'aims', 'to', 'discover', 'different', 'representations', 'of', 'the', 'same', 'semantic', 'relation', 'ie', 'similar', 'dependency', 'pathsfor', 'example', 'dirt', 'lin', 'and', 'pantel', '2001', 'aims', 'to', 'discover', 'different', 'representations', 'of', 'the', 'same', 'semantic', 'relation', 'using', 'distributional', 'similarity', 'of', 'dependency', 'paths'], ['the', 'annotation', 'was', 'performed', 'manually', 'using', 'the', 'brat', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012', 'the', 'annotation', 'was', 'performed', 'using', 'the', 'brat', '2', 'tool', 'stenetorp', 'et', 'al', '2012'], ['system', 'proposed', 'by', 'li', 'et', 'al', '2006', '', 'uses', 'a', 'semanticvector', 'approach', 'to', 'measure', 'sentence', 'similaritya', 'similar', 'semantic', 'similarity', 'measure', 'proposed', 'by', 'li', 'et', 'al', '2006', '', 'uses', 'a', 'semanticvector', 'approach', 'to', 'measure', 'sentence', 'similarity'], ['this', 'corpus', 'contains', 'around', '11000', 'nps', 'annotated', 'for', 'information', 'status', 'including', '663', 'bridging', 'nps', 'and', 'their', 'antecedents', 'in', '50', 'texts', 'taken', 'from', 'the', 'wsj', 'portion', 'of', 'the', 'ontonotes', 'corpus', '', 'weischedel', 'et', 'al', '2011it', 'consists', 'of', '50', 'texts', 'taken', 'from', 'the', 'wsj', 'portion', 'of', 'the', 'ontonotes', 'corpus', 'weischedel', 'et', 'al', '2011', 'with', 'almost', '11000', 'nps', 'annotated', 'for', 'information', 'status', 'including', '663', 'bridging', 'nps', 'and', 'their', 'antecedent'], ['all', 'modules', 'take', 'as', 'input', 'the', 'corpus', 'documents', 'preprocessed', 'with', 'a', 'partofspeech', 'tagger', '4', 'and', 'shallow', 'parser', '5', 'punyakanok', 'and', 'roth', '2001', 'all', 'components', 'take', 'as', 'input', 'the', 'corpus', 'documents', 'preprocessed', 'with', 'a', 'partofspeech', 'tagger', '2', 'and', 'shallow', 'parser', '3', 'punyakanok', 'and', 'roth', '2001'], ['from', 'the', 'pioneering', 'work', 'of', 'rapp', '1995', '', 'contextual', 'similarity', 'has', 'been', 'used', 'for', 'ble', 'for', 'a', 'long', 'timefrom', 'the', 'pioneering', 'work', 'of', 'rapp', '1995', '', 'ble', 'from', 'comparable', 'corpora', 'has', 'been', 'studied', 'for', 'a', 'long', 'time'], ['europarl', 'koehn', '2005', 'is', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'run', 'our', 'experiments', 'on', 'europarl', 'koehn', '2005', '', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['tesla', 'translation', 'evaluation', 'of', 'sentences', 'with', 'linearprogrammingbased', 'analysis', 'was', 'first', 'proposed', 'in', 'liu', 'et', 'al', '2010', 'teslaf', 'was', 'called', 'tesla', 'in', 'liu', 'et', 'al', '2010'], ['for', 'building', 'the', 'baseline', 'smt', 'system', 'we', 'used', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', '', 'in', 'its', 'standard', 'setupfor', 'building', 'our', 'smt', 'systems', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'was', 'used', 'in', 'its', 'standard', 'setup'], ['rhetorical', 'structure', 'theory', 'rst', 'mann', 'and', 'thompson', '1988', '', 'one', 'of', 'the', 'most', 'influential', 'theories', 'of', 'discourse', 'represents', 'texts', 'by', 'labeled', 'hierarchical', 'structures', 'called', 'discourse', 'trees', 'dtsrhetorical', 'structure', 'theory', 'rst', 'mann', 'and', 'thompson', '1988', '', 'one', 'of', 'the', 'most', 'influential', 'theories', 'of', 'discourse', 'posits', 'a', 'tree', 'representation', 'of', 'a', 'discourse', 'known', 'as', 'a', 'discourse', 'tree', 'dt'], ['in', 'addition', 'the', 'fixdiscount', 'method', 'in', 'foster', 'et', 'al', '2006', 'for', 'phrase', 'table', 'smoothing', 'is', 'also', 'usedin', 'addition', 'the', 'fixdiscount', 'method', 'foster', 'et', 'al', '2006', 'for', 'phrase', 'table', 'smoothing', 'was', 'also', 'used'], ['memorybased', 'language', 'processing', 'daelemans', 'and', 'van', 'den', 'bosch', '2005', 'is', 'based', 'on', 'the', 'idea', 'that', 'nlp', 'problems', 'can', 'be', 'solved', 'by', 'reuse', 'of', 'solved', 'examples', 'of', 'the', 'problem', 'stored', 'in', 'memorymemorybased', 'language', 'processing', 'daelemans', 'and', 'van', 'den', 'bosch', '2005', 'is', 'based', 'on', 'the', 'idea', 'that', 'nlp', 'problems', 'can', 'be', 'solved', 'by', 'reuse', 'of', 'solved', 'examples', 'of', 'the', 'problem', 'in', 'memory'], ['we', 'calculate', 'statistical', 'significance', 'of', 'performance', 'differences', 'using', 'stratified', 'shuffling', 'yeh', '2000', 'we', 'tested', 'the', 'significance', 'of', 'differences', 'using', 'stratified', 'shuffling', 'yeh', '2000'], ['for', 'instance', 'machine', 'translation', 'mt', 'systems', 'can', 'benefit', 'from', 'training', 'on', 'sentences', 'extracted', 'from', 'parallel', 'or', 'comparable', 'documents', 'retrieved', 'from', 'the', 'web', 'munteanu', 'and', 'marcu', '2005', 'in', 'addition', 'machine', 'translation', 'mt', 'systems', 'can', 'be', 'improved', 'by', 'training', 'on', 'sentences', 'extracted', 'from', 'parallel', 'or', 'comparable', 'documents', 'mined', 'from', 'the', 'web', 'munteanu', 'and', 'marcu', '2005'], ['1', 'with', '2', 'regularization', 'using', 'adagrad', 'duchi', 'et', 'al', '2011', '1', 'regularization', 'using', 'adagrad', 'duchi', 'et', 'al', '2011'], ['why', 'does', 'the', 'lr', 'model', 'outperform', 'berkeley', '13', 'the', 'muc', 'vilain', 'et', 'al', '1995', 'score', 'is', 'the', 'minimum', 'number', 'of', 'links', 'between', 'mentions', 'to', 'be', 'inserted', 'or', 'deleted', 'when', 'mapping', 'the', 'output', 'to', 'a', 'gold', 'standardthe', 'muc', 'score', 'vilain', 'et', 'al', '1995', 'counts', 'the', 'minimum', 'number', 'of', 'links', 'between', 'mentions', 'to', 'be', 'inserted', 'or', 'deleted', 'when', 'mapping', 'a', 'system', 'response', 'to', 'a', 'gold', 'standard', 'key', 'set'], ['in', 'the', 'context', 'of', 'this', 'paper', 'we', 'will', 'be', 'focusing', 'on', 'the', 'subset', 'tree', 'sst', 'kernel', 'described', 'in', 'collins', 'and', 'duffy', '2002', '', 'which', 'relies', 'on', 'a', 'fragment', 'definition', 'that', 'does', 'not', 'allow', 'to', 'break', 'productionwe', 'will', 'focus', 'on', 'the', 'syntactic', 'tree', 'kernel', 'described', 'in', 'collins', 'and', 'duffy', '2002', '', 'which', 'relies', 'on', 'a', 'fragment', 'definition', 'that', 'does', 'not', 'allow', 'to', 'break', 'production', 'rules'], ['europarl', 'koehn', '2005', 'is', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentthe', 'europarl', 'corpus', 'koehn', '2005', 'is', 'built', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['we', 'have', 'used', 'foma', 'a', 'free', 'software', 'tool', 'to', 'specify', 'finitestate', 'automata', 'and', 'transducers', 'hulden', '2009', 'the', 'module', 'was', 'implemented', 'using', 'foma', 'a', 'free', 'software', 'tool', 'to', 'specify', 'finitestate', 'automata', 'and', 'transducers', 'hulden', '2009'], ['we', 'used', 'the', 'same', 'test', 'set', 'used', 'in', 'li', 'et', 'al', '2004', 'for', 'our', 'testing', '5', 'we', 'used', 'the', 'same', 'test', 'data', 'as', 'in', 'li', 'et', 'al', '2004'], ['the', 'penn', 'discourse', 'treebank', 'pdtb', 'prasad', 'et', 'al', '2008', 'is', 'a', 'large', 'corpus', 'annotated', 'with', 'discourse', 'relations', 'covering', 'the', 'wall', 'street', 'journal', 'part', 'of', 'the', 'penn', 'treebankpenn', 'discourse', 'treebank', 'the', 'penn', 'discourse', 'treebank', 'pdtb', 'is', 'a', 'corpus', 'of', 'wall', 'street', 'journal', 'articles', 'annotated', 'with', 'discourse', 'relations', 'prasad', 'et', 'al', '2008'], ['we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'a', 'gb', 'implementation', 'of', 'the', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011'], ['since', 'the', 'commonly', 'used', 'word', 'similarity', 'datasets', 'contain', 'a', 'small', 'number', 'of', 'word', 'pairs', 'we', 'also', 'use', 'the', 'men', 'dataset', 'bruni', 'et', 'al', '2012', 'of', '3000', 'word', 'pairs', 'sampled', 'from', 'words', 'that', 'occur', 'at', 'least', '700', 'times', 'in', 'a', 'large', 'web', 'corpusthe', 'second', 'is', 'the', 'men', 'dataset', 'bruni', 'et', 'al', '2012', 'of', '3000', 'words', 'pairs', 'sampled', 'from', 'words', 'that', 'occur', 'at', 'least', '700', 'times', 'in', 'a', 'large', 'web', 'corpus'], ['the', 'training', 'data', 'of', 'the', 'shared', 'task', 'is', 'the', 'nucle', 'corpus', 'dahlmeier', 'et', 'al', '2013', '', 'which', 'contains', 'essays', 'written', 'by', 'learners', 'of', 'english', 'the', 'training', 'data', 'released', 'by', 'the', 'task', 'organizers', 'comes', 'from', 'the', 'nucle', 'corpus', 'dahlmeier', 'et', 'al', '2013', '', 'which', 'contains', 'essays', 'written', 'by', 'learners', 'of', 'english', 'as', 'a', 'foreign', 'language'], ['all', 'system', 'implementation', 'was', 'done', 'using', 'python', 'and', 'the', 'opensource', 'machine', 'learning', 'toolkit', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'all', 'of', 'the', 'machine', 'learning', 'was', 'done', 'using', 'scikitlearn', 'pedregosa', 'et', 'al', '2011'], ['it', 'has', 'been', 'shown', 'that', 'a', 'diverse', 'set', 'of', 'predictions', 'can', 'be', 'used', 'to', 'help', 'improve', 'decoder', 'accuracy', 'for', 'various', 'problems', 'in', 'nlp', 'henderson', 'and', 'brill', '1999', 'it', 'has', 'been', 'long', 'identified', 'in', 'nlp', 'that', 'a', 'diverse', 'set', 'of', 'solutions', 'from', 'a', 'decoder', 'can', 'be', 'reranked', 'or', 'recombined', 'in', 'order', 'to', 'improve', 'the', 'accuracy', 'in', 'various', 'problems', 'henderson', 'and', 'brill', '1999'], ['in', 'the', '2013', 'system', 'we', 'had', 'used', 'sentistrength', 'lexicon', 'thelwall', 'et', 'al', '2010', 'in', 'our', 'system', 'we', 'used', 'the', 'sentiment', 'lexicon', 'provided', 'by', 'sentistrength', 'thelwall', 'et', 'al', '2010'], ['finally', 'we', 'also', 'compare', 'the', 'quality', 'of', 'the', 'candidate', 'phrase', 'embeddings', 'with', 'word', 'embeddings', 'dhillon', 'et', 'al', '2011', 'by', 'adding', 'them', 'as', 'features', 'in', 'a', 'crf', 'based', 'sequence', 'taggerwe', 'also', 'compared', 'the', 'quality', 'of', 'the', 'candidate', 'phrase', 'embeddings', 'with', 'the', 'wordlevel', 'embeddings', 'by', 'adding', 'them', 'as', 'features', 'dhillon', 'et', 'al', '2011', 'along', 'with', 'the', 'baseline', 'features', 'in', 'the', 'crf', 'tagger'], ['these', 'methods', 'are', 'based', 'on', 'the', 'distributional', 'hypothesis', 'which', 'states', 'that', 'words', 'appearing', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meaning', 'harris', '1954', 'corpusbased', 'vsms', 'follow', 'the', 'standard', 'distributional', 'hypothesis', 'which', 'states', 'that', 'words', 'appearing', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meaning', 'harris', '1954'], ['all', 'annotations', 'were', 'done', 'using', 'the', 'brat', 'rapid', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012', 'the', 'annotations', 'were', 'made', 'using', 'the', 'brat', 'rapid', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012'], ['compared', 'to', 'wordnet', 'fellbaum', '1998', '', 'there', 'are', 'similarities', 'but', 'also', 'significant', 'differencescompared', 'to', 'wordnet', 'fellbaum', '1998', '', 'there', 'are', 'similarities', 'as', 'well', 'as', 'considerable', 'differences'], ['for', 'training', 'we', 'use', 'adam', 'kingma', 'and', 'ba', '2015', 'for', 'optimization', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0001we', 'use', 'adam', 'kingma', 'and', 'ba', '2015', 'for', 'optimization', 'with', 'initial', 'learning', 'rate', 'of', '0001'], ['for', 'our', 'classifier', 'we', 'use', 'svms', 'specifically', 'the', 'liblinear', 'svm', 'software', 'package', 'fan', 'et', 'al', '2008', '', 'which', 'is', 'wellsuited', 'to', 'text', 'classification', 'tasks', 'with', 'large', 'numbers', 'of', 'features', 'and', 'large', 'numberspecifically', 'we', 'use', 'the', 'liblinear', 'svm', 'package', 'fan', 'et', 'al', '2008', 'as', 'it', 'is', 'wellsuited', 'to', 'text', 'classification', 'tasks', 'with', 'large', 'numbers', 'of', 'features', 'and', 'texts'], ['the', 'first', 'two', 'experiments', 'concern', 'the', 'prediction', 'of', 'the', 'sentiment', 'of', 'movie', 'reviews', 'in', 'the', 'stanford', 'sentiment', 'treebank', 'socher', 'et', 'al', '2013the', 'first', 'two', 'experiments', 'involve', 'predicting', 'the', 'sentiment', 'of', 'movie', 'reviews', 'socher', 'et', 'al', '2013'], ['we', 'used', 'only', 'the', 'nonensembled', 'lefttoright', 'run', 'ie', 'no', 'righttoleft', 'rescoring', 'as', 'done', 'by', 'sennrich', 'et', 'al', '2016a', 'with', 'beam', 'size', 'of', '5', '5', 'taking', 'just', 'the', 'singlebest', 'outputwe', 'used', 'only', 'the', 'nonensembled', 'lefttoright', 'run', 'ie', 'no', 'righttoleft', 'rescoring', 'as', 'done', 'by', 'sennrich', 'et', 'al', '2016a', 'with', 'beam', 'size', 'of', '12', 'default', 'value'], ['we', 'used', 'only', 'the', 'nonensembled', 'lefttoright', 'run', 'sennrich', 'et', 'al', '2016a', 'with', 'beam', 'size', 'of', '5', '5', 'taking', 'just', 'the', 'singlebest', 'outputwe', 'used', 'only', 'the', 'nonensembled', 'lefttoright', 'run', 'sennrich', 'et', 'al', '2016a', 'with', 'beam', 'size', 'of', '12', 'default', 'value'], ['the', 'msd', 'morphological', 'coding', 'system', 'was', 'developed', 'for', 'a', 'bunch', 'of', 'languages', 'including', 'hungarian', 'erjavec', '2004', 'the', 'msd', 'morphological', 'coding', 'system', 'is', 'a', 'positional', 'coding', 'system', 'developed', 'for', 'several', 'languages', 'erjavec', '2004'], ['the', 'phrase', 'tables', 'were', 'generated', 'by', 'means', 'of', 'symmetrised', 'word', 'alignments', 'obtained', 'with', 'giza', 'och', 'and', 'ney', '2003', 'the', 'phrase', 'table', 'was', 'generated', 'employing', 'symmetrised', 'word', 'alignments', 'obtained', 'with', 'giza', 'och', 'and', 'ney', '2003'], ['word', 'alignment', 'is', 'performed', 'using', 'giza', 'och', 'and', 'ney', '2003', 'word', 'alignment', 'is', 'performed', 'with', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'experimented', 'with', 'several', 'levels', 'of', 'cluster', 'granularity', 'using', 'development', 'data', 'and', 'following', 'koo', 'et', 'al', '2008following', 'koo', 'et', 'al', '2008', '', 'we', 'also', 'experimented', 'with', 'using', 'two', 'sets', 'of', 'cluster', 'labels', 'with', 'different', 'levels', 'of', 'granularity'], ['raghavan', 'et', 'al', '2007', 'measure', 'the', 'benefit', 'from', 'feature', 'feedback', 'as', 'the', 'gain', 'in', 'the', 'learning', 'speed', 'with', 'feature', 'feedbackraghavan', 'et', 'al', '2007', '', 'evaluate', 'benefit', 'from', 'feature', 'feedback', 'in', 'terms', 'of', 'the', 'gain', 'in', 'learning', 'speed'], ['we', 'built', 'a', 'modified', 'kneserney', 'smoothed', '5gram', 'language', 'model', 'using', 'the', 'english', 'side', 'of', 'the', 'training', 'data', 'and', 'performed', 'querying', 'with', 'kenlm', 'heafield', '2011', 'training', 'and', 'querying', 'of', 'a', 'modified', 'kneserney', 'smoothed', '5', 'gram', 'language', 'model', 'are', 'done', 'on', 'the', 'english', 'side', 'of', 'the', 'training', 'data', 'using', 'kenlm', 'heafield', '2011'], ['a', 'formal', 'pacstyle', 'analysis', 'can', 'be', 'found', 'in', 'ando', 'and', 'zhang', '2004', 'the', 'formal', 'derivation', 'can', 'be', 'found', 'in', 'ando', 'and', 'zhang', '2004'], ['the', 'first', 'model', 'we', 'introduce', 'is', 'based', 'on', 'the', 'recurrent', 'neural', 'network', 'language', 'model', 'of', 'mikolov', 'et', 'al', '2010', 'the', 'model', 'as', 'described', 'thus', 'far', 'is', 'identical', 'to', 'the', 'recurrent', 'neural', 'network', 'language', 'model', 'rnnlm', 'of', 'mikolov', 'et', 'al', '2010'], ['the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'classifier', 'experiments', 'were', 'carried', 'out', 'using', 'the', 'svmlight', 'software', 'joachims', '1999', 'available', 'at', 'httpsvmlightjoachimsorg', 'with', 'a', 'polynomial', 'kernel', '2', 'degree3the', 'classifier', 'evaluations', 'were', 'carried', 'out', 'using', 'the', 'svmlight', 'software', 'joachims', '1999', 'available', 'at', 'httpsvmlightjoachimsorg', 'with', 'the', 'default', 'linear', 'kernel', 'for', 'the', 'standard', 'feature', 'evaluation'], ['the', 'training', 'data', 'of', 'the', 'shared', 'task', 'is', 'the', 'nucle', 'corpus', 'dahlmeier', 'et', 'al', '2013', '', 'which', 'contains', 'essays', 'written', 'by', 'learners', 'of', 'englishthe', 'training', 'data', 'for', 'the', 'task', 'is', 'from', 'the', 'nucle', 'corpus', 'dahlmeier', 'et', 'al', '2013', '', 'an', 'errortagged', 'collection', 'of', 'essays', 'written', 'by', 'nonnative', 'learners', 'of', 'english'], ['saldo', 'borin', 'et', 'al', '2013', 'is', 'the', 'largest', 'freely', 'available', 'lexical', 'resource', 'for', 'swedishsaldo', 'borin', 'et', 'al', '2013', 'is', 'the', 'most', 'comprehensive', 'open', 'lexical', 'resource', 'for', 'swedish'], ['for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['however', 'those', 'stringtotree', 'systems', 'run', 'slowly', 'in', 'cubic', 'time', 'huang', 'et', 'al', '2006', 'however', 'such', 'stringtotree', 'systems', 'run', 'slowly', 'in', 'cubic', 'time', 'huang', 'et', 'al', '2006'], ['the', '5gram', 'target', 'language', 'model', 'was', 'trained', 'using', 'kenlm', 'heafield', '2011', 'we', 'trained', 'an', 'english', '5gram', 'language', 'model', 'using', 'kenlm', 'heafield', '2011'], ['for', 'all', 'experiments', 'we', 'used', 'the', 'moses', 'smt', 'system', 'koehn', 'et', 'al', '2007', 'for', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'evaluate', 'our', 'method', 'on', 'the', 'following', 'data', 'sets', 'bullet', 'ontonotesdev', 'development', 'set', 'of', 'the', 'ontonotes', 'data', 'provided', 'by', 'the', 'conll2012', 'shared', 'task', 'pradhan', 'et', 'al', '2012', 'bullet', 'ontonotestest', 'test', 'set', 'of', 'the', 'ontonotes', 'data', 'provided', 'by', 'the', 'conll2012', 'shared', 'task', 'pradhan', 'et', 'al', '2012'], ['we', 'use', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'implement', 'our', 'modelswe', 'use', 'the', 'stateoftheart', 'phrasebased', 'machine', 'translation', 'system', 'moses', 'koehn', 'et', 'al', '2007', 'to', 'perform', 'our', 'machine', 'translation', 'experiments'], ['but', 'we', 'randomly', 'selected', '90', 'of', 'the', 'training', 'data', 'used', 'in', 'li', 'et', 'al', '2004', 'as', 'our', 'training', 'data', 'and', 'the', 'remainder', 'as', 'the', 'development', 'data', 'as', 'shown', 'in', 'table', '5', 'since', 'the', 'training', 'data', 'used', 'in', 'li', 'et', 'al', '2004', 'is', 'identical', 'as', 'the', 'union', 'of', 'our', 'training', 'and', 'development', 'data', 'we', 'denoted', 'it', 'as', 'traindev', 'in', 'table', '6'], ['the', 'bleu', 'score', 'measures', 'the', 'precision', 'of', 'ngrams', 'over', 'all', 'n', 'to', '4', 'in', 'our', 'case', 'with', 'respect', 'to', 'a', 'reference', 'translation', 'with', 'a', 'penalty', 'for', 'short', 'translations', 'papineni', 'et', 'al', '2001', 'bleu', 'score', 'this', 'score', 'measures', 'the', 'precision', 'of', 'unigrams', 'bigrams', 'trigrams', 'and', 'fourgrams', 'with', 'respect', 'to', 'a', 'whole', 'set', 'of', 'reference', 'translations', 'with', 'a', 'penalty', 'for', 'too', 'short', 'sentences', 'papineni', 'et', 'al', '2001'], ['the', 'training', 'data', 'of', 'the', 'shared', 'task', 'is', 'the', 'nucle', 'corpus', 'dahlmeier', 'et', 'al', '2013', '', 'which', 'contains', 'essays', 'written', 'by', 'learners', 'of', 'englishthe', 'training', 'data', 'provided', 'for', 'the', 'task', 'is', 'a', 'subset', 'of', 'the', 'nucle', 'v23', 'corpus', 'dahlmeier', 'et', 'al', '2013', '', 'which', 'comprises', 'essays', 'written', 'in', 'english', 'by', 'students', 'at', 'the', 'national', 'university', 'of', 'singapore'], ['test', 'data', 'was', 'drawn', 'from', 'the', 'open', 'american', 'national', 'corpus', 'ide', 'and', 'suderman', '2004', '', 'oanc', 'across', 'a', 'variety', 'of', 'genres', 'and', 'from', 'both', 'the', 'spoken', 'and', 'written', 'portions', 'of', 'the', 'corpuswe', 'selected', 'the', 'dataset', 'of', 'jurgens', 'and', 'klapaftis', '2013', 'which', 'was', 'drawn', 'from', 'the', 'open', 'american', 'national', 'corpus', 'oanc', 'ide', 'and', 'suderman', '2004', 'across', 'a', 'variety', 'of', 'genres', 'and', 'from', 'both', 'the', 'spoken', 'and', 'written', 'portions', 'of', 'the', 'corpus'], ['we', 'measure', 'statistical', 'significance', 'using', '95', 'confidence', 'intervals', 'computed', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'the', 'statistical', 'significance', 'tests', 'using', '95', 'confidence', 'interval', 'are', 'measured', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004'], ['we', 'use', 'the', 'english', 'portion', 'of', 'the', 'ace', '2005', 'relation', 'extraction', 'dataset', 'walker', 'et', 'al', '2006', 'we', 'evaluate', 'our', 'relation', 'extraction', 'system', 'on', 'the', 'english', 'portion', 'of', 'the', 'ace', '2005', 'corpus', 'walker', 'et', 'al', '2006'], ['this', 'data', 'was', 'collected', 'for', 'the', '2014', 'semeval', 'competition', 'marelli', 'et', 'al', '2014', 'and', 'consists', 'of', '9927', 'sentence', 'pairs', 'with', '4500', 'for', 'training', '500', 'as', 'a', 'development', 'setsentences', 'involving', 'compositional', 'knowledge', 'sick', 'is', 'from', 'task', '1', 'of', 'the', '2014', 'semeval', 'competition', 'marelli', 'et', 'al', '2014', 'and', 'consists', 'of', '9927', 'annotated', 'sentence', 'pairs', 'with', '4500', 'for', 'training', '500', 'as', 'a', 'development', 'set'], ['the', 'parsing', 'model', 'used', 'for', 'intrasentential', 'parsing', 'is', 'a', 'dynamic', 'conditional', 'random', 'field', 'dcrf', 'sutton', 'et', 'al', '2007', 'shown', 'in', 'figure', '7', 'our', 'novel', 'parsing', 'model', 'is', 'the', 'dynamic', 'conditional', 'random', 'field', 'dcrf', 'sutton', 'et', 'al', '2007', 'shown', 'in', 'figure', '2'], ['latent', 'dirichlet', 'allocation', 'lda', 'is', 'a', 'generative', 'model', 'which', 'considers', 'a', 'document', 'model', 'salton', '1989', 'as', 'a', 'mixture', 'probability', 'of', 'latent', 'topics', 'combination', 'of', 'latent', 'topics', 'ent', 'dirichlet', 'allocation', 'lda', 'is', 'a', 'generative', 'model', 'which', 'considers', 'a', 'document', 'model', 'seen', 'as', 'a', 'bag', 'of', 'words', 'salton', '1989', 'as', 'a', 'mixture', 'probability', 'of', 'latent', 'topics'], ['distributional', 'hypothesis', 'theory', 'harris', '1954', 'indicates', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'context', 'tend', 'to', 'have', 'similar', 'meaningsit', 'therefore', 'follows', 'the', 'distributional', 'hypothesis', 'harris', '1954', 'which', 'states', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meanings'], ['distributional', 'hypothesis', 'theory', 'harris', '1954', 'indicates', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'context', 'tend', 'to', 'have', 'similar', 'meaningsit', 'therefore', 'follows', 'the', 'distributional', 'hypothesis', 'harris', '1954', 'which', 'states', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meanings'], ['in', '2009', 'yefang', 'wang', 'wang', 'et', 'al', '2009', 'used', 'cascading', 'classifiers', 'on', 'manually', 'annotated', 'data', 'which', 'fetched', 'fscore', 'of', '0832in', '2009', 'yefang', 'wang', 'wang', 'et', 'al', '2009', 'used', 'cascading', 'classifiers', 'on', 'manually', 'annotated', 'data', 'and', 'achieved', 'around', '832', 'accuracy'], ['we', 'built', 'a', '5gram', 'language', 'model', 'on', 'the', 'english', 'side', 'of', 'qcatrain', 'using', 'kenlm', 'heafield', '2011', 'we', 'built', 'a', 'modified', 'kneserney', 'smoothed', '5gram', 'language', 'model', 'using', 'the', 'english', 'side', 'of', 'the', 'training', 'data', 'and', 'performed', 'querying', 'with', 'kenlm', 'heafield', '2011'], ['the', 'remaining', 'three', 'models', 'are', 'all', 'naive', 'bayes', 'classifiers', 'trained', 'on', 'the', 'google', 'web', '1t', '5gram', 'corpus', 'henceforth', 'google', 'corpus', '', 'brants', 'and', 'franz', '2006', 'the', 'other', 'models', 'are', 'trained', 'on', 'native', 'english', 'data', 'the', 'google', 'web', '1t', '5gram', 'corpus', 'henceforth', 'google', 'brants', 'and', 'franz', '2006', '', 'with', 'the', 'naive', 'bayes', 'nb', 'algorithm'], ['we', 'apply', 'bootstrapping', 'kozareva', 'et', 'al', '2008', 'on', 'the', 'word', 'graphs', 'by', 'manually', 'selecting', '10', 'seeds', 'for', 'concrete', 'and', 'abstract', 'words', 'see', 'table', '10', 'we', 'then', 'apply', 'bootstrapping', 'kozareva', 'et', 'al', '2008', 'on', 'the', 'noun', 'and', 'adjective', 'graphs', 'by', 'selecting', '10', 'seeds', 'for', 'visual', 'and', 'nonvisual', 'nouns', 'and', 'adjectives', 'see', 'table', '1'], ['these', 'methods', 'are', 'based', 'on', 'the', 'distributional', 'hypothesis', 'which', 'states', 'that', 'words', 'appearing', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meaning', 'harris', '1954', 'it', 'therefore', 'follows', 'the', 'distributional', 'hypothesis', 'harris', '1954', 'which', 'states', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meanings'], ['these', 'results', 'verify', 'the', 'benefit', 'of', 'using', 'ltag', 'based', 'features', 'and', 'confirm', 'the', 'hypothesis', 'that', 'ltag', 'based', 'features', 'provide', 'a', 'novel', 'set', 'of', 'abstract', 'features', 'that', 'complement', 'the', 'hand', 'selected', 'features', 'collins', '2000', 'our', 'hypothesis', 'is', 'that', 'the', 'ltag', 'based', 'features', 'provide', 'a', 'novel', 'set', 'of', 'abstract', 'features', 'that', 'complement', 'the', 'hand', 'selected', 'features', 'from', 'collins', '2000', 'and', 'the', 'ltag', 'based', 'features', 'will', 'help', 'improve'], ['rg65', 'rubenstein', 'and', 'goodenough', '1965', 'has', '65', 'word', 'pairsrg65', 'rubenstein', 'and', 'goodenough', '1965', 'is', 'set', 'of', '65', 'word', 'pairs'], ['the', 'significance', 'tests', 'were', 'performed', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004', 'statistical', 'significance', 'tests', 'are', 'performed', 'using', 'bootstrap', 'resampling', 'koehn', '2004'], ['the', 'default', 'phrasal', 'search', 'algorithm', 'is', 'cube', 'pruning', 'huang', 'and', 'chiang', '2007', 'the', 'search', 'is', 'typically', 'carried', 'out', 'using', 'the', 'cube', 'pruning', 'algorithm', 'huang', 'and', 'chiang', '2007'], ['indomain', 'data', 'is', 'mainly', 'used', 'to', 'solve', 'the', 'problem', 'of', 'data', 'sparseness', 'sun', 'and', 'xu', '2011', 'indomain', 'data', 'only', 'solves', 'the', 'problem', 'of', 'data', 'sparseness', 'sun', 'and', 'xu', '2011'], ['all', 'experiments', 'were', 'carried', 'out', 'using', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'all', 'the', 'experiments', 'are', 'carried', 'out', 'in', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['first', 'we', 'apply', 'heuristics', 'to', 'determine', 'number', 'and', 'gender', 'based', 'on', 'word', 'lists', 'wordnet', 'miller', '1990', 'and', 'partofspeech', 'tagswe', 'then', 'apply', 'heuristics', 'to', 'determine', 'number', 'and', 'gender', 'for', 'the', 'characters', 'based', 'on', 'word', 'lists', 'word', 'net', 'miller', '1990', 'and', 'pos', 'tags'], ['rank', 'svm', 'joachims', '2002', 'is', 'a', 'method', 'based', 'on', 'support', 'vector', 'machines', 'svms', 'for', 'which', 'we', 'use', 'only', 'linear', 'kernels', 'to', 'keep', 'complexity', 'lowfor', 'this', 'task', 'we', 'use', 'ranksvm', 'joachims', '2002', 'which', 'is', 'a', 'method', 'based', 'on', 'support', 'vector', 'machines', 'svms'], ['a', 'more', 'detailed', 'description', 'of', 'the', 'task', 'can', 'be', 'found', 'in', 'nakov', 'et', 'al', '2017', 'a', 'precise', 'description', 'of', 'the', 'corpus', 'and', 'metrics', 'can', 'be', 'found', 'in', 'task3', 'description', 'paper', 'nakov', 'et', 'al', '2017'], ['a', 'tree', 'kernel', 'function', 'is', 'a', 'convolution', 'kernel', 'haussler', '1999', 'defined', 'over', 'pairs', 'of', 'treestree', 'kernel', 'tk', 'functions', 'are', 'convolution', 'kernels', 'haussler', '1999', 'defined', 'over', 'pairs', 'of', 'trees'], ['we', 'build', 'upon', 'our', 'previous', 'approach', 'for', 'joint', 'concept', 'disambiguation', 'and', 'clustering', 'fahrni', 'and', 'strube', '2012', 'for', 'disambiguation', 'and', 'clustering', 'we', 'build', 'upon', 'our', 'previous', 'work', 'fahrni', 'and', 'strube', '2012'], ['rouge2', 'metric', 'lin', '2004', 'is', 'used', 'for', 'the', 'evaluation', 'we', 'used', 'the', 'rouge1', 'evaluation', 'metric', 'lin', '2004'], ['we', 'used', 'mallet', 'software', 'mccallum', '2002', 'for', 'crf', 'experimentswe', 'used', 'mallet', 'toolkit', 'mccallum', '2002', 'for', 'crf', 'implementation'], ['we', 'exploit', 'a', 'transitionbased', 'framework', 'with', 'global', 'learning', 'and', 'beamsearch', 'decoding', 'to', 'implement', 'the', 'joint', 'model', 'zhang', 'and', 'clark', '2011', 'our', 'joint', 'parsing', 'model', 'exploits', 'a', 'transitionbased', 'framework', 'with', 'global', 'learning', 'and', 'beamsearch', 'decoding', 'zhang', 'and', 'clark', '2011', '', 'extended', 'from', 'a', 'arcstandard', 'transitionbased', 'parsing', 'model'], ['the', 'annotation', 'was', 'performed', 'using', 'the', 'brat', '2', 'tool', 'stenetorp', 'et', 'al', '2012', 'the', 'annotations', 'were', 'made', 'using', 'the', 'brat', 'rapid', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012'], ['for', 'building', 'the', 'word', 'alignment', 'models', 'we', 'use', 'mgiza', 'gao', 'and', 'vogel', '2008', 'to', 'build', 'the', 'word', 'alignment', 'models', 'we', 'used', 'the', 'mgiza', 'package', 'gao', 'and', 'vogel', '2008'], ['the', 'reliability', 'of', 'the', 'annotation', 'was', 'evaluated', 'using', 'the', 'kappa', 'statistic', 'carletta', '1996', 'we', 'evaluated', 'annotation', 'reliability', 'by', 'using', 'the', 'kappa', 'statistic', 'carletta', '1996'], ['it', 'therefore', 'follows', 'the', 'distributional', 'hypothesis', 'harris', '1954', 'which', 'states', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meaningsdistributional', 'models', 'of', 'meaning', 'follow', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'which', 'states', 'that', 'two', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'have', 'similar', 'meanings'], ['this', 'dataset', 'is', 'composed', 'of', '35', 'triplets', 'of', 'sentences', 'from', 'the', 'eyetracking', 'experiment', 'experiment', '1', 'in', 'traxler', 'et', 'al', '2002', '', 'for', 'a', 'total', 'of', '105', 'sentencesthis', 'dataset', 'is', 'composed', 'of', '32', 'sentence', 'quadruplets', 'from', 'experiments', '2', 'eyetracking', 'and', '3', 'selfpaced', 'reading', 'in', 'traxler', 'et', 'al', '2002', '', 'for', 'a', 'total', 'of', '120', 'sentences'], ['the', 'significance', 'tests', 'were', 'performed', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004', 'a', 'statistical', 'significance', 'test', 'was', 'performed', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004'], ['we', 'use', 'the', 'standard', 'alignment', 'tool', 'giza', 'och', 'and', 'ney', '2003', 'to', 'word', 'align', 'the', 'parallel', 'datawe', 'use', 'the', 'giza', 'tool', 'och', 'and', 'ney', '2003', 'to', 'align', 'words', 'in', 'our', 'parallel', 'corpora'], ['we', 'use', 'the', 'standard', 'alignment', 'tool', 'giza', 'och', 'and', 'ney', '2003', 'to', 'word', 'align', 'the', 'parallel', 'datawe', 'use', 'the', 'giza', 'tool', 'och', 'and', 'ney', '2003', 'to', 'align', 'words', 'in', 'our', 'parallel', 'corpora'], ['the', 'kernels', 'are', 'combined', 'using', 'gaussian', 'process', 'regression', 'gpr', 'rasmussen', 'and', 'williams', '2006', 'gaussian', 'process', 'regression', 'gpr', 'rasmussen', 'and', 'williams', '2006'], ['to', 'overcome', 'this', 'agirre', 'et', 'al', '2009', 'used', 'mapreduce', 'infrastructure', 'with', '2000', 'cores', 'to', 'compute', 'pairwise', 'similarities', 'of', 'words', 'on', 'a', 'corpus', 'of', 'roughly', '16', 'terawordsin', 'another', 'work', 'a', 'corpus', 'of', 'roughly', '16', 'terawords', 'was', 'used', 'by', 'agirre', 'et', 'al', '2009', '', 'to', 'compute', 'pairwise', 'similarities', 'of', 'the', 'words', 'in', 'the', 'test', 'sets', 'using', 'the', 'mapreduce', 'infrastructure', 'on', '2000', 'cores'], ['we', 'measure', 'statistical', 'significance', 'using', '95', 'confidence', 'intervals', 'computed', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'we', 'calculated', 'significance', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004'], ['we', 'use', 'the', 'stanford', 'dependency', 'parser', 'chen', 'and', 'manning', '2014', 'at', 'this', 'stage', 'and', 'have', 'not', 'experimented', 'with', 'alternatives', 'in', 'this', 'work', 'we', 'use', 'the', 'stanford', 'neural', 'dependency', 'parser', 'chen', 'and', 'manning', '2014'], ['next', 'a', 'tweet', 'was', 'tokenized', 'and', 'fed', 'into', 'madamira', 'pasha', 'et', 'al', '2014', '', 'a', 'morphological', 'analysis', 'tool', 'for', 'arabic', 'textmadamira', 'pasha', 'et', 'al', '2014', 'is', 'a', 'morphological', 'analysis', 'and', 'disambiguation', 'tool', 'of', 'arabic'], ['yarowsky', '1995', 'has', 'proposed', 'a', 'bootstrapping', 'method', 'for', 'word', 'sense', 'disambiguation', 'yarowsky', '1995', 'proposed', 'such', 'a', 'method', 'for', 'word', 'sense', 'disambiguation', 'which', 'we', 'refer', 'to', 'as', 'monolingual', 'bootstrapping'], ['we', 'also', 'list', 'the', 'previous', 'stateoftheart', 'performance', 'from', 'a', 'conventional', 'smt', 'system', 'durrani', 'et', 'al', '2014', 'with', 'the', 'bleu', 'of', '370we', 'also', 'list', 'the', 'results', 'from', 'smt', 'model', 'durrani', 'et', 'al', '2014', 'as', 'a', 'comparison'], ['we', 'used', 'adam', 'kingma', 'and', 'ba', '2014', 'with', 'a', 'learning', 'rate', 'of', '00002we', 'use', 'the', 'adam', 'kingma', 'and', 'ba', '2014', 'algorithm', 'to', 'minimize', 'the', 'sum', 'of', 'the', 'loss', 'for', 'p', 'asv', 'and', 'p', 'af', '', 'with', 'a', 'learning', 'rate', 'of', '10'], ['distributional', 'semantics', 'is', 'based', 'on', 'the', 'idea', 'that', 'firth', '1957', 'in', 'other', 'words', 'the', 'meaning', 'of', 'a', 'word', 'is', 'related', 'to', 'the', 'contexts', 'it', 'appears', 'inword', 'cooccurence', 'statistics', 'you', 'shall', 'know', 'a', 'word', 'by', 'the', 'company', 'it', 'keeps', 'firth', '1957'], ['in', 'order', 'to', 'estimate', 'the', 'basic', 'lexical', 'similarity', 'function', 'employed', 'in', 'the', 'sum', 'ssc', 'and', 'sptk', 'operators', 'a', 'cooccurrence', 'word', 'space', 'is', 'acquired', 'through', 'the', 'distributional', 'analysis', 'of', 'the', 'ukwac', 'corpus', 'baroni', 'et', 'al', '2009', 'the', 'cooccurrence', 'word', 'space', 'is', 'acquired', 'through', 'the', 'distributional', 'analysis', 'of', 'the', 'ukwac', 'corpus', 'baroni', 'et', 'al', '2009'], ['we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'the', 'scikitlearn', 'toolkit', 'to', 'train', 'our', 'classifiers', 'pedregosa', 'et', 'al', '2011'], ['the', 'translation', 'model', 'was', 'trained', 'by', 'giza', 'och', 'and', 'ney', '2003', '', 'and', 'the', 'trigram', 'was', 'trained', 'by', 'the', 'cmucambridge', 'statistical', 'language', 'modeling', 'toolkit', 'v2', 'clarkson', 'and', 'rosenfeld', '1997', 'the', 'probability', 'pe', 'is', 'computed', 'using', 'a', 'simple', 'trigram', 'language', 'model', 'that', 'was', 'trained', 'using', 'the', 'cmu', 'language', 'modeling', 'toolkit', 'clarkson', 'and', 'rosenfeld', '1997'], ['this', 'is', 'a', 'generalization', 'of', 'the', 'operator', 'id', 'in', 'kaplan', 'and', 'kay', '1994', 'this', 'is', 'similar', 'to', 'the', 'operator', 'intro', 'in', 'kaplan', 'and', 'kay', '1994'], ['we', 'used', 'standard', 'classifiers', 'available', 'in', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'a', 'gb', 'implementation', 'of', 'the', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011'], ['we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011', 'for', 'the', 'linear', 'logistic', 'regression', 'implementation', 'we', 'used', 'scikitlearn', 'pedregosa', 'et', 'al', '2011'], ['for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'finally', 'we', 'used', 'moses', 'toolkit', 'as', 'phrasebased', 'reference', 'koehn', 'et', 'al', '2007'], ['4', 'word', 'alignments', 'are', 'created', 'by', 'aligning', 'the', 'data', 'in', 'both', 'directions', 'with', 'giza', '5', 'and', 'symmetrizing', 'the', 'two', 'trained', 'alignments', 'och', 'and', 'ney', '2003', 'baseline', 'word', 'alignments', 'were', 'obtained', 'by', 'running', 'giza', 'in', 'both', 'directions', 'and', 'symmetrizing', 'using', 'the', 'growdiagfinaland', 'heuristic', 'och', 'and', 'ney', '2003'], ['the', 'perplexity', 'achieved', 'by', 'the', '6', 'gram', 'nn', 'lm', 'in', 'the', 'spanish', 'newstest08', 'development', 'set', 'was', '116', 'versus', '94', 'obtained', 'with', 'a', 'standard', '6gram', 'language', 'model', 'with', 'interpolation', 'and', 'kneserney', 'smoothing', 'kneser', 'and', 'ney', '1995', 'the', 'language', 'model', 'is', 'a', '5gram', 'with', 'interpolation', 'and', 'kneserney', 'smoothing', 'kneser', 'and', 'ney', '1995'], ['we', 'used', 'standard', 'classifiers', 'available', 'in', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'the', 'scikitlearn', 'toolkit', 'to', 'train', 'our', 'classifiers', 'pedregosa', 'et', 'al', '2011'], ['statistical', 'machine', 'translation', 'is', 'typically', 'performed', 'using', 'phrasebased', 'systems', 'koehn', 'et', 'al', '2007', 'it', 'is', 'a', 'standard', 'phrasebased', 'machine', 'translation', 'model', 'koehn', 'et', 'al', '2007'], ['wordnet', 'miller', 'et', 'al', '1990', 'is', 'an', 'online', 'hierarchical', 'lexical', 'database', 'which', 'contains', 'semantic', 'information', 'about', 'english', 'wordsthe', 'wordnet', 'online', 'lexical', 'database', 'miller', 'et', 'al', '1990'], ['we', 'use', 'the', 'scikit', 'implementation', 'of', 'random', 'forest', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'a', 'gb', 'implementation', 'of', 'the', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011'], ['we', 'lemmatise', 'the', 'head', 'of', 'each', 'constituent', 'with', 'treetagger', 'schmid', '1994', 'we', 'used', 'the', 'stuttgart', 'treetagger', 'schmid', '1994', 'to', 'lemmatise', 'constituent', 'heads'], ['our', 'text', 'processing', 'uses', 'the', 'natural', 'language', 'toolkit', 'nltk', 'bird', 'et', 'al', '2009', 'partofspeech', 'tagging', 'was', 'accomplished', 'using', 'the', 'natural', 'language', 'toolkit', 'nltk', 'bird', 'et', 'al', '2009'], ['they', 'used', 'the', 'webbased', 'annotation', 'tool', 'brat', 'stenetorp', 'et', 'al', '2012', 'for', 'the', 'annotation', 'the', 'annotation', 'was', 'performed', 'manually', 'using', 'the', 'brat', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012'], ['our', 'system', 'participated', 'in', 'semeval2013', 'task', '2', 'sentiment', 'analysis', 'in', 'twitter', 'wilson', 'et', 'al', '2013', 'we', 'participated', 'in', 'both', 'subtask', 'a', 'and', 'b', 'of', 'semeval2013', 'task', '2', 'sentiment', 'analysis', 'in', 'twitter', 'wilson', 'et', 'al', '2013', 'with', 'an', 'adaptation', 'of', 'our', 'existing', 'system'], ['the', 'english', 'text', 'was', 'tokenized', 'using', 'the', 'word', 'tokenize', 'routine', 'from', 'nltk', 'bird', 'et', 'al', '2009', 'we', 'tokenise', 'the', 'text', 'using', 'the', 'default', 'tokeniser', 'from', 'nltk', 'bird', 'et', 'al', '2009'], ['the', 'webpages', 'were', 'parsed', 'using', 'the', 'stanford', 'corenlp', 'software', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['statistical', 'machine', 'translation', 'is', 'typically', 'performed', 'using', 'phrasebased', 'systems', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'english', 'side', 'was', 'tokenized', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'trained', 'an', 'english', '5gram', 'language', 'model', 'using', 'kenlm', 'heafield', '2011', 'we', 'built', 'a', 'trigram', 'language', 'model', 'with', 'kneserney', 'smoothing', 'using', 'kenlm', 'toolkit', 'heafield', '2011'], ['all', 'of', 'the', 'text', 'data', 'from', 'reddit', 'was', 'tokenized', 'using', 'the', 'nltk', 'tokenizer', 'bird', 'et', 'al', '2009', 'we', 'tokenise', 'the', 'text', 'using', 'the', 'default', 'tokeniser', 'from', 'nltk', 'bird', 'et', 'al', '2009'], ['our', 'machine', 'translation', 'systems', 'are', 'trained', 'using', 'moses', '3', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'build', 'upon', 'our', 'previous', 'approach', 'for', 'joint', 'concept', 'disambiguation', 'and', 'clustering', 'fahrni', 'and', 'strube', '2012', 'scopeignorant', 'disambig', 'our', 'previous', 'mlnbased', 'approach', 'for', 'concept', 'disambiguation', 'fahrni', 'and', 'strube', '2012'], ['we', 'used', 'adam', 'kingma', 'and', 'ba', '2014', 'with', 'a', 'learning', 'rate', 'of', '00002', 'we', 'use', 'adam', 'kingma', 'and', 'ba', '2014', 'with', 'a', 'learning', 'rate', 'of', '4e4', 'and', 'a', 'batch', 'size', 'of', '32'], ['for', 'all', 'experiments', 'we', 'used', 'the', 'moses', 'smt', 'system', 'koehn', 'et', 'al', '2007', 'for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'first', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'statistical', 'machine', 'translation'], ['for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'develop', 'translation', 'models', 'using', 'the', 'phrasebased', 'moses', 'koehn', 'et', 'al', '2007', 'smt', 'systemwe', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'tnt', 'brants', '2000', '', 'trained', 'on', 'the', 'negra', 'training', 'setwe', 'employed', 'the', 'tnt', 'tagger', 'brants', '2000', 'which', 'was', 'trained', 'on', 'the', 'spective', 'conll', 'training', 'data'], ['the', 'webpages', 'were', 'parsed', 'using', 'the', 'stanford', 'corenlp', 'software', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['we', 'develop', 'translation', 'models', 'using', 'the', 'phrasebased', 'moses', 'koehn', 'et', 'al', '2007', 'smt', 'systemwe', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'term', 'frequency', 'count', 'is', 'normalized', 'with', 'the', 'inverse', 'document', 'frequency', 'in', 'the', 'test', 'collection', 'salton', 'and', 'buckley', '1988', 'tfidf', 'is', 'a', 'standard', 'statistical', 'method', 'that', 'combines', 'the', 'frequency', 'of', 'a', 'term', 'in', 'a', 'particular', 'document', 'with', 'its', 'inverse', 'document', 'frequency', 'in', 'general', 'use', 'salton', 'and', 'buckley', '1988'], ['we', 'assessed', 'the', 'statistical', 'significance', 'of', 'differences', 'in', 'score', 'with', 'an', 'approximate', 'randomization', 'test', '8', 'noreen', '1989', '', 'indicating', 'a', 'significant', 'impact', 'in', 'bold', 'fontwe', 'assess', 'statistical', 'significance', 'of', 'the', 'difference', 'in', 'f', '1', 'score', 'for', 'two', 'approaches', 'via', 'an', 'approximate', 'randomization', 'test', 'noreen', '1989'], ['a', 'framework', 'for', 'human', 'error', 'analysis', 'and', 'error', 'classification', 'has', 'been', 'proposed', 'in', 'vilar', 'et', 'al', '2006', 'and', 'a', 'detailed', 'analysis', 'of', 'the', 'obtained', 'results', 'has', 'been', 'carried', 'outa', 'framework', 'for', 'human', 'error', 'analysis', 'has', 'been', 'proposed', 'in', 'vilar', 'et', 'al', '2006', '', 'but', 'as', 'every', 'human', 'evaluation', 'this', 'is', 'also', 'a', 'time', 'consuming', 'task'], ['for', 'example', 'chang', 'et', 'al', '2009', 'found', 'that', 'the', 'probability', 'of', 'heldout', 'documents', 'is', 'not', 'always', 'a', 'good', 'predictor', 'of', 'human', 'judgmentschang', 'et', 'al', '2009', 'stated', 'that', 'one', 'reason', 'is', 'that', 'the', 'objective', 'function', 'of', 'topic', 'models', 'does', 'not', 'always', 'correlate', 'well', 'with', 'human', 'judgments'], ['on', 'the', 'chinese', 'side', 'we', 'used', 'the', 'morphological', 'analyzer', 'described', 'in', 'kruengkrai', 'et', 'al', '2009', 'trained', 'on', 'the', 'training', 'data', 'of', 'ctb', 'tp', 'to', 'perform', 'word', 'segmentation', 'and', 'pos', 'tagging', 'and', 'used', 'the', 'firstthe', 'mma', 'system', 'kruengkrai', 'et', 'al', '2009', 'trained', 'on', 'the', 'training', 'data', 'was', 'used', 'to', 'perform', 'word', 'segmentation', 'and', 'tagging', 'and', 'the', 'baseline', 'parser', 'was', 'used', 'to', 'parse', 'the', 'sentences', 'in', 'the', 'gigaword', 'corpus'], ['conducted', 'using', 'the', 'moses', 'phrasebased', 'decoder', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['conducted', 'using', 'the', 'moses', 'phrasebased', 'decoder', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'conducted', 'using', 'the', 'moses', 'phrasebased', 'decoder', 'koehn', 'et', 'al', '2007'], ['then', 'the', 'processed', 'data', 'was', 'performed', 'for', 'tokenization', 'pos', 'tagging', 'parsing', 'stemming', 'and', 'lemmatization', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['we', 'applied', 'bootstrap', 'resampling', 'koehn', '2004', 'to', 'measure', 'statistical', 'significance', '', 'p', '', '005', 'of', 'our', 'models', 'compared', 'to', 'a', 'baselinewe', 'measure', 'significance', 'of', 'results', 'using', 'bootstrap', 'resampling', 'at', 'p', '', '005', 'koehn', '2004'], ['this', 'system', 'uses', 'the', 'attentional', 'encoderdecoder', 'architecture', 'described', 'by', 'bahdanau', 'et', 'al', '2015', '', 'building', 'on', 'work', 'by', 'sutskever', 'et', 'al', '2014', 'we', 'followed', 'the', 'encoderdecoder', 'architecture', 'with', 'attention', 'proposed', 'by', 'bahdanau', 'et', 'al', '2015'], ['the', 'language', 'model', 'is', 'a', '5gram', 'kenlm', 'heafield', '2011', 'model', 'trained', 'using', 'lmplz', 'with', 'modified', 'kneserney', 'smoothing', 'and', 'no', 'pruningwe', 'built', 'a', 'trigram', 'language', 'model', 'with', 'kneserney', 'smoothing', 'using', 'kenlm', 'toolkit', 'heafield', '2011'], ['weighted', 'finite', 'state', 'transducers', 'fsts', 'used', 'in', 'our', 'model', 'are', 'constructed', 'with', 'openfst', 'allauzen', 'et', 'al', '2007', 'the', 'decoder', 'is', 'implemented', 'with', 'weighted', 'finite', 'state', 'transducers', 'wfsts', 'using', 'standard', 'operations', 'available', 'in', 'the', 'openfst', 'libraries', 'allauzen', 'et', 'al', '2007'], ['weighted', 'finite', 'state', 'transducers', 'fsts', 'used', 'in', 'our', 'model', 'are', 'constructed', 'with', 'openfst', 'allauzen', 'et', 'al', '2007', 'the', 'decoder', 'is', 'implemented', 'with', 'weighted', 'finite', 'state', 'transducers', 'wfsts', 'using', 'standard', 'operations', 'available', 'in', 'the', 'openfst', 'libraries', 'allauzen', 'et', 'al', '2007'], ['then', 'the', 'processed', 'data', 'was', 'performed', 'for', 'tokenization', 'pos', 'tagging', 'parsing', 'stemming', 'and', 'lemmatization', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['it', 'is', 'a', 'modification', 'of', 'the', 'model', 'proposed', 'by', 'mintz', 'et', 'al', '2009', 'our', 'first', 'baseline', 'is', 'mi09', 'a', 'distantly', 'supervised', 'classifier', 'based', 'on', 'the', 'work', 'of', 'mintz', 'et', 'al', '2009'], ['we', 'use', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', '', 'the', 'machine', 'learning', 'library', 'for', 'python', 'for', 'implementing', 'the', 'different', 'approacheswe', 'used', 'the', 'scikitlearn', 'machine', 'learning', 'library', 'pedregosa', 'et', 'al', '2011', 'for', 'both', 'implementing', 'our', 'classification', 'models', 'and', 'performing', 'statistical', 'feature', 'selection'], ['we', 'use', 'the', 'universal', 'pos', 'tagset', 'upos', 'of', 'petrov', 'et', 'al', '2012', 'this', 'in', 'turn', 'relies', 'on', 'the', 'same', 'underlying', 'feature', 'model', 'typically', 'drawing', 'from', 'a', 'shared', 'partofspeech', 'pos', 'representation', 'such', 'as', 'the', 'universal', 'pos', 'tagset', 'of', 'petrov', 'et', 'al', '2012'], ['we', 'use', 'the', 'universal', 'pos', 'tagset', 'upos', 'of', 'petrov', 'et', 'al', '2012', 'this', 'in', 'turn', 'relies', 'on', 'the', 'same', 'underlying', 'feature', 'model', 'typically', 'drawing', 'from', 'a', 'shared', 'partofspeech', 'pos', 'representation', 'such', 'as', 'the', 'universal', 'pos', 'tagset', 'of', 'petrov', 'et', 'al', '2012'], ['the', 'crf', 'is', 'trained', 'using', 'decisions', 'from', 'the', 'following', 'underlying', 'components', 'bullet', 'madamira', 'is', 'a', 'publicly', 'available', 'tool', 'for', 'morphological', 'analysis', 'and', 'disambiguation', 'of', 'eda', 'and', 'msa', 'text', 'pasha', 'et', 'al', '2014madamira', 'pasha', 'et', 'al', '2014', 'is', 'a', 'morphological', 'analysis', 'and', 'disambiguation', 'tool', 'of', 'arabic'], ['for', 'language', 'modeling', 'we', 'trained', 'a', 'separate', '5gram', 'kneserney', 'smoothed', 'lm', 'model', 'on', 'the', 'target', 'ie', 'english', 'side', 'of', 'the', 'training', 'bitext', 'using', 'kenlm', 'heafield', '2011', 'we', 'built', 'a', 'modified', 'kneser', 'ney', 'smoothed', '5gram', 'language', 'model', 'using', 'the', 'english', 'side', 'of', 'the', 'training', 'data', 'and', 'performed', 'querying', 'with', 'kenlm', 'heafield', '2011', '7'], ['with', 'the', 'training', 'script', 'of', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'preprocessed', 'the', 'training', 'corpora', 'with', 'scripts', 'included', 'in', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['the', 'word', 'alignment', 'was', 'trained', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'configuration', 'growdiagfinaland', 'alignment', 'symmetrization', 'methodwe', 'performed', 'word', 'alignment', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'default', 'growdiagfinaland', 'alignment', 'symmetrization', 'method'], ['we', 'use', 'linear', 'svms', 'from', 'liblinear', 'and', 'svms', 'with', 'rbf', 'kernel', 'from', 'libsvm', 'chang', 'and', 'lin', '2011', 'as', 'our', 'learner', 'we', 'use', 'libsvm', 'with', 'a', 'linear', 'kernel', 'chang', 'and', 'lin', '2011'], ['we', 'specify', 'the', 'hierarchical', 'aligner', 'in', 'terms', 'of', 'a', 'deduction', 'system', 'shieber', 'et', 'al', '1995', 'we', 'specify', 'our', 'dynamic', 'programming', 'algorithm', 'as', 'a', 'deduction', 'system', 'shieber', 'et', 'al', '1995'], ['we', 'use', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parse', 'trees', 'and', 'dependency', 'relationswe', 'use', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'for', 'extracting', 'dependency', 'path', 'and', 'partofspeech', 'features'], ['in', 'this', 'work', 'we', 'use', 'the', 'stanford', 'neural', 'dependency', 'parser', 'chen', 'and', 'manning', '2014', 'in', 'order', 'to', 'detect', 'the', 'object', 'pronouns', 'we', 'employ', 'stanford', 'parser', 'chen', 'and', 'manning', '2014'], ['we', 'use', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parse', 'trees', 'and', 'dependency', 'relationswe', 'also', 'obtain', 'the', 'dependency', 'parse', 'of', 'the', 'sentences', 'using', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006'], ['the', 'learning', 'algorithm', 'used', 'in', 'our', 'coreference', 'engine', 'is', 'c45', 'quinlan', '1993', 'the', 'question', 'classifier', 'used', 'in', 'the', 'experiments', 'is', 'the', 'c45', 'decision', 'tree', 'classifier', 'quinlan', '1993'], ['we', 'use', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parse', 'trees', 'and', 'dependency', 'relationswe', 'use', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'for', 'extracting', 'dependency', 'path', 'and', 'partofspeech', 'features'], ['distributional', 'semantics', 'see', 'cohen', 'and', 'widdows', '2009', 'for', 'an', 'overview', 'is', 'based', 'on', 'the', 'observation', 'that', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'tend', 'to', 'be', 'semantically', 'related', 'harris', '1954', 'the', 'former', 'that', 'is', 'the', 'most', 'popular', 'relies', 'on', 'the', 'distributional', 'hypothesis', 'that', 'puts', 'forward', 'the', 'idea', 'that', 'words', 'with', 'similar', 'meaning', 'tend', 'to', 'occur', 'in', 'similar', 'contexts', 'harris', '1954'], ['their', 'work', 'is', 'part', 'of', 'the', 'stateoftheart', 'arabic', 'morphological', 'tagger', 'madamira', 'pasha', 'et', 'al', '2014', 'madamira', 'pasha', 'et', 'al', '2014', 'is', 'a', 'morphological', 'analysis', 'and', 'disambiguation', 'tool', 'of', 'arabic'], ['we', 'use', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parse', 'trees', 'and', 'dependency', 'relationswe', 'also', 'obtain', 'the', 'dependency', 'parse', 'of', 'the', 'sentences', 'using', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006'], ['phrasal', 'follows', 'the', 'loglinear', 'approach', 'to', 'phrasebased', 'translation', 'och', 'and', 'ney', '2004', 'in', 'which', 'the', 'decision', 'rule', 'has', 'the', 'familiar', 'linear', 'form', '', 'arg', 'max', 'e', 'w', 'e', 'f', '1the', 'loglinear', 'approach', 'to', 'phrasebased', 'translation', 'och', 'and', 'ney', '2004', 'directly', 'models', 'the', 'predictive', 'translation', 'distribution', 'pef', '', 'w', '', '1', 'zf', 'exp', 'w', 'e', 'f', '1', 'where', 'e', 'is', 'the', 'target', 'string'], ['training', 'data', 'are', 'based', 'on', 'a', 'concatenation', 'of', '18', 'postagged', 'english', 'corpora', '2', 'from', 'the', 'childes', 'database', 'macwhinney', '2000', 'both', 'cds', 'corpora', 'are', 'available', 'from', 'the', 'childes', 'database', 'macwhinney', '2000'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['5gram', 'language', 'models', 'of', 'turkish', 'and', 'english', 'were', 'trained', 'using', 'kenlm', 'heafield', '2011', 'we', 'built', 'a', '5gram', 'language', 'model', 'on', 'the', 'english', 'side', 'of', 'qcatrain', 'using', 'kenlm', 'heafield', '2011'], ['we', 'used', 'the', 'relation', 'classification', 'dataset', 'of', 'the', 'semeval', '2010', 'task', '8', 'hendrickx', 'et', 'al', '2010', 'we', 'evaluated', 'our', 'model', 'on', 'a', 'semantic', 'relation', 'classification', 'task', 'semeval', '2010', 'task', '8', 'hendrickx', 'et', 'al', '2010'], ['we', 'then', 'run', 'word', 'alignment', 'with', 'giza', 'och', 'and', 'ney', '2003', 'in', 'both', 'directions', 'with', 'the', 'default', 'parameters', 'used', 'in', 'moseswe', 'performed', 'word', 'alignment', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'default', 'growdiagfinaland', 'alignment', 'symmetrization', 'method'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'all', 'experiments', 'we', 'used', 'the', 'moses', 'smt', 'system', 'koehn', 'et', 'al', '2007'], ['the', 'significance', 'tests', 'were', 'performed', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004', 'the', 'statistical', 'significance', 'tests', 'using', '95', 'confidence', 'interval', 'are', 'measured', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004'], ['all', 'experiments', 'were', 'carried', 'out', 'using', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', '', 'in', 'its', 'standard', 'nonmonotonic', 'configuration', 'the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['these', 'sentences', 'have', 'then', 'be', 'fed', 'into', 'an', 'efficient', 'hpsg', 'parser', 'pet', 'callmeier', '2000', '', 'with', 'erg', 'loadedthe', 'sentences', 'are', 'fed', 'into', 'the', 'pet', 'hpsg', 'parser', 'callmeier', '2000', 'with', 'the', 'gg', 'loaded'], ['the', 'language', 'model', 'is', 'a', '5gram', 'kenlm', 'heafield', '2011', 'model', 'trained', 'using', 'lmplz', 'with', 'modified', 'kneserney', 'smoothing', 'and', 'no', 'pruningthe', '5gram', 'target', 'language', 'model', 'was', 'trained', 'using', 'kenlm', 'heafield', '2011'], ['cohen', 'et', 'al', '2012', '', 'present', 'a', 'spectral', 'algorithm', 'for', 'lpcfg', 'estimation', 'but', 'the', 'transformation', 'of', 'the', 'lpcfg', 'model', 'and', 'its', 'spectral', 'algorithm', 'to', 'rhmms', 'is', 'awkward', 'and', 'opaquefirst', 'we', 'present', 'an', 'algorithm', 'for', 'estimating', 'lpcfgs', 'akin', 'to', 'the', 'spectral', 'algorithm', 'of', 'cohen', 'et', 'al', '2012', '', 'but', 'simpler', 'to', 'understand', 'and', 'implement'], ['distributional', 'hypothesis', 'theory', 'harris', '1954', 'indicates', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'context', 'tend', 'to', 'have', 'similar', 'meaningssimilar', 'row', 'vectors', 'in', 't', 'indicate', 'similar', 'context', 'of', 'two', 'terms', 'in', 'the', 'domain', 'and', 'terms', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meanings', 'harris', '1954'], ['distributional', 'hypothesis', 'theory', 'harris', '1954', 'indicates', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'context', 'tend', 'to', 'have', 'similar', 'meaningssimilar', 'row', 'vectors', 'in', 't', 'indicate', 'similar', 'context', 'of', 'two', 'terms', 'in', 'the', 'domain', 'and', 'terms', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meanings', 'harris', '1954'], ['we', 'use', 'the', 'stanford', 'dependency', 'parser', 'marneffe', 'et', 'al', '2006', 'we', 'first', 'use', 'a', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'parse', 'each', 'sentence', 'and', 'extract', 'the', 'set', 'of', 'dependency', 'relations', 'associated', 'with', 'the', 'sentence'], ['we', 'also', 'replicated', 'the', 'experiment', 'of', 'holmqvist', 'et', 'al', '2012', 'on', 'this', 'datasetwe', 'also', 'carried', 'out', 'a', 'chunkreordering', 'pbsmt', 'experiment', 'where', 'the', 'chunks', 'are', 'reordered', 'based', 'on', 'the', 'final', 'alignments', 'obtained', 'by', '1pass', 'experiment', 'of', 'holmqvist', 'et', 'al', '2012'], ['we', 'develop', 'translation', 'models', 'using', 'the', 'phrasebased', 'moses', 'koehn', 'et', 'al', '2007', 'smt', 'systemconducted', 'using', 'the', 'moses', 'phrasebased', 'decoder', 'koehn', 'et', 'al', '2007'], ['pos', 'tagging', 'was', 'performed', 'with', 'treetagger', 'schmid', '1994', 'the', 'test', 'set', 'was', 'tagged', 'with', 'the', 'french', 'treetagger', 'schmid', '1994'], ['we', 'used', 'the', 'maltparser', 'nivre', 'et', 'al', '2007', 'for', 'parsing', 'experimentsfor', 'the', 'parsing', 'experimens', 'i', 'used', 'maltparser', 'nivre', 'et', 'al', '2007', '', 'version', '181'], ['classification', 'uses', 'the', 'scikitlearn', 'python', 'package', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'a', 'gb', 'implementation', 'of', 'the', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011'], ['we', 'use', 'the', 'stanford', 'dependency', 'parser', 'marneffe', 'et', 'al', '2006', 'these', 'features', 'were', 'obtained', 'using', 'the', 'stanford', 'parser', '2', 'marneffe', 'et', 'al', '2006'], ['we', 'used', 'adam', 'kingma', 'and', 'ba', '2014', 'with', 'a', 'learning', 'rate', 'of', '00002we', 'used', 'adam', 'as', 'the', 'optimizer', 'kingma', 'and', 'ba', '2014'], ['word', 'alignment', 'is', 'performed', 'using', 'giza', 'och', 'and', 'ney', '2003', 'this', 'is', 'done', 'using', 'ibm', 'model', '1', 'brown', 'et', 'al', '1993', 'and', 'giza', 'och', 'and', 'ney', '2003'], ['the', 'test', 'set', 'was', 'tagged', 'with', 'the', 'french', 'treetagger', 'schmid', '1994', 'for', 'the', 'french', 'side', 'the', 'treetagger', 'schmid', '1994', 'was', 'used'], ['for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'conducted', 'using', 'the', 'moses', 'phrasebased', 'decoder', 'koehn', 'et', 'al', '2007'], ['pos', 'tagging', 'was', 'performed', 'with', 'treetagger', 'schmid', '1994', 'pos', 'tagging', 'is', 'performed', 'using', 'the', 'ims', 'tree', 'tagger', 'schmid', '1994'], ['for', 'building', 'the', 'word', 'alignment', 'models', 'we', 'use', 'mgiza', 'gao', 'and', 'vogel', '2008', 'for', 'word', 'alignments', 'we', 'used', 'mgiza', 'gao', 'and', 'vogel', '2008'], ['the', 'polish', 'data', 'is', 'taken', 'from', 'the', 'europarl', 'corpus', 'koehn', '2005', 'all', 'the', 'data', 'come', 'from', 'europarl', 'koehn', '2005'], ['the', 'germantoenglish', 'corpus', 'is', 'europarl', 'v7', 'koehn', '2005', 'the', 'polish', 'data', 'is', 'taken', 'from', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['distributional', 'models', 'of', 'meaning', 'follow', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'which', 'states', 'that', 'two', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'have', 'similar', 'meaningsdistributional', 'similarity', 'relies', 'on', 'the', 'distributional', 'hypothesis', 'that', 'similar', 'terms', 'appear', 'in', 'similar', 'contexts', 'harris', '1954'], ['conducted', 'using', 'the', 'moses', 'phrasebased', 'decoder', 'koehn', 'et', 'al', '2007', 'the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['europarl', '2', 'koehn', '2005', '', 'it', 'is', 'a', 'corpus', 'of', 'parallel', 'texts', 'in', '11', 'languages', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament', 'in', 'our', 'mt', 'experiments', 'we', 'translate', 'french', 'into', 'spanish', 'and', 'use', 'the', 'following', 'corpora', 'to', 'learn', 'our', 'translation', 'systems', 'bullet', 'europarl', 'corpus', 'koehn', '2005', '', 'the', 'europarl', 'parallel', 'corpus', 'is', 'extract'], ['we', 'conducted', 'statistical', 'significance', 'tests', 'for', 'bleu', 'between', 'our', 'best', 'domainadapted', 'system', 'the', 'baseline', 'and', 'the', 'three', 'thirdparty', 'systems', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'with', '1000', 'statistical', 'significance', 'is', 'tested', 'on', 'the', 'bleu', 'metric', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'with', 'n', '', '1000', 'and', 'p', '', '005'], ['the', 'english', 'side', 'was', 'tokenized', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'will', 'be', 'created', 'by', 'the', 'moses', 'smt', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'develop', 'translation', 'models', 'using', 'the', 'phrasebased', 'moses', 'koehn', 'et', 'al', '2007', 'smt', 'systemwe', 'use', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'implement', 'our', 'models'], ['the', 'baseline', 'will', 'be', 'created', 'by', 'the', 'moses', 'smt', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['the', 'resulting', 'matrix', 'is', 'weighted', 'using', 'pointwise', 'mutual', 'information', 'church', 'and', 'hanks', '1990', 'a', 'popular', 'measure', 'of', 'this', 'association', 'is', 'pointwise', 'mutual', 'information', 'pmi', 'church', 'and', 'hanks', '1990'], ['the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'questions', 'are', 'translated', 'using', 'a', 'phrasebased', 'system', 'built', 'using', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'mo', 'set'], ['the', 'morphosyntactic', 'tagging', 'has', 'been', 'made', 'with', 'the', 'tree', 'tagger', 'schmid', '1994', 'pos', 'tagging', 'is', 'performed', 'using', 'the', 'ims', 'tree', 'tagger', 'schmid', '1994'], ['for', 'the', 'determination', 'of', 'pos', 'tags', 'we', 'use', 'the', 'stuttgart', 'treetagger', 'schmid', '1994', 'for', 'both', 'we', 'use', 'treetagger', 'schmid', '1994', 'with', 'language', 'dependent', 'models', 'ie', 'french', 'model', 'for', 'french', 'texts', 'english', 'for', 'english', 'texts'], ['the', 'polish', 'data', 'is', 'taken', 'from', 'the', 'europarl', 'corpus', 'koehn', '2005', 'the', 'europarl', 'corpus', 'koehn', '2005', 'is', 'built', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['all', 'annotations', 'were', 'done', 'using', 'the', 'brat', 'rapid', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012', 'the', 'annotation', 'was', 'performed', 'using', 'the', 'brat', '2', 'tool', 'stenetorp', 'et', 'al', '2012'], ['the', 'spanishenglish', 's2e', 'training', 'corpus', 'was', 'drawn', 'from', 'the', 'europarl', 'collection', 'koehn', '2005', 'the', 'polish', 'data', 'is', 'taken', 'from', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'used', 'tnt', 'brants', '2000', '', 'trained', 'on', 'the', 'negra', 'training', 'sethcrc', 'is', 'tagged', 'with', 'tnt', 'brants', '2000', '', 'trained', 'on', 'the', 'full', 'ptb'], ['for', 'all', 'experiments', 'we', 'used', 'the', 'moses', 'smt', 'system', 'koehn', 'et', 'al', '2007', 'for', 'germanenglish', 'we', 'also', 'have', 'a', 'system', 'based', 'on', 'moses', 'koehn', 'et', 'al', '2007'], ['two', 'basenp', 'data', 'sets', 'have', 'been', 'put', 'forward', 'by', 'ramshaw', 'and', 'marcus', '1995', 'an', 'alternative', 'representation', 'for', 'basenps', 'has', 'been', 'put', 'forward', 'by', 'ramshaw', 'and', 'marcus', '1995'], ['both', 'of', 'our', 'systems', 'were', 'based', 'on', 'the', 'moses', 'decoder', 'koehn', 'et', 'al', '2007', 'the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'will', 'be', 'created', 'by', 'the', 'moses', 'smt', 'toolkit', 'koehn', 'et', 'al', '2007'], ['corpusbased', 'meaning', 'representations', 'rely', 'on', 'the', 'distributional', 'hypothesis', 'which', 'assumes', 'that', 'words', 'occurring', 'in', 'a', 'similar', 'set', 'of', 'contexts', 'are', 'also', 'similar', 'in', 'meaning', 'harris', '1954', 'distributional', 'models', 'of', 'meaning', 'follow', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'which', 'states', 'that', 'two', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'have', 'similar', 'meanings'], ['recently', 'naim', 'et', 'al', '2014', '', 'proposed', 'an', 'unsupervised', 'learning', 'algorithm', 'for', 'automatically', 'aligning', 'sentences', 'in', 'a', 'document', 'with', 'corresponding', 'video', 'segmentsrecently', 'naim', 'et', 'al', '2014', 'proposed', 'a', 'fully', 'unsupervised', 'approach', 'for', 'aligning', 'wetlab', 'experiment', 'videos', 'with', 'associated', 'text', 'protocols', 'without', 'any', 'direct', 'supervision'], ['a', 'distributional', 'similarity', 'model', 'is', 'constructed', 'based', 'on', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'words', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'share', 'similar', 'meaningsdistributional', 'models', 'of', 'meaning', 'follow', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'which', 'states', 'that', 'two', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'have', 'similar', 'meanings'], ['this', 'paper', 'describes', 'the', 'details', 'of', 'our', 'system', 'that', 'participated', 'in', 'the', 'subtask', 'a', 'of', 'semeval2014', 'task', '9', 'sentiment', 'analysis', 'in', 'twitter', 'rosenthal', 'et', 'al', '2014', 'in', 'the', 'following', 'we', 'will', 'describe', 'the', 'system', 'with', 'which', 'we', 'participated', 'in', 'the', 'message', 'polarity', 'classification', 'subtask', 'of', 'sentiment', 'analysis', 'in', 'twitter', 'task', '9', 'of', 'semeval', '2014', 'rosenthal', 'et', 'al', '2014'], ['we', 'used', 'the', 'phrasebased', 'model', 'moses', 'koehn', 'et', 'al', '2007', 'for', 'the', 'experiments', 'with', 'all', 'the', 'standard', 'settings', 'including', 'a', 'lexicalized', 'reordering', 'model', 'and', 'a', '5gram', 'language', 'modelit', 'was', 'built', 'with', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'using', 'the', '14', 'standard', 'core', 'features', 'including', 'a', '5gram', 'language', 'model'], ['combinatory', 'categorial', 'grammar', 'ccg', 'is', 'a', 'linguistic', 'formalism', 'that', 'represents', 'both', 'the', 'syntax', 'and', 'semantics', 'of', 'language', 'steedman', '1996', 'combinatory', 'categorial', 'grammar', 'ccg', 'steedman', '1996', 'is', 'a', 'lexicalized', 'grammar', 'formalism', 'that', 'has', 'been', 'used', 'for', 'both', 'broad', 'coverage', 'syntactic', 'parsing', 'and', 'semantic', 'parsing'], ['we', 'used', 'the', 'random', 'forests', 'implementation', 'of', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011', 'with', '50', 'estimatorswe', 'used', 'the', 'linear', 'svm', 'implementation', 'with', 'default', 'parameters', 'and', 'random', 'forest', 'with', '50', 'trees', 'both', 'available', 'at', 'scikitlearn', 'pedregosa', 'et', 'al', '2011'], ['the', 'proposed', 'model', 'extends', 'the', 'lda', 'framework', 'of', 'blei', 'et', 'al', '2003', 'the', 'article', 'of', 'blei', 'et', 'al', '2003', 'compares', 'lda', 'with', 'plsi', 'and', 'mixture', 'unigram', 'models', 'using', 'the', 'perplexity', 'of', 'the', 'model'], ['we', 'used', 'the', 'training', 'section', 'of', 'the', 'dataset', 'from', 'gimpel', 'et', 'al', '2011', 'gimpel', 'et', 'al', '2011', '', 'provided', 'a', 'dataset', 'of', 'postagged', 'tweets', 'consisting', 'almost', 'entirely', 'of', 'tweets', 'sampled', 'from', 'one', 'particular', 'day', 'october', '27', '2010'], ['5gram', 'language', 'models', 'of', 'turkish', 'and', 'english', 'were', 'trained', 'using', 'kenlm', 'heafield', '2011', 'in', 'order', 'to', 'evaluate', 'the', 'fluency', 'of', 'each', 'system', 'we', 'train', '5gram', 'language', 'models', 'for', 'each', 'language', 'using', 'kenlm', 'heafield', '2011'], ['the', 'corpora', 'are', 'first', 'tokenized', 'and', 'lowercased', 'using', 'the', 'moses', 'scripts', 'then', 'lemmatized', 'and', 'tagged', 'by', 'partofspeech', 'pos', 'using', 'the', 'treetagger', 'schmid', '1994', 'the', 'corpus', 'was', 'then', 'automatically', 'tagged', 'with', 'partofspeech', 'information', 'using', 'treetagger', 'schmid', '1994'], ['the', 'statistical', 'significance', 'test', 'is', 'also', 'carried', 'out', 'with', 'paired', 'bootstrap', 'resampling', 'method', 'with', 'p', '', '0001', 'intervals', 'koehn', '2004', 'the', 'improvement', 'is', 'statistically', 'significant', 'according', 'to', 'paired', 'bootstrap', 'resampling', 'test', 'koehn', '2004'], ['the', 'realisation', 'ranking', 'component', 'is', 'an', 'svm', 'ranking', 'model', 'implemented', 'with', 'svmrank', 'a', 'support', 'vector', 'machinebased', 'learning', 'tool', 'joachims', '2006', 'the', 'learner', 'is', 'implemented', 'as', 'a', 'ranking', 'component', 'trained', 'with', 'svmrank', 'joachims', '2006'], ['the', 'statistical', 'significance', 'test', 'is', 'also', 'carried', 'out', 'with', 'paired', 'bootstrap', 'resampling', 'method', 'with', 'p', '', '0001', 'intervals', 'koehn', '2004', 'a', 'statistical', 'significance', 'test', 'was', 'performed', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004'], ['we', 'use', 'the', 'nltk', 'toolkit', 'loper', 'and', 'bird', '2002', 'to', 'extract', 'the', 'numerical', 'quantity', 'from', 'each', 'sentencewe', 'use', 'the', 'punkt', 'sentence', 'splitter', 'from', 'nltk', 'loper', 'and', 'bird', '2002', 'to', 'perform', 'both', 'sentence', 'and', 'word', 'segmentation', 'on', 'each', 'text', 'chunk'], ['we', 'mark', 'the', 'source', 'tokens', 'to', 'which', 'each', 'target', 'unk', 'symbol', 'is', 'most', 'aligned', 'with', 'the', 'method', 'of', 'luong', 'et', 'al', '2015', 'we', 'find', 'this', 'method', 'provides', 'an', 'additional', '15', 'bleu', 'points', 'which', 'is', 'consistent', 'with', 'the', 'conclusion', 'in', 'luong', 'et', 'al', '2015'], ['we', 'rely', 'on', 'the', 'hybrid', 'aligned', 'lexical', 'semantic', 'resource', 'proposed', 'by', 'faralli', 'et', 'al', '2016', 'to', 'perform', 'wsdin', 'particular', 'the', 'contribution', 'of', 'this', 'paper', 'is', 'a', 'new', 'unsupervised', 'knowledgebased', 'approach', 'to', 'wsd', 'based', 'on', 'the', 'hybrid', 'aligned', 'resource', 'har', 'introduced', 'by', 'faralli', 'et', 'al', '2016'], ['to', 'help', 'improve', 'the', 'information', 'extraction', 'tools', 'a', 'corpus', '', 'called', 'bioscope', 'has', 'been', 'annotated', 'for', 'speculation', 'negation', 'and', 'its', 'linguistic', 'scopes', 'in', 'biomedical', 'texts', 'szarvas', 'et', 'al', '2008', 'the', 'bioscope', 'corpus', 'is', 'a', 'manually', 'annotated', 'corpus', 'for', 'speculation', 'and', 'negation', 'keywords', 'token', 'level', 'and', 'their', 'linguistic', 'scopes', 'sentence', 'level', 'szarvas', 'et', 'al', '2008'], ['the', 'module', 'of', 'coreference', 'resolution', 'included', 'in', 'the', 'ixa', 'pipeline', 'is', 'loosely', 'based', 'on', 'the', 'stanford', 'multi', 'sieve', 'pass', 'system', 'lee', 'et', 'al', '2013', 'we', 'apply', 'the', 'stanford', 'coreference', 'resolution', 'system', 'lee', 'et', 'al', '2013'], ['we', 'run', 'our', 'experiments', 'on', 'europarl', 'koehn', '2005', '', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamenteuroparl', '2', 'koehn', '2005', '', 'it', 'is', 'a', 'corpus', 'of', 'parallel', 'texts', 'in', '11', 'languages', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['we', 'used', 'the', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'implementation', 'of', 'these', 'classifiers', 'with', 'their', 'default', 'parameter', 'settings', 'for', 'our', 'experimentswe', 'used', 'a', 'gb', 'implementation', 'of', 'the', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011'], ['we', 'used', 'the', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'implementation', 'of', 'these', 'classifiers', 'with', 'their', 'default', 'parameter', 'settings', 'for', 'our', 'experimentswe', 'used', 'the', 'scikitlearn', 'toolkit', 'to', 'train', 'our', 'classifiers', 'pedregosa', 'et', 'al', '2011'], ['automatic', 'multidocument', 'summarization', 'mds', 'aims', 'at', 'selecting', 'the', 'relevant', 'information', 'from', 'multiple', 'documents', 'on', 'the', 'same', 'topic', 'to', 'produce', 'a', 'summary', 'mani', '2001', 'automatic', 'text', 'summarization', 'aims', 'to', 'automatically', 'produce', 'a', 'short', 'and', 'wellorganized', 'summary', 'of', 'single', 'or', 'multiple', 'documents', 'mani', '2001'], ['additionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'the', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'implementation', 'of', 'these', 'classifiers', 'with', 'their', 'default', 'parameter', 'settings', 'for', 'our', 'experimentswe', 'used', 'the', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'implementation', 'of', 'svrs', 'and', 'the', 'skll', 'toolkit'], ['additionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'the', 'phrasebased', 'smt', 'system', 'we', 'adopted', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'train', 'classifiers', 'for', 'each', 'of', 'the', 'above', 'feature', 'types', 'and', 'for', 'the', 'full', 'feature', 'set', 'on', 'the', 'training', 'set', 'of', 'each', 'corpus', 'using', 'the', 'default', 'configuration', 'of', 'the', 'naive', 'bayes', 'implementation', 'of', 'weka', '', 'hall', 'et', 'al', '2009', 'we', 'train', 'random', 'forest', 'classifiers', 'breiman', '2001', 'using', 'weka', 'hall', 'et', 'al', '2009', 'for', 'each', 'step', 'and', 'also', 'for', 'the', 'joint', 'model'], ['in', 'addition', 'the', 'corpus', 'was', 'lemmatised', 'using', 'the', 'treetagger', 'lemmatizer', 'schmid', '1994', '1', 'the', 'corpus', 'is', 'lemmatised', 'and', 'tagged', 'by', 'partofspeech', 'on', 'both', 'sides', 'using', 'the', 'treetagger', 'schmid', '1994'], ['they', 'are', 'based', 'on', 'distributional', 'hypothesis', 'which', 'works', 'under', 'the', 'assumption', 'that', 'similar', 'words', 'occur', 'in', 'similar', 'contexts', 'harris', '1954', 'distributional', 'models', 'of', 'meaning', 'follow', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'which', 'states', 'that', 'two', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'have', 'similar', 'meanings'], ['we', 'used', 'the', 'same', 'test', 'set', 'used', 'in', 'li', 'et', 'al', '2004', 'for', 'our', 'testing', '5', 'but', 'we', 'randomly', 'selected', '90', 'of', 'the', 'training', 'data', 'used', 'in', 'li', 'et', 'al', '2004', 'as', 'our', 'training', 'data', 'and', 'the', 'remainder', 'as', 'the', 'development', 'data', 'as', 'shown', 'in', 'table', '5'], ['we', 'develop', 'translation', 'models', 'using', 'the', 'phrasebased', 'moses', 'koehn', 'et', 'al', '2007', 'smt', 'systemadditionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'nonlocal', 'features', 'based', 'on', 'finkel', 'et', 'al', '2005', 'for', 'example', 'finkel', 'et', 'al', '2005', 'enabled', 'the', 'use', 'of', 'nonlocal', 'features', 'by', 'using', 'gibbs', 'sampling'], ['for', 'building', 'the', 'baseline', 'smt', 'system', 'we', 'used', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', '', 'in', 'its', 'standard', 'setupfor', 'all', 'experiments', 'we', 'used', 'the', 'moses', 'smt', 'system', 'koehn', 'et', 'al', '2007'], ['for', 'translation', 'we', 'use', 'moses', 'koehn', 'et', 'al', '2007', 'with', 'lexicalized', 'reordering', 'step', 'and', 'the', 'proposed', 'model', 'with', 'latent', 'derivations', 'laderfor', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['to', 'recognize', 'explicit', 'connectives', 'we', 'construct', 'a', 'list', 'of', 'existing', 'connectives', 'labeled', 'in', 'the', 'penn', 'discourse', 'treebank', 'prasad', 'et', 'al', '2008a', 'in', 'addition', 'we', 'show', 'that', 'the', 'latent', 'representation', 'coheres', 'well', 'with', 'the', 'characterization', 'of', 'discourse', 'connectives', 'in', 'the', 'penn', 'discourse', 'treebank', 'prasad', 'et', 'al', '2008'], ['to', 'construct', 'language', 'models', 'and', 'measure', 'perplexity', '', 'we', 'use', 'srilm', 'stolcke', '2002', 'with', 'interpolated', 'modified', 'kneserney', 'discounting', 'chen', 'and', 'goodman', '1996', 'and', 'with', 'a', 'fixed', 'vocabulary', 'both', 'language', 'models', 'use', 'modified', 'kneserney', 'smoothing', 'chen', 'and', 'goodman', '1996'], ['we', 'use', 'gibbs', 'sampling', 'to', 'estimate', 'the', 'distributions', 'of', 'n', 'and', 'm', '', 'integrating', 'out', 'the', 'multinomial', 'parameters', 'griffiths', 'and', 'steyvers', '2004', 'we', 'use', 'the', 'gibbs', 'sampling', 'based', 'lda', 'griffiths', 'and', 'steyvers', '2004'], ['arabizi', 'is', 'not', 'a', 'letterbased', 'transliteration', 'from', 'the', 'arabic', 'script', 'as', 'is', 'for', 'example', 'the', 'buckwalter', 'transliteration', 'buckwalter', '2004', '1', 'arabic', 'transliteration', 'is', 'presented', 'in', 'the', 'buckwalter', 'scheme', 'buckwalter', '2004'], ['to', 'construct', 'language', 'models', 'and', 'measure', 'perplexity', '', 'we', 'use', 'srilm', 'stolcke', '2002', 'with', 'interpolated', 'modified', 'kneserney', 'discounting', 'chen', 'and', 'goodman', '1996', 'and', 'with', 'a', 'fixed', 'vocabulary', 'both', 'language', 'models', 'use', 'modified', 'kneserney', 'smoothing', 'chen', 'and', 'goodman', '1996'], ['we', 'build', 'upon', 'our', 'previous', 'markov', 'logic', 'based', 'approach', 'for', 'joint', 'concept', 'disambiguation', 'and', 'clustering', 'fahrni', 'and', 'strube', '2012', 'scopeignorant', 'disambig', 'our', 'previous', 'mlnbased', 'approach', 'for', 'concept', 'disambiguation', 'fahrni', 'and', 'strube', '2012'], ['we', 'use', 'the', 'feedforward', 'neural', 'probabilistic', 'language', 'model', 'architecture', 'of', 'vaswani', 'et', 'al', '2013', '', 'as', 'shown', 'in', 'figure', '4', 'we', 'follow', 'the', 'neural', 'network', 'architecture', 'of', 'vaswani', 'et', 'al', '2013', '', 'using', 'two', 'hidden', 'layers', 'of', 'rectified', 'linear', 'units'], ['the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'trained', 'a', 'number', 'of', 'frenchenglish', 'smt', 'systems', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'in', 'its', 'default', 'setting'], ['for', 'building', 'the', 'baseline', 'smt', 'system', 'we', 'used', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', '', 'in', 'its', 'standard', 'setupfor', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'smt', 'translations', 'a', 'phrasebased', 'smt', 'system', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'and', 'the', 'whole', 'spanishenglish', 'dataset', 'except', 'the', 'sentences', 'in', 'the', 'test', 'set', 'was', 'used', 'to', 'translated', 'the'], ['following', 'resource', 'collection', 'and', 'construction', 'a', 'smt', 'model', 'for', 'englishbrazilianportuguese', 'was', 'trained', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'using', 'its', 'baseline', 'settingswe', 'trained', 'a', 'model', 'using', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'on', 'the', 'training', 'data', 'as', 'our', 'baseline', 'system'], ['one', 'semiautomatic', 'approach', 'to', 'evaluation', 'is', 'rouge', 'lin', 'and', 'hovy', '2003', '', 'which', 'is', 'primarily', 'based', 'on', 'ngram', 'cooccurrence', 'between', 'automatic', 'and', 'human', 'summariesthe', 'method', 'rouge', 'lin', 'and', 'hovy', '2003', '', 'is', 'based', 'on', 'ngram', 'overlap', 'between', 'the', 'systemproduced', 'and', 'ideal', 'summaries'], ['we', 'use', 'the', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'to', 'compute', 'the', 'language', 'model', 'score', 'for', 'a', 'sentencewe', 'used', 'the', 'unigram', 'counts', 'from', 'the', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'to', 'determine', 'the', 'frequency', 'of', 'use', 'of', 'each', 'candidate', 'synonym'], ['the', 'semeval2015', 'aspect', 'based', 'sentiment', 'analysis', 'task', 'is', 'a', 'continuation', 'of', 'semeval2014', 'task', '4', 'pontiki', 'et', 'al', '2014', 'this', 'paper', 'describes', 'the', 'approach', 'of', 'the', 'semantic', 'analyis', 'project', 'snap', 'to', 'task', '4', 'of', 'semeval', '2014', 'aspect', 'based', 'sentiment', 'analysis', 'pontiki', 'et', 'al', '2014'], ['the', '2009', 'bio', 'nlp', 'shared', 'task', 'kim', 'et', 'al', '2009', 'aimed', 'at', 'extracting', 'biologicalevents', 'where', 'one', 'of', 'the', 'event', 'types', 'was', 'gene', 'expressionthe', 'recent', 'bionlp', '2009', 'shared', 'task', 'bionlp09st', 'on', 'event', 'extraction', 'kim', 'et', 'al', '2009', 'focused', 'on', 'event', 'types', 'of', 'varying', 'complexity'], ['the', 'constituent', 'context', 'model', 'ccm', 'for', 'inducing', 'constituency', 'parses', 'klein', 'and', 'manning', '2002', 'was', 'the', 'first', 'unsupervised', 'approach', 'to', 'surpass', 'a', 'rightbranching', 'baseline', 'the', 'ccm', 'is', 'a', 'generative', 'model', 'for', 'the', 'unsupervised', 'induction', 'of', 'binary', 'constituency', 'parses', 'over', 'sequences', 'of', 'partofspeech', 'pos', 'tags', 'klein', 'and', 'manning', '2002'], ['the', 'ccm', 'is', 'a', 'generative', 'model', 'for', 'the', 'unsupervised', 'induction', 'of', 'binary', 'constituency', 'parses', 'over', 'sequences', 'of', 'partofspeech', 'pos', 'tags', 'klein', 'and', 'manning', '2002', 'the', 'idea', 'of', 'representing', 'a', 'constituent', 'by', 'its', 'yield', 'and', 'a', 'different', 'definition', 'of', 'context', 'is', 'used', 'by', 'the', 'ccm', 'unsupervised', 'parsing', 'model', 'klein', 'and', 'manning', '2002'], ['we', 'use', 'the', 'implementation', 'provided', 'by', 'tai', 'et', 'al', '2015', '', 'changing', 'only', 'the', 'dependency', 'parses', 'that', 'are', 'fed', 'to', 'their', 'modelwe', 'use', 'the', 'dependency', 'tree', 'long', 'shortterm', 'memory', 'network', 'treelstm', 'proposed', 'by', 'tai', 'et', 'al', '2015', '', 'simply', 'replacing', 'their', 'default', 'dependency', 'parser', 'with', 'our', 'version', 'that', 'maps', 'unseen', 'words'], ['europarl', 'koehn', '2005', 'is', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'use', 'the', 'french', 'english', 'parallel', 'corpus', 'approximately', '12', 'million', 'sentences', 'from', 'the', 'corpus', 'of', 'european', 'parliamentary', 'proceedings', 'koehn', '2005', 'as', 'the', 'data', 'on', 'which', 'pivoting', 'is', 'performed', 'to', 'text'], ['the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'is', 'used', 'for', 'extracting', 'features', 'from', 'the', 'dependency', 'parse', 'treeswe', 'used', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'parse', 'the', 'sentences', 'that', 'contain', 'a', 'candidate', 'speculation', 'keyword', 'and', 'extracted', 'the', 'following', 'features', 'from', 'the', 'dependency', 'parse', 'trees'], ['the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'is', 'used', 'for', 'extracting', 'features', 'from', 'the', 'dependency', 'parse', 'treeswe', 'used', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'parse', 'the', 'sentences', 'that', 'contain', 'a', 'candidate', 'speculation', 'keyword', 'and', 'extracted', 'the', 'following', 'features', 'from', 'the', 'dependency', 'parse', 'trees'], ['the', 'major', 'part', 'of', 'data', 'comes', 'from', 'current', 'and', 'upcoming', 'full', 'releases', 'of', 'the', 'europarl', 'data', 'set', 'koehn', '2005', 'the', 'data', 'used', 'for', 'the', 'experiments', 'described', 'in', 'this', 'paper', 'comes', 'predominantly', 'from', 'bible', 'translations', '', 'wikipedia', 'and', 'the', 'europarl', 'corpus', 'of', 'european', 'parliamentary', 'proceedings', 'koehn', '2005'], ['we', 'compare', 'the', 'proposed', 'model', 'to', 'our', 'implementation', 'of', 'the', 'iobesbased', 'model', 'described', 'in', 'collobert', 'et', 'al', '2011', '', 'applied', 'to', 'mwe', 'tagging', 'the', 'model', 'architecture', 'shown', 'in', 'figure', '1', '', 'is', 'a', 'slight', 'variant', 'of', 'the', 'cnn', 'architecture', 'of', 'collobert', 'et', 'al', '2011'], ['it', 'is', 'a', 'phrasebased', 'system', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'and', 'trainedtuned', 'using', 'only', 'the', 'preprocessed', 'tokenised', 'lowercased', 'parallel', 'data', 'provided', 'for', 'the', 'shared', 'tasksmt', 'translations', 'a', 'phrasebased', 'smt', 'system', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'and', 'the', 'whole', 'spanishenglish', 'dataset', 'except', 'the', 'sentences', 'in', 'the', 'test', 'set', 'was', 'used', 'to', 'translated', 'the', 'shared', 'task'], ['in', 'this', 'section', 'we', 'first', 'discuss', 'the', 'hybrid', 'tree', 'model', 'of', 'lu', 'et', 'al', '2008', '', 'and', 'introduce', 'a', 'novel', 'extensionin', 'lu', 'et', 'al', '2008', '', 'the', 'mixgram', 'model', 'an', 'interpolation', 'between', 'the', 'unigram', 'model', 'and', 'the', 'bigram', 'model', 'was', 'also', 'considered', 'when', 'parsing', 'novel', 'sentences', 'which', 'yielded', 'a', 'better', 'performance'], ['the', 'europarl', 'corpus', 'koehn', '2005', 'is', 'built', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'use', 'the', 'french', 'english', 'parallel', 'corpus', 'approximately', '12', 'million', 'sentences', 'from', 'the', 'corpus', 'of', 'european', 'parliamentary', 'proceedings', 'koehn', '2005', 'as', 'the', 'data', 'on', 'which', 'pivoting', 'is', 'performed', 'to', 'text'], ['the', 'europarl', 'corpus', 'koehn', '2005', 'is', 'built', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'use', 'the', 'french', 'english', 'parallel', 'corpus', 'approximately', '12', 'million', 'sentences', 'from', 'the', 'corpus', 'of', 'european', 'parliamentary', 'proceedings', 'koehn', '2005', 'as', 'the', 'data', 'on', 'which', 'pivoting', 'is', 'performed', 'to', 'text'], ['we', 'use', 'adam', 'kingma', 'and', 'ba', '2015', 'for', 'optimisation', 'with', 'initial', 'learning', 'rate', 'of', '0001each', 'model', 'was', 'trained', 'during', '50', 'epochs', 'using', 'the', 'adaptive', 'variant', 'of', 'stochastic', 'gradient', 'descent', 'adam', 'kingma', 'and', 'ba', '2015', 'with', 'an', 'initial', 'learning', 'rate', 'of', '0001'], ['we', 'use', 'the', 'opensource', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'build', 'a', 'standard', 'phrasebased', 'smt', 'system', 'which', 'extracts', 'up', 'to', '8', 'words', 'phrases', 'in', 'the', 'moses', 'phrase', 'tablewe', 'use', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'implement', 'our', 'models'], ['we', 'used', 'the', 'mkcls', 'tool', 'in', 'giza', 'och', 'and', 'ney', '2003', 'to', 'learn', 'the', 'word', 'classeswe', 'also', 'used', 'giza', 'word', 'alignment', 'tool', 'och', 'and', 'ney', '2003', 'on', 'the', 'same', 'files', 'and', 'collected', 'figures', 'pertaining', 'to', 'the', 'alignment', 'of', 'proper', 'names', 'in', 'hindi', 'and', 'english'], ['we', 'then', 'use', 'the', 'phrase', 'extraction', 'utility', 'in', 'the', 'moses', 'statistical', 'machine', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'extract', 'a', 'phrase', 'table', 'which', 'operates', 'over', 'characters', 'we', 'use', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'implement', 'our', 'models'], ['phrase', 'pairs', 'are', 'extracted', 'from', 'ibm4', 'alignments', 'obtained', 'with', 'giza', 'och', 'and', 'ney', '2003', 'phrase', 'pairs', 'were', 'extracted', 'from', 'symmetrized', 'word', 'alignments', 'and', 'distortions', 'generated', 'by', 'giza', 'och', 'and', 'ney', '2003', 'using', 'the', 'combination', 'of', 'heuristics', 'growdiagfinaland', 'and', 'msdbidirectiona'], ['specifically', 'we', 'build', 'off', 'the', 'bayesian', 'block', 'hmms', 'used', 'by', 'ritter', 'et', 'al', '2010', 'for', 'modeling', 'twitter', 'conversations', 'which', 'will', 'be', 'our', 'primary', 'baselineour', 'work', 'is', 'motivated', 'by', 'the', 'bayesian', 'hmm', 'approach', 'of', 'ritter', 'et', 'al', '2010', '', '', 'the', 'model', 'we', 'refer', 'to', 'as', 'the', 'block', 'hmm', 'bhmm', '', 'and', 'we', 'consider', 'this', 'our', 'primary', 'baseline'], ['like', 'cho', '', 'chai', '2000', '', 'our', 'analysis', 'also', 'provides', 'the', 'same', 'explanation', 'for', 'various', 'scrambled', 'sentences', 'such', 'as', 'the', 'double', 'accusative', 'construction', 'dac', 'in', 'koreangiven', 'the', 'lp', 'constraints', 'in', '4', '7', 'and', '8', 'we', 'can', 'provide', 'a', 'simpler', 'explanation', 'to', 'various', 'scrambled', 'sentences', 'including', 'the', 'alleged', 'counterexamples', 'to', 'the', 'analysis', 'of', 'cho', '', 'chai', '2000'], ['we', 'propose', 'a', 'relatively', 'simple', 'and', 'nuanced', 'unsupervised', 'model', 'inspired', 'by', 'the', 'monolingual', 'weighted', 'matrix', 'factorization', 'wmf', 'model', 'proposed', 'in', 'guo', 'and', 'diab', '2012', '', 'which', 'we', 'extend', 'to', 'the', 'crosslinas', 'mentioned', 'above', 'we', 'extend', 'the', 'wmf', 'model', 'proposed', 'in', 'guo', 'and', 'diab', '2012', 'to', 'bilingual', 'and', 'multilingual', 'settings', 'by', 'forcing', 'the', 'two', 'monolingual', 'components', 'to', 'use', 'a', 'shared', 'factor'], ['our', 'work', 'is', 'also', 'related', 'to', 'bunescu', 'and', 'mooney', '2005', '', 'where', 'the', 'similarity', 'between', 'the', 'words', 'on', 'the', 'path', 'connecting', 'two', 'entities', 'in', 'the', 'dependency', 'graph', 'is', 'used', 'to', 'devise', 'a', 'kernel', 'functionthe', 'dependency', 'path', 'is', 'the', 'shortest', 'path', 'between', 'the', 'two', 'entities', 'in', 'a', 'dependency', 'parse', 'graph', 'and', 'has', 'been', 'shown', 'to', 'be', 'important', 'for', 'relation', 'extraction', 'bunescu', 'and', 'mooney', '2005'], ['motivated', 'by', 'previous', 'work', 'we', 'include', 'a', 'frequency', 'count', 'of', '17', 'discourse', 'markers', 'which', 'were', 'found', 'to', 'be', 'the', 'most', 'common', 'across', 'the', 'argue', 'corpus', 'abbott', 'et', 'al', '2011', 'outside', 'of', 'the', 'rhetorical', 'features', 'the', 'discourse', 'markers', 'which', 'are', 'found', 'to', 'be', 'the', 'most', 'useful', 'in', 'our', 'experiments', 'agree', 'with', 'those', 'found', 'in', 'the', 'ar', 'gue', 'corpus', 'abbott', 'et', 'al', '2011'], ['the', 'highest', 'performance', 'levels', 'were', 'achieved', 'using', 'a', 'sequential', 'minimal', 'optimization', 'algorithm', 'for', 'training', 'a', 'support', 'vector', 'classifier', 'using', 'polynomial', 'kernels', 'platt', '1998', 'table', '8', 'to', 'table', '12', 'show', 'the', 'macroaverage', 'f', 'macroavg', 'scores', 'obtained', 'after', '10', 'crossvalidation', 'using', 'sequential', 'minimal', 'optimization', 'algorithm', 'j', 'platt', '1998', 'for', 'training', 'a', 'support', 'vector', 'mach'], ['in', 'this', 'rest', 'of', 'this', 'paper', 'we', 'discuss', 'related', 'work', 'the', 'methods', 'for', 'each', 'system', 'and', 'experiments', 'and', 'results', 'for', 'each', 'subtask', 'using', 'the', 'data', 'provided', 'by', 'semeval2014', 'task', '9', 'sentiment', 'analysis', 'in', 'twithis', 'paper', 'describes', 'the', 'details', 'of', 'our', 'system', 'that', 'participated', 'in', 'the', 'subtask', 'a', 'of', 'semeval2014', 'task', '9', 'sentiment', 'analysis', 'in', 'twitter', 'rosenthal', 'et', 'al', '2014'], ['in', 'principle', 'classifiers', 'trained', 'on', 'pdtb', 'data', 'can', 'be', 'applied', 'directly', 'to', 'label', 'connectives', 'over', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005', 'used', 'for', 'training', 'and', 'testing', 'smtwe', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['in', 'the', 'next', 'section', 'we', 'briefly', 'review', 'modeling', 'of', 'transition', 'probabilities', 'in', 'a', 'conventional', 'hmm', 'alignment', 'model', 'vogel', 'et', 'al', '1996', 'och', 'and', 'ney', '2000a', 'we', 'briefly', 'review', 'the', 'hmm', 'based', 'word', 'alignment', 'models', 'vogel', '1996', 'och', 'and', 'ney', '2000a'], ['this', 'result', 'is', 'statistically', 'significant', 'at', 'p', '', '005', 'according', 'to', 'bootstrap', 'resampling', 'test', 'koehn', '2004', 'the', 'improvement', 'is', 'statistically', 'significant', 'according', 'to', 'paired', 'bootstrap', 'resampling', 'test', 'koehn', '2004'], ['starting', 'with', 'textrank', 'mihalcea', 'and', 'tarau', '2004', '', 'graphbased', 'ranking', 'methods', 'are', 'becoming', 'the', 'most', 'widely', 'used', 'unsupervised', 'approach', 'for', 'keyphrase', 'extractionin', 'the', 'unsupervised', 'approach', 'graphbased', 'ranking', 'methods', 'are', 'stateoftheart', 'mihalcea', 'and', 'tarau', '2004'], ['because', 'of', 'our', 'experience', 'with', 'the', 'weka', 'package', 'hall', 'et', 'al', '2009', 'we', 'chose', 'this', 'tool', 'for', 'implementation', 'especially', 'we', 'use', 'the', 'weka', 'hall', 'et', 'al', '2009', 'implementation', 'of', 'the', 'simple', 'kmeans', 'for', 'our', 'experiments'], ['dropout', 'srivastava', 'et', 'al', '2014', 'is', 'implemented', 'with', 'a', 'dropout', 'rate', 'of', '02', 'to', 'prevent', 'the', 'model', 'from', 'overfittingdropout', 'srivastava', 'et', 'al', '2014', 'is', 'a', 'very', 'effective', 'regularization', 'technique', 'to', 'prevent', 'overfitting', 'of', 'a', 'network'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'our', 'smt', 'experiments', 'we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['as', 'mentioned', 'in', 'section', '3', 'we', 'obtained', 'dependencies', 'from', 'the', 'output', 'of', 'the', 'stanford', 'parser', 'de', 'marneffe', 'and', 'manning', '2008', 'we', 'make', 'use', 'of', 'dependency', 'parse', 'information', 'from', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'and', 'manning', '2008'], ['we', 'tokenize', 'and', 'truecase', 'all', 'of', 'the', 'corpora', 'using', 'code', 'released', 'with', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'preprocessed', 'the', 'training', 'corpora', 'with', 'scripts', 'included', 'in', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['to', 'obtain', 'these', 'we', 'use', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'and', 'the', 'forced', 'alignment', 'from', 'section', '39we', 'use', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'for', 'extracting', 'dependency', 'path', 'and', 'partofspeech', 'features'], ['the', 'nmt', 'models', 'are', 'trained', 'using', 'adam', 'optimizer', 'kingma', 'and', 'ba', '2014', 'with', 'an', 'initial', 'learning', 'rate', 'of', '00001all', 'parameters', 'are', 'learned', 'by', 'adam', 'optimizer', 'kingma', 'and', 'ba', '2014', 'with', 'the', 'learning', 'rate', '0001'], ['the', 'improved', 'alignments', 'gave', 'a', 'gain', 'of', 'table', '8', '', 'hierarchical', 'lexicalized', 'reordering', 'model', 'galley', 'and', 'manning', '2008', 'one', 'of', 'the', 'phrasebased', 'systems', 'moreover', 'utilizes', 'a', 'lexicalized', 'reordering', 'model', 'galley', 'and', 'manning', '2008'], ['the', 'statistical', 'significance', 'tests', 'using', '95', 'confidence', 'interval', 'are', 'measured', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'a', 'statistical', 'significance', 'test', 'was', 'performed', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004'], ['to', 'obtain', 'these', 'we', 'use', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'and', 'the', 'forced', 'alignment', 'from', 'section', '39we', 'use', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'for', 'extracting', 'dependency', 'path', 'and', 'partofspeech', 'features'], ['we', 'use', 'the', 'adagrad', 'algorithm', 'duchi', 'et', 'al', '2011', 'to', 'optimize', 'the', 'conditional', 'marginal', 'loglikelihood', 'of', 'the', 'datawe', 'use', 'online', 'learning', 'to', 'train', 'model', 'parameters', '', 'updating', 'the', 'parameters', 'using', 'the', 'adagrad', 'algorithm', 'duchi', 'et', 'al', '2011'], ['the', 'other', 'is', 'from', 'jeffrey', 'pennington', 'et', 'al', '2014', '', 'the', 'dimension', 'of', 'word', 'embedding', 'is', '100we', 'use', 'the', '100dimensional', 'glove', 'word', 'embeddings', 'from', 'jeffrey', 'pennington', 'et', 'al', '2014'], ['we', 'use', 'the', 'adagrad', 'optimizer', 'duchi', 'et', 'al', '2011', '', 'with', 'initial', 'learning', 'rate', 'set', 'to', '01we', 'use', 'online', 'learning', 'to', 'train', 'model', 'parameters', '', 'updating', 'the', 'parameters', 'using', 'the', 'adagrad', 'algorithm', 'duchi', 'et', 'al', '2011'], ['we', 'experiment', 'with', 'the', 'phrasebased', 'statistical', 'machine', 'translation', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'in', 'order', 'to', 'train', 'a', 'japanese', 'english', 'system', 'and', 'to', 'show', 'the', 'influence', 'of', 'the', 'expanded', 'parallelwe', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'build', 'a', 'phrase', 'based', 'machine', 'translation', 'system', 'with', 'a', 'traditional', '5gram', 'lm', 'trained', 'on', 'the', 'target', 'side', 'of', 'our', 'bitext'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'smt', 'systems', 'were', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'follow', 'the', 'definition', 'in', 'cohen', 'et', 'al', '2012', 'of', 'lpcfgswe', 'also', 'use', 'these', 'perturbation', 'schemes', 'to', 'create', 'multiple', 'models', 'for', 'the', 'algorithm', 'of', 'cohen', 'et', 'al', '2012'], ['the', 'language', 'model', 'is', 'a', '5gram', 'kenlm', 'heafield', '2011', 'model', 'trained', 'using', 'lmplz', 'with', 'modified', 'kneserney', 'smoothing', 'and', 'no', 'pruningwe', 'trained', 'an', 'english', '5gram', 'language', 'model', 'using', 'kenlm', 'heafield', '2011'], ['as', 'mentioned', 'in', 'section', '3', 'we', 'obtained', 'dependencies', 'from', 'the', 'output', 'of', 'the', 'stanford', 'parser', 'de', 'marneffe', 'and', 'manning', '2008', 'we', 'make', 'use', 'of', 'dependency', 'parse', 'information', 'from', 'the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'and', 'manning', '2008'], ['also', 'we', 'evaluate', 'on', 'the', 'rte', 'part', 'of', 'the', 'sick', 'dataset', 'marelli', 'et', 'al', '2014', 'and', 'show', 'that', 'our', 'approach', 'leads', 'to', 'improvementsthe', 'second', 'is', 'the', 'rte', 'part', 'of', 'the', 'sick', 'dataset', 'marelli', 'et', 'al', '2014'], ['our', 'model', 'has', 'a', 'siamese', 'structure', 'bromley', 'et', 'al', '1993', 'with', 'two', 'subnetworks', 'each', 'processing', 'a', 'sentence', 'in', 'parallelmost', 'previous', 'work', 'use', 'sentence', 'modeling', 'with', 'a', 'siamese', 'structure', 'bromley', 'et', 'al', '1993'], ['the', 'dialogue', 'act', 'labelling', 'of', 'the', 'corpus', 'follows', 'the', 'date', 'tagging', 'scheme', 'walker', 'et', 'al', '2001', 'the', 'classes', 'used', 'to', 'train', 'the', 'date', 'tagger', 'are', 'derived', 'directly', 'from', 'the', 'date', 'tagging', 'scheme', 'walker', 'et', 'al', '2001c'], ['we', 'also', 'obtain', 'the', 'dependency', 'parse', 'of', 'the', 'sentences', 'using', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'examine', 'the', 'effect', 'of', 'normalization', 'on', 'dependency', 'parsing', 'we', 'employ', 'the', 'stanford', 'dependency', 'parser', '3', 'marneffe', 'et', 'al', '2006'], ['the', 'resulting', 'matrix', 'is', 'weighted', 'using', 'pointwise', 'mutual', 'information', 'church', 'and', 'hanks', '1990', 'we', 'construct', 'wordword', 'cooccurrence', 'matrix', 'x', 'every', 'element', 'in', 'the', 'matrix', 'is', 'the', 'pointwise', 'mutual', 'information', 'between', 'the', 'two', 'words', 'church', 'and', 'hanks', '1990'], ['we', 'extract', 'structured', 'facts', 'using', 'two', 'methods', 'clausie', 'del', 'corro', 'and', 'gemulla', '2013', 'and', 'sedona', 'detailed', 'later', 'in', 'sec', '4', 'also', 'see', 'fig', '1we', 'extract', 'facts', 'from', 'captions', 'using', 'clausie', 'del', 'corro', 'and', 'gemulla', '2013', 'and', 'our', 'proposed', 'sedonanlp', 'system'], ['dropout', 'srivastava', 'et', 'al', '2014', 'is', 'implemented', 'with', 'a', 'dropout', 'rate', 'of', '02', 'to', 'prevent', 'the', 'model', 'from', 'overfittingnext', 'the', 'output', 'of', 'the', 'maxpooling', 'layer', 'is', 'passed', 'to', 'a', 'dropout', 'layer', 'srivastava', 'et', 'al', '2014'], ['the', 'population', 'distribution', 'was', 'estimated', 'by', 'the', 'bootstrap', 'method', 'cohen', '1995', 'the', 'bootstrap', 'sampling', 'method', 'provides', 'a', 'way', 'for', 'artificially', 'establishing', 'a', 'sampling', 'distribution', 'for', 'a', 'statistic', 'when', 'the', 'distribution', 'is', 'not', 'known', 'cohen', '1995'], ['the', 'statistical', 'significance', 'test', 'is', 'also', 'carried', 'out', 'with', 'paired', 'bootstrap', 'resampling', 'method', 'with', 'p', '', '0001', 'intervals', 'koehn', '2004', 'the', 'significance', 'testing', 'is', 'performed', 'by', 'paired', 'bootstrap', 'resampling', 'koehn', '2004'], ['the', 'bootstrap', 'sampling', 'method', 'provides', 'a', 'way', 'for', 'artificially', 'establishing', 'a', 'sampling', 'distribution', 'for', 'a', 'statistic', 'when', 'the', 'distribution', 'is', 'not', 'known', 'cohen', '1995', 'the', 'population', 'distribution', 'was', 'estimated', 'by', 'the', 'bootstrap', 'method', 'cohen', '1995'], ['for', 'medical', 'we', 'use', 'the', 'biomedical', 'data', 'from', 'emea', 'tiedemann', '2009', 'for', 'frenchenglish', 'experiments', 'we', 'used', 'the', 'emea', 'parallel', 'corpus', 'tiedemann', '2009', '', 'which', 'are', 'medical', 'documents', 'from', 'the', 'european', 'medecines', 'agency'], ['the', 'mt', 'experiments', 'were', 'carried', 'out', 'using', 'the', 'standard', 'loglinear', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'translation', 'system', 'is', 'trained', 'using', 'the', 'well', 'known', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'the', 'theory', 'of', 'cho', '', 'chai', '2000', 'to', 'be', 'complete', 'we', 'have', 'proposed', 'a', 'new', 'type', 'of', 'marker', 'and', 'the', 'adjunct', 'lp', 'constraint', 'in', 'conjunction', 'with', 'their', 'argument', 'lp', 'constraintthen', 'we', 'revise', 'the', 'two', 'lp', 'constraints', 'of', 'cho', '', 'chai', '2000', 'and', 'add', 'the', 'adjunct', 'lp', 'constraint'], ['3', 'with', 'these', 'trees', 'fixed', 'the', 'partial', 'derivatives', 'with', 'respect', 'to', 'parameters', 'are', 'computed', 'via', 'the', 'backpropagation', 'through', 'structures', 'algorithm', 'goller', 'and', 'kuchler', '1996', 'derivatives', 'are', 'computed', 'efficiently', 'via', 'backpropagation', 'through', 'structure', 'goller', 'and', 'kuchler', '1996'], ['many', 'researchers', 'have', 'considered', 'generating', 'paraphrases', 'by', 'mining', 'the', 'web', 'guided', 'by', 'the', 'distributional', 'hypothesis', 'which', 'states', 'that', 'words', 'occurring', 'in', 'similar', 'contexts', 'tend', 'to', 'have', 'similar', 'meanings', 'harris', '1954', 'distributional', 'models', 'of', 'meaning', 'follow', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'which', 'states', 'that', 'two', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'have', 'similar', 'meanings'], ['we', 'used', 'weka', 'hall', 'et', 'al', '2009', 'for', 'all', 'our', 'classification', 'experimentswe', 'use', 'the', 'weka', 'toolkit', 'hall', 'et', 'al', '2009', 'for', 'our', 'supervised', 'learning', 'experiments'], ['bilingual', 'corpora', 'the', 'corpus', 'used', 'in', 'the', 'following', 'experiments', 'is', 'the', 'basic', 'travel', 'expression', 'corpus', 'takezawa', 'et', 'al', '2002', 'the', 'experiments', 'are', 'carried', 'out', 'on', 'a', 'subset', 'of', 'the', 'basic', 'travel', 'expression', 'corpus', 'btec', 'takezawa', 'et', 'al', '2002', '', 'as', 'it', 'is', 'used', 'for', 'the', 'supplied', 'data', 'track', 'condition', 'of', 'the', 'iwslt', 'evaluation', 'campaign'], ['first', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'statistical', 'machine', 'translationthe', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'is', 'used', 'for', 'phrase', 'extraction'], ['another', 'parallel', 'corpus', 'is', 'the', 'jrcacquis', 'multilingual', 'parallel', 'corpus', 'steinberger', 'et', 'al', '2006', 'corpus', 'steinberger', 'et', 'al', '2006'], ['the', 'parallel', 'corpus', 'is', 'wordaligned', 'using', 'giza', 'och', 'and', 'ney', '2003', 'this', 'is', 'done', 'using', 'ibm', 'model', '1', 'brown', 'et', 'al', '1993', 'and', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'thus', 'cast', 'msc', 'as', 'a', 'semantic', 'sentence', 'classification', 'task', 'in', 'a', 'cnn', 'architecture', 'adopting', 'the', 'onelayer', 'cnn', 'model', 'of', 'kim', '2014', 'a', 'variant', 'of', 'collobert', 'et', 'al', '2011', 'we', 'compare', 'the', 'proposed', 'model', 'to', 'our', 'implementation', 'of', 'the', 'iobesbased', 'model', 'described', 'in', 'collobert', 'et', 'al', '2011', '', 'applied', 'to', 'mwe', 'tagging'], ['all', 'these', 'features', 'are', 'inherited', 'from', 'moses', 'koehn', 'et', 'al', '2007', 'all', 'the', 'experiments', 'are', 'carried', 'out', 'in', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005', 'for', 'our', 'experiments', 'we', 'use', '40000', 'sentences', 'from', 'europarl', 'koehn', '2005', 'for', 'each', 'language', 'pair', 'following', 'the', 'basic', 'setup', 'of', 'tiedemann', '2014'], ['the', 'texts', 'were', 'first', 'automatically', 'segmented', 'and', 'tokenized', '10', 'and', 'then', 'they', 'were', 'partofspeech', 'tagged', 'by', 'tnt', 'tagger', 'brants', '2000', '', 'which', 'was', 'trained', 'on', 'the', 'respective', 'conll', 'training', 'data', 'the', 'filethe', 'data', 'was', 'tagged', 'using', 'tnt', 'brants', '2000', '', 'using', 'a', 'model', 'trained', 'on', 'the', 'wall', 'street', 'journal'], ['secondly', 'holmqvist', 'et', 'al', '2012', 'reordered', 'source', 'words', 'based', 'on', 'word', 'alignment', 'whereas', 'we', 'suggest', 'reordering', 'source', 'chunksholmqvist', 'et', 'al', '2012', 'presented', 'a', 'method', 'where', 'source', 'text', 'is', 'reordered', 'to', 'replicate', 'the', 'target', 'word', 'order', 'based', 'on', 'word', 'alignment'], ['for', 'language', 'modeling', 'we', 'use', 'the', 'english', 'gigaword', 'corpus', 'with', '5gram', 'lm', 'implemented', 'with', 'the', 'kenlm', 'toolkit', 'heafield', '2011', 'language', 'model', 'for', 'all', '3scfg', 'systems', 'we', 'use', 'a', '4gram', 'kneserney', 'smoothed', 'language', 'model', 'trained', 'using', 'the', 'kenlm', 'toolkit', 'heafield', '2011'], ['for', 'our', 'lda', 'implementations', 'we', 'use', 'mallet', 'mccallum', '2002', 'for', 'training', 'bilingual', 'topic', 'models', 'we', 'use', 'mallet', 'toolkit', 'mccallum', '2002'], ['we', 'use', 'the', 'standard', 'stanfordstyle', 'set', 'of', 'dependency', 'labels', 'de', 'marneffe', 'et', 'al', '2006', 'we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006'], ['next', 'we', 'evaluate', 'how', 'well', 'the', 'complexity', 'measures', 'proposed', 'in', '', 'raghavan', 'et', 'al', '2007', 'correlate', 'with', 'improvement', 'in', 'performance', 'and', 'improvement', 'in', 'learning', 'ratewe', 'also', 'compare', 'annotation', 'strategies', 'in', 'terms', 'of', 'the', 'learning', 'rate', 'similar', 'to', 'raghavan', 'et', 'al', '2007', '', 'except', 'that', 'we', 'estimate', 'and', 'compare', 'the', 'maximum', 'improvement', 'in', 'the', 'learning', 'rate'], ['syntax', 'in', 'epec', 'is', 'annotated', 'following', 'the', 'dependency', 'based', 'formalism', 'used', 'in', 'the', 'prague', 'dependency', 'treebank', 'which', 'was', 'also', 'used', 'in', 'the', 'german', 'negra', 'corpus', 'skut', 'et', 'al', '1997', 'for', 'the', 'german', 'experiments', 'we', 'used', 'the', 'negra', 'corpus', 'skut', 'et', 'al', '1997'], ['we', 'used', 'the', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'implementation', 'of', 'svrs', 'and', 'the', 'skll', 'toolkitwe', 'use', 'the', 'scikit', 'implementation', 'of', 'svm', 'pedregosa', 'et', 'al', '2011'], ['we', 'used', 'the', 'scikitlearn', 'toolkit', 'to', 'train', 'our', 'classifiers', 'pedregosa', 'et', 'al', '2011', 'for', 'indeplogistic', 'we', 'used', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'to', 'train', 'classifiers'], ['the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'trained', 'a', 'standard', 'moses', 'baseline', 'koehn', 'et', 'al', '2007', 'on', 'the', 'same', 'training', 'data', 'and', 'used', 'the', 'same', '4', 'gram', 'language', 'model', 'to', 'generate', 'responses'], ['these', 'features', 'were', 'obtained', 'using', 'the', 'stanford', 'parser', '2', 'marneffe', 'et', 'al', '2006', 'we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006'], ['we', 'built', 'a', '5gram', 'language', 'model', 'on', 'the', 'english', 'side', 'of', 'qcatrain', 'using', 'kenlm', 'heafield', '2011', 'one', 'is', 'a', '3gram', 'language', 'model', 'built', 'using', 'kenlm', 'heafield', '2011', 'and', 'trained', 'over', 'a', 'modified', 'version', 'of', 'the', 'annotated', 'corpus', 'in', 'which', 'every', 'it', 'is', 'concatenated', 'with', 'its', 'type', 'eg', 'it', 'event'], ['in', 'the', 'other', 'side', 'the', 'french', 'corpus', 'is', 'partofspeech', 'pos', 'tagged', 'by', 'using', 'treetagger', 'tool', 'schmid', '1994', 'for', 'annotating', 'text', 'with', 'partofspeech', 'and', 'lemma', 'information1', 'the', 'corpus', 'is', 'lemmatised', 'and', 'tagged', 'by', 'partofspeech', 'on', 'both', 'sides', 'using', 'the', 'treetagger', 'schmid', '1994'], ['the', 'comlex', 'syntax', 'dictionary', 'grishman', 'et', 'al', '1994', 'the', 'comlex', 'syntax', 'dictionary', 'grishman', 'et', 'al', '1994', 'brown', 'corpus', 'kuera', 'and', 'francis', '1967', 'has', 'been', 'used', 'as', 'a', 'reference', 'corpus', 'in', 'many', 'computational', 'applications'], ['we', 'lemmatise', 'the', 'head', 'of', 'each', 'constituent', 'with', 'treetagger', 'schmid', '1994', 'pos', 'tagging', 'was', 'performed', 'with', 'the', 'treetagger', 'schmid', '1994'], ['with', 'the', 'kenlm', 'toolkit', 'heafield', '2011', 'for', 'language', 'modeling', 'we', 'used', 'the', 'kenlm', 'toolkit', 'heafield', '2011', 'for', 'standard', 'ngram', 'modeling', 'with', 'an', 'ngram', 'length', 'of', '5'], ['we', 'briefly', 'review', 'the', 'hmm', 'based', 'word', 'alignment', 'models', 'vogel', '1996', 'och', 'and', 'ney', '2000', 'och', 'is', 'the', 'hmm', 'alignment', 'model', 'of', 'och', 'and', 'ney', '2000'], ['we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'marneffe', 'et', 'al', '2006', 'these', 'features', 'were', 'obtained', 'using', 'the', 'stanford', 'parser', '2', 'marneffe', 'et', 'al', '2006'], ['we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006', 'we', 'use', 'the', 'standard', 'stanfordstyle', 'set', 'of', 'dependency', 'labels', 'de', 'marneffe', 'et', 'al', '2006'], ['all', 'the', 'summaries', 'are', 'evaluated', 'using', 'rouge', 'lin', '2004', 'for', 'the', 'summarization', 'task', 'we', 'compare', 'results', 'using', 'rouge', 'lin', '2004'], ['the', 'reported', 'confidence', 'intervals', 'were', 'estimated', 'using', 'bootstrap', 'resampling', 'koehn', '2004', 'we', 'calculated', 'significance', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004'], ['we', 'implemented', 'charwnn', 'using', 'the', 'theano', 'library', 'bergstra', 'et', 'al', '2010', 'gradients', 'computed', 'using', 'the', 'automatic', 'differentiation', 'facilities', 'of', 'theano', 'bergstra', 'et', 'al', '2010', 'which', 'implements', 'a', 'generalized', 'bptt'], ['for', 'the', 'linear', 'logistic', 'regression', 'implementation', 'we', 'used', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'for', 'indeplogistic', 'we', 'used', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'to', 'train', 'classifiers'], ['the', 'tagger', 'we', 'use', 'is', 'tnt', 'brants', '2000', '', 'a', 'hidden', 'markov', 'trigram', 'tagger', 'which', 'was', 'trained', 'on', 'the', 'spoken', 'dutch', 'corpus', 'cgn', 'internal', 'release', '6the', 'data', 'was', 'tagged', 'using', 'tnt', 'brants', '2000', '', 'using', 'a', 'model', 'trained', 'on', 'the', 'wall', 'street', 'journal'], ['we', 'have', 'used', 'the', 'implementation', 'described', 'in', 'schapire', 'and', 'singer', '1999', 'with', 'decision', 'trees', 'of', 'depth', 'fixed', 'to', '31', 'regarding', 'the', 'learning', 'algorithm', 'we', 'used', 'generalized', 'adaboost', 'with', 'realvalued', 'weak', 'classifiers', 'which', 'constructs', 'an', 'ensemble', 'of', 'decision', 'trees', 'of', 'fixed', 'depth', 'schapire', 'and', 'singer', '1999'], ['we', 'built', 'a', '5gram', 'language', 'model', 'on', 'the', 'english', 'side', 'of', 'qcatrain', 'using', 'kenlm', 'heafield', '2011', 'we', 'build', 'our', 'pbsmt', 'systems', 'in', 'a', 'standard', 'way', 'using', 'the', 'moses', 'system', '', 'kenlm', 'for', 'language', 'modelling', 'heafield', '2011', '', 'and', 'standard', 'lexical', 'reordering', 'model'], ['delphin', 'minimal', 'recursion', 'semantics', 'dm', 'as', 'part', 'of', 'the', 'full', 'hpsg', 'sign', 'the', 'erg', 'also', 'makes', 'available', 'a', 'logicalform', 'representation', 'of', 'propositional', 'semantics', 'in', 'the', 'format', 'of', 'minimal', 'recursion', 'semantas', 'output', 'the', 'grammar', 'delivers', 'detailed', 'semantic', 'representations', 'in', 'the', 'form', 'of', 'minimal', 'recursion', 'semantics', 'copestake', 'et', 'al', '2005'], ['like', 'the', 'conll2006', 'shared', 'task', 'the', '2007', 'shared', 'task', 'focuses', 'on', 'dependency', 'parsing', 'and', 'aims', 'at', 'comparing', 'stateoftheart', 'machine', 'learning', 'algorithms', 'applied', 'to', 'this', 'task', 'nivre', 'et', 'al', '2007', 'one', 'of', 'the', 'first', 'venues', 'at', 'which', 'domain', 'adaptation', 'was', 'targeted', 'was', 'the', '2007', 'conll', 'shared', 'task', 'on', 'dependency', 'parsing', 'nivre', 'et', 'al', '2007'], ['like', 'cho', '', 'chai', '2000', '', 'our', 'analysis', 'also', 'provides', 'the', 'same', 'explanation', 'for', 'various', 'scrambled', 'sentences', 'such', 'as', 'the', 'double', 'accusative', 'construction', 'dac', 'in', 'koreanhowever', 'like', 'cho', '', 'chai', '2000', '', 'our', 'analysis', 'can', 'also', 'predict', 'that', 'the', 'scrambled', 'sentence', '19c', 'is', 'grammatical', 'whereas', '19b', 'is', 'ungrammatical'], ['the', 'phrase', 'table', 'is', 'extracted', 'from', 'a', 'bilingual', 'text', 'aligned', 'on', 'the', 'word', 'level', 'using', 'eg', 'giza', '', 'och', 'and', 'ney', '2003', 'a', 'core', 'component', 'of', 'every', 'pbsmt', 'system', 'is', 'the', 'phrase', 'table', 'which', 'contains', 'bilingual', 'phrase', 'pairs', 'extracted', 'from', 'a', 'bilingual', 'corpus', 'after', 'word', 'alignment', 'och', 'and', 'ney', '2003'], ['google', 'web', '1t', 'brants', 'and', 'franz', '2006', 'has', 'been', 'used', 'to', 'calculate', 'term', 'idf', 'which', 'is', 'used', 'as', 'a', 'measure', 'of', 'the', 'importance', 'of', 'the', 'termsthe', 'google', 'web', '1t', 'data', 'brants', 'and', 'franz', '2006', 'has', 'been', 'shown', 'to', 'give', 'a', 'higher', 'score', 'however', 'this', 'data', 'was', 'not', 'available', 'during', 'the', 'course', 'of', 'this', 'research'], ['we', 'first', 'use', 'a', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'parse', 'each', 'sentence', 'and', 'extract', 'the', 'set', 'of', 'dependency', 'relations', 'associated', 'with', 'the', 'sentence', 'a', 'complete', 'set', 'of', 'parser', 'tags', 'and', 'the', 'method', 'used', 'to', 'map', 'from', 'a', 'constituent', 'to', 'a', 'typed', 'dependency', 'grammar', 'can', 'be', 'found', 'in', 'de', 'marneffe', 'et', 'al', '2006'], ['we', 'first', 'use', 'a', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'parse', 'each', 'sentence', 'and', 'extract', 'the', 'set', 'of', 'dependency', 'relations', 'associated', 'with', 'the', 'sentence', 'a', 'complete', 'set', 'of', 'parser', 'tags', 'and', 'the', 'method', 'used', 'to', 'map', 'from', 'a', 'constituent', 'to', 'a', 'typed', 'dependency', 'grammar', 'can', 'be', 'found', 'in', 'de', 'marneffe', 'et', 'al', '2006'], ['these', 'analyses', 'provide', 'an', 'alternative', 'but', 'theoretically', 'more', 'reasonable', 'explanation', 'to', 'the', 'findings', 'of', 'liang', 'et', 'al', '2006', '', 'while', 'they', 'blame', 'unreasonable', 'gold', 'derivations', 'for', 'the', 'failure', 'of', 'standliang', 'et', 'al', '2006', 'observe', 'that', 'standard', 'update', 'performs', 'worse', 'than', 'local', 'update', 'which', 'they', 'attribute', 'to', 'the', 'fact', 'that', 'the', 'former', 'often', 'update', 'towards', 'a', 'gold', 'derivation', 'made', 'up', 'of', 'unreasonable', 'r'], ['we', 'experiment', 'with', 'the', 'phrasebased', 'statistical', 'machine', 'translation', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'in', 'order', 'to', 'train', 'a', 'japanese', 'english', 'system', 'and', 'to', 'show', 'the', 'influence', 'of', 'the', 'expanded', 'parallelwe', 'present', 'a', 'system', 'that', 'takes', 'a', 'general', 'moses', 'statistical', 'machine', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'and', 'adapts', 'it', 'to', 'the', 'biomedical', 'domain'], ['for', 'our', 'corpus', 'we', 'randomly', 'selected', 'documents', 'from', 'the', 'washington', 'section', 'of', 'the', 'new', 'york', 'times', 'corpus', 'sandhaus', '2008', 'from', 'the', 'year', '2007new', 'york', 'times', 'consists', 'of', '500', 'random', 'sentences', 'from', 'the', 'new', 'york', 'times', 'corpus', 'sandhaus', '2008', '', 'year', '2007', 'we', 'selected', 'only', 'sentences', 'that', 'contained', 'at', 'least', 'one', 'named', 'entity', 'according', 'to', 'the', 'stanf'], ['they', 'used', 'the', 'webbased', 'annotation', 'tool', 'brat', 'stenetorp', 'et', 'al', '2012', 'for', 'the', 'annotation', 'we', 'consider', 'the', 'following', 'types', 'of', 'implicit', 'referents', '', 'the', 'brat', 'tool', 'stenetorp', 'et', 'al', '2012', 'was', 'used', 'for', 'annotation'], ['all', 'corpora', 'were', 'taken', 'from', 'the', 'childes', 'database', 'macwhinney', '2000', 'the', 'evaluation', 'set', 'was', 'comprised', 'of', 'adult', 'utterances', 'from', 'the', 'brown', '1973', 'data', 'of', 'the', 'childes', 'database', 'macwhinney', '2000'], ['the', 'system', 'generated', 'tweets', 'were', 'evaluated', 'using', 'rouge', 'measures', 'lin', '2004', 'the', 'summaries', 'from', 'the', 'above', 'algorithm', 'for', 'the', 'qfmds', 'were', 'evaluated', 'based', 'on', 'rouge', 'metrics', 'lin', '2004'], ['the', 'toefl', 'synonym', 'selection', 'task', 'is', 'to', 'select', 'the', 'semantically', 'closest', 'word', 'to', 'a', 'target', 'from', 'a', 'list', 'of', 'four', 'candidates', 'landauer', 'and', 'dumais', '1997', 'the', 'toefl', 'synonym', 'dataset', 'landauer', 'and', 'dumais', '1997', 'consists', 'of', '80', 'question', 'words', 'for', 'each', 'of', 'which', '4', 'answer', 'words', 'are', 'given', 'and', 'the', 'task', 'is', 'to', 'select', 'the', 'answer', 'word', 'most', 'similar', 'to', 'the', 'question'], ['we', 'calculate', 'our', 'features', 'using', 'the', 'kenlm', 'toolkit', 'heafield', '2011', '1', 'we', 'used', 'the', 'kenlm', 'toolkit', 'heafield', '2011', 'to', 'build', 'all', 'language', 'models', 'used', 'in', 'this', 'work', 'ie', 'both', 'for', 'data', 'selection', 'and', 'for', 'the', 'mt', 'systems', 'used', 'for', 'extrinsic', 'evaluation'], ['we', 'use', 'the', 'scikit', 'implementation', 'of', 'random', 'forest', 'pedregosa', 'et', 'al', '2011', 'the', 'regressor', 'used', 'is', 'a', 'random', 'forest', 'regressor', 'in', 'the', 'implementation', 'provided', 'by', 'scikitlearn', 'pedregosa', 'et', 'al', '2011'], ['for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005', 'the', 'statistical', 'dictionary', 'for', 'this', 'task', 'was', 'extracted', 'from', 'the', 'englishfrench', 'europarl', '7', 'corpus', 'koehn', '2005'], ['the', 'statistical', 'significance', 'tests', 'using', '95', 'confidence', 'interval', 'are', 'measured', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'we', 'used', 'bootstrap', 'resampling', 'for', 'testing', 'statistical', 'significance', 'koehn', '2004'], ['we', 'then', 'run', 'word', 'alignment', 'with', 'giza', 'och', 'and', 'ney', '2003', 'in', 'both', 'directions', 'with', 'the', 'default', 'parameters', 'used', 'in', 'moseswe', 'used', 'the', 'giza', 'software', 'och', 'and', 'ney', '2003', 'to', 'do', 'the', 'word', 'alignments'], ['we', 'built', 'a', '5gram', 'language', 'model', 'on', 'the', 'english', 'side', 'of', 'qcatrain', 'using', 'kenlm', 'heafield', '2011', 'we', 'calculate', 'our', 'features', 'using', 'the', 'kenlm', 'toolkit', 'heafield', '2011'], ['we', 'used', 'giza', 'och', 'and', 'ney', '2003', 'to', 'align', 'the', 'words', 'in', 'the', 'corpuswe', 'then', 'run', 'word', 'alignment', 'with', 'giza', 'och', 'and', 'ney', '2003', 'in', 'both', 'directions', 'with', 'the', 'default', 'parameters', 'used', 'in', 'moses'], ['the', 'phrase', 'tables', 'were', 'generated', 'by', 'means', 'of', 'symmetrised', 'word', 'alignments', 'obtained', 'with', 'giza', 'och', 'and', 'ney', '2003', 'the', 'word', 'alignment', 'was', 'obtained', 'by', 'running', 'giza', 'och', 'and', 'ney', '2003'], ['zhu', 'et', 'al', '2013', 'applied', 'kalman', 'filter', 'model', 'to', 'learn', 'and', 'estimate', 'user', 'intentions', 'in', 'their', 'humancomputer', 'interactive', 'word', 'segmentation', 'frameworkin', 'the', 'scenario', 'of', 'humancomputer', 'interactive', 'chinese', 'word', 'segmentation', 'the', 'kalman', 'filter', 'approach', 'proposed', 'by', 'zhu', 'et', 'al', '2013', '', 'the', 'md', 'value', 'and', 'the', 'horizontal', 'axis', 'denotes', 'the', 'occurrence', 'of'], ['koo', 'et', 'al', '2008', '', 'have', 'proposed', 'to', 'use', 'word', 'clusters', 'as', 'features', 'to', 'improve', 'graphbased', 'statistical', 'dependency', 'parsing', 'for', 'english', 'and', 'czechin', 'order', 'to', 'reduce', 'the', 'amount', 'of', 'annotated', 'data', 'to', 'train', 'a', 'dependency', 'parser', 'koo', 'et', 'al', '2008', 'used', 'word', 'clusters', 'computed', 'from', 'unlabelled', 'data', 'as', 'features', 'for', 'training', 'a', 'parser'], ['more', 'recently', 'pasha', 'et', 'al', '2014', 'created', 'madamira', 'a', 'system', 'for', 'morphological', 'analysis', 'and', 'disambiguation', 'of', 'arabic', 'this', 'system', 'can', 'be', 'used', 'to', 'improve', 'the', 'accuracy', 'of', 'spelling', 'checking', 'system', 'esnext', 'the', 'files', 'are', 'processed', 'with', 'the', 'morphological', 'analysis', 'and', 'disambiguation', 'system', 'madamira', 'pasha', 'et', 'al', '2014', 'that', 'corrects', 'a', 'common', 'class', 'of', 'spelling', 'errors'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settingsfor', 'building', 'our', 'smt', 'systems', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'was', 'used', 'in', 'its', 'standard', 'setup'], ['bullet', 'naive', 'bayesnb', 'we', 'use', 'binomial', 'variant', 'with', 'laplace', 'smoothing', 'parameter', '', '1', 'pedregosa', 'et', 'al', '2011', 'we', 'use', 'the', 'bernoulli', 'naive', 'bayes', 'classifier', 'in', 'scikit', 'with', 'the', 'default', 'settings', 'pedregosa', 'et', 'al', '2011'], ['we', 'use', 'adagrad', 'duchi', 'et', 'al', '2011', '', 'with', 'the', 'initial', 'learning', 'rate', 'set', 'to', '', '', '05we', 'use', 'online', 'learning', 'to', 'train', 'model', 'parameters', '', 'updating', 'the', 'parameters', 'using', 'the', 'adagrad', 'algorithm', 'duchi', 'et', 'al', '2011'], ['the', 'mt', 'experiments', 'were', 'carried', 'out', 'using', 'the', 'standard', 'loglinear', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'adagrad', 'method', 'duchi', 'et', 'al', '2011', 'to', 'automatically', 'update', 'the', 'learning', 'rate', 'for', 'each', 'parameterwe', 'use', 'online', 'learning', 'to', 'train', 'model', 'parameters', '', 'updating', 'the', 'parameters', 'using', 'the', 'adagrad', 'algorithm', 'duchi', 'et', 'al', '2011'], ['among', 'the', 'existing', 'sensetagged', 'corpora', 'the', 'semcor', 'corpus', 'miller', 'et', 'al', '1994', 'is', 'one', 'of', 'the', 'most', 'widely', 'usedthe', 'semcor', 'corpus', 'miller', 'et', 'al', '1994', 'is', 'one', 'of', 'the', 'few', 'currently', 'available', 'manually', 'senseannotated', 'corpora', 'for', 'wsd'], ['we', 'use', 'the', 'liblinear', 'support', 'vector', 'machine', 'svm', 'chang', 'and', 'lin', '2011', 'classifier', 'for', 'training', 'and', 'run', '5fold', 'crossvalidation', 'for', 'evaluationa', 'linearkernel', 'support', 'vector', 'machine', 'chang', 'and', 'lin', '2011', 'classifier', 'is', 'trained', 'on', 'the', 'available', 'training', 'data'], ['we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'tokenize', 'and', 'truecase', 'all', 'of', 'the', 'corpora', 'using', 'code', 'released', 'with', 'moses', 'koehn', 'et', 'al', '2007', '1'], ['we', 'built', 'a', 'sourcetotarget', 'pbsmt', 'model', 'from', 'the', 'bilingual', 'domain', 'corpus', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'dictionaries', 'are', 'automatically', 'generated', 'via', 'word', 'alignment', 'using', 'giza', 'och', 'and', 'ney', '2000', 'on', 'parallel', 'corporawe', 'then', 'made', 'use', 'of', 'the', 'giza', 'software', 'och', 'and', 'ney', '2000', 'to', 'perform', 'word', 'alignment', 'on', 'the', 'parallel', 'corpora'], ['there', 'are', 'two', 'main', 'approaches', 'to', 'processing', 'nonstandard', 'data', 'normalization', 'and', 'domain', 'adaptation', 'eisenstein', '2013', 'there', 'are', 'two', 'approaches', 'proposed', 'in', 'the', 'literature', 'to', 'handle', 'this', 'problem', 'eisenstein', '2013'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'trained', 'a', 'model', 'using', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'on', 'the', 'training', 'data', 'as', 'our', 'baseline', 'system'], ['the', 'task', 'of', 'identifying', 'mentions', 'to', 'medical', 'concepts', 'in', 'free', 'text', 'and', 'mapping', 'these', 'mentions', 'to', 'a', 'knowledge', 'base', 'was', 'recently', 'proposed', 'in', 'shareclef', 'ehealth', 'evaluation', 'lab', '2013', 'suominen', 'et', 'al', '2013', 'we', 'found', 'the', 'largest', 'of', 'such', 'corpus', 'to', 'be', 'the', 'dataset', 'from', 'the', 'task', 'to', 'recognize', 'disorder', 'mentions', 'in', 'clinical', 'text', 'initially', 'organized', 'by', 'shareclef', 'ehealth', 'evaluation', 'lab', 'shel', 'in', '2013', '', 'suominen', 'et', 'al', '2013'], ['we', 'built', 'a', 'sourcetotarget', 'pbsmt', 'model', 'from', 'the', 'bilingual', 'domain', 'corpus', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007'], ['we', 'also', 'use', 'madatokan', 'habash', 'et', 'al', '2009', 'to', 'preprocess', 'and', 'tokenize', 'the', 'arabic', 'side', 'of', 'the', 'corpuswe', 'use', 'the', 'mada', 'package', 'habash', 'et', 'al', '2009', 'to', 'collect', 'the', 'stem', 'and', 'the', 'morphological', 'features', 'of', 'the', 'hypothesis', 'and', 'reference', 'translation'], ['both', 'training', 'and', 'testing', 'data', 'consist', 'of', 'pubmed', 'abstracts', 'extracted', 'from', 'the', 'genia', 'corpus', 'kim', 'et', 'al', '2008', 'the', 'data', 'provided', 'for', 'the', 'shared', 'task', 'is', 'prepared', 'from', 'the', 'genia', 'corpus', 'kim', 'et', 'al', '2008'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'built', 'a', 'sourcetotarget', 'pbsmt', 'model', 'from', 'the', 'bilingual', 'domain', 'corpus', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['to', 'extract', 'our', 'partofspeech', 'pos', 'features', 'we', 'first', 'tag', 'the', 'transcripts', 'using', 'the', 'nltk', 'pos', 'tagger', 'bird', 'et', 'al', '2009', 'we', 'used', 'the', 'brill', 'tagger', 'provided', 'by', 'nltk', 'for', 'our', 'pos', 'tagging', 'bird', 'et', 'al', '2009'], ['the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'tokenize', 'and', 'truecase', 'all', 'of', 'the', 'corpora', 'using', 'code', 'released', 'with', 'moses', 'koehn', 'et', 'al', '2007', '1'], ['we', 'trained', 'a', 'model', 'using', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'on', 'the', 'training', 'data', 'as', 'our', 'baseline', 'systemour', 'baseline', 'is', 'a', 'phrasebased', 'mt', 'system', 'trained', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'seed', 'and', 'test', 'paradigms', 'we', 'used', 'verbal', 'inflectional', 'paradigms', 'from', 'the', 'celex', 'morphological', 'database', 'baayen', 'et', 'al', '1995', 'for', 'english', 'we', 'used', 'the', 'celex', 'database', 'baayen', 'et', 'al', '1995', 'to', 'segment', 'english', 'words', 'into', 'morphemes'], ['then', 'we', 'did', 'word', 'alignment', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'default', 'growdiagfinaland', 'alignment', 'symmetrization', 'methodwe', 'then', 'run', 'word', 'alignment', 'with', 'giza', 'och', 'and', 'ney', '2003', 'in', 'both', 'directions', 'with', 'the', 'default', 'parameters', 'used', 'in', 'moses'], ['pang', 'et', 'al', '2002', 'have', 'reported', 'the', 'effectiveness', 'of', 'applying', 'machine', 'learning', 'techniques', 'to', 'the', 'pn', 'classificationpang', 'et', 'al', '2002', '', 'use', 'machine', 'learning', 'methods', 'nb', 'svm', 'and', 'maxent', 'to', 'detect', 'sentiments', 'on', 'movie', 'reviews'], ['we', 'train', 'the', 'concept', 'identification', 'stage', 'using', 'infinite', 'ramp', 'loss', '1', 'with', 'adagrad', 'duchi', 'et', 'al', '2011', 'we', 'use', 'online', 'learning', 'to', 'train', 'model', 'parameters', '', 'updating', 'the', 'parameters', 'using', 'the', 'adagrad', 'algorithm', 'duchi', 'et', 'al', '2011'], ['for', 'the', 'contextual', 'check', 'we', 'use', 'the', 'google', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'which', 'contains', 'counts', 'for', 'ngrams', 'from', 'unigrams', 'through', 'to', 'fivegrams', 'obtained', 'from', 'over', '1', 'trillion', 'word', 'tokethe', 'web1t', 'corpus', 'brants', 'and', 'franz', '2006', 'is', 'a', 'dataset', 'consisting', 'of', 'the', 'counts', 'for', 'ngrams', 'obtained', 'from', '1', 'trillion', '10', '12', 'words', 'of', 'english', 'web', 'text', 'subject', 'to', 'a', 'minimum', 'occurrence', 'threshold', '20'], ['we', 'built', 'a', 'sourcetotarget', 'pbsmt', 'model', 'from', 'the', 'bilingual', 'domain', 'corpus', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['bullet', 'naive', 'bayesnb', 'we', 'use', 'binomial', 'variant', 'with', 'laplace', 'smoothing', 'parameter', '', '1', 'pedregosa', 'et', 'al', '2011', 'bullet', 'logistic', 'regressionlr', 'we', 'use', 'logistic', 'regression', 'with', 'l2', 'loss', 'penalty', 'and', 'c1', 'pedregosa', 'et', 'al', '2011'], ['we', 'built', 'a', 'sourcetotarget', 'pbsmt', 'model', 'from', 'the', 'bilingual', 'domain', 'corpus', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'tokenize', 'and', 'frequentcase', 'the', 'data', 'with', 'the', 'standard', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['an', 'unpruned', 'modified', 'kneserneysmoothed', '4gram', 'language', 'model', 'is', 'estimated', 'using', 'the', 'kenlm', 'toolkit', 'heafield', 'et', 'al', '2013', 'the', 'language', 'model', 'is', 'a', 'standard', '5gram', 'model', 'estimated', 'from', 'the', 'monolingual', 'data', 'using', 'modified', 'kneserney', 'smoothing', 'without', 'pruning', 'applying', 'kenlm', 'tools', 'heafield', 'et', 'al', '2013'], ['for', 'native', 'data', 'several', 'teams', 'make', 'use', 'of', 'the', 'web', '1t', '5gram', 'corpus', 'henceforth', 'web1t', 'brants', 'and', 'franz', '2006', 'we', 'used', 'the', 'unigram', 'counts', 'from', 'the', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'to', 'determine', 'the', 'frequency', 'of', 'use', 'of', 'each', 'candidate', 'synonym'], ['we', 'measure', 'statistical', 'significance', 'using', '95', 'confidence', 'intervals', 'computed', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'the', 'column', 'pairci', 'shows', '95', 'confidence', 'intervals', 'relative', 'to', 'the', 'primary', 'system', 'using', 'the', 'paired', 'bootstrap', 'resampling', 'method', 'koehn', '2004'], ['1', 'the', 'models', 'are', 'constructed', 'using', 'c45', 'decision', 'tree', 'classifiers', 'as', 'implemented', 'within', 'weka', 'hall', 'et', 'al', '2009', '', 'with', 'default', 'parameter', 'settingsthe', 'weka', 'smo', 'implementation', 'of', 'svm', 'hall', 'et', 'al', '2009', 'was', 'used', 'as', 'classifier', 'with', 'default', 'parameter', 'settings'], ['it', 'therefore', 'follows', 'the', 'distributional', 'hypothesis', 'harris', '1954', 'which', 'states', 'that', 'words', 'that', 'occur', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meaningsdistributional', 'similarity', 'relies', 'on', 'the', 'distributional', 'hypothesis', 'that', 'similar', 'terms', 'appear', 'in', 'similar', 'contexts', 'harris', '1954'], ['consider', 'the', 'two', 'examples', 'below', 'drawn', 'from', 'the', 'penn', 'discourse', 'treebank', 'pdtb', 'prasad', 'et', 'al', '2008', '', 'of', 'a', 'causal', 'and', 'a', 'contrast', 'relation', 'respectively', 'pitler', 'and', 'nenkova', '2008', 'used', 'discourse', 'relations', 'of', 'the', 'penn', 'discourse', 'treebank', 'prasad', 'et', 'al', '2008', 'as', 'a', 'feature'], ['ble', 'is', 'based', 'on', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'stating', 'that', 'words', 'with', 'similar', 'meaning', 'have', 'similar', 'distributions', 'across', 'languages', 'distributional', 'similarity', 'relies', 'on', 'the', 'distributional', 'hypothesis', 'that', 'similar', 'terms', 'appear', 'in', 'similar', 'contexts', 'harris', '1954'], ['we', 'use', 'the', 'bleu', 'score', 'as', 'primary', 'criterion', 'which', 'is', 'optimized', 'on', 'the', 'development', 'set', 'using', 'the', 'downhill', 'simplex', 'algorithm', 'press', 'et', 'al', '2002', 'the', 'optimization', 'is', 'done', 'using', 'the', 'downhill', 'simplex', 'algorithm', 'from', 'the', 'numerical', 'recipes', 'book', 'press', 'et', 'al', '2002'], ['the', 'relation', 'prediction', 'task', 'of', 'science', 'ie', 'is', 'challenging', 'and', 'quite', 'different', 'from', 'other', 'semantic', 'relation', 'prediction', 'task', 'like', 'semeval2010', 'task', '8', 'hendrickx', 'et', 'al', '2009', 'the', 'semeval2010', 'task', '8', 'dataset', 'is', 'a', 'widely', 'used', 'benchmark', 'for', 'relation', 'classification', 'hendrickx', 'et', 'al', '2009'], ['machine', 'translation', 'system', 'settings', 'we', 'used', 'a', 'phrasebased', 'statistical', 'machine', 'translation', 'model', 'as', 'implemented', 'in', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'machine', 'translationwe', 'compare', 'the', 'model', 'against', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', '', 'applied', 'to', 'phoneme', 'sequences'], ['we', 'use', 'the', 'mrs', 'analyses', 'that', 'are', 'produced', 'by', 'the', 'hpsg', 'english', 'resource', 'grammar', 'erg', 'flickinger', '2000', 'the', 'experiments', 'are', 'carried', 'out', 'on', 'a', 'broadcoverage', 'linguisticallyprecise', 'hpsg', 'grammar', 'for', 'english', 'the', 'lingo', 'english', 'resource', 'grammar', 'erg', 'copestake', 'and', 'flickinger', '2000'], ['in', 'each', 'case', 'the', 'improvement', 'of', 'ebmt', 'tm', '', 'smt', 'over', 'the', 'baseline', 'smt', 'is', 'statistically', 'significant', 'reliability', 'of', '98', 'using', 'bootstrap', 'resampling', 'koehn', '2004', 'in', 'all', 'cases', 'results', 'are', 'statistically', 'significant', '99', 'following', 'the', 'pair', 'bootstrap', 'resampling', 'koehn', '2004'], ['to', 'capture', 'typical', 'partof', 'bridging', 'see', 'example', '2', 'we', 'extract', 'a', 'list', 'of', '45', 'nouns', 'which', 'specify', 'building', 'parts', 'eg', 'room', 'or', 'roof', 'from', 'the', 'general', 'inquirer', 'lexicon', 'stone', 'et', 'al', '1966', 'we', 'extract', 'a', 'list', 'containing', 'around', '4000', 'relational', 'nouns', 'from', 'wordnet', 'and', 'a', 'list', 'containing', 'around', '500', 'nouns', 'that', 'specify', 'professional', 'roles', 'from', 'the', 'general', 'inquirer', 'lexicon', 'stone', 'et', 'al', '1966'], ['the', 'surfacesyntactic', 'representation', '', 'p', 'was', 'a', 'standard', 'firstorder', 'edge', 'factorization', 'using', 'the', 'same', 'features', 'as', 'mcdonald', 'et', 'al', '2005', 'the', 'number', 'of', 'features', 'extracted', 'from', 'the', 'pdt', 'training', 'set', 'was', '13', '450', '672', 'using', 'the', 'feature', 'set', 'outlined', 'by', 'mcdonald', 'et', 'al', '2005'], ['in', 'our', 'work', 'like', 'hernault', 'et', 'al', '2010', '', 'we', 'also', 'consider', 'the', 'discourse', 'segmentation', 'task', 'as', 'a', 'sequence', 'labeling', 'problemsimilar', 'to', 'the', 'work', 'of', 'hernault', 'et', 'al', '2010', '', 'our', 'base', 'model', 'uses', 'conditional', 'random', 'fields', '1', 'to', 'learn', 'a', 'sequence', 'labeling', 'model'], ['we', 'use', 'the', 'data', 'that', 'were', 'recorded', 'and', 'preprocessed', 'by', 'mitchell', 'et', 'al', '2008', '', 'available', 'for', 'download', 'in', 'their', 'supporting', 'online', 'material1', 'full', 'details', 'of', 'the', 'experimental', 'protocol', 'data', 'acquisition', 'and', 'preprocessing', 'can', 'be', 'found', 'in', 'mitchell', 'et', 'al', '2008', 'and', 'the', 'supporting', 'material'], ['we', 'trained', 'nonprojective', 'dependency', 'parsers', 'for', '6', 'languages', 'using', 'the', 'conllx', 'shared', 'task', 'datasets', 'buchholz', 'and', 'marsi', '2006', '', 'arabic', 'danish', 'dutch', 'japanese', 'slovene', 'and', 'spanishfor', 'nonprojective', 'parsing', 'experiments', 'four', 'languages', 'from', 'the', 'conll', 'x', 'shared', 'task', 'are', 'used', 'danish', 'dutch', 'slovene', 'and', 'swedish', 'buchholz', 'and', 'marsi', '2006'], ['our', 'lp', 'constraints', 'based', 'on', 'the', 'new', 'type', 'marker', 'are', 'compatible', 'with', 'those', 'of', 'cho', '', 'chai', '2000', 'on', 'the', 'one', 'hand', 'and', 'are', 'able', 'to', 'deal', 'with', 'the', 'scrambling', 'between', 'adjuncts', 'and', 'arguments', 'on', 'the', 'other', 'handfor', 'the', 'theory', 'of', 'cho', '', 'chai', '2000', 'to', 'be', 'complete', 'we', 'have', 'proposed', 'a', 'new', 'type', 'of', 'marker', 'and', 'the', 'adjunct', 'lp', 'constraint', 'in', 'conjunction', 'with', 'their', 'argument', 'lp', 'constraint'], ['the', 'wsj', 'grammar', 'covers', 'the', 'upenn', 'wall', 'street', 'journal', 'wsj', 'treebank', 'sentences', 'marcus', 'et', 'al', '1994', 'the', 'approach', 'presented', 'in', 'this', 'paper', 'is', 'a', 'first', 'attempt', 'to', 'scale', 'up', 'stochastic', 'parsing', 'systems', 'based', 'on', 'linguistically', 'finegrained', 'handcoded', 'grammars', 'to', 'the', 'upenn', 'wall', 'street', 'journal', 'henceforth', 'wsj'], ['statistically', 'significant', 'results', 'calculated', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'for', 'bleu', 'and', 'nist', 'are', 'indicated', 'with', 'symbols', 'p', '', '001', 'and', 'p', '', '005we', 'calculated', 'significance', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004'], ['in', 'testing', 'we', 'used', 'minimum', 'bayes', 'risk', 'decoding', 'kumar', 'and', 'byrne', '2004', '', 'cube', 'pruning', 'and', 'the', 'operation', 'sequence', 'model', 'durrani', 'et', 'al', '2011', 'selects', 'the', 'translation', 'with', 'minimum', 'bayes', 'risk', 'kumar', 'and', 'byrne', '2004'], ['selects', 'the', 'translation', 'with', 'minimum', 'bayes', 'risk', 'kumar', 'and', 'byrne', '2004', 'additionally', 'we', 'will', 'compare', 'two', 'decision', 'rules', 'the', 'common', 'maximum', 'aposteriori', 'map', 'decision', 'rule', 'and', 'the', 'minimum', 'bayes', 'risk', 'mbr', 'decision', 'rule', 'kumar', 'and', 'byrne', '2004'], ['we', 'use', 'a', 'prototypebased', 'selectional', 'preference', 'model', 'erk', '2007', 'we', 'build', 'on', 'a', 'recent', 'selectional', 'preference', 'model', 'erk', '2007', 'that', 'bases', 'its', 'generalisations', 'on', 'word', 'similarity', 'in', 'a', 'vector', 'space'], ['we', 'used', '10fold', 'crossvalidation', 'set', 'the', 'confidence', 'interval', 'to', '95', 'and', 'used', 'the', 'jackknifing', 'procedure', 'for', 'multiannotation', 'evaluation', 'lin', '2004', 'rouge2', 'metric', 'lin', '2004', 'is', 'used', 'for', 'the', 'evaluation'], ['the', 'brown', 'corpus', 'tagged', 'with', 'wordnet', 'senses', 'miller', 'et', 'al', '1993', 'the', 'senses', 'in', 'wordnet', 'are', 'ordered', 'according', 'to', 'the', 'frequency', 'data', 'in', 'the', 'manually', 'tagged', 'resource', 'sem', 'cor', 'miller', 'et', 'al', '1993'], ['as', 'an', 'implementation', 'we', 'use', 'svm', 'light', 'joachims', '1999', 'as', 'a', 'supervised', 'classifier', 'for', 'vsm', 'and', 'wiki', 'we', 'chose', 'support', 'vector', 'machines', 'using', 'svm', 'light', 'joachims', '1999'], ['for', 'translation', 'tables', '', 'the', 'moses', 'system', 'koehn', 'et', 'al', '2007', 'as', 'well', 'as', 'portage', 'offer', 'model', 'filtering', 'moses', 'offline', 'portage', 'offline', 'andor', 'at', 'load', 'timewe', 'compare', 'the', 'model', 'against', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', '', 'applied', 'to', 'phoneme', 'sequences'], ['the', 'particle', 'filter', 'of', 'canini', 'et', 'al', '2009', 'rejuvenates', 'over', 'independent', 'draws', 'from', 'the', 'history', 'by', 'storing', 'all', 'past', 'observations', 'and', 'statesthe', 'particle', 'filter', 'studied', 'empirically', 'by', 'canini', 'et', 'al', '2009', '', 'stored', 'the', 'entire', 'history', 'incurring', 'linear', 'storage', 'complexity', 'in', 'the', 'size', 'of', 'the', 'stream'], ['we', 'describe', 'an', 'approximation', 'to', 'the', 'bleu', 'score', 'papineni', 'et', 'al', '2001', 'that', 'will', 'satisfy', 'these', 'conditions', 'we', 'here', 'describe', 'a', 'linear', 'approximation', 'to', 'the', 'logbleu', 'score', 'papineni', 'et', 'al', '2001', 'which', 'allows', 'such', 'a', 'decomposition'], ['we', 'applied', 'the', 'naive', 'bayes', 'probabilistic', 'supervised', 'learning', 'algorithm', 'from', 'the', 'weka', 'machine', 'learning', 'library', 'hall', 'et', 'al', '2009', 'we', 'conducted', 'experiments', 'using', 'multinomial', 'naive', 'bayes', 'classifier', 'implemented', 'in', 'the', 'weka', 'toolkit', 'hall', 'et', 'al', '2009'], ['we', 'minimize', 'the', 'cross', 'entropy', 'loss', 'using', 'gradientbased', 'optimization', 'and', 'the', 'adam', 'update', 'rule', 'kingma', 'and', 'ba', '2014', 'the', 'network', 'is', 'trained', 'using', 'sgd', 'with', 'shuffled', 'minibatches', 'using', 'the', 'adam', 'update', 'rule', 'kingma', 'and', 'ba', '2014'], ['statistical', 'significance', 'in', 'bleu', 'score', 'difference', 'was', 'measured', 'by', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'statistical', 'significance', 'of', 'the', 'difference', 'between', 'systems', 'is', 'computed', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'p', '', '005', '1', '000', 'iterations'], ['we', 'measure', 'statistical', 'significance', 'using', '95', 'confidence', 'intervals', 'computed', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'for', 'all', 'results', 'we', 'computed', 'their', 'confidence', 'intervals', 'p', '', '005', 'by', 'means', 'of', 'bootstrap', 'resampling', 'koehn', '2004'], ['3', 'as', 'verbs', 'we', 'take', 'all', 'tags', 'that', 'map', 'to', 'v', 'in', 'the', 'universal', 'tag', 'mappings', 'from', 'petrov', 'et', 'al', '2012', 'in', 'the', 'pos', 'tag', 'level', 'we', 'basically', 'used', 'the', 'universal', 'tagset', 'proposed', 'by', 'petrov', 'et', 'al', '2012', '', 'in', 'mapping', 'original', 'tags', 'into', 'universal', 'ones'], ['we', 'use', 'the', 'mrs', 'analyses', 'that', 'are', 'produced', 'by', 'the', 'hpsg', 'english', 'resource', 'grammar', 'erg', 'flickinger', '2000', 'here', 'we', 'use', '1', 'the', 'link', 'grammar', 'parser', '8', 'and', '2', 'the', 'hpsg', 'english', 'resource', 'grammar', 'copestake', 'and', 'flickinger', '2000', 'and', 'pet', 'parser'], ['we', 'measure', 'statistical', 'significance', 'using', '95', 'confidence', 'intervals', 'computed', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'for', 'statistical', 'significance', 'testing', 'we', 'use', 'a', 'paired', 'bootstrap', 'resampling', 'method', 'proposed', 'in', 'koehn', '2004'], ['on', 'semantic', 'role', 'labeling', 'gildea', 'and', 'jurafsky', '2002', 'first', 'presented', 'a', 'system', 'based', 'on', 'a', 'statistical', 'classifier', 'which', 'is', 'trained', 'on', 'a', 'handannotated', 'corpora', 'framenetgildea', 'and', 'jurafsky', '2002', '', 'were', 'the', 'first', 'to', 'describe', 'a', 'statistical', 'system', 'trained', 'on', 'the', 'data', 'from', 'the', 'framenet', 'project', 'to', 'automatically', 'assign', 'semantic', 'roles'], ['we', 'apply', 'the', 'stochastic', 'gradient', 'descent', 'algorithm', 'with', 'minibatches', 'and', 'the', 'adadelta', 'update', 'rule', 'zeiler', '2012', 'we', 'use', 'a', 'minibatch', 'size', 'of', '100', 'and', 'use', 'adadelta', 'zeiler', '2012', 'as', 'the', 'gradient', 'update', 'method'], ['we', 'used', 'the', 'linear', 'svm', 'implementation', 'with', 'default', 'parameters', 'and', 'random', 'forest', 'with', '50', 'trees', 'both', 'available', 'at', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'we', 'use', 'the', 'scikit', 'implementation', 'of', 'svm', 'pedregosa', 'et', 'al', '2011'], ['we', 'test', 'the', 'statistical', 'significance', 'of', 'differences', 'between', 'various', 'mt', 'systems', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004', 'statistical', 'significance', 'tests', 'are', 'performed', 'using', 'bootstrap', 'resampling', 'koehn', '2004'], ['in', 'particular', 'the', 'parser', 'implements', 'the', 'arcstandard', 'parsing', 'algorithm', 'nivre', '2004', '1', 'the', 'parser', 'implements', 'the', 'arcstandard', 'algorithm', 'nivre', '2004', 'and', 'it', 'therefore', 'makes', 'use', 'of', 'a', 'stack', 'and', 'a', 'buffer'], ['we', 'use', 'adadelta', 'zeiler', '2012', 'to', 'update', 'the', 'parameters', 'during', 'trainingin', 'the', 'training', 'procedure', 'we', 'use', 'adadelta', 'zeiler', '2012', 'to', 'update', 'model', 'parameters', 'with', 'a', 'minibatch', 'size', '80'], ['we', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005', 'the', 'systems', 'for', 'the', 'english', '', 'spanish', 'translation', 'tasks', 'were', 'trained', 'on', 'the', 'sentencealigned', 'europarl', 'corpus', 'koehn', '2005'], ['ccg', 'is', 'a', 'lexicalized', 'theory', 'of', 'grammar', 'steedman', '2001', 'the', 'input', 'of', 'the', 'boxer', 'system', 'is', 'a', 'syntactic', 'analysis', 'in', 'the', 'form', 'of', 'a', 'derivation', 'of', 'combinatorial', 'categorial', 'grammar', 'ccg', 'steedman', '2001'], ['the', 'results', 'for', 'teslam', 'and', 'teslaf', 'have', 'previously', 'been', 'reported', 'in', 'liu', 'et', 'al', '2010', '3', 'tesla', 'translation', 'evaluation', 'of', 'sentences', 'with', 'linearprogrammingbased', 'analysis', 'was', 'first', 'proposed', 'in', 'liu', 'et', 'al', '2010'], ['the', 'semantic', 'representation', 'is', 'minimal', 'recursion', 'semantics', 'copestake', 'et', 'al', '2005', 'the', 'erg', 'produces', 'minimal', 'recursion', 'semantics', 'mrs', 'copestake', 'et', 'al', '2005', 'analyses', 'which', 'are', 'flat', 'structures', 'that', 'explicitly', 'encode', 'predicate', 'argument', 'relations', 'and', 'other', 'data'], ['germanet', 'gn', 'is', 'the', 'german', 'counterpart', 'to', 'wn', 'hamp', 'and', 'feldweg', '1997', 'future', 'work', 'could', 'be', 'to', 'extend', 'the', 'german', 'dataset', 'by', 'adding', 'additional', 'resources', 'to', 'the', 'llr', 'for', 'instance', 'germanet', 'hamp', 'and', 'feldweg', '1997'], ['baroni', 'et', 'al', '2002', '', 'report', 'that', '47', 'of', 'the', 'vocabulary', 'types', 'in', 'the', 'apa', 'corpus', '2', 'were', 'compounds', 'baroni', 'et', 'al', '2002', '', 'analyzed', 'the', '28', 'million', 'words', 'german', 'apa', 'news', 'corpus', 'and', 'discovered', 'that', 'compounds', 'account', 'for', '47', 'of', 'the', 'word', 'types', 'but', 'only', '7', 'of', 'the', 'overall', 'token', 'count', 'with', '83', 'of', 'compounds'], ['both', 'language', 'models', 'use', 'modified', 'kneserney', 'smoothing', 'chen', 'and', 'goodman', '1996', '5gram', 'language', 'models', 'are', 'trained', 'over', 'the', 'targetside', 'of', 'the', 'training', 'data', 'using', 'srilm', 'stolcke', '2002', 'with', 'modified', 'kneserney', 'discounting', 'chen', 'and', 'goodman', '1996'], ['we', 'used', 'mallet', 'mccallum', '2002', 'for', 'this', 'experiment', 'and', 'the', 'same', 'trainingtesting', 'partitions', 'used', 'in', 'the', 'experiment', 'reported', 'in', 'table', '3', '10', 'we', 'used', 'the', 'pltm', 'implementation', 'in', 'mallet', 'mccallum', '2002'], ['we', 'calculated', 'significance', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'the', 'statistical', 'significance', 'test', 'is', 'also', 'carried', 'out', 'with', 'paired', 'bootstrap', 'resampling', 'method', 'with', 'p', '', '0001', 'intervals', 'koehn', '2004'], ['additionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'finally', 'we', 'used', 'moses', 'toolkit', 'as', 'phrasebased', 'reference', 'koehn', 'et', 'al', '2007'], ['for', 'building', 'the', 'baseline', 'smt', 'system', 'we', 'used', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', '', 'in', 'its', 'standard', 'setupwe', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settings'], ['this', 'results', 'in', 'the', 'semantic', 'triple', 'shotmanbird', 'lemmatized', 'to', 'shootmanbird', 'using', 'the', 'stanford', 'corenlp', 'lemmatizer', 'manning', 'et', 'al', '2014', 'we', 'also', 'lemmatized', 'all', 'words', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['we', 'used', 'standard', 'classifiers', 'available', 'in', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011', '3', 'we', 'used', 'logistic', 'regression', 'though', 'linear', 'svm', 'showed', 'almost', 'the', 'same', 'results', 'for', 'the', 'final', 'reranking', 'the', 'implementation', 'was', 'taken', 'from', 'scikitlearn', 'package', 'pedregosa', 'et', 'al', '2011'], ['rhetorical', 'structure', 'theory', 'mann', 'and', 'thompson', '1988', 'belongs', 'to', 'the', 'first', 'sortthe', 'closest', 'area', 'to', 'our', 'work', 'consists', 'of', 'investigations', 'of', 'discourse', 'relations', 'in', 'the', 'context', 'of', 'rhetorical', 'structure', 'theory', 'mann', 'and', 'thompson', '1988'], ['both', 'language', 'models', 'use', 'modified', 'kneserney', 'smoothing', 'chen', 'and', 'goodman', '1996', '5gram', 'language', 'models', 'are', 'trained', 'over', 'the', 'targetside', 'of', 'the', 'training', 'data', 'using', 'srilm', 'stolcke', '2002', 'with', 'modified', 'kneserney', 'discounting', 'chen', 'and', 'goodman', '1996'], ['we', 'train', 'the', 'concept', 'identification', 'stage', 'using', 'infinite', 'ramp', 'loss', 'with', 'adagrad', 'duchi', 'et', 'al', '2011', '1', 'using', 'adagrad', 'duchi', 'et', 'al', '2011'], ['for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'all', 'our', 'translation', 'systems', 'are', 'based', 'on', 'moses', 'koehn', 'et', 'al', '2007', 'and', 'standard', 'components', 'for', 'training', 'and', 'tuning', 'the', 'models'], ['we', 'use', 'rouge', 'score', 'as', 'our', 'evaluation', 'metric', 'lin', '2004', 'with', 'standard', 'options', '8', 'we', 'expect', 'this', 'restriction', 'is', 'more', 'consistent', 'with', 'the', 'rouge', 'evaluation', 'metric', 'used', 'for', 'summarization', 'lin', '2004'], ['for', 'building', 'the', 'baseline', 'smt', 'system', 'we', 'used', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', '', 'in', 'its', 'standard', 'setupfor', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'europarl', 'parallel', 'corpus', 'koehn', '2005', 'as', 'the', 'bitext', 'from', 'which', 'to', 'extract', 'the', 'auto', 'matic', 'bilingual', 'lexiconswe', 'extract', 'our', 'paraphrase', 'grammar', 'from', 'the', 'frenchenglish', 'portion', 'of', 'the', 'europarl', 'corpus', 'version', '5', 'koehn', '2005'], ['we', 'use', 'ridge', 'regression', 'rr', 'with', 'l2norm', 'regularization', 'and', 'support', 'vector', 'regression', 'svr', 'with', 'an', 'rbf', 'kernel', 'from', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'bullet', 'logistic', 'regressionlr', 'we', 'use', 'logistic', 'regression', 'with', 'l2', 'loss', 'penalty', 'and', 'c1', 'pedregosa', 'et', 'al', '2011'], ['all', 'data', 'used', 'in', 'our', 'experiments', 'are', 'sentencesplit', 'lowercased', 'and', 'tokenize', 'using', 'the', 'corenlp', 'toolkit', 'manning', 'et', 'al', '2014', 'we', 'tokenize', 'english', 'data', 'and', 'segment', 'chinese', 'data', 'using', 'the', 'stanford', 'corenlp', 'toolkit', 'manning', 'et', 'al', '2014'], ['since', 'the', 'phrase', 'table', 'contains', 'lemmas', 'the', 'wikipedia', 'corpus', 'was', 'lemmatised', 'using', 'the', 'treetagger', 'schmid', '1994', '1', 'the', 'corpus', 'is', 'lemmatised', 'and', 'tagged', 'by', 'partofspeech', 'on', 'both', 'sides', 'using', 'the', 'treetagger', 'schmid', '1994'], ['for', 'this', 'purpose', 'we', 'used', 'the', 'logistic', 'regression', 'classifier', 'from', 'scikitlearn', 'with', 'l2', 'regularisation', 'pedregosa', 'et', 'al', '2011', '1', 'we', 'use', 'logistic', 'regression', 'with', 'l2', 'regularization', 'implemented', 'using', 'the', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011'], ['all', 'our', 'systems', 'are', 'contrasted', 'with', 'a', 'standard', 'phrasebased', 'system', 'built', 'with', 'moses', 'koehn', 'et', 'al', '2007', 'our', 'machine', 'translation', 'systems', 'this', 'year', 'are', 'a', 'departure', 'from', 'our', 'previous', 'moses', 'koehn', 'et', 'al', '2007', 'based', 'systems', 'from', 'wmt16'], ['we', 'test', 'our', 'metrics', 'in', 'the', 'setting', 'of', 'the', 'wmt', '2009', 'evaluation', 'task', 'callisonburch', 'et', 'al', '2009', 'the', 'compared', 'systems', 'are', 'evaluated', 'on', 'the', 'englishtogerman', '13', 'news', 'translation', 'task', 'of', 'wmt', '2009', 'callisonburch', 'et', 'al', '2009'], ['these', 'classifiers', 'have', 'been', 'used', 'in', 'related', 'work', 'by', 'pang', 'et', 'al', '2002', 'we', 'evaluated', 'our', 'method', 'with', 'movie', 'review', 'documents', 'that', 'were', 'used', 'in', 'pang', 'et', 'al', '2002'], ['one', 'is', 'from', 'turian', 'et', 'al', '2010', '', 'the', 'dimension', 'of', 'word', 'embedding', 'is', '50for', 'example', 'turian', 'et', 'al', '2010', '', 'showed', 'that', 'the', 'optimal', 'dimensionality', 'for', 'word', 'embeddings', 'is', 'taskspecific'], ['word', 'alignment', 'using', 'giza', 'toolkit', 'och', 'and', 'ney', '2000', '', 'the', 'default', 'configuration', 'as', 'available', 'in', 'training', 'scripts', 'for', 'mosesword', 'alignment', 'is', 'performed', 'by', 'giza', 'och', 'and', 'ney', '2000', 'in', 'both', 'directions', 'with', 'the', 'default', 'setting'], ['we', 'found', 'that', 'using', 'adagrad', 'duchi', 'et', 'al', '2011', 'to', 'update', 'the', 'parameters', 'is', 'very', 'effective', '1', 'using', 'adagrad', 'duchi', 'et', 'al', '2011'], ['the', 'penn', 'discourse', 'treebank', 'pdtb', 'prasad', 'et', 'al', '2008', 'is', 'a', 'large', 'corpus', 'annotated', 'with', 'discourse', 'relations', 'covering', 'the', 'wall', 'street', 'journal', 'part', 'of', 'the', 'penn', 'treebankone', 'of', 'the', 'most', 'important', 'resources', 'for', 'discourse', 'connectives', 'in', 'english', 'is', 'the', 'penn', 'discourse', 'treebank', 'prasad', 'et', 'al', '2008'], ['the', 'system', 'was', 'trained', 'on', 'the', 'english', 'and', 'danish', 'part', 'of', 'the', 'europarl', 'corpus', 'version', '3', 'koehn', '2005', 'the', 'europarl', 'data', 'set', 'consists', 'of', '707', 'sentences', 'of', 'the', 'german', 'part', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['for', 'example', 'turian', 'et', 'al', '2010', 'compared', 'brown', 'clusters', 'cw', 'embeddings', 'and', 'hlbl', 'embeddings', 'in', 'ner', 'and', 'chunking', 'tasksturian', 'et', 'al', '2010', 'applied', 'word', 'embeddings', 'to', 'chunking', 'and', 'named', 'entity', 'recognition', 'ner'], ['we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007', 'in', 'particular', 'we', 'use', 'moses', 'koehn', 'et', 'al', '2007', 'to', 'train', 'a', 'monolingual', 'phrasebased', 'mt', 'system', 'on', 'the', 'paralex', 'corpus'], ['past', 'experiences', 'on', 'this', 'system', 'have', 'shown', 'that', 'the', 'random', 'forest', 'breiman', '2001', 'outperforms', 'other', 'regression', 'algorithms', 'like', 'support', 'vector', 'regression', 'or', 'multilayer', 'perceptronour', 'submitted', 'system', 'for', 'the', 'second', 'task', 'is', 'based', 'on', 'random', 'forest', 'breiman', '2001'], ['we', 'trained', 'a', 'number', 'of', 'frenchenglish', 'smt', 'systems', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'in', 'its', 'default', 'settingwe', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'train', 'standard', 'phrasebased', 'systems', 'with', 'default', 'configurations'], ['bullet', 'zhang2015', 'zhang', 'et', 'al', '2015', 'proposed', 'to', 'use', 'shallow', 'convolutional', 'neural', 'networks', 'to', 'model', 'two', 'arguments', 'respectivelyzhang', 'et', 'al', '2015', 'explore', 'a', 'shallow', 'convolutional', 'neural', 'network', 'and', 'achieve', 'competitive', 'performance'], ['all', 'data', 'used', 'in', 'our', 'experiments', 'are', 'sentencesplit', 'lowercased', 'and', 'tokenize', 'using', 'the', 'corenlp', 'toolkit', 'manning', 'et', 'al', '2014', 'we', 'tokenize', 'english', 'data', 'and', 'segment', 'chinese', 'data', 'using', 'the', 'stanford', 'corenlp', 'toolkit', 'manning', 'et', 'al', '2014'], ['the', 'major', 'part', 'of', 'data', 'comes', 'from', 'current', 'and', 'upcoming', 'film', 'releases', 'of', 'the', 'europarl', 'dataset', 'koehn', '2005', 'the', 'source', 'of', 'bilingual', 'data', 'used', 'in', 'the', 'experiments', 'is', 'the', 'europarl', 'collection', 'koehn', '2005'], ['we', 'expect', 'this', 'restriction', 'is', 'more', 'consistent', 'with', 'the', 'rouge', 'evaluation', 'metric', 'used', 'for', 'summarization', 'lin', '2004', 'rouge', 'lin', '2004', 'is', 'the', 'fully', 'automatic', 'metric', 'commonly', 'used', 'to', 'evaluate', 'the', 'text', 'summarization', 'results'], ['additionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'train', 'standard', 'phrasebased', 'systems', 'with', 'default', 'configurations'], ['additionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'decoder', 'searches', 'for', 'the', 'best', 'translation', 'given', 'a', 'set', 'of', 'models', 'h', 'm', 'e', 'i', '1', '', 's', 'k', '1', '', 'f', 'j', '1', 'by', 'maximizing', 'the', 'loglinear', 'feature', 'score', 'och', 'and', 'ney', '2004', '', 'e', 'i', '1', '', 'arg', 'max', 'ie', 'i', '1', 'm', 'm1the', 'loglinear', 'approach', 'to', 'phrasebased', 'translation', 'och', 'and', 'ney', '2004', 'directly', 'models', 'the', 'predictive', 'translation', 'distribution', 'pef', '', 'w', '', '1', 'zf', 'exp', 'w', 'e', 'f', '1', 'where', 'e', 'is', 'the', 'target', 'string'], ['the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'train', 'standard', 'phrasebased', 'systems', 'with', 'default', 'configurations'], ['the', 'usages', 'from', 'the', 'ukwac', 'are', 'tokenised', 'and', 'lemmatised', 'using', 'treetagger', 'schmid', '1994', '', 'as', 'provided', 'by', 'the', 'corpus3', 'all', 'english', 'data', 'are', 'pos', 'tagged', 'and', 'lemmatised', 'using', 'the', 'treetagger', 'schmid', '1994'], ['to', 'quantify', 'the', 'redundancy', 'of', 'structures', 'we', 'partof', 'speech', 'tagger', 'the', 'english', 'gigaword', 'corpus', 'graff', 'and', 'cieri', '2003', 'the', 'trigram', 'target', 'language', 'model', 'is', 'trained', 'from', 'the', 'xinhua', 'portion', 'of', 'english', 'gigaword', 'corpus', 'graff', 'and', 'cieri', '2003'], ['baseline', 'word', 'alignments', 'were', 'obtained', 'by', 'running', 'giza', 'in', 'both', 'directions', 'and', 'symmetrizing', 'using', 'the', 'growdiagfinaland', 'heuristic', 'och', 'and', 'ney', '2003', 'translation', 'models', 'were', 'trained', 'over', 'the', 'bilingual', 'data', 'that', 'was', 'automatically', 'wordaligned', 'using', 'giza', 'och', 'and', 'ney', '2003', 'in', 'both', 'directions', '', 'and', 'the', 'diaggrowfinal', 'heuristic', 'was', 'used', 'to', 'ldc200'], ['the', 'results', 'displayed', 'in', 'table', '3', 'are', 'obtained', 'with', 'the', 'smo', 'classifier', 'trained', 'using', 'the', 'weka', 'library', 'hall', 'et', 'al', '2009', 'on', 'our', 'downloaded', 'semeval', '2013', 'development', 'and', 'training', 'corpora', '7595', 'tweetsthe', 'core', 'model', 'is', 'a', 'decision', 'tree', 'classifier', 'trained', 'on', 'the', 'qalb', 'parallel', 'training', 'data', 'using', 'weka', 'hall', 'et', 'al', '2009'], ['we', 'trained', 'a', 'number', 'of', 'frenchenglish', 'smt', 'systems', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'in', 'its', 'default', 'settingwe', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'performance', 'of', 'our', 'algorithm', 'is', 'compared', 'with', 'the', 'disambiguation', 'accuracy', 'obtained', 'with', 'a', 'variation', 'of', 'the', 'lesk', 'algorithm', '3', 'lesk', '1986', '', 'which', 'selects', 'the', 'meaning', 'of', 'an', 'openclass', 'word', 'by', 'findinote', 'that', 'word', 'sense', 'disambiguation', 'is', 'performed', 'using', 'the', 'lesk', 'algorithm', 'lesk', '1986'], ['we', 'use', 'the', 'scikit', 'implementation', 'of', 'svm', 'pedregosa', 'et', 'al', '2011', 'we', 'use', 'a', 'wellknown', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'implementation', 'of', 'random', 'forests', 'and', 'svms', 'in', 'python', 'as', 'well', 'as', 'a', 'very', 'effective', 'gradient', 'boosting', 'trees', 'implementation', 'from', 'the', 'xgboost', 'libr'], ['for', 'example', 'turian', 'et', 'al', '2010', 'showed', 'that', 'the', 'optimal', 'dimensionality', 'for', 'word', 'embeddings', 'is', 'task', 'specific', 'turian', 'et', 'al', '2010', 'applied', 'word', 'embeddings', 'to', 'chunking', 'and', 'named', 'entity', 'recognition', 'ner'], ['ease', 'uses', 'nltk', 'bird', 'et', 'al', '2009', 'for', 'pos', 'tagging', 'and', 'stemming', 'a', 'spell', 'for', 'spell', 'checking', 'and', 'wordnet', 'fellbaum', '1998', 'to', 'get', 'the', 'synonyms', 'we', 'used', 'the', 'brill', 'tagger', 'provided', 'by', 'nltk', 'for', 'our', 'pos', 'tagging', 'bird', 'et', 'al', '2009'], ['all', 'our', 'translation', 'systems', 'are', 'based', 'on', 'moses', 'koehn', 'et', 'al', '2007', 'and', 'standard', 'components', 'for', 'training', 'and', 'tuning', 'the', 'modelsall', 'our', 'systems', 'are', 'contrasted', 'with', 'a', 'standard', 'phrasebased', 'system', 'built', 'with', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007'], ['additionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'thank', 'gbor', 'recski', 'has', 'research', 'institute', 'for', 'linguistics', 'for', 'performing', 'the', 'pca', 'on', 'the', 'operators', 'of', 'the', 'socher', 'et', 'al', '2013', 'cvgwe', 'employed', 'the', 'stateoftheart', 'sentiment', 'annotator', 'by', 'socher', 'et', 'al', '2013', 'for', 'this', 'purpose'], ['it', 'has', 'been', 'shown', 'in', 'previous', 'work', 'on', 'relation', 'extraction', 'that', 'the', 'shortest', 'dependency', 'path', 'between', 'any', 'two', 'entities', 'captures', 'the', 'information', 'required', 'to', 'assert', 'a', 'relationship', 'between', 'them', 'bunescu', 'and', 'mooney', '2005', 'our', 'work', 'is', 'also', 'related', 'to', 'bunescu', 'and', 'mooney', '2005', '', 'where', 'the', 'similarity', 'between', 'the', 'words', 'on', 'the', 'path', 'connecting', 'two', 'entities', 'in', 'the', 'dependency', 'graph', 'is', 'used', 'to', 'devise', 'a', 'kernel', 'function'], ['these', 'algorithms', 'were', 'used', 'to', 'participate', 'in', 'the', 'the', 'expression', 'level', 'task', 'subtask', 'a', 'and', 'message', 'level', 'task', 'subtask', 'b', 'of', 'the', 'semeval2014', 'task', '9', 'sentiment', 'analysis', 'in', 'twitter', 'rosenthal', 'et', 'al', 'the', 'system', 'that', 'obtained', 'the', 'best', 'performance', 'in', 'the', 'sentiment', 'analysis', 'at', 'message', 'level', 'task', 'of', 'semeval', '2013', 'nakov', 'et', 'al', '2013', 'and', '2014', 'rosenthal', 'et', 'al', '2014', 'mined', 'twitter', 'to', 'build', 'big', 'sent'], ['phrasebased', 'smt', 'system', 'a', 'standard', 'non', 'factored', 'phrasebased', 'smt', 'system', 'was', 'built', 'using', 'the', 'open', 'source', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'parameters', 'set', 'similar', 'to', 'those', 'of', 'neubig', '2011smt', 'translations', 'a', 'phrasebased', 'smt', 'system', 'built', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'and', 'the', 'whole', 'spanishenglish', 'dataset', 'except', 'the', 'sentences', 'in', 'the', 'test', 'set', 'was', 'used', 'to', 'translated', 'the'], ['for', 'the', 'theory', 'of', 'cho', '', 'chai', '2000', 'to', 'be', 'complete', 'we', 'have', 'proposed', 'a', 'new', 'type', 'of', 'marker', 'and', 'the', 'adjunct', 'lp', 'constraint', 'in', 'conjunction', 'with', 'their', 'argument', 'lp', 'constraintgiven', 'the', 'lp', 'constraints', 'in', '4', '7', 'and', '8', 'we', 'can', 'provide', 'a', 'simpler', 'explanation', 'to', 'various', 'scrambled', 'sentences', 'including', 'the', 'alleged', 'counterexamples', 'to', 'the', 'analysis', 'of', 'cho', '', 'chai', '2000'], ['all', 'linguistic', 'annotations', 'needed', 'for', 'features', 'pos', 'chunks', '7', '', 'parses', 'are', 'from', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'part', 'of', 'speech', 'tagging', 'and', 'named', 'entity', 'recognition', 'for', 'comment', 'plausibility', 'features', 'are', 'carried', 'out', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['algorithm', '5', 'shows', 'a', 'passiveaggressive', 'algorithm', 'for', 'the', 'structured', 'output', 'crammer', 'et', 'al', '2006', 'to', 'build', 'a', 'parser', 'we', 'use', 'a', 'structured', 'classifier', 'to', 'approximate', 'the', 'oracle', 'and', 'apply', 'the', 'passiveaggressive', 'pa', 'algorithm', 'crammer', 'et', 'al', '2006', 'for', 'parameter', 'estimation'], ['the', 'task', 'is', 'part', 'of', 'the', 'semantic', 'evaluation', '2012', 'workshop', 'agirre', 'et', 'al', '2012', 'semantic', 'textual', 'similarity', 'sts', 'is', 'the', 'task', 'of', 'judging', 'the', 'similarity', 'of', 'a', 'pair', 'of', 'sentences', 'on', 'a', 'scale', 'from', '1', 'to', '5', 'agirre', 'et', 'al', '2012'], ['we', 'used', 'the', 'same', 'annotation', 'guidelines', 'as', 'zaidan', 'et', 'al', '2007', 'we', 'also', 'use', 'a0', 'to', 'compare', 'against', 'the', 'masking', 'svm', 'method', 'and', 'svm', 'baseline', 'of', 'zaidan', 'et', 'al', '2007'], ['the', 'stanford', 'parser', '1', 'marneffe', 'et', 'al', '2006', 'was', 'used', 'to', 'produce', 'all', 'dependency', 'parsesthe', 'same', 'kind', 'of', 'process', 'was', 'applied', 'to', 'the', 'penn', 'treebank', 'using', 'the', 'stanford', 'conversion', 'system', 'to', 'produce', 'dependency', 'annotations', 'de', 'marneffe', 'et', 'al', '2006'], ['1', 'after', 'tokenization', '', 'we', 'lemmatize', 'and', 'stem', 'tweets', 'and', 'remove', 'stopwords', 'from', 'each', 'tweet', 'using', 'the', 'nltk', 'toolkit', 'bird', 'et', 'al', '2009', 'for', 'reuters', 'we', 'segmented', 'and', 'tokenized', 'the', 'data', 'using', 'nltk', 'bird', 'et', 'al', '2009'], ['machine', 'translation', 'system', 'settings', 'we', 'used', 'a', 'phrasebased', 'statistical', 'machine', 'translation', 'model', 'as', 'implemented', 'in', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'machine', 'translationwe', 'use', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'implement', 'our', 'models'], ['the', 'phrase', 'table', 'is', 'extracted', 'from', 'a', 'bilingual', 'text', 'aligned', 'on', 'the', 'word', 'level', 'using', 'eg', 'giza', '', 'och', 'and', 'ney', '2003', 'the', 'corpus', 'is', 'first', 'wordaligned', 'using', 'a', 'word', 'alignment', 'heuristic', 'och', 'and', 'ney', '2003'], ['this', 'architecture', 'is', 'very', 'similar', 'to', 'the', 'framework', 'of', 'uima', 'ferrucci', 'and', 'lally', '2004', '3', 'dkpro', 'is', 'a', 'collection', 'of', 'software', 'components', 'for', 'natural', 'language', 'processing', 'based', 'on', 'the', 'apache', 'uima', 'framework', 'ferrucci', 'and', 'lally', '2004'], ['besides', 'using', 'sentistrength', 'we', 'use', 'the', 'lexicon', 'approach', 'proposed', 'by', 'hu', 'and', 'liu', '2004', 'in', 'order', 'to', 'construct', 'the', 'lexical', 'prior', 'knowledge', 'matrix', 'u', '0', '', 'we', 'use', 'the', 'sentiment', 'lexicon', 'generated', 'by', 'hu', 'and', 'liu', '2004'], ['run1', 'we', 'firstly', 'use', 'the', 'stanford', 'corenlp', 'toolkit', '3', 'manning', 'et', 'al', '2014', 'to', 'split', 'each', 'token', 'for', 'the', 'sentence', 'pairs', 'in', 'the', 'evaluation', 'datawe', 'use', 'the', 'stanford', 'corenlp', 'caseless', 'tagger', 'for', 'partofspeech', 'tagging', 'manning', 'et', 'al', '2014'], ['by', 'setting', 'n', 'inw', 'and', 'enwn1', 'for', 'all', 'nodes', 'the', 'generalized', 'kernel', 'can', 'be', 'converted', 'to', 'the', 'kernel', 'proposed', 'in', 'collins', 'and', 'duffy', '2001', 'the', 'proof', 'is', 'similar', 'to', 'the', 'proof', 'for', 'the', 'tree', 'kernel', 'in', 'collins', 'and', 'duffy', '2001'], ['2', 'we', 'tested', 'the', 'difference', 'in', 'performance', 'for', 'statistical', 'significance', 'using', 'an', 'approximate', 'randomization', 'procedure', 'yeh', '2000', 'with', '10000', 'iterations', 'we', 'calculate', 'statistical', 'significance', 'of', 'performance', 'differences', 'using', 'stratified', 'shuffling', 'yeh', '2000'], ['run1', 'we', 'firstly', 'use', 'the', 'stanford', 'corenlp', 'toolkit', '3', 'manning', 'et', 'al', '2014', 'to', 'split', 'each', 'token', 'for', 'the', 'sentence', 'pairs', 'in', 'the', 'evaluation', 'datawe', 'use', 'the', 'stanford', 'corenlp', 'caseless', 'tagger', 'for', 'partofspeech', 'tagging', 'manning', 'et', 'al', '2014'], ['all', 'linguistic', 'annotations', 'needed', 'for', 'features', 'pos', 'chunks', '7', '', 'parses', 'are', 'from', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'part', 'of', 'speech', 'tagging', 'and', 'named', 'entity', 'recognition', 'for', 'comment', 'plausibility', 'features', 'are', 'carried', 'out', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['in', 'erkan', 'and', 'radev', '2004', '', 'the', 'concept', 'of', 'graph', 'based', 'centrality', 'was', 'used', 'to', 'rank', 'a', 'set', 'of', 'sentences', 'in', 'producing', 'generic', 'multidocument', 'summariesit', 'was', 'also', 'the', 'model', 'used', 'to', 'rank', 'sentences', 'in', 'erkan', 'and', 'radev', '2004'], ['significance', 'was', 'tested', 'using', 'a', 'paired', 'bootstrap', 'koehn', '2004', 'with', '1000', 'samples', 'p002statistical', 'significance', 'is', 'tested', 'on', 'the', 'bleu', 'metric', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'with', 'n', '', '1000', 'and', 'p', '', '005'], ['for', 'seed', 'and', 'test', 'paradigms', 'we', 'used', 'verbal', 'inflectional', 'paradigms', 'from', 'the', 'celex', 'morphological', 'database', 'baayen', 'et', 'al', '1995', 'to', 'validate', 'this', 'measure', 'we', 'computed', 'the', 'cosine', 'similarity', 'between', 'words', 'and', 'their', 'morphological', 'parents', 'from', 'the', 'celex2', 'database', 'baayen', 'et', 'al', '1995'], ['we', 'used', 'kenlm', 'heafield', '2011', 'to', 'create', '3gram', 'language', 'models', 'with', 'kneserney', 'smoothing', 'on', 'the', 'target', 'side', 'of', 'the', 'bilingual', 'training', 'corpora1', 'we', 'used', 'the', 'kenlm', 'toolkit', 'heafield', '2011', 'to', 'build', 'all', 'language', 'models', 'used', 'in', 'this', 'work', 'ie', 'both', 'for', 'data', 'selection', 'and', 'for', 'the', 'mt', 'systems', 'used', 'for', 'extrinsic', 'evaluation'], ['we', 'built', 'a', '5gram', 'language', 'model', 'on', 'the', 'english', 'side', 'of', 'qcatrain', 'using', 'kenlm', 'heafield', '2011', 'we', 'used', 'kenlm', 'heafield', '2011', 'to', 'create', '3gram', 'language', 'models', 'with', 'kneserney', 'smoothing', 'on', 'the', 'target', 'side', 'of', 'the', 'bilingual', 'training', 'corpora'], ['an', 'estimate', 'of', 'the', 'likelihood', 'of', 'a', 'verb', 'taking', 'a', 'event', 'subject', 'was', 'computed', 'over', 'the', 'annotated', 'english', 'gigaword', 'v5', 'corpus', 'napoles', 'et', 'al', '2012', 'in', 'the', 'news', 'column', '', 'we', 'show', 'the', 'statistics', 'of', 'a', 'subset', 'of', 'annotated', 'english', 'gigaword', 'napoles', 'et', 'al', '2012'], ['we', 'use', 'the', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'to', 'compute', 'the', 'language', 'model', 'score', 'for', 'a', 'sentencesince', 'the', 'googleapi', 'is', 'not', 'available', 'any', 'more', 'we', 'use', 'the', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'to', 'extract', 'the', 'google', 'distance', 'feature'], ['machine', 'translation', 'system', 'settings', 'we', 'used', 'a', 'phrasebased', 'statistical', 'machine', 'translation', 'model', 'as', 'implemented', 'in', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'machine', 'translationthe', 'moses', 'smt', 'toolkit', 'koehn', 'et', 'al', '2007', 'provides', 'a', 'complete', 'statistical', 'translation', 'system', 'distributed', 'under', 'the', 'lgpl', 'license'], ['machine', 'translation', 'system', 'settings', 'we', 'used', 'a', 'phrasebased', 'statistical', 'machine', 'translation', 'model', 'as', 'implemented', 'in', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'machine', 'translationwe', 'trained', 'a', 'model', 'using', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'on', 'the', 'training', 'data', 'as', 'our', 'baseline', 'system'], ['a', 'chunk', 'is', 'a', 'minimal', '', 'nonrecursive', 'structure', 'consisting', 'of', 'correlated', 'groups', 'of', 'words', 'bharati', 'et', 'al', '2006', 'the', 'chunk', 'label', 'tagset', 'is', 'a', 'coarser', 'version', 'of', 'anncorra', 'tagset', 'bharati', 'et', 'al', '2006'], ['mc30', 'a', 'subset', 'of', 'rg65', 'dataset', 'with', '30', 'word', 'pairs', 'miller', 'and', 'charles', '1991', 'we', 'use', 'a', 'set', 'of', '30', 'word', 'pairs', 'from', 'a', 'study', 'carried', 'out', 'by', 'miller', 'and', 'charles', '1991'], ['we', 'obtained', 'newspeg', 'judgments', 'using', 'the', 'brat', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012', 'from', 'two', 'annotators', '3', 'all', 'annotations', 'were', 'done', 'using', 'the', 'brat', 'rapid', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012'], ['we', 'transformed', 'the', 'parse', 'trees', 'in', 'ontonotes', 'into', 'syntactic', 'dependencies', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'we', 'use', 'the', 'stanford', 'corenlp', 'caseless', 'tagger', 'for', 'partofspeech', 'tagging', 'manning', 'et', 'al', '2014'], ['gradient', 'clipping', 'heuristic', 'to', 'prevent', 'the', 'exploding', 'gradient', 'problem', 'graves', '2013', 'for', 'the', 'exploding', 'gradient', 'problem', 'numerical', 'stability', 'can', 'be', 'achieved', 'by', 'clipping', 'the', 'gradients', 'graves', '2013'], ['we', 'obtained', 'newspeg', 'judgments', 'using', 'the', 'brat', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012', 'from', 'two', 'annotators', '3', 'the', 'annotations', 'were', 'made', 'using', 'the', 'brat', 'rapid', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012'], ['we', 'then', 'describe', 'in', 'more', 'detail', 'a', 'modern', 'chinese', 'corpus', 'the', 'penn', 'chinese', 'treebank', 'xue', 'et', 'al', '2005', 'for', 'chinese', 'we', 'use', 'the', 'penn', 'chinese', 'treebank', 'version', '51', 'ctb', 'xue', 'et', 'al', '2005'], ['in', 'order', 'to', 'assess', 'statistical', 'significance', 'of', 'the', 'obtained', 'results', 'we', 'use', 'the', 'paired', 'bootstrap', 'resampling', 'method', 'koehn', '2004', 'which', 'estimates', 'the', 'probability', 'pvalue', 'that', 'a', 'measured', 'difference', 'for', 'statistical', 'significance', 'testing', 'we', 'use', 'a', 'paired', 'bootstrap', 'resampling', 'method', 'proposed', 'in', 'koehn', '2004'], ['statistical', 'significance', 'in', 'bleu', 'score', 'difference', 'was', 'measured', 'by', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'significance', 'was', 'tested', 'using', 'a', 'paired', 'bootstrap', 'koehn', '2004', 'with', '1000', 'samples', 'p002'], ['the', 'article', 'system', 'builds', 'on', 'the', 'elements', 'of', 'the', 'system', 'described', 'in', 'rozovskaya', 'and', 'roth', '2010c', 'the', 'preposition', 'classifier', 'uses', 'a', 'combined', 'system', 'building', 'on', 'work', 'described', 'in', '', 'and', 'rozovskaya', 'and', 'roth', '2010b'], ['for', 'calculating', 'the', 'required', 'frequencies', 'we', 'use', 'the', 'web1t', 'corpus', '6', 'brants', 'and', 'franz', '2006', 'as', 'a', 'practical', 'approximation', 'we', 'use', 'bigram', 'counts', 'from', 'the', 'web', '1t', 'corpus', 'brants', 'and', 'franz', '2006'], ['phrase', 'pairs', 'were', 'extracted', 'from', 'symmetrized', 'word', 'alignments', 'and', 'distortions', 'generated', 'by', 'giza', 'och', 'and', 'ney', '2003', 'using', 'the', 'combination', 'of', 'heuristics', 'growdiagfinaland', 'and', 'msdbidirectionalword', 'alignments', 'were', 'created', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'ran', 'all', 'of', 'our', 'experiments', 'in', 'weka', 'hall', 'et', 'al', '2009', 'using', 'logistic', 'regressionweka', 'hall', 'et', 'al', '2009', 'which', 'contains', 'the', 'implementation', 'of', 'all', 'three', 'algorithms', 'was', 'used', 'in', 'our', 'study'], ['both', 'corpora', 'were', 'extracted', 'from', 'the', 'open', 'parallel', 'corpus', 'opus', 'tiedemann', '2012', 'the', 'parallel', 'data', 'were', 'taken', 'from', 'opus', 'tiedemann', '2012', '', 'which', 'provides', 'sentencealigned', 'corpora', 'with', 'annotation'], ['we', 'ran', 'all', 'of', 'our', 'experiments', 'in', 'weka', 'hall', 'et', 'al', '2009', 'using', 'logistic', 'regressionwe', 'conducted', 'experiments', 'using', 'multinomial', 'naive', 'bayes', 'classifier', 'implemented', 'in', 'the', 'weka', 'toolkit', 'hall', 'et', 'al', '2009'], ['the', 'spanishenglish', 's2e', 'training', 'corpus', 'was', 'drawn', 'from', 'the', 'europarl', 'collection', 'koehn', '2005', 'the', 'experiments', 'focus', 'on', 'translation', 'from', 'german', 'to', 'english', 'using', 'the', 'europarl', 'data', 'koehn', '2005'], ['type', 'system', 'extends', 'the', 'type', 'system', 'that', 'is', 'built', 'into', 'the', 'uima', '6', 'framework', 'ferrucci', 'and', 'lally', '2004', 'this', 'architecture', 'is', 'very', 'similar', 'to', 'the', 'framework', 'of', 'uima', 'ferrucci', 'and', 'lally', '2004'], ['for', 'other', 'languages', 'we', 'use', 'the', 'corpora', 'made', 'available', 'for', 'the', 'conllx', 'shared', 'task', 'buchholz', 'and', 'marsi', '2006', 'we', 'use', 'the', 'conllx', 'buchholz', 'and', 'marsi', '2006', 'distribution', 'data', 'from', 'seven', 'different', 'languages', 'arabic', 'bulgarian', 'dutch', 'portuguese', 'slovene', 'spanish', 'and', 'swedish'], ['table', '5', 'compares', 'our', 'reordering', 'model', 'with', 'a', 'reimplementation', 'of', 'the', 'reordering', 'model', 'proposed', 'in', 'tromble', 'and', 'eisner', '2009', 'we', 'note', 'that', 'our', 'model', 'outperforms', 'the', 'model', 'proposed', 'in', 'tromble', 'and', 'eisner', '2009', 'in', 'all', 'cases'], ['table', '5', 'compares', 'our', 'reordering', 'model', 'with', 'a', 'reimplementation', 'of', 'the', 'reordering', 'model', 'proposed', 'in', 'tromble', 'and', 'eisner', '2009', 'we', 'note', 'that', 'our', 'model', 'outperforms', 'the', 'model', 'proposed', 'in', 'tromble', 'and', 'eisner', '2009', 'in', 'all', 'cases'], ['these', 'results', 'contradict', 'those', 'given', 'in', 'zelenko', 'et', 'al', '2003', '', 'where', 'the', 'sparse', 'kernel', 'achieves', '23', 'better', 'f1', 'performance', 'than', 'the', 'contiguous', 'kernelzelenko', 'et', 'al', '2003', 'have', 'shown', 'the', 'contiguous', 'kernel', 'to', 'be', 'computable', 'in', 'omn', 'and', 'the', 'sparse', 'kernel', 'in', 'omn', '3', '', 'where', 'm', 'and', 'n', 'are', 'the', 'number', 'of', 'children', 'in', 'trees', 't', '1', 'and', 't', '2', 'respectively'], ['for', 'example', 'ontonotes', 'hovy', 'et', 'al', '2006', '', 'a', 'largescale', 'annotation', 'project', 'chose', 'this', 'optionto', 'this', 'end', 'a', 'recent', 'largescale', 'annotation', 'effort', 'called', 'the', 'ontonotes', 'project', 'hovy', 'et', 'al', '2006', 'was', 'started'], ['as', 'for', 'ej', 'translation', 'we', 'use', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'english', 'abstraction', 'treeswe', 'use', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parse', 'trees', 'and', 'dependency', 'relations'], ['for', 'both', 'english', 'and', 'german', 'we', 'used', 'the', 'partof', 'speech', 'tagger', 'treetagger', 'schmid', '1994', 'to', 'obtain', 'postagsonly', 'for', 'german', 'data', 'did', 'we', 'used', 'the', 'treetagger', 'schmid', '1994', 'tokenizer'], ['we', 'train', 'our', 'model', 'on', 'a', 'subset', 'of', 'the', 'wackypedia', 'en', '6', 'corpus', 'baroni', 'et', 'al', '2009', 'we', 'built', 'a', 'knowledge', 'base', 'v', '2', 'r', '1', 'using', 'the', 'frwac', 'corpus', 'baroni', 'et', 'al', '2009'], ['the', 'questions', 'are', 'translated', 'using', 'a', 'phrasebased', 'system', 'built', 'using', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'mo', 'setthe', 'first', 'two', 'baselines', 'are', 'standard', 'systems', 'using', 'pbmt', 'or', 'hiero', 'trained', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'questions', 'are', 'translated', 'using', 'a', 'phrasebased', 'system', 'built', 'using', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'mo', 'setthe', 'decoder', 'is', 'built', 'on', 'top', 'of', 'an', 'opensource', 'phrasebased', 'smt', 'decoder', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'measure', 'statistical', 'significance', 'using', '95', 'confidence', 'intervals', 'computed', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'significance', 'was', 'tested', 'using', 'a', 'paired', 'bootstrap', 'koehn', '2004', 'with', '1000', 'samples', 'p002'], ['as', 'for', 'ej', 'translation', 'we', 'use', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'english', 'abstraction', 'treeswe', 'use', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parse', 'trees', 'and', 'dependency', 'relations'], ['words', 'were', 'downcased', 'and', 'lemmatized', 'using', 'the', 'wordnet', 'lemmatizer', 'in', 'the', 'nltk', '2', 'toolkit', 'bird', 'et', 'al', '2009', 'for', 'reuters', 'we', 'segmented', 'and', 'tokenized', 'the', 'data', 'using', 'nltk', 'bird', 'et', 'al', '2009'], ['english', 'sentences', 'are', 'parsed', 'into', 'dependency', 'structures', 'by', 'stanford', 'parser', 'marneffe', 'et', 'al', '2006', 'the', 'grammatical', 'relations', 'are', 'all', 'the', 'collapsed', 'dependencies', 'produced', 'by', 'the', 'stanford', 'dependency', 'parser', 'marneffe', 'et', 'al', '2006'], ['both', 'of', 'our', 'systems', 'were', 'based', 'on', 'the', 'moses', 'decoder', 'koehn', 'et', 'al', '2007', 'the', 'decoder', 'is', 'built', 'on', 'top', 'of', 'an', 'opensource', 'phrasebased', 'smt', 'decoder', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'also', 'used', 'anew', 'bradley', 'and', 'lang', '1999', 'for', 'bootstrapping', 'the', 'affective', 'lexicon', 'expansion', 'processfinally', 'the', 'anew', 'lexicon', 'bradley', 'and', 'lang', '1999', 'is', 'used', 'for', 'selecting', 'the', 'initial', 'set', 'of', 'seed', 'words', 'of', '1'], ['translation', 'at', 'the', 'discomt', '2015', 'workshop', 'hardmeier', 'et', 'al', '2015', 'the', 'nlp', 'group', 'of', 'the', 'idiap', 'research', 'institute', 'participated', 'in', 'both', 'subtasks', 'of', 'the', 'discomt', '2015', 'shared', 'task', 'pronounfocused', 'translation', 'and', 'pronoun', 'prediction', 'hardmeier', 'et', 'al', '2015'], ['establishing', 'and', 'maintaining', 'common', 'ground', 'is', 'a', 'complicated', 'process', 'even', 'for', 'human', 'interlocutors', 'clark', '1996', 'an', 'example', 'of', 'such', 'a', 'pragmatic', 'factor', 'is', 'common', 'ground', 'clark', '1996'], ['sentiwordnet', 'score', 'senti', 'we', 'used', 'the', 'senti', 'wordnet', 'baccianella', 'et', 'al', '2010', 'lexical', 'resource', 'to', 'assign', 'scores', 'for', 'each', 'word', 'based', 'on', 'three', 'sentiments', 'ie', 'positive', 'negative', 'and', 'objective', 'respecwe', 'then', 'have', 'assigned', 'a', 'sentiment', 'score', 'using', 'the', 'sentiwordnet', 'baccianella', 'et', 'al', '2010', 'lexical', 'resource', 'to', 'each', 'word', 'in', 'the', 'set', 'of', 'retrieved', 'snippets'], ['europarl', '2', 'koehn', '2005', '', 'it', 'is', 'a', 'corpus', 'of', 'parallel', 'texts', 'in', '11', 'languages', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament', 'we', 'used', 'a', 'subset', 'of', 'the', 'data', 'provided', 'for', 'the', 'second', 'workshop', 'on', 'statistical', 'machine', 'translation', '2', '', 'which', 'consists', 'mainly', 'of', 'texts', 'from', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'also', 'carried', 'out', 'a', 'chunkreordering', 'pbsmt', 'experiment', 'where', 'the', 'chunks', 'are', 'reordered', 'based', 'on', 'the', 'final', 'alignments', 'obtained', 'by', '1pass', 'experiment', 'of', 'holmqvist', 'et', 'al', '2012', 'holmqvist', 'et', 'al', '2012', 'presented', 'a', 'method', 'where', 'source', 'text', 'is', 'reordered', 'to', 'replicate', 'the', 'target', 'word', 'order', 'based', 'on', 'word', 'alignment'], ['wsi', 'is', 'generally', 'considered', 'as', 'an', 'unsupervised', 'clustering', 'task', 'under', 'the', 'distributional', 'hypothesis', 'harris', '1954', 'that', 'the', 'word', 'meaning', 'is', 'reflected', 'by', 'the', 'set', 'of', 'contexts', 'in', 'which', 'it', 'appearsdistributional', 'models', 'of', 'meaning', 'follow', 'the', 'distributional', 'hypothesis', 'harris', '1954', '', 'which', 'states', 'that', 'two', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'have', 'similar', 'meanings'], ['the', 'data', 'used', 'for', 'the', 'experiments', 'described', 'in', 'this', 'paper', 'comes', 'predominantly', 'from', 'bible', 'translations', '', 'wikipedia', 'and', 'the', 'europarl', 'corpus', 'of', 'european', 'parliamentary', 'proceedings', 'koehn', '2005', '', 'europarl', '2', 'koehn', '2005', '', 'it', 'is', 'a', 'corpus', 'of', 'parallel', 'texts', 'in', '11', 'languages', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['for', 'the', 'language', 'model', 'we', 'used', 'the', 'kenlm', 'toolkit', 'heafield', '2011', 'to', 'create', 'a', '5gram', 'language', 'model', 'on', 'the', 'target', 'side', 'of', 'the', 'europarl', 'corpus', 'v7', 'with', 'approximately', '54m', 'tokens', 'with', 'kneserney', 'smoolanguage', 'model', 'for', 'all', '3scfg', 'systems', 'we', 'use', 'a', '4gram', 'kneserney', 'smoothed', 'language', 'model', 'trained', 'using', 'the', 'kenlm', 'toolkit', 'heafield', '2011'], ['first', 'used', 'by', 'blitzer', 'et', 'al', '2007', '', 'the', 'mds', 'dataset', 'contains', '4', 'different', 'types', 'of', 'product', 'reviews', 'taken', 'from', 'amazoncom', 'including', 'books', 'dvds', 'electronics', 'and', 'kitchen', 'appliances', 'with', '1000', 'positive', 'to', 'evaluate', 'da', 'for', 'sentiment', 'classification', 'we', 'use', 'the', 'amazon', 'product', 'reviews', 'collected', 'by', 'blitzer', 'et', 'al', '2007', '', 'for', 'four', 'different', 'product', 'categories', 'books', 'b', 'dvds', 'd', 'electronic', 'items', 'e', 'and'], ['the', 'most', 'famous', 'example', 'would', 'probably', 'be', 'the', 'europarl', 'corpus', 'koehn', '2005', 'for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['for', 'language', 'modeling', 'we', 'trained', 'a', 'separate', '5gram', 'kneserney', 'smoothed', 'lm', 'model', 'on', 'the', 'target', 'ie', 'english', 'side', 'of', 'the', 'training', 'bitext', 'using', 'kenlm', 'heafield', '2011', 'in', 'order', 'to', 'evaluate', 'the', 'fluency', 'of', 'each', 'system', 'we', 'train', '5gram', 'language', 'models', 'for', 'each', 'language', 'using', 'kenlm', 'heafield', '2011'], ['they', 'used', 'the', 'webbased', 'annotation', 'tool', 'brat', 'stenetorp', 'et', 'al', '2012', 'for', 'the', 'annotation', 'the', 'annotations', 'were', 'made', 'using', 'the', 'brat', 'rapid', 'annotation', 'tool', 'stenetorp', 'et', 'al', '2012'], ['the', 'classification', 'was', 'conducted', 'using', 'different', 'scikitlearn', 'algorithms', 'pedregosa', 'et', 'al', '2011', 'the', 'svm', 'models', 'were', 'trained', 'using', 'the', 'scikitlearn', 'toolkit', '4', 'pedregosa', 'et', 'al', '2011'], ['the', 'europarl', 'corpus', 'koehn', '2005', 'is', 'built', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['for', 'language', 'modeling', 'we', 'trained', 'a', 'separate', '5gram', 'kneserney', 'smoothed', 'lm', 'model', 'on', 'the', 'target', 'ie', 'english', 'side', 'of', 'the', 'training', 'bitext', 'using', 'kenlm', 'heafield', '2011', 'the', 'language', 'model', 'is', 'a', '5gram', 'kenlm', 'heafield', '2011', 'model', 'trained', 'using', 'lmplz', 'with', 'modified', 'kneserney', 'smoothing', 'and', 'no', 'pruning'], ['the', 'spanishenglish', 's2e', 'training', 'corpus', 'was', 'drawn', 'from', 'the', 'europarl', 'collection', 'koehn', '2005', 'we', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'use', 'the', 'stanford', 'corenlp', 'caseless', 'tagger', 'for', 'partofspeech', 'tagging', 'manning', 'et', 'al', '2014', 'we', 'also', 'lemmatized', 'all', 'words', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['all', 'of', 'our', 'models', 'are', 'trained', 'using', 'nematus', 'sennrich', 'et', 'al', '2017', 'we', 'trained', 'our', 'basic', 'neural', 'machine', 'translation', 'systems', 'labeled', 'base', 'in', 'table', '3', 'with', 'nematus', 'sennrich', 'et', 'al', '2017'], ['we', 'use', 'giza', 'och', 'and', 'ney', '2003', 'with', 'its', 'default', 'parameters', 'to', 'produce', 'phrase', 'alignmentswe', 'used', 'the', 'giza', 'software', 'och', 'and', 'ney', '2003', 'to', 'do', 'the', 'word', 'alignments'], ['we', 'are', 'working', 'with', 'standard', 'tools', 'as', 'dissect', 'dinu', 'et', 'al', '2013', 'the', 'matrix', 'is', 'weighted', 'with', 'ppmi', 'as', 'implemented', 'in', 'dissect', 'dinu', 'et', 'al', '2013'], ['they', 'are', 'based', 'on', 'distributional', 'hypothesis', 'which', 'works', 'under', 'the', 'assumption', 'that', 'similar', 'words', 'occur', 'in', 'similar', 'contexts', 'harris', '1954', 'corpusbased', 'vsms', 'follow', 'the', 'standard', 'distributional', 'hypothesis', 'which', 'states', 'that', 'words', 'appearing', 'in', 'the', 'same', 'contexts', 'tend', 'to', 'have', 'similar', 'meaning', 'harris', '1954'], ['its', 'segmentation', 'model', 'is', 'a', 'classbased', 'hidden', 'markov', 'model', 'hmm', 'model', 'zhang', 'et', 'al', '2003', 'for', 'chinese', 'a', 'segmentation', 'model', 'zhang', 'et', 'al', '2003', 'is', 'used', 'for', 'detecting', 'word', 'boundaries'], ['the', 'edit', 'distance', 'kernel', 'was', 'trained', 'with', 'libsvm', 'chang', 'and', 'lin', '2011', 'as', 'our', 'learner', 'we', 'use', 'libsvm', 'with', 'a', 'linear', 'kernel', 'chang', 'and', 'lin', '2011'], ['the', 'corpora', 'are', 'first', 'tokenized', 'and', 'lowercased', 'using', 'the', 'moses', 'scripts', 'then', 'lemmatized', 'and', 'tagged', 'by', 'partofspeech', 'pos', 'using', 'the', 'treetagger', 'schmid', '1994', 'the', 'corpus', 'was', 'converted', 'from', 'xml', 'to', 'raw', 'text', 'various', 'string', 'normalization', 'operations', 'were', 'then', 'applied', 'and', 'the', 'corpus', 'was', 'lemmatized', 'using', 'treetagger', 'schmid', '1994'], ['for', 'example', 'ontonotes', 'hovy', 'et', 'al', '2006', '', 'a', 'largescale', 'annotation', 'project', 'chose', 'this', 'optionfor', 'example', 'the', 'ontonotes', 'hovy', 'et', 'al', '2006', 'project', 'opted', 'for', 'this', 'approach'], ['we', 'use', 'the', 'opensource', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'build', 'a', 'standard', 'phrasebased', 'smt', 'system', 'which', 'extracts', 'up', 'to', '8', 'words', 'phrases', 'in', 'the', 'moses', 'phrase', 'tablethe', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'is', 'used', 'to', 'train', 'a', 'phrase', 'based', 'smt', 'system', 'with', 'the', 'parallel', 'data', 'previously', 'introduced'], ['in', 'our', 'experiments', 'we', 'use', 'the', 'liblinear', 'package', 'fan', 'et', 'al', '2008', 'to', 'solve', 'the', 'primal', 'problem', 'with', 'l', '2', 'regularization', 'and', 'l', '2', 'lossspecifically', 'we', 'use', 'the', 'liblinear', 'svm', 'package', 'fan', 'et', 'al', '2008', 'as', 'it', 'is', 'wellsuited', 'to', 'text', 'classification', 'tasks', 'with', 'large', 'numbers', 'of', 'features', 'and', 'texts'], ['the', 'spanishenglish', 's2e', 'training', 'corpus', 'was', 'drawn', 'from', 'the', 'europarl', 'collection', 'koehn', '2005', 'for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['the', 'icsi', 'meeting', 'corpus', 'janin', 'et', 'al', '2003', 'is', 'a', 'corpus', 'of', 'text', 'transcripts', 'of', 'research', 'meetingsthe', 'icsi', 'meeting', 'corpus', 'the', 'icsi', 'meeting', 'corpus', 'janin', 'et', 'al', '2003', 'is', '75', 'transcribed', 'meetings'], ['we', 'use', 'rouge', 'lin', '2004', 'for', 'evaluating', 'the', 'content', 'of', 'summarieswe', 'use', 'rouge', 'metric', 'lin', '2004', 'to', 'evaluate', 'generated', 'timelines', 'against', 'reference', 'summaries'], ['the', 'measure', 'selected', 'is', 'the', 'normalised', 'pearson', 'correlation', 'agirre', 'et', 'al', '2012', 'the', 'task', 'is', 'part', 'of', 'the', 'semantic', 'evaluation', '2012', 'workshop', 'agirre', 'et', 'al', '2012'], ['the', 'main', 'corpora', 'we', 'use', 'are', 'europarl', 'koehn', '2005', 'and', 'the', 'canadian', 'hansardfor', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'exploit', 'this', 'monolingual', 'data', 'for', 'training', 'as', 'described', 'in', 'sennrich', 'et', 'al', '2016a', 'we', 'also', 'used', 'automatically', 'backtranslated', 'indomain', 'monolingual', 'data', 'sennrich', 'et', 'al', '2016a'], ['we', 'use', 'the', 'stanford', 'corenlp', 'caseless', 'tagger', 'for', 'partofspeech', 'tagging', 'manning', 'et', 'al', '2014', 'we', 'also', 'lemmatized', 'all', 'words', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['this', 'is', 'a', 'corpusbased', 'metric', 'relying', 'on', 'the', 'distributional', 'hypothesis', 'of', 'meaning', 'suggesting', 'that', 'similarity', 'of', 'context', 'implies', 'similarity', 'of', 'meaning', 'z', 'harris', '1954', 'corpusbased', 'meaning', 'representations', 'rely', 'on', 'the', 'distributional', 'hypothesis', 'which', 'assumes', 'that', 'words', 'occurring', 'in', 'a', 'similar', 'set', 'of', 'contexts', 'are', 'also', 'similar', 'in', 'meaning', 'harris', '1954'], ['similarly', 'turian', 'et', 'al', '2010', 'evaluated', 'three', 'different', 'word', 'representations', 'on', 'ner', 'and', 'chunking', 'and', 'concluded', 'that', 'unsupervised', 'word', 'representations', 'improved', 'ner', 'and', 'chunkingturian', 'et', 'al', '2010', '', 'evaluate', 'different', 'techniques', 'for', 'inducing', 'word', 'representations', 'and', 'detail', 'significant', 'improvements', 'for', 'supervised', 'ner', 'and', 'chunking', 'systems', 'when', 'also', 'incorporating', 'word', 'embedding'], ['331', 'reference', 'system', 'we', 'compare', 'a', 'number', 'of', 'analogical', 'devices', 'to', 'the', 'stateoftheart', 'statistical', 'translation', 'engine', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'then', 'use', 'the', 'phrase', 'extraction', 'utility', 'in', 'the', 'moses', 'statistical', 'machine', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'extract', 'a', 'phrase', 'table', 'which', 'operates', 'over', 'characters'], ['for', 'training', 'svm', 'classifiers', 'we', 'used', 'libsvm', 'package', 'chang', 'and', 'lin', '2001', 'we', 'used', 'libsvm', 'to', 'implement', 'our', 'own', 'svm', 'for', 'regression', 'chang', 'and', 'lin', '2001'], ['the', 'word', 'alignment', 'was', 'trained', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'configuration', 'growdiagfinalandthe', 'word', 'alignment', 'was', 'obtained', 'by', 'running', 'giza', 'och', 'and', 'ney', '2003'], ['the', 'corpus', 'is', 'first', 'wordaligned', 'using', 'a', 'word', 'alignment', 'heuristic', 'och', 'and', 'ney', '2003', 'the', 'word', 'alignment', 'was', 'obtained', 'by', 'running', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'perform', 'bootstrap', 'resampling', 'with', 'bounds', 'estimation', 'as', 'described', 'in', 'koehn', '2004', 'we', 'used', 'bootstrap', 'resampling', 'for', 'testing', 'statistical', 'significance', 'koehn', '2004'], ['we', 'perform', 'bootstrap', 'resampling', 'with', 'bounds', 'estimation', 'as', 'described', 'by', 'koehn', '2004', 'we', 'used', 'bootstrap', 'resampling', 'for', 'testing', 'statistical', 'significance', 'koehn', '2004'], ['markov', 'logic', 'networks', 'mln', 'richardson', 'and', 'domingos', '2006', 'is', 'adopted', 'for', 'learning', 'and', 'predicationmarkov', 'logic', 'networks', 'mln', 'richardson', 'and', 'domingos', '2006', 'are', 'one', 'of', 'the', 'statistical', 'relational', 'learning', 'frameworks'], ['with', 'the', 'training', 'script', 'of', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settings'], ['we', 'train', 'for', '15', 'epochs', 'using', 'minibatch', 'stochastic', 'gradient', 'descent', 'the', 'adadelta', 'update', 'rule', 'zeiler', '2012', '', 'and', 'early', 'stoppingwe', 'initialize', 'parameters', 'uniformly', 'using', 'the', 'range', '005', '005', 'for', 'layer', 'parameters', 'and', '001', '001', 'for', 'embeddings', 'and', 'train', 'the', 'model', 'using', 'stochastic', 'gradient', 'descent', 'sgd', 'with', 'learning', 'rates'], ['this', 'confirms', 'the', 'finding', 'of', 'liu', 'et', 'al', '2012', 'that', 'language', 'model', 'and', 'lexicalized', 'reordering', 'models', 'only', 'have', 'modest', 'effects', 'on', 'translation', 'retrievalmore', 'recently', 'liu', 'et', 'al', '2012', 'have', 'proposed', 'a', 'new', 'translation', 'retrieval', 'architecture', 'that', 'depends', 'only', 'on', 'monolingual', 'corpora'], ['for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005', 'we', 'evaluate', 'our', 'method', 'by', 'means', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'exploit', 'this', 'monolingual', 'data', 'for', 'training', 'as', 'described', 'in', 'sennrich', 'et', 'al', '2016a', 'we', 'also', 'used', 'automatically', 'backtranslated', 'indomain', 'monolingual', 'data', 'sennrich', 'et', 'al', '2016a'], ['our', 'decoder', 'is', 'a', 'stack', 'decoder', 'similar', 'to', 'koehn', 'et', 'al', '2003', 'phrase', 'extraction', 'was', 'performed', 'following', 'koehn', 'et', 'al', '2003'], ['for', 'the', 'english', 'spanish', 'and', 'frenchenglish', 'systems', 'we', 'used', 'parallel', 'training', 'data', 'from', 'the', 'europarl', 'and', 'news', 'commentary', 'corpora', 'as', 'well', 'as', 'the', 'ted', 'corpus', 'cettolo', 'et', 'al', '2012', 'in', 'our', 'lowresource', 'condition', 'we', 'trained', 'an', 'smt', 'system', 'using', 'only', 'training', 'data', 'from', 'the', 'ted', 'corpus', 'cettolo', 'et', 'al', '2012'], ['the', 'word', 'alignment', 'was', 'trained', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'configuration', 'growdiagfinalandword', 'alignment', 'was', 'done', 'with', 'giza', 'och', 'and', 'ney', '2003', 'for', 'both', 'systems'], ['rhetorical', 'structure', 'theory', 'rst', 'mann', 'and', 'thompson', '1988', 'represents', 'the', 'coherence', 'structure', 'of', 'a', 'text', 'by', 'a', 'labeled', 'tree', 'called', 'discourse', 'tree', 'dt', 'as', 'shown', 'in', 'figure', '1', 'the', 'closest', 'area', 'to', 'our', 'work', 'consists', 'of', 'investigations', 'of', 'discourse', 'relations', 'in', 'the', 'context', 'of', 'rhetorical', 'structure', 'theory', 'mann', 'and', 'thompson', '1988'], ['the', 'dataset', 'used', 'for', 'the', 'experiments', 'reported', 'in', 'this', 'paper', 'has', 'been', 'prepared', 'by', 'reyes', 'et', 'al', '2013', 'the', 'other', 'groups', 'have', 'been', 'used', 'already', 'for', 'example', 'by', 'carvalho', 'et', 'al', '2009', 'or', 'reyes', 'et', 'al', '2013', 'yet', 'our', 'implementation', 'is', 'different', 'in', 'most', 'of', 'the', 'cases'], ['a', 'good', 'data', 'source', 'for', 'this', 'is', 'the', 'europarl', 'corpus', 'koehn', '2005', 'for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['the', 'most', 'famous', 'example', 'would', 'probably', 'be', 'the', 'europarl', 'corpus', 'koehn', '2005', 'we', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['previously', 'socher', 'et', 'al', '2011', '', 'used', 'a', 'recursive', 'autoencoder', 'to', 'similarly', 'obtain', 'a', 'vector', 'representation', 'of', 'each', 'sentence', 'again', 'combining', 'other', 'lexical', 'similarity', 'features', 'to', 'improve', 'the', 'resultsfor', 'the', 'msrp', 'task', 'socher', 'et', 'al', '2011', 'used', 'a', 'recursive', 'neural', 'network', 'to', 'model', 'each', 'sentence', '', 'recursively', 'computing', 'the', 'representation', 'for', 'the', 'sentence', 'from', 'the', 'representations', 'of', 'its', 'constituents'], ['the', 'first', 'source', 'is', 'the', 'conll', '2003', 'shared', 'task', 'date', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003', 'and', 'the', 'second', 'source', 'is', 'the', '2004', 'nist', 'automatic', 'content', 'extraction', 'weischedel', '2004', 'aidayago', 'is', 'derived', 'from', 'the', 'conll2003', 'shared', 'task', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'build', 'a', 'phrase', 'based', 'machine', 'translation', 'system', 'with', 'a', 'traditional', '5gram', 'lm', 'trained', 'on', 'the', 'target', 'side', 'of', 'our', 'bitextfor', 'this', 'purpose', 'we', 'use', 'the', 'moses', 'toolkit', 'to', 'build', 'a', 'phrasebased', 'statistical', 'mt', 'system', 'koehn', 'et', 'al', '2007', '', 'with', 'training', 'data', 'from', 'the', 'translation', 'task', 'of', 'the', 'wmt', '2013', 'workshop', 'bojar', 'et', 'al', '2'], ['we', 'therefore', 'propose', 'an', 'alternative', 'method', 'based', 'on', 'correlation', 'matrices', 'computed', 'from', 'the', 'bleu', 'performance', 'measure', 'papineni', 'et', 'al', '2001', 'we', 'measure', 'translation', 'quality', 'via', 'the', 'bleu', 'score', 'papineni', 'et', 'al', '2001'], ['we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'the', 'linear', 'svm', 'implementation', 'with', 'default', 'parameters', 'and', 'random', 'forest', 'with', '50', 'trees', 'both', 'available', 'at', 'scikitlearn', 'pedregosa', 'et', 'al', '2011'], ['finally', 'the', 'conll2003', 'shared', 'task', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003', 'is', 'an', 'figure', '1', '', 'search', 'results', 'for', 'the', 'queries', 'michael', 'jackson', 'and', 'michael', 'jackson', 'footballeraidayago', 'is', 'derived', 'from', 'the', 'conll2003', 'shared', 'task', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003'], ['further', 'we', 'sentencesplit', 'tokenized', 'and', 'lemmatized', 'the', 'text', 'in', 'wikipedia', 'and', 'gigaword', 'using', 'stanford', 'corenlp', 'toolkit', '360', 'manning', 'et', 'al', '2014', 'we', 'also', 'lemmatized', 'all', 'words', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['the', 'corpora', 'are', 'first', 'tokenized', 'and', 'lowercased', 'using', 'the', 'moses', 'scripts', 'then', 'lemmatized', 'and', 'tagged', 'by', 'partofspeech', 'pos', 'using', 'the', 'treetagger', 'schmid', '1994', 'all', 'novels', 'were', 'lemmatized', 'and', 'postagged', 'using', 'treetagger', 'schmid', '1994'], ['we', 'use', 'adadelta', 'zeiler', '2012', 'to', 'update', 'the', 'parameters', 'during', 'trainingwe', 'also', 'use', 'adadelta', 'zeiler', '2012', 'to', 'optimize', 'the', 'learning', 'of', '', 'which', 'is', 'a', 'effective', 'method', 'to', 'train', 'the', 'neural', 'networks'], ['the', 'first', 'source', 'is', 'the', 'conll', '2003', 'shared', 'task', 'date', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003', 'and', 'the', 'second', 'source', 'is', 'the', '2004', 'nist', 'automatic', 'content', 'extraction', 'weischedel', '2004', 'aidayago', 'is', 'derived', 'from', 'the', 'conll2003', 'shared', 'task', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003'], ['we', 'used', 'a', '2009', 'snapshot', 'of', 'wikipedia', '2', 'which', 'was', 'pos', 'tagged', 'and', 'lemmatized', 'using', 'the', 'treetagger', 'schmid', '1994', 'all', 'novels', 'were', 'lemmatized', 'and', 'postagged', 'using', 'treetagger', 'schmid', '1994'], ['recently', 'hovy', 'et', 'al', '2013', 'utilized', 'word', 'embeddings', 'by', 'collobert', 'et', 'al', '2011', 'for', 'capturing', 'coherence', 'and', 'contextual', 'features', 'for', 'supervised', 'metaphor', 'detectionhovy', 'et', 'al', '2013', 'applied', 'tree', 'kernels', 'to', 'metaphor', 'detection'], ['finally', 'the', 'conll2003', 'shared', 'task', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003', 'is', 'an', 'figure', '1', '', 'search', 'results', 'for', 'the', 'queries', 'michael', 'jackson', 'and', 'michael', 'jackson', 'footballeraidayago', 'is', 'derived', 'from', 'the', 'conll2003', 'shared', 'task', 'tjong', 'kim', 'sang', 'and', 'de', 'meulder', '2003'], ['we', 'run', 'our', 'experiments', 'on', 'europarl', 'koehn', '2005', '', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'applied', 'bootstrap', 'resampling', 'koehn', '2004', 'to', 'measure', 'statistical', 'significance', '', 'p', '', '005', 'of', 'our', 'models', 'compared', 'to', 'a', 'baselinewe', 'used', 'bootstrap', 'resampling', 'for', 'testing', 'statistical', 'significance', 'koehn', '2004'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settingsthe', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'is', 'used', 'to', 'train', 'a', 'phrase', 'based', 'smt', 'system', 'with', 'the', 'parallel', 'data', 'previously', 'introduced'], ['the', 'statistical', 'significance', 'test', 'is', 'also', 'carried', 'out', 'with', 'paired', 'bootstrap', 'resampling', 'method', 'with', 'p', '', '0001', 'intervals', 'koehn', '2004', 'we', 'used', 'bootstrap', 'resampling', 'for', 'testing', 'statistical', 'significance', 'koehn', '2004'], ['this', 'model', 'is', 'motivated', 'by', 'vector', 'space', 'model', 'salton', 'et', 'al', '1975', 'one', 'of', 'the', 'bestknown', 'methods', 'of', 'representing', 'a', 'document', 'for', 'similarity', 'computation', 'between', 'documents', 'is', 'the', 'vector', 'space', 'model', 'salton', 'et', 'al', '1975'], ['the', 'data', 'was', 'segmented', 'into', 'basenp', 'parts', 'and', 'nonbasenp', 'parts', 'in', 'a', 'similar', 'fashion', 'as', 'the', 'data', 'used', 'by', 'ramshaw', 'and', 'marcus', '1995', 'we', 'have', 'used', 'the', 'basenp', 'data', 'presented', 'in', 'ramshaw', 'and', 'marcus', '1995', '2'], ['the', 'njuparser', 'is', 'based', 'on', 'the', 'stateofthe', 'art', 'mat', 'parser', 'mcdonald', '2006', 'the', 'second', 'algorithm', 'denoted', 'glotr', 'is', 'the', 'chuliu', 'edmonds', 'algorithm', 'for', 'maximal', 'spanning', 'tree', 'implemented', 'in', 'the', 'mstparser', 'mcdonald', '2006'], ['one', 'is', 'a', '3gram', 'language', 'model', 'built', 'using', 'kenlm', 'heafield', '2011', 'and', 'trained', 'over', 'a', 'modified', 'version', 'of', 'the', 'annotated', 'corpus', 'in', 'which', 'every', 'it', 'is', 'concatenated', 'with', 'its', 'type', 'eg', 'it', 'eventtraining', 'and', 'querying', 'of', 'a', 'modified', 'kneserney', 'smoothed', '5', 'gram', 'language', 'model', 'are', 'done', 'on', 'the', 'english', 'side', 'of', 'the', 'training', 'data', 'using', 'kenlm', 'heafield', '2011', '7'], ['the', 'word', 'alignment', 'was', 'obtained', 'by', 'running', 'giza', 'och', 'and', 'ney', '2003', 'baseline', 'word', 'alignments', 'were', 'obtained', 'by', 'running', 'giza', 'in', 'both', 'directions', 'and', 'symmetrizing', 'using', 'the', 'growdiagfinaland', 'heuristic', 'och', 'and', 'ney', '2003'], ['translations', 'for', 'english', 'words', 'in', 'the', 'lexical', 'sample', 'are', 'extracted', 'from', 'a', 'semiautomatic', 'word', 'alignment', 'of', 'sentences', 'from', 'the', 'europarl', 'parallel', 'corpus', 'koehn', '2005', 'in', 'order', 'to', 'improve', 'the', 'robustness', 'of', 'the', 'word', 'alignments', 'the', 'documents', 'were', 'concatenated', 'into', 'a', 'single', 'file', 'together', 'with', 'englishfrench', 'parallel', 'data', 'from', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['all', 'the', 'language', 'models', 'lm', 'used', 'in', 'our', 'experiments', 'are', '5grams', 'modified', 'kneserney', 'smoothed', 'lms', 'trained', 'using', 'kenlm', 'heafield', 'et', 'al', '2013', 'the', 'language', 'models', 'are', 'estimated', 'using', 'the', 'kenlm', 'toolkit', 'heafield', 'et', 'al', '2013', 'with', 'modified', 'kneserney', 'smoothing'], ['in', 'all', 'experiments', 'we', 'use', 'the', 'svm', 'classifier', 'with', 'sequential', 'minimal', 'optimization', 'smo', 'implementation', 'available', 'in', 'the', 'weka', 'package', 'hall', 'et', 'al', '2009', 'for', 'all', 'experiments', 'we', 'use', 'a', 'decisiontree', 'classifier', 'as', 'implemented', 'in', 'weka', 'hall', 'et', 'al', '2009', 'toolkit'], ['we', 'use', 'the', 'stanford', 'parser', 'to', 'generate', 'a', 'dg', 'for', 'each', 'sentence', 'de', 'marneffe', 'et', 'al', '2006', 'we', 'used', 'the', 'lexicalized', 'dependency', 'parser', 'in', 'the', 'stanford', 'statistical', 'natural', 'language', 'parser', 'ver203', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parses', 'for', 'the', 'data', 'sets'], ['preprocessing', 'we', 'tokenized', 'the', 'english', 'side', 'of', 'all', 'bitexts', 'for', 'language', 'modeling', 'using', 'the', 'standard', 'tokenizer', 'of', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007', 'we', 'use', 'moses', '7', 'koehn', 'et', 'al', '2007', 'to', 'implement', 'this', 'model', 'by', 'setting', 'the', 'source', 'and', 'target', 'sides', 'to', 'be', 'kana', 'sequences'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'first', 'competitive', 'learning', 'based', 'system', 'is', 'described', 'in', 'soon', 'et', 'al', '2001', 'to', 'solve', 'coreference', 'we', 'used', 'a', 'variation', 'of', 'the', 'closest', 'antecedent', 'approach', 'described', 'in', 'soon', 'et', 'al', '2001'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'predicted', 'collapsed', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'extract', 'arguments', 'of', 'the', 'verbs', 'and', 'used', 'only', 'a', 'subset', 'of', 'dependents', 'of', 'a', 'verbwe', 'extract', 'syntactic', 'dependencies', 'using', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'and', 'use', 'its', 'collapsed', 'dependency', 'format'], ['we', 'select', 'as', 'a', 'generalpurpose', 'corpus', 'europarl', 'v7', 'koehn', '2005', '', 'with', '197m', 'parallel', 'sentenceswe', 'run', 'our', 'experiments', 'on', 'europarl', 'koehn', '2005', '', 'a', 'multilingual', 'parallel', 'corpus', 'which', 'is', 'described', 'in', 'detail', 'in', 'section', '31'], ['we', 'used', 'the', 'lexicalized', 'dependency', 'parser', 'in', 'the', 'stanford', 'statistical', 'natural', 'language', 'parser', 'ver203', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parses', 'for', 'the', 'data', 'setsin', 'our', 'experiments', '', 'we', 'used', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'create', 'dependency', 'parses'], ['the', 'result', 'of', 'the', 'operation', 'is', 'equivalent', 'to', 'weighted', 'composition', 'of', 'the', 'lexicon', 'and', 'the', 'weighted', 'intersection', 'of', 'the', 'rules', 'as', 'defined', 'in', 'allauzen', 'et', 'al', '2007', 'the', 'openfst', 'library', 'is', 'used', 'to', 'perform', 'all', 'of', 'the', 'wfst', 'operations', 'allauzen', 'et', 'al', '2007'], ['the', 'distributional', 'hypothesis', 'of', 'meaning', 'harris', '1954', 'is', 'a', 'widelyused', 'approach', 'for', 'estimating', 'term', 'similaritythis', 'is', 'a', 'corpusbased', 'metric', 'relying', 'on', 'the', 'distributional', 'hypothesis', 'of', 'meaning', 'suggesting', 'that', 'similarity', 'of', 'context', 'implies', 'similarity', 'of', 'meaning', 'z', 'harris', '1954'], ['preprocessing', 'we', 'tokenized', 'the', 'english', 'side', 'of', 'all', 'bitexts', 'for', 'language', 'modeling', 'using', 'the', 'standard', 'tokenizer', 'of', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['one', 'is', 'from', 'turian', 'et', 'al', '2010', '', 'the', 'dimension', 'of', 'word', 'embedding', 'is', '50for', 'brown', 'the', 'features', 'are', 'the', 'prefix', 'features', 'extracted', 'from', 'word', 'clusters', 'in', 'the', 'same', 'way', 'as', 'turian', 'et', 'al', '2010'], ['the', 'thresholds', 'were', 'thoroughly', 'selected', 'depending', 'on', 'our', 'analysis', 'for', 'the', 'wordnet', 'hierarchy', 'and', 'semantic', 'similarity', 'measures', 'pedersen', 'et', 'al', '2004', 'for', 'a', 'pair', 'of', 'words', 'wordnet', 'provides', 'a', 'series', 'of', 'measures', 'of', 'the', 'semantic', 'similarity', 'pedersen', 'et', 'al', '2004'], ['indomain', 'smt', 'we', 'used', 'the', 'parallel', 'corpus', 'section', '3', 'to', 'train', 'an', 'enpt', 'phrasebased', 'smt', 'system', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'our', 'baseline', 'is', 'a', 'phrasebased', 'mt', 'system', 'trained', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['the', 'statistical', 'significance', 'test', 'is', 'also', 'carried', 'out', 'with', 'paired', 'bootstrap', 'resampling', 'method', 'with', 'p', '', '0001', 'intervals', 'koehn', '2004', 'we', 'measure', 'significance', 'of', 'results', 'using', 'bootstrap', 'resampling', 'at', 'p', '', '005', 'koehn', '2004'], ['we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007', '331', 'reference', 'system', 'we', 'compare', 'a', 'number', 'of', 'analogical', 'devices', 'to', 'the', 'stateoftheart', 'statistical', 'translation', 'engine', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'a', '2009', 'snapshot', 'of', 'wikipedia', '2', 'which', 'was', 'pos', 'tagged', 'and', 'lemmatized', 'using', 'the', 'treetagger', 'schmid', '1994', '3', 'all', 'english', 'data', 'are', 'pos', 'tagged', 'and', 'lemmatised', 'using', 'the', 'treetagger', 'schmid', '1994'], ['the', 'text', 'was', 'preprocessed', 'using', 'wp2txt', '6', 'to', 'remove', 'markup', 'and', 'then', 'tokenized', 'with', 'the', 'stanford', 'tokenizer', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['we', 'run', 'our', 'experiments', 'on', 'europarl', 'koehn', '2005', '', 'a', 'multilingual', 'parallel', 'corpus', 'which', 'is', 'described', 'in', 'detail', 'in', 'section', '31europarl', 'koehn', '2005', 'is', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['we', 'use', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', '', 'the', 'machine', 'learning', 'library', 'for', 'python', 'for', 'implementing', 'the', 'different', 'approacheslogistic', 'regression', 'implemented', 'in', 'python', 'with', 'the', 'scikitlearn', 'machine', 'learning', 'library', 'pedregosa', 'et', 'al', '2011', '', 'is', 'used', 'for', 'all', 'classification', 'models'], ['the', 'way', 'they', 'were', 'added', 'is', 'similar', 'to', 'incorporating', 'the', 'negation', 'effect', 'described', 'by', 'pang', 'et', 'al', '2002', 'in', 'this', 'paper', 'we', 'use', 'the', 'subjectivity', 'corpus', 'by', 'pang', 'et', 'al', '2002'], ['we', 'used', 'the', 'lexicalized', 'dependency', 'parser', 'in', 'the', 'stanford', 'statistical', 'natural', 'language', 'parser', 'ver203', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parses', 'for', 'the', 'data', 'setsin', 'our', 'experiments', '', 'we', 'used', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'create', 'dependency', 'parses'], ['we', 'modified', 'the', 'implementation', 'of', 'the', 'swell', 'java', 'package', '4', 'of', 'dhillon', 'et', 'al', '2015', 'we', 'also', 'compare', 'our', 'word', 'embeddings', 'with', 'the', 'eigen', 'word', 'embeddings', 'of', 'dhillon', 'et', 'al', '2015', '', 'without', 'any', 'prior', 'knowl', 'edge'], ['the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'is', 'used', 'for', 'extracting', 'features', 'from', 'the', 'dependency', 'parse', 'treeswe', 'used', 'the', 'lexicalized', 'dependency', 'parser', 'in', 'the', 'stanford', 'statistical', 'natural', 'language', 'parser', 'ver203', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parses', 'for', 'the', 'data', 'sets'], ['the', 'first', 'work', 'on', 'this', 'topic', 'was', 'done', 'back', 'in', 'the', 'eighties', 'church', '1988', 'the', 'dutch', 'version', 'was', 'tagged', 'automatically', 'using', 'a', 'tagger', 'inspired', 'on', 'the', 'english', 'tagger', 'described', 'in', 'church', '1988'], ['the', 'openfst', 'library', 'is', 'used', 'to', 'perform', 'all', 'of', 'the', 'wfst', 'operations', 'allauzen', 'et', 'al', '2007', 'the', 'decoder', 'is', 'implemented', 'with', 'weighted', 'finite', 'state', 'transducers', 'wfsts', 'using', 'standard', 'operations', 'available', 'in', 'the', 'openfst', 'libraries', 'allauzen', 'et', 'al', '2007'], ['for', 'our', 'experiments', 'we', 'used', 'translated', 'movie', 'subtitles', 'from', 'the', 'opus', 'corpus', 'tiedemann', '2009b', 'for', 'chineseenglish', 'experiments', 'we', 'used', 'the', 'openoffice3', 'oo3', 'parallel', 'corpus', 'tiedemann', '2009', '', 'which', 'is', 'oo3', 'computer', 'office', 'productivity', 'software', 'documentation'], ['the', 'moses15', 'result', 'is', 'obtained', 'by', 'applying', 'the', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'over', 'letter', 'strings', 'with', '15character', 'context', 'windowsthe', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'openfst', 'library', 'is', 'used', 'to', 'perform', 'all', 'of', 'the', 'wfst', 'operations', 'allauzen', 'et', 'al', '2007', 'the', 'decoder', 'is', 'implemented', 'with', 'weighted', 'finite', 'state', 'transducers', 'wfsts', 'using', 'standard', 'operations', 'available', 'in', 'the', 'openfst', 'libraries', 'allauzen', 'et', 'al', '2007'], ['we', 'select', 'as', 'a', 'generalpurpose', 'corpus', 'europarl', 'v7', 'koehn', '2005', '', 'with', '197m', 'parallel', 'sentenceswe', 'run', 'our', 'experiments', 'on', 'europarl', 'koehn', '2005', '', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['we', 'also', 'compare', 'our', 'word', 'embeddings', 'with', 'the', 'eigen', 'word', 'embeddings', 'of', 'dhillon', 'et', 'al', '2015', '', 'without', 'any', 'prior', 'knowl', 'edgedhillon', 'et', 'al', '2015', 'used', 'cca', 'to', 'derive', 'word', 'embeddings', 'through', 'the', 'following', 'procedure'], ['the', 'relationship', 'between', 'language', 'and', 'sentiment', 'is', 'an', 'active', 'area', 'of', 'investigation', 'pang', 'and', 'lee', '2008', 'the', 'rapid', 'growth', 'of', 'usergenerated', 'content', 'much', 'of', 'which', 'is', 'sentimentladen', 'has', 'fueled', 'an', 'interest', 'in', 'sentiment', 'analysis', 'pang', 'and', 'lee', '2008', 'liu', '2010'], ['support', 'vector', 'machines', 'svm', 'are', 'one', 'of', 'the', 'binary', 'classifiers', 'based', 'on', 'maximum', 'margin', 'strategy', 'introduced', 'by', 'vapnik', 'vapnik', '1995', 'these', 'classifiers', 'are', 'based', 'on', 'a', 'discriminative', 'model', 'support', 'vector', 'machine', 'svm', '6', 'vapnik', '1995'], ['in', 'barankov', 'and', 'tamchyna', '2014', 'we', 'experimented', 'with', 'targeted', 'paraphrasing', 'using', 'the', 'freely', 'available', 'smt', 'system', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['to', 'learn', 'the', 'parameters', 'of', 'the', 'model', 'we', 'minimize', 'the', 'crossentropy', 'loss', 'as', 'the', 'training', 'objective', 'using', 'the', 'adam', 'optimization', 'algorithm', 'kingma', 'and', 'ba', '2014', 'for', 'optimization', 'we', 'employed', 'the', 'adam', '6', 'httpnlpstanfordedusoftwarecorenlpshtml', 'algorithm', 'kingma', 'and', 'ba', '2014', 'to', 'update', 'parameters'], ['for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'was', 'used', 'for', 'word', 'alignment', 'and', 'phrase', 'translation', 'probabilities', 'were', 'estimated', 'from', 'them', 'by', 'the', 'moses', 'system', 'koehn', 'et', 'al', '2007'], ['the', 'text', 'was', 'preprocessed', 'using', 'wp2txt', '6', 'to', 'remove', 'markup', 'and', 'then', 'tokenized', 'with', 'the', 'stanford', 'tokenizer', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['the', 'task', 'reported', 'on', 'here', 'is', 'to', 'produce', 'propbank', 'kingsbury', 'and', 'palmer', '2002', 'labels', 'given', 'the', 'features', 'provided', 'for', 'the', 'conll2005', 'closed', 'task', 'carreras', 'and', 'm', 'arquez', '2005a', 'standard', 'for', 'predicate', 'argument', 'annotation', 'is', 'provided', 'in', 'the', 'propbank', 'project', 'kingsbury', 'and', 'palmer', '2002'], ['for', 'both', 'systems', 'the', 'used', 'training', 'data', 'is', 'from', 'the', '4th', 'version', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005', 'and', 'the', 'news', 'commentary', 'corpusthe', 'source', 'of', 'bilingual', 'data', 'used', 'in', 'the', 'experiments', 'is', 'the', 'europarl', 'collection', 'koehn', '2005'], ['we', 'use', 'the', 'stanford', 'parser', 'to', 'generate', 'a', 'dg', 'for', 'each', 'sentence', 'de', 'marneffe', 'et', 'al', '2006', 'we', 'used', 'the', 'lexicalized', 'dependency', 'parser', 'in', 'the', 'stanford', 'statistical', 'natural', 'language', 'parser', 'ver203', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parses', 'for', 'the', 'data', 'sets'], ['we', 'used', 'predicted', 'collapsed', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'extract', 'arguments', 'of', 'the', 'verbs', 'and', 'used', 'only', 'a', 'subset', 'of', 'dependents', 'of', 'a', 'verbwe', 'extract', 'syntactic', 'dependencies', 'using', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'and', 'use', 'its', 'collapsed', 'dependency', 'format'], ['we', 'tokenize', 'english', 'data', 'and', 'segment', 'chinese', 'data', 'using', 'the', 'stanford', 'corenlp', 'toolkit', 'manning', 'et', 'al', '2014', '2', 'further', 'we', 'sentencesplit', 'tokenized', 'and', 'lemmatized', 'the', 'text', 'in', 'wikipedia', 'and', 'gigaword', 'using', 'stanford', 'corenlp', 'toolkit', '360', 'manning', 'et', 'al', '2014'], ['the', 'evaluation', 'emphasis', 'in', 'multidocument', 'summarization', 'has', 'been', 'on', 'evaluating', 'content', 'not', 'readability', '', 'using', 'manual', '', 'as', 'well', 'as', 'automatic', 'lin', 'and', 'hovy', '2003', 'methodsrouge', 'lin', 'and', 'hovy', '2003', 'has', 'been', 'adopted', 'as', 'a', 'standard', 'evaluation', 'metric', 'in', 'various', 'summarization', 'tasks'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrase', 'based', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'all', 'experiments', 'were', 'carried', 'out', 'using', 'the', 'opensource', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', '', 'in', 'its', 'standard', 'nonmonotonic', 'configuration'], ['the', 'similarity', 'function', 'is', 'here', 'the', 'smoothed', 'partial', 'tree', 'kernel', 'sptk', 'proposed', 'in', 'croce', 'et', 'al', '2011', 'we', 'directly', 'apply', 'the', 'smoothed', 'partial', 'treekernel', 'sptk', 'proposed', 'in', 'croce', 'et', 'al', '2011', '', 'to', 'estimate', 'the', 'similarity', 'among', 'a', 'specific', 'tree', 'representation'], ['we', 'tokenize', 'english', 'data', 'and', 'segment', 'chinese', 'data', 'using', 'the', 'stanford', 'corenlp', 'toolkit', 'manning', 'et', 'al', '2014', '2', 'further', 'we', 'sentencesplit', 'tokenized', 'and', 'lemmatized', 'the', 'text', 'in', 'wikipedia', 'and', 'gigaword', 'using', 'stanford', 'corenlp', 'toolkit', '360', 'manning', 'et', 'al', '2014'], ['the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'is', 'used', 'for', 'extracting', 'features', 'from', 'the', 'dependency', 'parse', 'treeswe', 'used', 'the', 'lexicalized', 'dependency', 'parser', 'in', 'the', 'stanford', 'statistical', 'natural', 'language', 'parser', 'ver203', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'obtain', 'parses', 'for', 'the', 'data', 'sets'], ['to', 'demonstrate', 'the', 'effect', 'of', 'the', 'proposed', 'method', 'we', 'use', 'the', 'stateoftheart', 'phrasebased', 'system', 'and', 'hierarchical', 'phrasebased', 'system', 'implemented', 'in', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'is', 'used', 'to', 'train', 'a', 'phrase', 'based', 'smt', 'system', 'with', 'the', 'parallel', 'data', 'previously', 'introduced'], ['331', 'reference', 'system', 'we', 'compare', 'a', 'number', 'of', 'analogical', 'devices', 'to', 'the', 'stateoftheart', 'statistical', 'translation', 'engine', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['preprocessing', 'we', 'tokenized', 'the', 'english', 'side', 'of', 'all', 'bitexts', 'for', 'language', 'modeling', 'using', 'the', 'standard', 'tokenizer', 'of', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'tokenize', 'and', 'frequentcase', 'the', 'data', 'with', 'the', 'standard', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['finally', 'it', 'is', 'also', 'noticeable', 'that', 'the', 'percentage', 'of', 'compounds', 'detected', 'in', 'the', 'training', 'set', 'is', 'similar', 'to', 'the', 'one', 'reported', 'by', 'baroni', 'et', 'al', '2002', 'and', 'referenced', 'to', 'in', 'section', '2baroni', 'et', 'al', '2002', '', 'report', 'that', '47', 'of', 'the', 'vocabulary', 'types', 'in', 'the', 'apa', 'corpus', '2', 'were', 'compounds'], ['as', 'a', 'learning', 'algorithm', 'we', 'adopt', 'a', 'ranking', 'svm', 'joachims', '2002', '', 'which', 'is', 'an', 'instance', 'of', 'preference', 'learningto', 'train', 'a', 'mentionpair', 'classifier', 'we', 'use', 'the', 'svm', 'learning', 'algorithm', 'from', 'the', 'svm', 'light', 'package', 'joachims', '2002', '', 'converting', 'all', 'multivalued', 'features', 'into', 'an', 'equivalent', 'set', 'of', 'binaryvalued', 'featur'], ['brain', 'images', 'are', 'quite', 'noisy', 'so', 'we', 'used', 'the', 'methodology', 'from', 'mitchell', 'et', 'al', '2008', 'to', 'select', 'the', 'most', 'stable', 'brain', 'image', 'features', 'for', 'each', 'of', 'the', '18', 'participantsfollowing', 'the', 'evaluation', 'paradigm', 'of', 'mitchell', 'et', 'al', '2008', '', 'linear', 'models', 'trained', 'on', 'corpusbased', 'features', 'are', 'used', 'to', 'predict', 'the', 'pattern', 'of', 'eeg', 'activity', 'for', 'unseen', 'concepts'], ['the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007', 'this', 'system', 'is', 'the', 'moses', 'decoder', 'koehn', 'et', 'al', '2007', 'with', 'a', 'translation', 'and', 'a', 'language', 'model', 'trained', 'with', 'no', 'additional', 'resources', 'other', 'than', 'the', 'official', 'data', 'provided', 'by', 'the', 'shared', 'task', 'organizers'], ['results', 'on', 'englishfrench', 'englishromanian', 'and', 'czech', 'english', 'alignment', 'show', 'that', 'our', 'model', 'significantly', 'outperforms', 'fast', 'align', 'a', 'popular', 'loglinear', 'reparameterization', 'of', 'ibm', 'model', '2', 'dyer', 'et', 'al', '2013', 'our', 'alignment', 'model', 'is', 'based', 'on', 'a', 'simple', 'variant', 'of', 'ibm', 'model', '2', 'where', 'the', 'alignment', 'distribution', 'is', 'only', 'controlled', 'by', 'two', 'parameters', '', 'and', 'p', '0', 'dyer', 'et', 'al', '2013'], ['the', 'work', 'presented', 'in', 'berger', 'et', 'al', '1996', 'that', 'is', 'based', 'on', 'the', 'a', '', 'concept', 'however', 'introduces', 'word', 'reordering', 'restrictions', 'in', 'order', 'to', 'reduce', 'the', 'overall', 'search', 'spacethe', 'original', 'reordering', 'constraint', 'in', 'berger', 'et', 'al', '1996', 'is', 'shown', 'to', 'be', 'a', 'special', 'case', 'of', 'a', 'more', 'general', 'restriction', 'scheme', 'in', 'which', 'the', 'word', 'reordering', 'constraints', 'are', 'expressed', 'in', 'terms', 'of', 'simple'], ['our', 'second', 'method', 'is', 'based', 'on', 'the', 'recurrent', 'neural', 'network', 'language', 'model', 'rnnlm', 'approach', 'to', 'learning', 'word', 'embeddings', 'of', 'mikolov', 'et', 'al', '2013a', 'and', 'mikolov', 'et', 'al', '2013b', '', 'using', 'the', 'word2vec', 'packagemikolov', 'et', 'al', '2013a', 'proposed', 'a', 'faster', 'skipgram', 'model', 'word2vec', '5', 'which', 'tries', 'to', 'maximize', 'classification', 'of', 'a', 'word', 'based', 'on', 'another', 'word', 'in', 'the', 'same', 'sentence'], ['the', 'susanne', 'corpus', 'is', 'a', 'modified', 'and', 'condensed', 'version', 'of', 'brown', 'corpus', 'francis', 'and', 'kucera', '1979', 'in', 'order', 'to', 'avoid', 'the', 'errors', 'introduced', 'by', 'tagger', 'the', 'susanne', 'corpus', 'is', 'used', 'as', 'the', 'training', 'and', 'testhe', 'corpus', 'consists', 'of', 'a', 'subset', 'of', 'the', 'brown', 'corpus', '700000', 'words', 'with', 'more', 'than', '200000', 'senseannotated', 'francis', 'and', 'kucera', '1979', '', 'and', 'it', 'has', 'been', 'partofspeechtagged', 'and', 'sensetagged'], ['the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'all', 'these', 'features', 'are', 'inherited', 'from', 'moses', 'koehn', 'et', 'al', '2007'], ['english', 'annotations', 'were', 'all', 'produced', 'using', 'the', 'stanford', 'core', 'nlp', 'toolkit', 'manning', 'et', 'al', '2014', 'we', 'also', 'lemmatized', 'all', 'words', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['the', 'parameters', 'are', 'estimated', 'by', 'gibbs', 'sampling', 'using', 'the', 'mallet', 'implementation', 'mccallum', '2002', '10', 'we', 'used', 'the', 'pltm', 'implementation', 'in', 'mallet', 'mccallum', '2002'], ['the', 'metaclassifier', 'is', 'a', 'linear', 'svm', 'fan', 'et', 'al', '2008', 'implemented', 'with', 'kelpliblinear', 'fan', 'et', 'al', '2008', '', 'a', 'library', 'for', 'large', 'svm', 'linear', 'classification', 'is', 'used', 'for', 'implementation'], ['we', 'use', 'adadelta', 'zeiler', '2012', 'to', 'update', 'the', 'parameters', 'during', 'trainingwe', 'use', 'a', 'minibatch', 'stochastic', 'gradient', 'descent', 'sgd', 'with', 'the', 'adadelta', 'update', 'rule', 'zeiler', '2012', '', 'apply', 'random', 'initialization', 'within', '001', '001', 'for', 'w', 'f', 'j', '', 'and', 'initialize', 'the', 'remaining', 'parameter'], ['we', 'train', 'a', 'support', 'vector', 'machine', 'svm', 'cortes', 'and', 'vapnik', '1995', 'on', 'the', 'tweets', 'provided', 'for', 'trainingspecifically', 'we', 'use', 'support', 'vector', 'machine', 'svm', 'cortes', 'and', 'vapnik', '1995', 'for', 'this', 'purpose'], ['parameter', 'tuning', 'is', 'carried', 'out', 'using', 'z', 'mert', 'zaidan', '2009', 'feature', 'weights', 'based', 'on', 'bleu', 'are', 'then', 'tuned', 'using', 'z', 'mert', 'zaidan', '2009'], ['brin', 'identifies', 'the', 'use', 'of', 'patterns', 'in', 'the', 'discovery', 'of', 'relations', 'on', 'the', 'web', 'brin', '1998', 'brin', 'proposed', 'the', 'bootstrapping', 'method', 'for', 'relation', 'discovery', 'brin', '1998'], ['we', 'adopt', 'the', 'setting', 'of', 'socher', 'et', 'al', '2012', 'neural', 'networks', 'are', 'first', 'used', 'in', 'this', 'task', 'in', 'socher', 'et', 'al', '2012'], ['the', 'publicly', 'available', 'tool', 'giza', 'was', 'used', 'to', 'align', 'the', 'letters', 'och', 'and', 'ney', '2003', 'we', 'used', 'the', 'giza', 'software', 'och', 'and', 'ney', '2003', 'to', 'do', 'the', 'word', 'alignments'], ['crfsuite', 'implementation', 'okazaki', '2007', 'we', 'trained', 'a', 'crf', 'tagger', 'using', 'crfsuite', '1', 'okazaki', '2007'], ['the', 'hmm', 'classifier', 'used', 'in', 'our', 'experiments', 'follows', 'the', 'algorithm', 'described', 'in', 'bikel', 'et', 'al', '1999', 'the', 'hmm', 'classifier', 'used', 'in', 'the', 'experiments', 'in', 'section', '4', 'follows', 'the', 'system', 'description', 'in', 'bikel', 'et', 'al', '1999', '', 'and', 'it', 'performs', 'sequence', 'classification', 'by', 'assigning', 'each', 'word', 'either', 'one', 'of', 'the', 'named'], ['we', 'computed', '4gram', 'lms', 'with', 'modified', 'kneserney', 'smoothing', 'kneser', 'and', 'ney', '1995', 'the', 'language', 'model', 'is', 'a', '5gram', 'with', 'interpolation', 'and', 'kneserney', 'smoothing', 'kneser', 'and', 'ney', '1995'], ['socher', 'et', 'al', '2011', 'come', 'closest', 'to', 'our', 'target', 'problemwe', 'plan', 'to', 'adapt', 'ideas', 'from', 'socher', 'et', 'al', '2011', 'for', 'this', 'task'], ['we', 'use', 'ridge', 'regression', 'rr', 'with', 'l2norm', 'regularization', 'and', 'support', 'vector', 'regression', 'svr', 'with', 'an', 'rbf', 'kernel', 'from', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'we', 'use', 'the', 'support', 'vector', 'machines', 'implementation', 'in', 'the', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011', 'to', 'perform', 'regression', 'svr', 'on', 'each', 'feature', 'set', 'with', 'either', 'rbf', 'kernels', 'and', 'parameters', 'optimise'], ['as', 'for', 'the', 'former', 'hereafter', 'it', 'is', 'referred', 'to', 'synpth', 'we', 'continue', 'to', 'use', 'a', 'dependency', 'version', 'of', 'the', 'pruning', 'algorithm', 'of', 'xue', 'and', 'palmer', '2004', 'for', 'argument', 'a', 'dependency', 'version', 'of', 'the', 'pruning', 'algorithm', 'in', 'xue', 'and', 'palmer', '2004', 'is', 'used', 'to', 'find', 'in', 'an', 'iterative', 'way', 'the', 'current', 'syntactic', 'head', 'and', 'its', 'siblings', 'in', 'a', 'parse', 'tree', 'in', 'a', 'constitue'], ['15', 'the', 'significance', 'tests', 'were', 'performed', 'using', 'the', 'bootstrap', 'resampling', 'method', 'koehn', '2004', 'we', 'used', 'bootstrap', 'resampling', 'for', 'testing', 'statistical', 'significance', 'koehn', '2004'], ['in', 'this', 'work', 'we', 'focus', 'on', 'learning', 'with', 'support', 'vector', 'machines', 'svms', 'vapnik', '1995', 'specifically', 'we', 'use', 'support', 'vector', 'machine', 'svm', 'cortes', 'and', 'vapnik', '1995', 'for', 'this', 'purpose'], ['we', 'used', 'treetagger', 'schmid', '1994', 'to', 'obtain', 'a', 'lemmatag', 'pair', 'for', 'each', 'russian', 'wordwe', 'used', 'the', 'stuttgart', 'treetagger', 'schmid', '1994', 'to', 'lemmatise', 'constituent', 'heads'], ['to', 'account', 'for', 'this', 'constraint', '', 'include', 'information', 'from', 'latent', 'semantic', 'analysis', 'deerwester', 'et', 'al', '1990', 'earlier', 'models', 'made', 'use', 'of', 'latent', 'semantic', 'analysis', 'lsa', 'deerwester', 'et', 'al', '1990'], ['the', 'system', 'was', 'trained', 'on', 'the', 'english', 'and', 'danish', 'part', 'of', 'the', 'europarl', 'corpus', 'version', '3', 'koehn', '2005', 'in', 'principle', 'classifiers', 'trained', 'on', 'pdtb', 'data', 'can', 'be', 'applied', 'directly', 'to', 'label', 'connectives', 'over', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005', 'used', 'for', 'training', 'and', 'testing', 'smt'], ['we', 'use', 'linear', 'svms', 'from', 'liblinear', 'and', 'svms', 'with', 'rbf', 'kernel', 'from', 'libsvm', 'chang', 'and', 'lin', '2011', 'the', 'edit', 'distance', 'kernel', 'was', 'trained', 'with', 'libsvm', 'chang', 'and', 'lin', '2011'], ['in', 'current', 'phrasebased', 'statistical', 'machine', 'translation', 'systems', 'such', 'as', 'moses', '1', 'koehn', 'et', 'al', '2007', '', 'the', 'translation', 'model', 'is', 'defined', 'in', 'terms', 'of', 'phrase', 'pairs', 'biphrases', 'extracted', 'from', 'a', 'bilingualit', 'is', 'a', 'standard', 'phrasebased', 'machine', 'translation', 'model', 'koehn', 'et', 'al', '2007'], ['we', 'train', 'a', 'support', 'vector', 'machine', 'svm', 'cortes', 'and', 'vapnik', '1995', 'on', 'the', 'tweets', 'provided', 'for', 'trainingspecifically', 'we', 'use', 'support', 'vector', 'machine', 'svm', 'cortes', 'and', 'vapnik', '1995', 'for', 'this', 'purpose'], ['the', 'word', 'alignment', 'was', 'obtained', 'by', 'running', 'giza', 'och', 'and', 'ney', '2003', 'the', 'word', 'alignment', 'is', 'created', 'by', 'giza', 'och', 'and', 'ney', '2003', '', 'the', 'intersection', 'symmetrization', 'is', 'used'], ['for', 'training', 'svm', 'classifiers', 'we', 'used', 'libsvm', 'package', 'chang', 'and', 'lin', '2001', 'the', 'svm', 'implementation', 'used', 'was', 'libsvm', 'chang', 'and', 'lin', '2001', '', 'and', 'default', 'parameters', 'were', 'used', 'radial', 'basis', 'function', 'kernel', 'cost', 'parameter', 'of', '1', 'and', 'a', 'gamma', 'value', 'of', 'the', 'inverse', 'of', 'the', 'number', 'of', 'd'], ['we', 'use', 'the', 'standard', 'alignment', 'tool', 'giza', 'och', 'and', 'ney', '2003', 'to', 'word', 'align', 'the', 'parallel', 'datawe', 'used', 'giza', 'och', 'and', 'ney', '2003', 'to', 'align', 'the', 'words', 'in', 'the', 'corpus'], ['we', 'use', 'the', 'standard', 'alignment', 'tool', 'giza', 'och', 'and', 'ney', '2003', 'to', 'word', 'align', 'the', 'parallel', 'datawe', 'used', 'the', 'giza', 'software', 'och', 'and', 'ney', '2003', 'to', 'do', 'the', 'word', 'alignments'], ['in', 'the', 'penn', 'discourse', 'treebank', '20', 'prasad', 'et', 'al', '2008', '', 'the', 'sense', 'is', 'called', 'chosen', 'alternativein', 'contrast', 'the', 'set', 'of', 'discourse', 'markers', 'in', 'our', 'work', 'is', 'fixed', 'for', 'english', '', 'we', 'use', '61', 'markers', 'annotated', 'in', 'the', 'penn', 'discourse', 'treebank', '20', 'prasad', 'et', 'al', '2008', '', 'for', 'german', 'we', 'use', '155', 'oneword', 'tr'], ['weka', 'hall', 'et', 'al', '2009', 'was', 'used', 'to', 'apply', 'learning', 'methods', 'to', 'extracted', 'featuresthe', 'svm', 'implementation', 'of', 'weka', 'hall', 'et', 'al', '2009', 'was', 'used', 'to', 'build', 'the', 'clte', 'model'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settingsthis', 'was', 'done', 'with', 'a', 'specific', 'tool', 'provided', 'with', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'standard', 'alignment', 'tool', 'giza', 'och', 'and', 'ney', '2003', 'to', 'word', 'align', 'the', 'parallel', 'datawe', 'used', 'the', 'giza', 'software', 'och', 'and', 'ney', '2003', 'to', 'do', 'the', 'word', 'alignments'], ['we', 'used', 'the', 'giza', 'software', 'och', 'and', 'ney', '2003', 'to', 'do', 'the', 'word', 'alignmentswe', 'performed', 'word', 'alignment', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'default', 'growdiagfinaland', 'alignment', 'symmetrization', 'method'], ['the', 'language', 'model', 'used', 'is', 'the', '5gram', 'corpus', 'from', 'google', 'books', 'michel', 'et', 'al', '2011', 'we', 'used', 'google', 'books', 'ngrams', 'michel', 'et', 'al', '2011', 'as', 'the', 'general', 'corpus'], ['english', 'annotations', 'were', 'all', 'produced', 'using', 'the', 'stanford', 'core', 'nlp', 'toolkit', 'manning', 'et', 'al', '2014', 'we', 'also', 'lemmatized', 'all', 'words', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['for', 'example', 'turian', 'et', 'al', '2010', 'compared', 'brown', 'clusters', 'cw', 'embeddings', 'and', 'hlbl', 'embeddings', 'in', 'ner', 'and', 'chunking', 'taskswe', 'evaluate', 'cw', 'word', 'embeddings', 'with', '25', '50', 'and', '100', 'dimensions', 'as', 'well', 'as', 'hlbl', 'word', 'embeddings', 'with', '50', 'and', '100', 'dimensions', 'that', 'are', 'introduced', 'in', 'turian', 'et', 'al', '2010', 'and', 'can', 'be', 'downloaded', 'here', '4'], ['distributional', 'representations', 'encode', 'an', 'expression', 'by', 'its', 'environment', 'assuming', 'the', 'context', 'dependent', 'nature', 'of', 'meaning', 'according', 'to', 'which', 'one', 'shall', 'know', 'a', 'word', 'by', 'the', 'company', 'it', 'keeps', 'firth', '1957', 'this', 'is', 'often', 'called', 'distributional', 'semantics', 'where', 'a', 'word', 'or', 'a', 'name', 'in', 'our', 'case', 'is', 'known', 'by', 'the', 'company', 'it', 'keeps', 'firth', '1957'], ['li', 'et', 'al', '2013', '', 'use', 'crowdsourcing', 'to', 'build', 'plot', 'graphsour', 'neural', 'network', 'is', 'similar', 'to', 'that', 'of', 'li', 'et', 'al', '2013'], ['we', 'use', 'the', 'standard', 'alignment', 'tool', 'giza', 'och', 'and', 'ney', '2003', 'to', 'word', 'align', 'the', 'parallel', 'datawe', 'used', 'giza', 'och', 'and', 'ney', '2003', 'to', 'align', 'the', 'words', 'in', 'the', 'corpus'], ['the', 'formalism', 'that', 'is', 'used', 'to', 'represent', 'the', 'semantics', 'in', 'the', 'delphin', 'grammars', 'is', 'minimal', 'recursion', 'semantics', 'mrs', 'copestake', 'et', 'al', '2005', 'meaning', 'representation', 'and', 'composition', 'in', 'the', 'erg', 'builds', 'on', 'the', 'formal', 'framework', 'of', 'minimal', 'recursion', 'semantics', 'mrs', 'copestake', 'et', 'al', '2005'], ['for', 'french', 'hungarian', 'polish', 'and', 'swedish', 'we', 'used', 'europarl', 'corpus', '1', 'koehn', '2005', 'for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['the', 'english', 'text', 'was', 'tokenized', 'using', 'the', 'word', 'tokenize', 'routine', 'from', 'nltk', 'bird', 'et', 'al', '2009', 'the', 'stopwords', 'are', 'taken', 'from', 'the', 'stopword', 'list', 'provided', 'by', 'the', 'nltk', 'toolkit', 'bird', 'et', 'al', '2009'], ['we', 'use', 'the', 'moses', 'decoder', 'koehn', 'et', 'al', '2007', 'as', 'a', 'representative', 'smt', 'decoder', 'inside', 'the', 'system', 'described', 'belowwe', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['specifically', 'the', 'sentence', 'compression', 'dataset', 'clarke', 'and', 'lapata', '2008', 'referred', 'as', 'cl08', 'is', 'used', 'for', 'subtree', 'deletion', 'model', 'training', 'arc', 'one', 'idea', 'is', 'to', 'apply', 'it', 'to', 'the', 'language', 'model', 'based', 'compression', 'method', 'clarke', 'and', 'lapata', '2008'], ['use', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'extract', 'a', 'set', 'of', 'dependencies', 'from', 'each', 'commentwe', 'extract', 'syntactic', 'dependencies', 'using', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'and', 'use', 'its', 'collapsed', 'dependency', 'format'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['in', 'practice', 'the', 'decoder', 'has', 'to', 'employ', 'beam', 'search', 'to', 'make', 'it', 'tractable', 'koehn', '2004', 'to', 'make', 'the', 'exponential', 'algorithm', 'practical', 'beam', 'search', 'is', 'the', 'standard', 'approximate', 'search', 'method', 'koehn', '2004'], ['the', 'source', 'of', 'bilingual', 'data', 'used', 'in', 'the', 'experiments', 'is', 'the', 'europarl', 'collection', 'koehn', '2005', 'the', 'europarl', 'data', 'set', 'consists', 'of', '707', 'sentences', 'of', 'the', 'german', 'part', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['the', 'tectogrammatical', 'annotation', 'layer', 'is', 'based', 'on', 'the', 'functional', 'generative', 'description', 'theory', 'sgall', 'et', 'al', '1986', 'our', 'approach', 'is', 'based', 'on', 'the', 'czech', 'linguistic', 'tradition', 'represented', 'mainly', 'in', 'sgall', 'et', 'al', '1986'], ['in', 'this', 'paper', 'we', 'use', 'the', 'subjectivity', 'corpus', 'by', 'pang', 'et', 'al', '2002', 'these', 'classifiers', 'have', 'been', 'used', 'in', 'related', 'work', 'by', 'pang', 'et', 'al', '2002'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'baseline', 'error', 'correction', 'system', 'using', 'the', 'moses', 'smt', 'system', 'koehn', 'et', 'al', '2007'], ['all', 'our', 'systems', 'are', 'contrasted', 'with', 'a', 'standard', 'phrasebased', 'system', 'built', 'with', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'train', 'standard', 'phrasebased', 'systems', 'with', 'default', 'configurations'], ['a', 'common', 'approach', 'to', 'computing', 'similarity', 'is', 'to', 'count', 'the', 'number', 'of', 'common', 'words', 'lesk', '1986', 'this', 'sense', 'similarity', 'measure', 'is', 'inspired', 'by', 'the', 'definition', 'of', 'the', 'lesk', 'algorithm', 'lesk', '1986'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'is', 'used', 'for', 'extracting', 'features', 'from', 'the', 'dependency', 'parse', 'treesin', 'our', 'experiments', '', 'we', 'used', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'create', 'dependency', 'parses'], ['we', 'tokenize', 'and', 'frequentcase', 'the', 'data', 'with', 'the', 'standard', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'implement', 'mert', 'and', 'mira', '1', '', 'and', 'directly', 'use', 'mira', '2', 'from', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'tokenize', 'and', 'frequentcase', 'the', 'data', 'with', 'the', 'standard', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'our', 'baseline', 'is', 'a', 'phrasebased', 'mt', 'system', 'trained', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'tokenize', 'and', 'frequentcase', 'the', 'data', 'with', 'the', 'standard', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'conducted', 'baseline', 'experiments', 'for', 'phrasebased', 'machine', 'translation', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['in', 'contrast', 'mcclosky', 'et', 'al', '2006', 'focus', 'on', 'large', 'seeds', 'and', 'exploit', 'a', 'rerankingparsermcclosky', 'et', 'al', '2006', '', 'use', 'selftraining', 'in', 'combination', 'with', 'a', 'pcfg', 'parser', 'and', 'reranking'], ['we', 'use', 'logistic', 'regression', 'with', 'l2', 'regularization', 'implemented', 'using', 'the', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011', 'bullet', 'logistic', 'regressionlr', 'we', 'use', 'logistic', 'regression', 'with', 'l2', 'loss', 'penalty', 'and', 'c1', 'pedregosa', 'et', 'al', '2011'], ['since', 'the', 'phrase', 'table', 'contains', 'lemmas', 'the', 'wikipedia', 'corpus', 'was', 'lemmatised', 'using', 'the', 'treetagger', 'schmid', '1994', '3', 'all', 'english', 'data', 'are', 'pos', 'tagged', 'and', 'lemmatised', 'using', 'the', 'treetagger', 'schmid', '1994'], ['we', 'use', 'logistic', 'regression', 'with', 'l2', 'regularization', 'implemented', 'using', 'the', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011', 'we', 'use', 'the', 'bernoulli', 'naive', 'bayes', 'classifier', 'in', 'scikit', 'with', 'the', 'default', 'settings', 'pedregosa', 'et', 'al', '2011'], ['the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'both', 'systems', 'are', 'based', 'on', 'the', 'moses', 'smt', 'toolkit', 'koehn', 'et', 'al', '2007', 'and', 'constructed', 'as', 'follows'], ['for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'tokenize', 'and', 'frequentcase', 'the', 'data', 'with', 'the', 'standard', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'train', 'standard', 'phrasebased', 'systems', 'with', 'default', 'configurations'], ['we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['europarl', 'koehn', '2005', 'is', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'extract', 'our', 'paraphrase', 'grammar', 'from', 'the', 'frenchenglish', 'portion', 'of', 'the', 'europarl', 'corpus', 'version', '5', 'koehn', '2005'], ['for', 'all', 'syntactic', 'parsers', 'we', 'used', 'the', 'basic', 'stanford', 'dependency', 'representation', 'de', 'marneffe', 'et', 'al', '2006', 'in', 'our', 'experiments', '', 'we', 'used', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'create', 'dependency', 'parses'], ['the', 'stanford', 'dependency', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'is', 'used', 'for', 'extracting', 'features', 'from', 'the', 'dependency', 'parse', 'treesin', 'our', 'experiments', '', 'we', 'used', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'create', 'dependency', 'parses'], ['we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'all', 'our', 'systems', 'are', 'contrasted', 'with', 'a', 'standard', 'phrasebased', 'system', 'built', 'with', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'crossentropy', 'loss', 'function', 'and', 'minibatch', 'adagrad', 'duchi', 'et', 'al', '2011', 'to', 'optimize', 'parameters', 'we', 'use', 'adagrad', 'duchi', 'et', 'al', '2011', '', 'with', 'the', 'initial', 'learning', 'rate', 'set', 'to', '', '', '05'], ['we', 'use', 'the', 'crossentropy', 'loss', 'function', 'and', 'minibatch', 'adagrad', 'duchi', 'et', 'al', '2011', 'to', 'optimize', 'parameters', 'we', 'train', 'the', 'concept', 'identification', 'stage', 'using', 'infinite', 'ramp', 'loss', '1', 'with', 'adagrad', 'duchi', 'et', 'al', '2011'], ['we', 'use', 'the', 'adagrad', 'method', 'duchi', 'et', 'al', '2011', 'to', 'automatically', 'update', 'the', 'learning', 'rate', 'for', 'each', 'parameterwe', 'use', 'the', 'crossentropy', 'loss', 'function', 'and', 'minibatch', 'adagrad', 'duchi', 'et', 'al', '2011', 'to', 'optimize', 'parameters'], ['socher', 'et', 'al', '2011', '', 'explored', 'using', 'recursive', 'autoencoders', 'raes', 'and', 'dynamic', 'pooling', 'for', 'paraphrase', 'detectionsocher', 'et', 'al', '2011', '', 'use', 'recursive', 'autoencoders', 'for', 'sentiment', 'analysis', 'on', 'the', 'sentence', 'level'], ['2', 'translation', 'model', 'moses', 'is', 'a', 'stateoftheart', 'toolkit', 'for', 'phrasebased', 'smt', 'systems', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['2', 'translation', 'model', 'moses', 'is', 'a', 'stateoftheart', 'toolkit', 'for', 'phrasebased', 'smt', 'systems', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['2', 'translation', 'model', 'moses', 'is', 'a', 'stateoftheart', 'toolkit', 'for', 'phrasebased', 'smt', 'systems', 'koehn', 'et', 'al', '2007', 'our', 'baseline', 'is', 'a', 'phrasebased', 'mt', 'system', 'trained', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['for', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['all', 'our', 'systems', 'are', 'contrasted', 'with', 'a', 'standard', 'phrasebased', 'system', 'built', 'with', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['for', 'all', 'syntactic', 'parsers', 'we', 'used', 'the', 'basic', 'stanford', 'dependency', 'representation', 'de', 'marneffe', 'et', 'al', '2006', 'in', 'our', 'experiments', '', 'we', 'used', 'the', 'stanford', 'parser', 'de', 'marneffe', 'et', 'al', '2006', 'to', 'create', 'dependency', 'parses'], ['can', 'be', 'evaluated', 'by', 'maximising', 'the', 'pseudolikelihood', 'on', 'a', 'training', 'corpus', 'malouf', '2002', 'the', 'parameters', 'can', 'be', 'efficiently', 'estimated', 'from', 'a', 'treebank', 'as', 'shown', 'by', 'malouf', '2002'], ['the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['2', 'translation', 'model', 'moses', 'is', 'a', 'stateoftheart', 'toolkit', 'for', 'phrasebased', 'smt', 'systems', 'koehn', 'et', 'al', '2007', 'all', 'our', 'systems', 'are', 'contrasted', 'with', 'a', 'standard', 'phrasebased', 'system', 'built', 'with', 'moses', 'koehn', 'et', 'al', '2007'], ['in', 'this', 'approach', 'we', 'used', 'the', 'nersuite', 'software', 'based', 'on', 'the', 'crfsuite', 'implementation', 'okazaki', '2007', 'nersuite', 'is', 'a', 'ner', 'system', 'that', 'is', 'built', 'on', 'top', 'of', 'the', 'crfsuite', 'okazaki', '2007'], ['2', 'translation', 'model', 'moses', 'is', 'a', 'stateoftheart', 'toolkit', 'for', 'phrasebased', 'smt', 'systems', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['these', 'efforts', 'focused', 'exclusively', 'on', 'the', 'meronymy', 'relation', 'as', 'used', 'in', 'wordnet', 'miller', 'et', 'al', '1990', 'experts', 'can', 'manually', 'specify', 'the', 'attributes', 'of', 'entities', 'as', 'in', 'the', 'wordnet', 'project', 'miller', 'et', 'al', '1990'], ['special', 'forms', 'of', 'relatedness', 'are', 'represented', 'in', 'the', 'lexical', 'entries', 'of', 'the', 'wordnet', 'lexical', 'database', 'miller', 'et', 'al', '1990', 'experts', 'can', 'manually', 'specify', 'the', 'attributes', 'of', 'entities', 'as', 'in', 'the', 'wordnet', 'project', 'miller', 'et', 'al', '1990'], ['we', 'train', 'our', 'model', 'on', 'a', 'subset', 'of', 'the', 'wackypedia', 'en', '6', 'corpus', 'baroni', 'et', 'al', '2009', 'we', 'used', 'the', 'publicly', 'available', 'wacky', 'corpus', 'baroni', 'et', 'al', '2009'], ['since', 'the', 'first', 'shared', 'task', 'on', 'recognising', 'textual', 'entailment', 'rte', 'dagan', 'et', 'al', '2005', 'was', 'organised', 'in', '2005', 'much', 'research', 'has', 'been', 'done', 'on', 'how', 'one', 'can', 'detect', 'entailment', 'between', 'natural', 'language', 'although', 'there', 'had', 'been', 'research', 'on', 'reasoning', 'expressed', 'in', 'natural', 'language', 'the', 'pascal', 'recognising', 'textual', 'entailment', 'rte', 'challenge', 'dagan', 'et', 'al', '2005', 'spurred', 'wide', 'interest', 'in', 'the', 'problem'], ['we', 'used', 'kbest', 'batch', 'mira', 'cherry', 'and', 'foster', '2012', 'for', 'tuningwe', 'tune', 'the', 'systems', 'using', 'kbest', 'batch', 'mira', 'cherry', 'and', 'foster', '2012'], ['we', 'used', 'the', 'mallet', 'toolkit', 'mccallum', '2002', 'for', 'learning', 'maximum', 'entropy', 'models', 'with', 'gaussian', 'priors', 'for', 'all', 'our', 'experimentswe', 'use', 'the', 'mallet', 'package', 'mccallum', '2002', 'for', 'experiments'], ['we', 'use', 'the', 'implementation', 'provided', 'by', 'crfsuite', '7', 'okazaki', '2007', 'for', 'both', 'training', 'and', 'classification', 'taskswe', 'use', 'crfsuite', 'okazaki', '2007', 'as', 'an', 'implementation', 'of', 'crf'], ['glish', 'source', 'with', 'target', 'french', 'by', 'using', 'giza', 'och', 'and', 'ney', '2003', 'word', 'alignments', 'were', 'created', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'used', 'rougen', 'lin', '2004', 'for', 'evaluation', 'of', 'summarieswe', 'expect', 'this', 'restriction', 'is', 'more', 'consistent', 'with', 'the', 'rouge', 'evaluation', 'metric', 'used', 'for', 'summarization', 'lin', '2004'], ['1', 'with', '2', 'regularization', 'using', 'adagrad', 'duchi', 'et', 'al', '2011', 'we', 'adopt', 'online', 'learning', 'updating', 'parameters', 'using', 'adagrad', 'duchi', 'et', 'al', '2011'], ['we', 'used', 'weka', 'hall', 'et', 'al', '2009', 'for', 'all', 'our', 'classification', 'experimentsweka', 'hall', 'et', 'al', '2009', 'which', 'contains', 'the', 'implementation', 'of', 'all', 'three', 'algorithms', 'was', 'used', 'in', 'our', 'study'], ['pos', 'tagging', 'was', 'performed', 'with', 'treetagger', 'schmid', '1994', 'treetagger', 'is', 'a', 'statistical', 'decision', 'treebased', 'pos', 'tagger', 'schmid', '1994'], ['word', 'alignments', 'on', 'the', 'parallel', 'corpus', 'are', 'performed', 'using', 'giza', 'och', 'and', 'ney', '2003', 'with', 'the', 'growdiagfinal', 'refinementthe', 'parallel', 'corpus', 'is', 'wordaligned', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011', 'classification', 'uses', 'the', 'scikitlearn', 'python', 'package', 'pedregosa', 'et', 'al', '2011'], ['the', 'lexical', 'sample', 'data', 'was', 'parsed', 'using', 'the', 'clark', 'and', 'curran', 'ccg', 'parser', 'clark', 'and', 'curran', '2004', 'it', 'builds', 'on', 'the', 'cc', 'ccg', 'parser', 'clark', 'and', 'curran', '2004'], ['for', 'postagging', 'we', 'used', 'the', 'stanford', 'postagger', 'toutanova', 'and', 'manning', '2000', 'next', 'we', 'replace', 'all', 'nouns', 'with', 'their', 'pos', 'tag', 'we', 'use', 'the', 'stanford', 'pos', 'tagger', 'toutanova', 'and', 'manning', '2000'], ['we', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005', 'all', 'the', 'data', 'come', 'from', 'europarl', 'koehn', '2005'], ['this', 'data', 'is', 'part', 'of', 'the', 'nucle', 'corpus', 'dahlmeier', 'et', 'al', '2013', 'the', 'training', 'data', 'released', 'by', 'the', 'task', 'organizers', 'comes', 'from', 'the', 'nucle', 'corpus', 'dahlmeier', 'et', 'al', '2013', '', 'which', 'contains', 'essays', 'written', 'by', 'learners', 'of', 'english', 'as', 'a', 'foreign', 'language', 'and', 'is', 'corrected', 'by'], ['the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007', 'conducted', 'using', 'the', 'moses', 'phrasebased', 'decoder', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'the', 'word', 'alignment', 'produced', 'by', 'giza', 'och', 'and', 'ney', '2000', 'out', 'of', 'an', 'ibm', 'model', '2och', 'is', 'the', 'hmm', 'alignment', 'model', 'of', 'och', 'and', 'ney', '2000'], ['for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005', 'all', 'the', 'data', 'come', 'from', 'europarl', 'koehn', '2005'], ['word', 'alignment', 'is', 'performed', 'using', 'giza', 'och', 'and', 'ney', '2003', 'glish', 'source', 'with', 'target', 'french', 'by', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['word', 'alignment', 'was', 'done', 'with', 'giza', 'och', 'and', 'ney', '2003', 'for', 'both', 'systemsword', 'alignment', 'is', 'performed', 'with', 'giza', 'och', 'and', 'ney', '2003'], ['the', 'evaluation', 'results', 'were', 'provided', 'by', 'the', 'organizers', 'using', 'their', 'evaluation', 'script', 'in', 'python', 'dahlmeier', 'and', 'ng', '2012', 'the', 'models', 'were', 'evaluated', 'using', 'm2', 'scorer', 'dahlmeier', 'and', 'ng', '2012'], ['in', 'future', 'work', 'we', 'can', 'therefore', 'incorporate', 'unsupervised', 'methods', 'of', 'nsw', 'classification', 'and', 'expansion', 'along', 'the', 'lines', 'of', 'the', 'automatic', 'dictionary', 'construction', 'method', 'presented', 'by', 'han', 'et', 'al', '2012', 'han', 'et', 'al', '2012', 'introduced', 'a', 'dictionary', 'based', 'method', 'and', 'an', 'automatic', 'normalisationdictionary', 'construction', 'method'], ['glish', 'source', 'with', 'target', 'french', 'by', 'using', 'giza', 'och', 'and', 'ney', '2003', 'word', 'alignment', 'is', 'done', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['we', 'conducted', 'statistical', 'significance', 'tests', 'for', 'bleu', 'between', 'our', 'best', 'domainadapted', 'system', 'the', 'baseline', 'and', 'the', 'three', 'thirdparty', 'systems', 'using', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'with', '1000', 'we', 'used', 'bootstrap', 'resampling', 'for', 'testing', 'statistical', 'significance', 'koehn', '2004'], ['the', 'word', 'alignment', 'was', 'obtained', 'by', 'running', 'giza', 'och', 'and', 'ney', '2003', 'word', 'alignment', 'is', 'performed', 'with', 'giza', 'och', 'and', 'ney', '2003'], ['word', 'alignment', 'was', 'done', 'with', 'giza', 'och', 'and', 'ney', '2003', 'for', 'both', 'systemsword', 'alignment', 'is', 'performed', 'using', 'giza', 'och', 'and', 'ney', '2003'], ['for', 'the', 'theory', 'of', 'cho', '', 'chai', '2000', 'to', 'be', 'complete', 'we', 'have', 'proposed', 'a', 'new', 'type', 'of', 'marker', 'and', 'the', 'adjunct', 'lp', 'constraint', 'in', 'conjunction', 'with', 'their', 'argument', 'lp', 'constraintin', 'section', '3', 'we', 'present', 'a', 'solution', 'to', 'the', 'alleged', 'counterexample', 'of', 'cho', '', 'chai', '2000', 'and', 'explain', 'some', 'representative', 'scrambled', 'sentences', 'in', 'korean', 'under', 'our', 'analysis'], ['in', 'order', 'to', 'compare', 'our', 'method', 'to', 'a', 'well', 'understood', 'phrase', 'baseline', 'we', 'present', 'a', 'method', 'that', 'tracts', 'phrases', 'by', 'harvesting', 'the', 'viterbi', 'path', 'from', 'an', 'hmm', 'alignment', 'model', 'vogel', 'et', 'al', '1996', 'in', 'our', 'experiments', 'we', 'investigated', 'both', 'weak', 'and', 'a', 'strong', 'initialisations', 'the', 'former', 'based', 'on', 'word', 'alignments', 'from', 'ibm', 'model', '1', 'and', 'the', 'latter', 'on', 'alignments', 'from', 'an', 'hmm', 'model', 'vogel', 'et', 'al', '1996'], ['we', 'examine', 'the', 'quality', 'of', 'translations', 'to', 'english', 'from', 'chinese', 'and', 'arabic', 'using', 'humantargeted', 'translation', 'edit', 'rates', 'hter', 'snover', 'et', 'al', '2006', '', 'which', 'roughly', 'captures', 'the', 'minimal', 'number', 'of', 'editsto', 'calculate', 'the', 'number', 'of', 'changes', 'we', 'used', 'a', 'modified', 'translation', 'edit', 'rate', 'ter', 'which', 'measures', 'the', 'number', 'of', 'edits', 'needed', 'to', 'transform', 'one', 'sentence', 'into', 'another', 'snover', 'et', 'al', '2006'], ['integer', 'linear', 'programming', 'ilp', 'has', 'recently', 'been', 'applied', 'to', 'inference', 'in', 'sequential', 'conditional', 'random', 'fields', 'roth', 'and', 'yih', '2004', '', 'this', 'has', 'allowed', 'the', 'use', 'of', 'truly', 'global', 'constraints', 'during', 'infesecond', 'to', 'avoid', 'the', 'error', 'propagation', 'problem', 'inherent', 'in', 'the', 'pipeline', 'approach', 'we', 'perform', 'joint', 'inference', 'over', 'the', 'outputs', 'of', 'the', 'aci', 'and', 'ri', 'classifiers', 'in', 'an', 'integer', 'linear', 'programming', 'ilp', 'frame'], ['for', 'that', 'purpose', 'we', 'use', 'the', 'word', 'analogy', 'task', 'proposed', 'by', 'mikolov', 'et', 'al', '2013a', '', 'which', 'measures', 'the', 'accuracy', 'on', 'answering', 'questions', 'like', 'what', 'is', 'the', 'word', 'that', 'is', 'similar', 'to', 'small', 'in', 'the', 'same', 'sensmikolov', 'et', 'al', '2013a', 'proposed', 'a', 'faster', 'skipgram', 'model', 'word2vec', '5', 'which', 'tries', 'to', 'maximize', 'classification', 'of', 'a', 'word', 'based', 'on', 'another', 'word', 'in', 'the', 'same', 'sentence'], ['the', 'corpus', 'is', 'first', 'wordaligned', 'using', 'a', 'word', 'alignment', 'heuristic', 'och', 'and', 'ney', '2003', 'a', 'core', 'component', 'of', 'every', 'pbsmt', 'system', 'is', 'the', 'phrase', 'table', 'which', 'contains', 'bilingual', 'phrase', 'pairs', 'extracted', 'from', 'a', 'bilingual', 'corpus', 'after', 'word', 'alignment', 'och', 'and', 'ney', '2003'], ['for', 'ranking', 'we', 'use', 'the', 'svm', 'rank', 'ranker', 'joachims', '2006', '', 'which', 'learns', 'a', 'sparse', 'weight', 'vector', 'that', 'minimizes', 'the', 'number', 'of', 'swapped', 'pairs', 'in', 'the', 'training', 'setwe', 'use', 'the', 'svm', 'rank', 'implementation', 'joachims', '2006', 'of', 'ranking', 'svm', 'in', 'this', 'paper'], ['parameters', 'are', 'updated', 'using', 'adagrad', 'duchi', 'et', 'al', '2011', 'with', 'a', 'learning', 'rate', 'of', '01all', 'models', 'were', 'trained', 'using', 'adagrad', 'duchi', 'et', 'al', '2011', 'with', 'an', 'initial', 'base', 'learning', 'rate', 'of', '01', 'which', 'we', 'exponentially', 'decayed', 'with', 'a', 'decade', 'of', '15', 'million', 'steps'], ['the', 'tagger', 'we', 'use', 'is', 'tnt', 'brants', '2000', '', 'a', 'hidden', 'markov', 'trigram', 'tagger', 'which', 'was', 'trained', 'on', 'the', 'spoken', 'dutch', 'corpus', 'cgn', 'internal', 'release', '6hcrc', 'is', 'tagged', 'with', 'tnt', 'brants', '2000', '', 'trained', 'on', 'the', 'full', 'ptb'], ['sentiment', 'score', 'of', 'the', 'last', 'post', 'of', 'the', 'observation', 'period', 'we', 'trained', 'an', 'l2', 'regularized', 'logistic', 'regression', 'from', 'liblinear', 'fan', 'et', 'al', '2008', 'using', 'the', 'data', 'collected', 'from', 'the', 'depression', 'forumwe', 'trained', 'the', 'classifiers', 'using', 'the', 'liblinear', 'implementation', 'fan', 'et', 'al', '2008', 'of', 'logistic', 'regres', 'sion'], ['for', 'german', 'english', 'we', 'also', 'have', 'a', 'system', 'based', 'on', 'moses', 'koehn', 'et', 'al', '2007', 'for', 'reference', 'we', 'also', 'show', 'the', 'mt', 'performance', 'of', 'the', 'phrase', 'based', 'stringtotree', 'and', 'treetostring', 'systems', 'which', 'are', 'based', 'on', 'the', 'opensource', 'gizamoses', 'pipeline', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'universal', 'pos', 'tagset', 'upos', 'of', 'petrov', 'et', 'al', '2012', 'for', 'example', 'petrov', 'et', 'al', '2012', '', 'build', 'supervised', 'pos', 'taggers', 'for', '22', 'languages', 'using', 'the', 'tnt', 'tagger', 'brants', '2000', '', 'with', 'an', 'average', 'accuracy', 'of', '952'], ['we', 'also', 'considered', 'an', 'ensemble', 'of', 'our', 'approach', 'and', 'that', 'of', 'berant', 'and', 'liang', '2014', 'we', 'chose', 'a', 'threshold', 'such', 'that', 'our', 'approach', 'predicts', '50', 'of', 'the', 'time', 'when', 'sq', 'a', 'is', 'above', 'its', 'value', 'and', 'the', 'other', '50', 'of', 'the', 'time', 'we', 'use', 'the', 'prediction', 'of', 'berant', 'and', 'liang', '2014', 'instead'], ['one', 'way', 'to', 'solve', 'this', 'problem', 'is', 'to', 'use', 'a', 'kernel', 'function', 'that', 'is', 'tailored', 'for', 'particular', 'nlp', 'applications', 'such', 'as', 'the', 'tree', 'kernel', 'collins', 'and', 'duffy', '2001', 'for', 'statistical', 'parsingthe', 'proof', 'is', 'similar', 'to', 'the', 'proof', 'for', 'the', 'tree', 'kernel', 'in', 'collins', 'and', 'duffy', '2001'], ['we', 'use', 'adam', 'kingma', 'and', 'ba', '2015', 'for', 'optimisation', 'with', 'initial', 'learning', 'rate', 'of', '0001we', 'train', 'with', 'the', 'adam', 'optimizer', 'kingma', 'and', 'ba', '2015', '', 'a', 'learning', 'rate', 'of', '00001', 'batch', 'size', 'of', '50', 'and', 'dropout', 'with', 'probability', '02', 'applied', 'to', 'the', 'hidden', 'layer'], ['we', 'build', 'our', 'pbsmt', 'systems', 'in', 'a', 'standard', 'way', 'using', 'the', 'moses', 'system', '', 'kenlm', 'for', 'language', 'modelling', 'heafield', '2011', '', 'and', 'standard', 'lexical', 'reordering', 'model', 'we', 'use', 'kenlm', '3', 'heafield', '2011', 'for', 'computing', 'the', 'target', 'language', 'model', 'score'], ['the', 'starting', 'point', 'for', 'our', 'model', 'is', 'the', 'skipgram', 'with', 'negative', 'sampling', 'sgns', 'objective', 'of', 'mikolov', 'et', 'al', '2013b', 'our', 'model', 'is', 'an', 'extension', 'of', 'the', 'contextual', 'bag', 'of', 'words', 'cbow', 'model', 'of', 'mikolov', 'et', 'al', '2013b', '', 'a', 'method', 'for', 'learning', 'vector', 'representations', 'of', 'words', 'based', 'on', 'their', 'distributional', 'contexts'], ['we', 'introduce', 'a', 'new', 'anaphoricity', 'detection', 'model', 'as', 'the', 'second', 'neural', 'model', 'using', 'a', 'long', 'short', 'term', 'memory', 'lstm', 'network', 'hochreiter', 'and', 'schmidhuber', '1997', 'we', 'generated', 'embeddings', 'by', 'training', 'a', 'characterlevel', 'longshort', 'term', 'memory', 'lstm', 'network', 'hochreiter', 'and', 'schmidhuber', '1997', '', 'using', 'it', 'as', 'an', 'encoder', 'on', 'the', 'turns', 'at', 'talk', 'from', 'our', 'corpus'], ['the', 'realisation', 'ranking', 'component', 'is', 'an', 'svm', 'ranking', 'model', 'implemented', 'with', 'svmrank', 'a', 'support', 'vector', 'machinebased', 'learning', 'tool', 'joachims', '2006', 'the', 'advantage', 'of', 'an', 'svm', 'rank', 'model', 'is', 'that', 'it', 'is', 'very', 'fast', 'and', 'it', 'has', 'been', 'shown', 'to', 'be', 'accurate', 'for', 'a', 'variety', 'of', 'ranking', 'problems', 'joachims', '2006'], ['wsi', 'is', 'generally', 'considered', 'as', 'an', 'unsupervised', 'clustering', 'task', 'under', 'the', 'distributional', 'hypothesis', 'harris', '1954', 'that', 'the', 'word', 'meaning', 'is', 'reflected', 'by', 'the', 'set', 'of', 'contexts', 'in', 'which', 'it', 'appearsaccording', 'to', 'the', 'distributional', 'hypothesis', 'of', 'meaning', 'harris', '1954', '', 'words', 'that', 'occur', 'in', 'similar', 'contexts', 'tend', 'to', 'be', 'semantically', 'similar'], ['we', 'run', 'our', 'experiments', 'on', 'europarl', 'koehn', '2005', '', 'a', 'multilingual', 'parallel', 'corpus', 'which', 'is', 'described', 'in', 'detail', 'in', 'section', '31our', 'experiments', 'were', 'carried', 'out', 'on', 'the', 'europarl', 'koehn', '2005', 'corpus', 'which', 'is', 'a', 'corpus', 'widely', 'used', 'in', 'smt', 'and', 'that', 'has', 'been', 'used', 'in', 'several', 'mt', 'evaluation', 'campaigns'], ['for', 'the', 'language', 'model', 'we', 'used', 'the', 'kenlm', 'toolkit', 'heafield', '2011', 'to', 'create', 'a', '5gram', 'language', 'model', 'on', 'the', 'target', 'side', 'of', 'the', 'europarl', 'corpus', 'v7', 'with', 'approximately', '54m', 'tokens', 'with', 'kneserney', 'smoothe', 'language', 'model', 'is', 'a', '5gram', 'kenlm', 'heafield', '2011', 'model', 'trained', 'using', 'lmplz', 'with', 'modified', 'kneserney', 'smoothing', 'and', 'no', 'pruning'], ['the', 'second', 'collection', 'is', 'constituted', 'by', 'the', 'genia', 'corpus', 'kim', 'et', 'al', '2003', '3', '', 'which', 'contains', '2000', 'abstracts', 'from', 'medline', 'a', 'total', 'of', '18546', 'sentences', 'genia', 'kim', 'et', 'al', '2003', 'is', 'a', 'collection', 'of', '2000', 'research', 'abstracts', 'selected', 'from', 'the', 'search', 'results', 'of', 'medline', 'database', 'using', 'keywords', 'mesh', 'terms', 'human', 'blood', 'cells', 'and', 'transcription', 'factors'], ['we', 'introduce', 'a', 'new', 'anaphoricity', 'detection', 'model', 'as', 'the', 'second', 'neural', 'model', 'using', 'a', 'long', 'short', 'term', 'memory', 'lstm', 'network', 'hochreiter', 'and', 'schmidhuber', '1997', 'we', 'generated', 'embeddings', 'by', 'training', 'a', 'characterlevel', 'longshort', 'term', 'memory', 'lstm', 'network', 'hochreiter', 'and', 'schmidhuber', '1997', '', 'using', 'it', 'as', 'an', 'encoder', 'on', 'the', 'turns', 'at', 'talk', 'from', 'our', 'corpus'], ['the', 'sentence', 'aligned', 'parallel', 'data', 'is', 'first', 'wordaligned', 'using', 'giza', 'in', 'both', 'sourcetarget', 'and', 'targetsource', 'directions', 'followed', 'by', 'the', 'application', 'of', 'traditional', 'symmetrisation', 'heuristics', 'och', 'and', 'ney', '2003', 'baseline', 'word', 'alignments', 'were', 'obtained', 'by', 'running', 'giza', 'in', 'both', 'directions', 'and', 'symmetrizing', 'using', 'the', 'growdiagfinaland', 'heuristic', 'och', 'and', 'ney', '2003'], ['all', 'the', 'language', 'models', 'lm', 'used', 'in', 'our', 'experiments', 'are', '5grams', 'modified', 'kneserney', 'smoothed', 'lms', 'trained', 'using', 'kenlm', 'heafield', 'et', 'al', '2013', 'for', 'the', 'language', 'model', 'we', 'used', 'all', 'monolingual', 'datasets', 'and', 'the', 'french', 'parts', 'of', 'the', 'parallel', 'datasets', 'and', 'trained', 'a', '5gram', 'language', 'model', 'with', 'modified', 'kneserney', 'smoothing', 'using', 'kenlm', 'heafield', 'et', 'al', '2013'], ['they', 'are', 'based', 'on', 'distributional', 'hypothesis', 'which', 'works', 'under', 'the', 'assumption', 'that', 'similar', 'words', 'occur', 'in', 'similar', 'contexts', 'harris', '1954', 'many', 'researchers', 'have', 'considered', 'generating', 'paraphrases', 'by', 'mining', 'the', 'web', 'guided', 'by', 'the', 'distributional', 'hypothesis', 'which', 'states', 'that', 'words', 'occurring', 'in', 'similar', 'contexts', 'tend', 'to', 'have', 'similar', 'meanings', 'harris', '1954'], ['finally', 'we', 'consider', 'the', 'europarl', 'corpus', 'v7', 'koehn', '2005', '', 'given', 'it', 'is', 'widely', 'used', 'in', 'the', 'mt', 'community', 'for', 'spanish', 'englishour', 'experiments', 'were', 'carried', 'out', 'on', 'the', 'europarl', 'koehn', '2005', 'corpus', 'which', 'is', 'a', 'corpus', 'widely', 'used', 'in', 'smt', 'and', 'that', 'has', 'been', 'used', 'in', 'several', 'mt', 'evaluation', 'campaigns'], ['for', 'the', 'contextual', 'check', 'we', 'use', 'the', 'google', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'which', 'contains', 'counts', 'for', 'ngrams', 'from', 'unigrams', 'through', 'to', 'fivegrams', 'obtained', 'from', 'over', '1', 'trillion', 'word', 'tokeorgwikifogindex', '8', 'for', 'word', 'frequency', 'we', 'use', 'the', 'unigrams', 'data', 'from', 'the', 'google', 'web1t', 'collection', 'brants', 'and', 'franz', '2006'], ['by', 'imposing', 'constraints', 'on', 'the', 'possible', 'word', 'reorderings', 'similar', 'to', 'that', 'described', 'in', 'berger', 'et', 'al', '1996', '', 'the', 'dpbased', 'approach', 'becomes', 'more', 'effective', 'when', 'the', 'constraints', 'are', 'applied', 'the', 'number', 'the', 'first', 'is', 'a', 'reimplementation', 'of', 'the', 'stackbased', 'decoder', 'described', 'in', 'berger', 'et', 'al', '1996'], ['surdeanu', 'et', 'al', '2012', 'propose', 'a', 'twolayer', 'multiinstance', 'multilabel', 'miml', 'framework', 'to', 'capture', 'the', 'dependencies', 'among', 'relationssurdeanu', 'et', 'al', '2012', 'proposed', 'a', 'novel', 'approach', 'to', 'multiinstance', 'multilabel', 'learning', 'for', 'relation', 'extraction', '', 'which', 'jointly', 'modeled', 'all', 'the', 'sentences', 'in', 'texts', 'and', 'all', 'labels', 'in', 'knowledge', 'bases', 'for'], ['we', 'have', 'theoretically', 'suggested', 'that', 'based', 'on', 'cho', '', 'chai', '2000', '', 'our', 'theory', 'can', 'be', 'a', 'complete', 'theory', 'of', 'scrambling', 'phenomenon', 'by', 'providing', 'the', 'new', 'type', 'marker', 'and', 'the', 'adjunct', 'lp', 'constraintthen', 'we', 'revise', 'the', 'two', 'lp', 'constraints', 'of', 'cho', '', 'chai', '2000', 'and', 'add', 'the', 'adjunct', 'lp', 'constraint'], ['for', 'the', 'contextual', 'check', 'we', 'use', 'the', 'google', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'which', 'contains', 'counts', 'for', 'ngrams', 'from', 'unigrams', 'through', 'to', 'fivegrams', 'obtained', 'from', 'over', '1', 'trillion', 'word', 'tokewe', 'used', 'the', 'unigram', 'counts', 'from', 'the', 'web', '1t', '5gram', 'corpus', 'brants', 'and', 'franz', '2006', 'to', 'determine', 'the', 'frequency', 'of', 'use', 'of', 'each', 'candidate', 'synonym'], ['rouge', 'lin', '2004', 'is', 'a', 'set', 'of', 'evaluation', 'metrics', 'used', 'for', 'automatic', 'summarizationwe', 'expect', 'this', 'restriction', 'is', 'more', 'consistent', 'with', 'the', 'rouge', 'evaluation', 'metric', 'used', 'for', 'summarization', 'lin', '2004'], ['here', 'we', 'review', 'the', 'parameters', 'of', 'the', 'standard', 'phrasebased', 'translation', 'model', 'koehn', 'et', 'al', '2007', 'the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'spanishenglish', 's2e', 'training', 'corpus', 'was', 'drawn', 'from', 'the', 'europarl', 'collection', 'koehn', '2005', 'the', 'systems', 'for', 'the', 'english', '', 'spanish', 'translation', 'tasks', 'were', 'trained', 'on', 'the', 'sentencealigned', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'convert', 'the', 'trees', 'in', 'both', 'treebanks', 'from', 'constituencies', 'to', 'labeled', 'dependencies', 'see', 'figure', '1', 'using', 'the', 'stanford', 'converter', 'which', 'produces', '46', 'types', 'of', 'labeled', 'dependencies', '1', 'de', 'marneffe', 'et', 'al', '2006', 'we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006'], ['we', 'convert', 'the', 'trees', 'in', 'both', 'treebanks', 'from', 'constituencies', 'to', 'labeled', 'dependencies', 'see', 'figure', '1', 'using', 'the', 'stanford', 'converter', 'which', 'produces', '46', 'types', 'of', 'labeled', 'dependencies', '1', 'de', 'marneffe', 'et', 'al', 'we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006'], ['we', 'use', 'kenlm', '3', 'heafield', '2011', 'for', 'computing', 'the', 'target', 'language', 'model', 'scorewe', 'use', 'a', '5gram', 'lm', 'trained', 'on', 'the', 'gigaword', 'corpus', 'and', 'use', 'kenlm', 'heafield', '2011', 'for', 'lm', 'scoring', 'during', 'decoding'], ['the', 'most', 'famous', 'example', 'would', 'probably', 'be', 'the', 'europarl', 'corpus', 'koehn', '2005', 'the', 'system', 'was', 'trained', 'on', 'the', 'english', 'and', 'danish', 'part', 'of', 'the', 'europarl', 'corpus', 'version', '3', 'koehn', '2005'], ['we', 'use', 'the', 'stanford', 'parser', 'with', 'stanford', 'dependencies', 'de', 'marneffe', 'et', 'al', '2006', 'we', 'convert', 'the', 'trees', 'in', 'both', 'treebanks', 'from', 'constituencies', 'to', 'labeled', 'dependencies', 'see', 'figure', '1', 'using', 'the', 'stanford', 'converter', 'which', 'produces', '46', 'types', 'of', 'labeled', 'dependencies', '1', 'de', 'marneffe', 'et', 'al', '2006'], ['additionally', 'we', 'train', 'phrasebased', 'machine', 'translation', 'models', 'with', 'our', 'alignments', 'using', 'the', 'popular', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'here', 'we', 'review', 'the', 'parameters', 'of', 'the', 'standard', 'phrasebased', 'translation', 'model', 'koehn', 'et', 'al', '2007'], ['the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'moses', 'decoder', 'koehn', 'et', 'al', '2007', 'was', 'used', 'for', 'generating', 'lattices', 'and', 'nbest', 'lists'], ['the', 'morpho', 'syntactically', 'annotated', 'corpus', 'we', 'used', 'is', 'a', 'variant', 'of', 'the', 'french', 'treebank', 'or', 'ftb', 'abeille', 'et', 'al', '2003', 'this', 'corresponds', 'to', 'a', 'new', 'version', 'of', 'the', 'french', 'treebank', 'abeille', 'et', 'al', '2003'], ['there', 'has', 'been', 'a', 'large', 'amount', 'of', 'work', 'on', 'sentiment', 'analysis', 'at', 'various', 'levels', 'of', 'granularity', 'pang', 'and', 'lee', '2008', 'for', 'a', 'detailed', 'survey', 'of', 'the', 'field', 'of', 'sentiment', 'analysis', 'see', 'pang', 'and', 'lee', '2008'], ['this', 'task', 'setup', 'is', 'further', 'described', 'in', 'the', 'task', 'description', 'paper', 'rosenthal', 'et', 'al', '2014', 'a', 'complete', 'description', 'of', 'the', 'training', 'and', 'test', 'datasets', 'can', 'be', 'found', 'at', 'the', 'task', 'description', 'paper', 'rosenthal', 'et', 'al', '2014'], ['the', 'baseline', 'will', 'be', 'created', 'by', 'the', 'moses', 'smt', 'toolkit', 'koehn', 'et', 'al', '2007', '18', 'our', 'baseline', 'is', 'the', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'run', 'over', 'letter', 'strings', 'rather', 'than', 'word', 'strings'], ['discourse', 'structure', 'in', 'summarization', 'rhetorical', 'structure', 'theory', 'rst', 'mann', 'and', 'thompson', '1988', 'represents', 'the', 'discourse', 'in', 'a', 'document', 'in', 'the', 'form', 'of', 'a', 'tree', 'figure', '1', 'causal', 'relations', 'are', 'among', 'discourse', 'relations', 'defined', 'by', 'rhetorical', 'structure', 'theory', 'mann', 'and', 'thompson', '1988'], ['it', 'builds', 'on', 'the', 'cc', 'ccg', 'parser', 'clark', 'and', 'curran', '2004', 'it', 'receives', 'ccg', 'derivations', 'from', 'the', 'cc', 'parser', 'clark', 'and', 'curran', '2004', '', 'and', 'maps', 'lexical', 'categories', 'and', 'combinatory', 'rules', 'into', 'semantic', 'descriptions', 'expressed', 'by', 'lambda', 'calculus'], ['we', 'use', 'boxer', 'bos', 'et', 'al', '2004', 'to', 'parse', 'natural', 'language', 'into', 'a', 'logical', 'formin', 'transforming', 'natural', 'language', 'text', 'to', 'logical', 'form', 'we', 'build', 'on', 'the', 'software', 'package', 'boxer', 'bos', 'et', 'al', '2004'], ['the', 'training', 'set', 'is', 'used', 'to', 'train', 'the', 'phrasebased', 'translation', 'model', 'and', 'language', 'model', 'for', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'use', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'to', 'implement', 'our', 'models'], ['we', 'perform', 'bootstrap', 'resampling', 'with', 'bounds', 'estimation', 'as', 'described', 'in', 'koehn', '2004', 'we', 'also', 'report', '95', 'confidence', 'intervals', 'ci', 'measured', 'on', '1000', 'iterations', 'of', 'bootstrap', 'resampling', 'with', 'replacement', 'koehn', '2004'], ['the', 'major', 'part', 'of', 'data', 'comes', 'from', 'current', 'and', 'upcoming', 'full', 'releases', 'of', 'the', 'europarl', 'data', 'set', 'koehn', '2005', 'the', 'europarl', 'corpus', 'koehn', '2005', 'is', 'built', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliament'], ['for', 'italian', 'we', 'use', 'the', 'word2vec', 'to', 'train', 'word', 'embeddings', 'on', 'the', 'europarl', 'italian', 'corpus', 'koehn', '2005', '2', '7', 'httpopennlpapacheorg', '523', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005', 'as', 'testing', 'data'], ['the', 'gate', 'pluginbased', 'architecture', 'cunningham', 'et', 'al', '2002', 'is', 'the', 'basis', 'for', 'the', 'platform', 'environment', 'gate', 'cunningham', 'et', 'al', '2002a', 'is', 'an', 'architecture', '', 'a', 'framework', 'and', 'a', 'development', 'environment', 'for', 'human', 'language', 'technology', 'modules', 'and', 'applications'], ['we', 'trained', 'a', '5gram', 'language', 'model', 'on', 'the', 'xinhua', 'section', 'of', 'the', 'english', 'gigaword', 'corpus', '306', 'million', 'words', 'using', 'the', 'srilm', 'toolkit', 'stolcke', '2002', 'with', 'the', 'modified', 'kneserney', 'smoothing', 'chen', 'and', 'goodman', '1996', 'both', 'language', 'models', 'use', 'modified', 'kneserney', 'smoothing', 'chen', 'and', 'goodman', '1996'], ['we', 'used', 'giza', 'och', 'and', 'ney', '2003', 'along', 'with', 'the', 'growing', 'heuristics', 'to', 'wordalign', 'the', 'training', 'cor', 'puswe', 'automatically', 'wordaligned', 'the', 'german', 'part', 'to', 'each', 'of', 'the', 'others', 'with', 'the', 'giza', 'tool', 'och', 'and', 'ney', '2003'], ['our', 'previous', 'mlnbased', 'approach', 'for', 'joint', 'disambiguation', 'and', 'clustering', 'of', 'concepts', 'fahrni', 'and', 'strube', '2012', 'joint', 'disambiguation', 'and', 'clustering', 'of', 'mentions', 'improves', 'the', 'disambiguation', 'results', 'for', 'both', 'the', 'scopeignorant', 'fahrni', 'and', 'strube', '2012', 'and', 'the', 'scopeaware', 'approach'], ['pang', 'et', 'al', '2002', 'compare', 'the', 'performance', 'of', 'three', 'commonly', 'used', 'machine', 'learning', 'models', 'naive', 'bayes', 'maximum', 'entropy', 'and', 'svmpang', 'et', 'al', '2002', '', 'use', 'machine', 'learning', 'methods', 'nb', 'svm', 'and', 'maxent', 'to', 'detect', 'sentiments', 'on', 'movie', 'reviews'], ['jans', 'et', 'al', '2012', 'focused', 'solely', 'on', 'the', 'narrative', 'cloze', 'test', 'as', 'an', 'end', 'goalwe', 'refer', 'to', 'the', 'system', 'of', 'jans', 'et', 'al', '2012', 'as', 'the', 'single', 'protagonist', 'system'], ['we', 'applied', 'the', 'naive', 'bayes', 'probabilistic', 'supervised', 'learning', 'algorithm', 'from', 'the', 'weka', 'machine', 'learning', 'library', 'hall', 'et', 'al', '2009', 'naive', 'bayes', 'and', 'decision', 'tree', 'models', 'were', 'built', 'with', 'the', 'weka', 'hall', 'et', 'al', '2009', 'package', 'for', 'decision', 'trees', 'we', 'used', 'the', 'j48', 'implementation'], ['we', 'therefore', 'seek', 'to', 'allow', 'quick', 'incremental', 'updates', 'of', 'the', 'rm', 'within', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'build', 'a', 'state', 'of', 'the', 'art', 'phrasebased', 'smt', 'system', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'use', 'the', 'moses', 'software', 'package', '5', 'to', 'train', 'a', 'pbmt', 'model', 'koehn', 'et', 'al', '2007', 'we', 'compare', 'the', 'model', 'against', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', '', 'applied', 'to', 'phoneme', 'sequences'], ['it', 'has', 'a', 'much', 'longer', 'average', 'sentence', 'length', 'than', 'penn', 'chinese', 'treebank', 'pctb', '33', 'compare', 'to', '28', 'xue', 'et', 'al', '2005', 'we', 'then', 'describe', 'in', 'more', 'detail', 'a', 'modern', 'chinese', 'corpus', 'the', 'penn', 'chinese', 'treebank', 'xue', 'et', 'al', '2005'], ['the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'the', 'first', 'two', 'baselines', 'are', 'standard', 'systems', 'using', 'pbmt', 'or', 'hiero', 'trained', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'corpora', 'are', 'tokenised', 'and', 'truecased', 'using', 'scripts', 'from', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'the', 'first', 'two', 'baselines', 'are', 'standard', 'systems', 'using', 'pbmt', 'or', 'hiero', 'trained', 'using', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'dropout', 'rate', 'was', 'set', 'to', '05', 'and', 'the', 'model', 'was', 'trained', 'via', 'adagrad', 'duchi', 'et', 'al', '2011', 'we', 'use', 'the', 'crossentropy', 'loss', 'function', 'and', 'minibatch', 'adagrad', 'duchi', 'et', 'al', '2011', 'to', 'optimize', 'parameters'], ['the', 'dropout', 'rate', 'was', 'set', 'to', '05', 'and', 'the', 'model', 'was', 'trained', 'via', 'adagrad', 'duchi', 'et', 'al', '2011', 'we', 'use', 'adagrad', 'duchi', 'et', 'al', '2011', '', 'with', 'the', 'initial', 'learning', 'rate', 'set', 'to', '', '', '05'], ['we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007', 'we', 'compare', 'the', 'model', 'against', 'the', 'moses', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', '', 'applied', 'to', 'phoneme', 'sequences'], ['they', 'had', 'shown', 'that', 'the', 'penn', 'discourse', 'treebank', 'pdtb', 'style', 'prasad', 'et', 'al', '2008', 'discourse', 'relations', 'are', 'usefulthe', 'penn', 'discourse', 'treebank', 'pdtb', 'prasad', 'et', 'al', '2008', 'includes', 'several', 'relation', 'types', 'that', 'are', 'relevant', 'to', 'causation', 'primarily', 'cause', 'and', 'reason'], ['they', 'had', 'shown', 'that', 'the', 'penn', 'discourse', 'treebank', 'pdtb', 'style', 'prasad', 'et', 'al', '2008', 'discourse', 'relations', 'are', 'usefulpitler', 'and', 'nenkova', '2008', 'used', 'discourse', 'relations', 'of', 'the', 'penn', 'discourse', 'treebank', 'prasad', 'et', 'al', '2008', 'as', 'a', 'feature'], ['mcdonald', 'et', 'al', '2005', 'present', 'a', 'technique', 'for', 'training', 'discriminative', 'models', 'for', 'dependency', 'parsing', 'the', 'details', 'of', 'parsing', 'model', 'were', 'presented', 'in', 'mcdonald', 'et', 'al', '2005', 'and', 'mcdonald', 'and', 'pereira', '2006'], ['we', 'transformed', 'the', 'parse', 'trees', 'in', 'ontonotes', 'into', 'syntactic', 'dependencies', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['2', 'translation', 'model', 'moses', 'is', 'a', 'stateoftheart', 'toolkit', 'for', 'phrasebased', 'smt', 'systems', 'koehn', 'et', 'al', '2007', 'the', 'decoder', 'is', 'built', 'on', 'top', 'of', 'an', 'opensource', 'phrasebased', 'smt', 'decoder', 'moses', 'koehn', 'et', 'al', '2007'], ['for', 'the', 'ontonotes', 'data', 'sets', 'same', 'speaker', 'lee', 'et', 'al', '2013', 'is', 'the', 'only', 'feature', 'for', 'resolving', 'pronounsthe', 'mention', 'detection', 'of', 'the', 'stanford', 'coreference', 'system', 'lee', 'et', 'al', '2013', 'is', 'used', 'for', 'the', 'ontonotes', 'data', 'sets'], ['it', 'was', 'one', 'of', 'the', 'best', 'parsers', 'in', 'the', 'conll', 'shared', 'task', '2009', 'haji', 'et', 'al', '2009', 'ctb6', 'is', 'used', 'as', 'the', 'chinese', 'data', 'set', 'in', 'the', 'conll', '2009', 'shared', 'task', 'haji', 'et', 'al', '2009'], ['we', 'transformed', 'the', 'parse', 'trees', 'in', 'ontonotes', 'into', 'syntactic', 'dependencies', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['results', 'are', 'reported', 'on', 'the', 'test', 'data', 'using', 'f1', 'computed', 'with', 'the', 'conll', 'scorer', 'dahlmeier', 'and', 'ng', '2012', 'we', 'report', 'f1', 'performance', 'scored', 'using', 'the', 'official', 'scorer', 'from', 'the', 'shared', 'task', 'dahlmeier', 'and', 'ng', '2012'], ['the', 'perplexity', 'achieved', 'by', 'the', '6gram', 'nn', 'lm', 'in', 'the', 'spanish', 'news2009', 'set', 'was', '281', 'versus', '145', 'obtained', 'with', 'the', 'standard', '6gram', 'language', 'model', 'with', 'interpolation', 'and', 'kneserney', 'smoothing', 'kneser', 'and', 'nethe', 'results', 'of', 'this', 'experiment', 'appear', 'in', 'table', '2', 'which', 'further', 'compares', 'these', 'models', 'with', 'the', 'following', 'baselines', '1', 'vanila', 'rnnlstm', 'and', '2', 'a', '6gram', 'lm', 'with', 'kneserney', 'smoothing', '3', 'kneser', 'and', 'ney'], ['we', 'address', 'the', 'qe', 'problem', 'as', 'a', 'regression', 'task', 'by', 'building', 'svm', 'models', 'with', 'an', 'epsilon', 'regressor', 'and', 'a', 'radial', 'basis', 'function', 'kernel', 'using', 'the', 'libsvm', 'toolkit', 'chang', 'and', 'lin', '2011', 'we', 'use', 'libsvm', 'chang', 'and', 'lin', '2011', 'as', 'the', 'svm', 'tool'], ['finally', 'it', 'is', 'also', 'noticeable', 'that', 'the', 'percentage', 'of', 'compounds', 'detected', 'in', 'the', 'training', 'set', 'is', 'similar', 'to', 'the', 'one', 'reported', 'by', 'baroni', 'et', 'al', '2002', 'and', 'referenced', 'to', 'in', 'section', '2baroni', 'et', 'al', '2002', 'also', 'pointed', 'out', 'that', 'the', 'small', 'percentage', 'of', 'compounds', 'detected', 'at', 'token', 'level', '7', 'suggested', 'that', 'many', 'of', 'them', 'are', 'productively', 'formed', 'hapax', 'legomena', 'or', 'very', 'rare', 'words'], ['facts', 'such', 'as', 'these', 'are', 'difficult', 'to', 'account', 'for', 'in', 'a', 'purely', 'semantic', 'theory', 'of', 'ellipsis', 'resolution', 'such', 'as', 'the', 'one', 'proposed', 'in', 'dalrymple', 'et', 'al', '1991', 'the', 'most', 'influential', 'of', 'the', 'semantic', 'approaches', 'to', 'ellpsis', 'has', 'been', 'the', 'higherorder', 'unification', 'approach', 'proposed', 'in', 'dalrymple', 'et', 'al', '1991'], ['all', 'these', 'reordering', 'models', 'are', 'tested', 'using', 'moses', 'koehn', 'et', 'al', '2007', '', 'except', 'that', 'the', 'neural', 'model', 'needs', 'an', 'additional', 'hypergraph', 'reranking', 'procedure', 'section', '33we', 'refer', 'to', 'that', 'model', 'as', 'moses', 'enes100k', '', 'because', 'it', 'was', 'trained', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['bullet', 'raesubj', 'socher', 'et', 'al', '2011', 'proposed', 'to', 'use', 'recursive', 'autoencoders', 'for', 'sentencelevel', 'predication', 'of', 'sentiment', 'label', 'distributionswe', 'follow', 'the', 'formulation', 'of', 'vector', 'composition', 'proposed', 'by', 'socher', 'et', 'al', '2011', '', 'except', 'that', 'we', 'do', 'not', 'stack', 'autoencoders', 'for', 'recursion'], ['we', 'train', 'a', 'ridge', 'regression', 'model', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', '', 'for', 'each', 'assignment', 'and', 'test', 'using', 'annotations', 'from', 'the', 'rest', 'as', 'training', 'examples', 'our', 'system', 'is', 'a', 'linear', 'model', 'estimated', 'using', 'ridge', 'regression', 'as', 'implemented', 'in', 'the', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011'], ['we', 'follow', 'the', 'protocols', 'in', 'collobert', 'et', 'al', '2011', '', 'using', 'the', 'concatenation', 'of', 'neighboring', 'embeddings', 'as', 'input', 'to', 'a', 'multilayer', 'neural', 'modelwe', 'use', 'srl', 'collobert', 'et', 'al', '2011', 'to', 'determine', 'the', 'agent', 'and', 'patient', 'arguments', 'of', 'an', 'event', 'mention'], ['this', 'is', 'equivalent', 'to', 'an', 'svm', 'with', 'the', 'compound', 'cluster', 'features', 'as', 'in', 'koo', 'et', 'al', '2008', 'the', 'sparsity', 'of', 'lexical', 'features', 'can', 'also', 'be', 'tackled', 'by', 'the', 'use', 'of', 'distributional', 'word', 'clusters', 'as', 'pioneered', 'by', 'koo', 'et', 'al', '2008'], ['all', 'experiments', 'are', 'conducted', 'using', 'the', 'moses', 'phrasebased', 'smt', 'system', 'koehn', 'et', 'al', '2007', 'with', 'a', 'maximum', 'phrase', 'length', 'of', '8for', 'the', 'comparisons', 'of', 'translation', 'quality', 'the', 'models', 'are', 'trained', 'up', 'using', 'a', 'phrasebased', 'translation', 'system', 'koehn', 'et', 'al', '2007', 'that', 'used', 'the', 'above', 'listed', 'models', 'to', 'align', 'the', 'data'], ['we', 'use', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'create', 'a', 'statistical', 'phrasebased', 'machine', 'translation', 'model', 'built', 'on', 'the', 'best', 'preprocessed', 'data', 'as', 'described', 'abovewe', 'refer', 'to', 'that', 'model', 'as', 'moses', 'enes100k', '', 'because', 'it', 'was', 'trained', 'using', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['all', 'experiments', 'were', 'on', 'english', 'part', 'of', 'speech', 'pos', 'tagging', 'on', 'the', 'postagged', 'wall', 'street', 'journal', 'text', 'in', 'the', 'penn', 'treebank', 'ptb', 'version', '3', 'marcus', 'et', 'al', '1994', 'these', 'techniques', 'were', 'evaluated', 'in', 'experiments', 'on', 'the', 'penn', 'treebank', 'marcus', 'et', 'al', '1994', 'with', 'the', 'widecoverage', 'hpsg', 'parser', 'developed', 'by'], ['we', 'use', 'the', 'word', 'alignments', 'to', 'construct', 'a', 'phrase', 'table', 'by', 'applying', 'the', 'consistent', 'phrase', 'pair', 'heuristic', 'och', 'and', 'ney', '2004', 'to', 'all', '5gramswe', 'use', 'the', 'intersection', 'of', 'direct', 'and', 'reverse', 'giza', 'och', 'and', 'ney', '2004', 'alignments', 'as', 'a', 'heuristic', 'rule', 'to', 'find', 'words', 'reliably', 'aligned', 'to', 'each', 'other'], ['we', 'estimated', 'a', 'hierarchical', 'mt', 'model', 'for', 'the', 'train', 'partition', 'with', 'the', 'standard', 'configuration', 'of', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'we', 'also', 'compare', 'with', 'the', 'standard', 'phrasebased', 'system', 'of', 'moses', 'koehn', 'et', 'al', '2007', '', 'with', 'standard', 'settings', 'except', 'for', 'the', 'ttable', 'limit', 'which', 'we', 'set', 'to', '100'], ['a', 'small', 'amount', 'of', 'labeled', 'data', 'is', 'used', 'to', 'map', 'the', 'induced', 'topics', 'to', 'realworld', 'senses', 'for', 'a', 'description', 'of', 'the', 'method', 'see', 'agirre', 'and', 'soroa', '2007', 'thus', 'inducing', 'a', 'number', 'of', 'clusters', 'similar', 'to', 'the', 'number', 'of', 'senses', 'is', 'not', 'a', 'requirement', 'for', 'good', 'results', 'agirre', 'and', 'soroa', '2007a'], ['we', 'use', 'the', 'word', 'alignments', 'to', 'construct', 'a', 'phrase', 'table', 'by', 'applying', 'the', 'consistent', 'phrase', 'pair', 'heuristic', 'och', 'and', 'ney', '2004', 'to', 'all', '5gramswe', 'use', 'the', 'intersection', 'of', 'direct', 'and', 'reverse', 'giza', 'och', 'and', 'ney', '2004', 'alignments', 'as', 'a', 'heuristic', 'rule', 'to', 'find', 'words', 'reliably', 'aligned', 'to', 'each', 'other'], ['1', 'for', 'a', 'reference', 'standard', 'text', 'we', 'used', 'the', 'written', 'portion', 'of', 'the', 'british', 'national', 'corpus', 'bnc', 'burnard', '2000', 'the', 'evaluation', 'corpus', 'is', 'a', 'subset', 'of', 'an', 'ungrammatical', 'version', 'of', 'the', 'british', 'national', 'corpus', 'bnc', 'a', '100', 'million', 'word', 'balanced', 'corpus', 'of', 'british', 'english', 'burnard', '2000'], ['we', 'worked', 'with', 'the', 'europarl', 'corpus', 'koehn', '2005', 'in', 'order', 'to', 'have', 'a', 'parallel', 'comparative', 'corpus', 'for', 'italian', 'and', 'spanishtranslations', 'for', 'english', 'words', 'in', 'the', 'lexical', 'sample', 'are', 'extracted', 'from', 'a', 'semiautomatic', 'word', 'alignment', 'of', 'sentences', 'from', 'the', 'europarl', 'parallel', 'corpus', 'koehn', '2005'], ['the', 'organization', 'from', 'semeval2013', 'task', '2', 'sentiment', 'analysis', 'in', 'twitter', 'provided', 'three', 'datasets', 'for', 'the', 'task', 'wilson', 'et', 'al', '2013', 'we', 'participated', 'in', 'both', 'subtask', 'a', 'and', 'b', 'of', 'semeval2013', 'task', '2', 'sentiment', 'analysis', 'in', 'twitter', 'wilson', 'et', 'al', '2013', 'with', 'an', 'adaptation', 'of', 'our', 'existing', 'system'], ['pending', 'a', 'planned', 'full', 'evaluation', 'using', 'the', 'moses', 'system', 'koehn', 'et', 'al', '2007', 'as', 'a', 'benchmark', 'we', 'tested', 'the', 'mt', 'method', 'outlined', 'above', 'on', 'two', 'simple', 'tasksto', 'test', 'our', 'method', 'we', 'conducted', 'two', 'lowresource', 'translation', 'experiments', 'using', 'the', 'phrasebased', 'mt', 'system', 'moses', 'koehn', 'et', 'al', '2007'], ['the', 'entity', 'transition', 'features', 'are', 'then', 'used', 'to', 'train', 'a', 'support', 'vector', 'machine', 'ranker', 'joachims', '2002', 'to', 'rank', 'the', 'source', 'documents', 'higher', 'than', 'the', 'permutationswe', 'use', 'the', 'support', 'vector', 'machine', 'svm', 'rank', 'algorithm', 'joachims', '2002', 'to', 'predict', 'a', 'rank', 'order', 'for', 'each', 'list', 'of', 'comments'], ['our', 'second', 'method', 'is', 'based', 'on', 'the', 'recurrent', 'neural', 'network', 'language', 'model', 'rnnlm', 'approach', 'to', 'learning', 'word', 'embeddings', 'of', 'mikolov', 'et', 'al', '2013a', 'and', 'mikolov', 'et', 'al', '2013b', '', 'using', 'the', 'word2vec', 'packagein', 'all', 'of', 'the', 'above', 'tasks', 'we', 'compare', 'the', 'neural', 'word', 'embeddings', 'of', 'mikolov', 'et', 'al', '2013a', 'with', 'two', 'vector', 'spaces', 'both', 'based', 'on', 'cooccurrence', 'counts', 'and', 'produced', 'by', 'standard', 'distributional', 'techniques'], ['we', 'apply', 'the', 'hidden', 'markov', 'model', 'hmm', 'viterbi', '1967', 'and', 'the', 'forwardbackward', 'algorithm', 'james', '1995', 'to', 'obtain', 'the', 'contextdependent', 'probabilitieswe', 'use', 'viterbi', 'algorithm', 'viterbi', '1967', 'to', 'decode', 'the', 'character', 'sentence'], ['we', 'report', 'bleu', 'papineni', 'et', 'al', '2001', 'of', 'translation', 'system', 'output', 'measured', 'against', 'the', 'original', 'english', 'querieswe', 'measure', 'translation', 'quality', 'via', 'the', 'bleu', 'score', 'papineni', 'et', 'al', '2001'], ['it', 'is', 'a', 'standard', 'phrasebased', 'machine', 'translation', 'model', 'koehn', 'et', 'al', '2007', 'all', 'our', 'systems', 'are', 'contrasted', 'with', 'a', 'standard', 'phrasebased', 'system', 'built', 'with', 'moses', 'koehn', 'et', 'al', '2007'], ['finally', 'we', 'used', 'moses', 'toolkit', 'as', 'phrasebased', 'reference', 'koehn', 'et', 'al', '2007', 'the', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['concept', 'similarity', 'is', 'computed', 'using', 'the', 'edgebased', 'path', 'similarity', 'pedersen', 'et', 'al', '2004', 'the', 'degree', 'of', 'similarity', 'between', 'two', 'similar', 'words', 'is', 'identified', 'using', 'wordnet', 'pedersen', 'et', 'al', '2004'], ['finally', 'we', 'used', 'moses', 'toolkit', 'as', 'phrasebased', 'reference', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['it', 'is', 'a', 'standard', 'phrasebased', 'machine', 'translation', 'model', 'koehn', 'et', 'al', '2007', 'we', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['2', 'translation', 'model', 'moses', 'is', 'a', 'stateoftheart', 'toolkit', 'for', 'phrasebased', 'smt', 'systems', 'koehn', 'et', 'al', '2007', 'finally', 'we', 'used', 'moses', 'toolkit', 'as', 'phrasebased', 'reference', 'koehn', 'et', 'al', '2007'], ['otherwise', 'it', 'is', 'measured', 'by', 'wordnet', 'similarity', 'package', 'pedersen', 'et', 'al', '2004', 'the', 'degree', 'of', 'similarity', 'between', 'two', 'similar', 'words', 'is', 'identified', 'using', 'wordnet', 'pedersen', 'et', 'al', '2004'], ['we', 'use', '5gram', 'language', 'models', 'with', 'kneserney', 'discounting', 'heafield', 'et', 'al', '2013', 'the', 'language', 'models', 'are', 'estimated', 'using', 'the', 'kenlm', 'toolkit', 'heafield', 'et', 'al', '2013', 'with', 'modified', 'kneserney', 'smoothing'], ['we', 'adopt', 'online', 'learning', 'updating', 'parameters', 'using', 'adagrad', 'duchi', 'et', 'al', '2011', 'we', 'use', 'the', 'crossentropy', 'loss', 'function', 'and', 'minibatch', 'adagrad', 'duchi', 'et', 'al', '2011', 'to', 'optimize', 'parameters'], ['we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'svm', 'implementations', 'from', 'scikitlearn', 'pedregosa', 'et', 'al', '2011', 'and', 'experimented', 'with', 'a', 'number', 'of', 'classifiers'], ['we', 'adopt', 'online', 'learning', 'updating', 'parameters', 'using', 'adagrad', 'duchi', 'et', 'al', '2011', 'we', 'train', 'the', 'concept', 'identification', 'stage', 'using', 'infinite', 'ramp', 'loss', '1', 'with', 'adagrad', 'duchi', 'et', 'al', '2011'], ['finally', 'we', 'used', 'moses', 'toolkit', 'as', 'phrasebased', 'reference', 'koehn', 'et', 'al', '2007', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'to', 'train', 'standard', 'phrasebased', 'systems', 'with', 'default', 'configurations'], ['europarl', 'koehn', '2005', 'is', 'a', 'multilingual', 'parallel', 'corpus', 'extracted', 'from', 'the', 'proceedings', 'of', 'the', 'european', 'parliamentwe', 'used', 'the', 'english', 'side', 'of', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['we', 'leave', 'the', 'thirdorder', 'models', 'koo', 'and', 'collins', '2010', 'for', 'a', 'future', 'study', 'koo10', 'stands', 'for', 'the', 'model', '1', 'in', 'koo', 'and', 'collins', '2010', 'which', 'is', 'a', 'thirdorder', 'model'], ['we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011', 'specifically', 'we', 'used', 'the', 'standard', 'gradient', 'boosting', 'regressor', 'in', 'the', 'scikitlearn', 'toolkit', '4', 'pedregosa', 'et', 'al', '2011'], ['for', 'english', 'and', 'french', 'a', 'model', 'was', 'trained', 'using', 'the', 'europarl', 'corpus', 'koehn', '2005', 'for', 'this', 'purpose', 'we', 'use', 'the', 'europarl', 'corpus', 'koehn', '2005'], ['empirically', 'we', 'show', 'that', 'our', 'model', 'beats', 'the', 'stateoftheart', 'systems', 'of', 'rush', 'et', 'al', '2015', '', 'on', 'multiple', 'data', 'setsvery', 'recently', 'rush', 'et', 'al', '2015', 'proposed', 'a', 'neural', 'attention', 'model', 'for', 'this', 'problem', 'using', 'a', 'new', 'data', 'set', 'for', 'training', 'and', 'showing', 'stateoftheart', 'performance', 'on', 'the', 'duc', 'tasks'], ['we', 'use', 'logistic', 'regression', 'with', 'l2', 'regularization', 'implemented', 'using', 'the', 'scikitlearn', 'toolkit', 'pedregosa', 'et', 'al', '2011', 'we', 'use', 'the', 'scikit', 'implementation', 'of', 'random', 'forest', 'pedregosa', 'et', 'al', '2011'], ['we', 'use', 'the', 'new', 'york', 'times', 'annotated', 'corpus', 'sandhaus', '2008', 'for', 'all', 'our', 'experimentsof', 'the', 'remaining', 'twelve', 'eight', 'came', 'from', 'a', 'large', 'set', 'of', 'possible', 'stimuli', 'we', 'collected', 'from', 'the', 'new', 'york', 'times', 'annotated', 'corpus', 'nytac', 'sandhaus', '2008', 'for', 'use', 'in', 'later', 'phases', 'of', 'the', 'experiment', '', 'wh'], ['we', 'use', 'the', 'nmf', 'and', 'tfidf', 'implementations', 'provided', 'by', 'scikitlearn', 'version', '014', 'pedregosa', 'et', 'al', '2011', 'we', 'used', 'the', 'implementation', 'of', 'the', 'scikitlearn', '2', 'module', 'pedregosa', 'et', 'al', '2011'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settingswe', 'built', 'phrasebased', 'machine', 'translation', 'systems', 'using', 'the', 'open', 'software', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['in', 'addition', 'the', 'data', 'was', 'tokenized', 'lemmatized', 'and', 'parsed', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014', 'we', 'also', 'lemmatized', 'all', 'words', 'using', 'stanford', 'corenlp', 'manning', 'et', 'al', '2014'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settingsthe', 'baseline', 'systems', 'are', 'built', 'with', 'the', 'opensource', 'phrasebased', 'smt', 'toolkit', 'moses', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settingsfor', 'training', 'the', 'translation', 'model', 'and', 'for', 'decoding', 'we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007'], ['we', 'used', 'the', 'moses', 'toolkit', 'koehn', 'et', 'al', '2007', 'with', 'its', 'default', 'settingsbullet', 'decoder', 'moses', 'koehn', 'et', 'al', '2007', 'with', 'default', 'options', '', 'except', 'for', 'the', 'distortion', 'limit', '620'], ['feature', 'selection', 'was', 'performed', 'using', 'chisquare', 'in', 'weka', 'hall', 'et', 'al', '2009', 'we', 'train', 'and', 'evaluate', 'the', 'classifiers', 'in', 'a', '10fold', 'crossvalidation', 'using', 'weka', 'hall', 'et', 'al', '2009'], ['barzilay', 'and', 'lapata', '2008', '', 'propose', 'an', 'entity', 'grid', 'model', 'which', 'represents', 'the', 'distribution', 'of', 'referents', 'in', 'a', 'discourse', 'for', 'sentence', 'orderingcheung', 'and', 'penn', '2010', 'extend', 'the', 'approach', 'of', 'filippova', 'and', 'strube', '2007', 'and', 'augment', 'a', 'sentenceinternal', 'constituent', 'ordering', 'model', 'with', 'sentenceexternal', 'features', 'inspired', 'from', 'the', 'entity', 'grid', 'model'], ['the', 'differences', 'in', 'bleu', 'points', 'are', '014', 'and', '016', 'which', 'are', 'not', 'statistically', 'significant', 'according', 'to', 'the', 'paired', 'bootstrap', 'resampling', 'method', 'koehn', '2004', 'statistically', 'significant', 'results', 'calculated', 'with', 'paired', 'bootstrap', 'resampling', 'koehn', '2004', 'for', 'bleu', 'and', 'nist', 'are', 'indicated', 'with', 'symbols', '', 'p', '', '001', 'and', 'p', '', '005'], ['this', 'type', 'of', 'nonlocal', 'feature', 'was', 'not', 'used', 'by', 'finkel', 'et', 'al', '2005', 'or', 'krishnan', 'and', 'manning', '2006for', 'example', 'finkel', 'et', 'al', '2005', 'enabled', 'the', 'use', 'of', 'nonlocal', 'features', 'by', 'using', 'gibbs', 'sampling'], ['this', 'type', 'of', 'nonlocal', 'feature', 'was', 'not', 'used', 'by', 'finkel', 'et', 'al', '2005', 'or', 'krishnan', 'and', 'manning', '2006the', 'resulting', 'performance', 'of', 'the', 'proposed', 'algorithm', 'with', 'nonlocal', 'features', 'is', 'higher', 'than', 'that', 'of', 'finkel', 'et', 'al', '2005', '', 'and', 'comparable', 'with', 'that', 'of', 'krishnan', 'and', 'manning', '2006'], ['distributional', 'semantics', 'is', 'based', 'on', 'the', 'idea', 'that', 'firth', '1957', 'in', 'other', 'words', 'the', 'meaning', 'of', 'a', 'word', 'is', 'related', 'to', 'the', 'contexts', 'it', 'appears', 'inword', 'cooccurence', 'statistics', 'you', 'shall', 'know', 'a', 'word', 'by', 'the', 'company', 'it', 'keeps', 'firth', '1957']]\n"
     ]
    }
   ],
   "source": [
    "myList = [i.split(' ') for i in words]\n",
    "print(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEMBUAT MODEL WORD2VEC\n",
    "word_model = Word2Vec(myList, vector_size=100, min_count=1, window=100, epochs=100, sg=0)\n",
    "word_model.save(\"word2vec100size.model\")\n",
    "#word_model.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3446, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Mengambil bobot word2vec\n",
    "embedding_matrix = word_model.wv.vectors\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = embedding_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[439,\n",
       "  523,\n",
       "  129,\n",
       "  123,\n",
       "  7,\n",
       "  1403,\n",
       "  1116,\n",
       "  9,\n",
       "  1404,\n",
       "  1,\n",
       "  603,\n",
       "  2140,\n",
       "  2141,\n",
       "  1,\n",
       "  755,\n",
       "  49,\n",
       "  888,\n",
       "  4,\n",
       "  1405,\n",
       "  1406,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  439,\n",
       "  523,\n",
       "  129,\n",
       "  123,\n",
       "  7,\n",
       "  1403,\n",
       "  1116,\n",
       "  9,\n",
       "  1404,\n",
       "  1,\n",
       "  755,\n",
       "  49,\n",
       "  888,\n",
       "  4,\n",
       "  1405,\n",
       "  1406,\n",
       "  2,\n",
       "  3,\n",
       "  117],\n",
       " [1,\n",
       "  118,\n",
       "  145,\n",
       "  13,\n",
       "  1,\n",
       "  1117,\n",
       "  183,\n",
       "  889,\n",
       "  2,\n",
       "  3,\n",
       "  156,\n",
       "  890,\n",
       "  756,\n",
       "  218,\n",
       "  5,\n",
       "  44,\n",
       "  63,\n",
       "  34,\n",
       "  95,\n",
       "  123,\n",
       "  757,\n",
       "  142,\n",
       "  891,\n",
       "  26,\n",
       "  1407,\n",
       "  118,\n",
       "  145,\n",
       "  13,\n",
       "  1,\n",
       "  1117,\n",
       "  183,\n",
       "  889,\n",
       "  2,\n",
       "  3,\n",
       "  156,\n",
       "  42,\n",
       "  240,\n",
       "  756,\n",
       "  218,\n",
       "  5,\n",
       "  44,\n",
       "  63,\n",
       "  34,\n",
       "  95,\n",
       "  123,\n",
       "  757,\n",
       "  142,\n",
       "  891,\n",
       "  26,\n",
       "  1408],\n",
       " [892,\n",
       "  157,\n",
       "  287,\n",
       "  1409,\n",
       "  1410,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  13,\n",
       "  7,\n",
       "  130,\n",
       "  671,\n",
       "  319,\n",
       "  1,\n",
       "  157,\n",
       "  5,\n",
       "  7,\n",
       "  136,\n",
       "  13,\n",
       "  2142,\n",
       "  31,\n",
       "  7,\n",
       "  1411,\n",
       "  1412,\n",
       "  2143,\n",
       "  157,\n",
       "  287,\n",
       "  1409,\n",
       "  1410,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  13,\n",
       "  7,\n",
       "  130,\n",
       "  671,\n",
       "  2144,\n",
       "  1,\n",
       "  157,\n",
       "  5,\n",
       "  7,\n",
       "  136,\n",
       "  31,\n",
       "  7,\n",
       "  1411,\n",
       "  1412,\n",
       "  758],\n",
       " [6,\n",
       "  262,\n",
       "  77,\n",
       "  80,\n",
       "  14,\n",
       "  604,\n",
       "  411,\n",
       "  31,\n",
       "  138,\n",
       "  8,\n",
       "  12,\n",
       "  36,\n",
       "  6,\n",
       "  262,\n",
       "  77,\n",
       "  80,\n",
       "  14,\n",
       "  604,\n",
       "  411,\n",
       "  31,\n",
       "  138,\n",
       "  26,\n",
       "  12,\n",
       "  36],\n",
       " [98,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  158,\n",
       "  130,\n",
       "  320,\n",
       "  8,\n",
       "  264,\n",
       "  44,\n",
       "  251,\n",
       "  146,\n",
       "  380,\n",
       "  277,\n",
       "  4,\n",
       "  321,\n",
       "  84,\n",
       "  241,\n",
       "  76,\n",
       "  8,\n",
       "  205,\n",
       "  146,\n",
       "  440,\n",
       "  252,\n",
       "  759,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  158,\n",
       "  130,\n",
       "  320,\n",
       "  8,\n",
       "  1,\n",
       "  264,\n",
       "  44,\n",
       "  251,\n",
       "  146,\n",
       "  277,\n",
       "  4,\n",
       "  321,\n",
       "  84,\n",
       "  241,\n",
       "  76,\n",
       "  8,\n",
       "  205,\n",
       "  146,\n",
       "  440,\n",
       "  252,\n",
       "  760],\n",
       " [6,\n",
       "  18,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  160,\n",
       "  200,\n",
       "  4,\n",
       "  55,\n",
       "  761,\n",
       "  118,\n",
       "  18,\n",
       "  7,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  160,\n",
       "  111,\n",
       "  136,\n",
       "  4,\n",
       "  219,\n",
       "  1,\n",
       "  75,\n",
       "  5,\n",
       "  55,\n",
       "  242,\n",
       "  762,\n",
       "  14,\n",
       "  1,\n",
       "  136],\n",
       " [1413,\n",
       "  2,\n",
       "  3,\n",
       "  288,\n",
       "  750,\n",
       "  68,\n",
       "  2145,\n",
       "  2146,\n",
       "  322,\n",
       "  103,\n",
       "  11,\n",
       "  1118,\n",
       "  893,\n",
       "  1414,\n",
       "  61,\n",
       "  23,\n",
       "  7,\n",
       "  894,\n",
       "  441,\n",
       "  2147,\n",
       "  2148,\n",
       "  1413,\n",
       "  2,\n",
       "  3,\n",
       "  288,\n",
       "  750,\n",
       "  68,\n",
       "  2149,\n",
       "  322,\n",
       "  2150,\n",
       "  11,\n",
       "  2151],\n",
       " [1,\n",
       "  895,\n",
       "  763,\n",
       "  895,\n",
       "  896,\n",
       "  211,\n",
       "  112,\n",
       "  605,\n",
       "  13,\n",
       "  524,\n",
       "  31,\n",
       "  1,\n",
       "  525,\n",
       "  212,\n",
       "  5,\n",
       "  2152,\n",
       "  476,\n",
       "  2153,\n",
       "  2154,\n",
       "  4,\n",
       "  2155,\n",
       "  895,\n",
       "  763,\n",
       "  2156,\n",
       "  68,\n",
       "  2157,\n",
       "  5,\n",
       "  1,\n",
       "  142,\n",
       "  211,\n",
       "  112,\n",
       "  605,\n",
       "  895,\n",
       "  896],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  348,\n",
       "  7,\n",
       "  70,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  19,\n",
       "  66,\n",
       "  23,\n",
       "  1,\n",
       "  606,\n",
       "  442,\n",
       "  28,\n",
       "  31,\n",
       "  138,\n",
       "  1415,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  262,\n",
       "  77,\n",
       "  80,\n",
       "  14,\n",
       "  604,\n",
       "  411,\n",
       "  31,\n",
       "  138,\n",
       "  8,\n",
       "  12,\n",
       "  36,\n",
       "  6,\n",
       "  262,\n",
       "  77,\n",
       "  80,\n",
       "  14,\n",
       "  604,\n",
       "  411,\n",
       "  31,\n",
       "  138,\n",
       "  26,\n",
       "  12,\n",
       "  36],\n",
       " [98,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  158,\n",
       "  130,\n",
       "  320,\n",
       "  8,\n",
       "  264,\n",
       "  44,\n",
       "  251,\n",
       "  146,\n",
       "  380,\n",
       "  277,\n",
       "  4,\n",
       "  321,\n",
       "  84,\n",
       "  241,\n",
       "  76,\n",
       "  8,\n",
       "  205,\n",
       "  146,\n",
       "  440,\n",
       "  252,\n",
       "  759,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  158,\n",
       "  130,\n",
       "  320,\n",
       "  8,\n",
       "  1,\n",
       "  264,\n",
       "  44,\n",
       "  251,\n",
       "  146,\n",
       "  277,\n",
       "  4,\n",
       "  321,\n",
       "  84,\n",
       "  241,\n",
       "  76,\n",
       "  8,\n",
       "  205,\n",
       "  146,\n",
       "  440,\n",
       "  252,\n",
       "  760],\n",
       " [98,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  158,\n",
       "  130,\n",
       "  320,\n",
       "  8,\n",
       "  1,\n",
       "  44,\n",
       "  264,\n",
       "  146,\n",
       "  380,\n",
       "  277,\n",
       "  4,\n",
       "  321,\n",
       "  84,\n",
       "  241,\n",
       "  76,\n",
       "  8,\n",
       "  205,\n",
       "  146,\n",
       "  440,\n",
       "  252,\n",
       "  759,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  158,\n",
       "  130,\n",
       "  320,\n",
       "  8,\n",
       "  1,\n",
       "  264,\n",
       "  44,\n",
       "  251,\n",
       "  146,\n",
       "  277,\n",
       "  4,\n",
       "  321,\n",
       "  84,\n",
       "  241,\n",
       "  76,\n",
       "  8,\n",
       "  205,\n",
       "  146,\n",
       "  440,\n",
       "  252,\n",
       "  760],\n",
       " [98,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  158,\n",
       "  130,\n",
       "  320,\n",
       "  8,\n",
       "  264,\n",
       "  44,\n",
       "  146,\n",
       "  380,\n",
       "  277,\n",
       "  4,\n",
       "  321,\n",
       "  84,\n",
       "  241,\n",
       "  76,\n",
       "  8,\n",
       "  205,\n",
       "  146,\n",
       "  440,\n",
       "  252,\n",
       "  759,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  158,\n",
       "  130,\n",
       "  320,\n",
       "  8,\n",
       "  1,\n",
       "  264,\n",
       "  44,\n",
       "  251,\n",
       "  146,\n",
       "  277,\n",
       "  4,\n",
       "  321,\n",
       "  84,\n",
       "  241,\n",
       "  76,\n",
       "  8,\n",
       "  205,\n",
       "  146,\n",
       "  440,\n",
       "  252,\n",
       "  760],\n",
       " [98,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  158,\n",
       "  130,\n",
       "  320,\n",
       "  8,\n",
       "  1,\n",
       "  44,\n",
       "  264,\n",
       "  146,\n",
       "  380,\n",
       "  277,\n",
       "  4,\n",
       "  321,\n",
       "  84,\n",
       "  241,\n",
       "  76,\n",
       "  8,\n",
       "  205,\n",
       "  146,\n",
       "  440,\n",
       "  252,\n",
       "  759,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  158,\n",
       "  130,\n",
       "  320,\n",
       "  8,\n",
       "  1,\n",
       "  264,\n",
       "  44,\n",
       "  251,\n",
       "  146,\n",
       "  277,\n",
       "  4,\n",
       "  321,\n",
       "  84,\n",
       "  241,\n",
       "  76,\n",
       "  8,\n",
       "  205,\n",
       "  146,\n",
       "  440,\n",
       "  252,\n",
       "  760],\n",
       " [6,\n",
       "  121,\n",
       "  672,\n",
       "  24,\n",
       "  323,\n",
       "  526,\n",
       "  897,\n",
       "  61,\n",
       "  128,\n",
       "  11,\n",
       "  412,\n",
       "  324,\n",
       "  221,\n",
       "  4,\n",
       "  322,\n",
       "  443,\n",
       "  4,\n",
       "  438,\n",
       "  64,\n",
       "  6,\n",
       "  121,\n",
       "  672,\n",
       "  24,\n",
       "  323,\n",
       "  128,\n",
       "  11,\n",
       "  412,\n",
       "  324,\n",
       "  221,\n",
       "  4,\n",
       "  322,\n",
       "  443,\n",
       "  4,\n",
       "  438,\n",
       "  64],\n",
       " [673,\n",
       "  1119,\n",
       "  99,\n",
       "  4,\n",
       "  2158,\n",
       "  154,\n",
       "  104,\n",
       "  289,\n",
       "  8,\n",
       "  898,\n",
       "  4,\n",
       "  899,\n",
       "  36,\n",
       "  673,\n",
       "  1119,\n",
       "  99,\n",
       "  4,\n",
       "  2159,\n",
       "  154,\n",
       "  104,\n",
       "  289,\n",
       "  8,\n",
       "  898,\n",
       "  4,\n",
       "  899,\n",
       "  36],\n",
       " [6,\n",
       "  607,\n",
       "  1,\n",
       "  106,\n",
       "  10,\n",
       "  7,\n",
       "  1416,\n",
       "  608,\n",
       "  14,\n",
       "  900,\n",
       "  83,\n",
       "  1417,\n",
       "  349,\n",
       "  1418,\n",
       "  4,\n",
       "  674,\n",
       "  16,\n",
       "  6,\n",
       "  607,\n",
       "  1,\n",
       "  106,\n",
       "  10,\n",
       "  7,\n",
       "  1416,\n",
       "  2160,\n",
       "  608,\n",
       "  14,\n",
       "  900,\n",
       "  83,\n",
       "  1417,\n",
       "  349,\n",
       "  1418,\n",
       "  4,\n",
       "  674,\n",
       "  16],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  1419,\n",
       "  161,\n",
       "  1420,\n",
       "  901,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  253,\n",
       "  4,\n",
       "  121,\n",
       "  902,\n",
       "  10,\n",
       "  124,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  1421,\n",
       "  1422,\n",
       "  303,\n",
       "  82,\n",
       "  138,\n",
       "  8,\n",
       "  1423,\n",
       "  2161,\n",
       "  903,\n",
       "  35,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  1419,\n",
       "  161,\n",
       "  1420,\n",
       "  901,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  253,\n",
       "  4,\n",
       "  121,\n",
       "  902,\n",
       "  10,\n",
       "  124,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  1421,\n",
       "  1422,\n",
       "  303,\n",
       "  82,\n",
       "  138,\n",
       "  8,\n",
       "  1423,\n",
       "  16],\n",
       " [48,\n",
       "  13,\n",
       "  609,\n",
       "  31,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  904,\n",
       "  48,\n",
       "  13,\n",
       "  609,\n",
       "  31,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  8,\n",
       "  905,\n",
       "  114,\n",
       "  904],\n",
       " [51,\n",
       "  24,\n",
       "  65,\n",
       "  31,\n",
       "  350,\n",
       "  31,\n",
       "  1,\n",
       "  52,\n",
       "  138,\n",
       "  8,\n",
       "  906,\n",
       "  43,\n",
       "  32,\n",
       "  232,\n",
       "  254,\n",
       "  2162,\n",
       "  65,\n",
       "  31,\n",
       "  350,\n",
       "  31,\n",
       "  1,\n",
       "  52,\n",
       "  138,\n",
       "  8,\n",
       "  906,\n",
       "  43,\n",
       "  32,\n",
       "  232,\n",
       "  254,\n",
       "  2163],\n",
       " [11,\n",
       "  605,\n",
       "  675,\n",
       "  290,\n",
       "  162,\n",
       "  1120,\n",
       "  1424,\n",
       "  14,\n",
       "  413,\n",
       "  764,\n",
       "  8,\n",
       "  765,\n",
       "  1425,\n",
       "  4,\n",
       "  765,\n",
       "  905,\n",
       "  898,\n",
       "  4,\n",
       "  899,\n",
       "  36,\n",
       "  11,\n",
       "  605,\n",
       "  7,\n",
       "  2164,\n",
       "  5,\n",
       "  290,\n",
       "  162,\n",
       "  1120,\n",
       "  1424,\n",
       "  14,\n",
       "  675,\n",
       "  764,\n",
       "  8,\n",
       "  765,\n",
       "  1425,\n",
       "  4,\n",
       "  765,\n",
       "  905,\n",
       "  898,\n",
       "  4,\n",
       "  899,\n",
       "  36],\n",
       " [1,\n",
       "  411,\n",
       "  5,\n",
       "  1,\n",
       "  766,\n",
       "  291,\n",
       "  767,\n",
       "  189,\n",
       "  162,\n",
       "  768,\n",
       "  13,\n",
       "  414,\n",
       "  169,\n",
       "  26,\n",
       "  68,\n",
       "  1121,\n",
       "  213,\n",
       "  5,\n",
       "  1426,\n",
       "  108,\n",
       "  170,\n",
       "  1427,\n",
       "  37,\n",
       "  1,\n",
       "  411,\n",
       "  5,\n",
       "  1,\n",
       "  766,\n",
       "  291,\n",
       "  767,\n",
       "  189,\n",
       "  162,\n",
       "  768,\n",
       "  13,\n",
       "  414,\n",
       "  169,\n",
       "  26,\n",
       "  68,\n",
       "  1121,\n",
       "  213,\n",
       "  5,\n",
       "  1426,\n",
       "  108,\n",
       "  325,\n",
       "  1427,\n",
       "  37],\n",
       " [6,\n",
       "  69,\n",
       "  14,\n",
       "  1,\n",
       "  243,\n",
       "  527,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  201,\n",
       "  7,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  1122,\n",
       "  676,\n",
       "  415,\n",
       "  5,\n",
       "  326,\n",
       "  4,\n",
       "  444,\n",
       "  14,\n",
       "  528,\n",
       "  769,\n",
       "  214,\n",
       "  9,\n",
       "  1,\n",
       "  529,\n",
       "  2165,\n",
       "  69,\n",
       "  14,\n",
       "  1,\n",
       "  243,\n",
       "  527,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  201,\n",
       "  7,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  1122,\n",
       "  676,\n",
       "  415,\n",
       "  5,\n",
       "  326,\n",
       "  4,\n",
       "  444,\n",
       "  14,\n",
       "  528,\n",
       "  769,\n",
       "  214,\n",
       "  9,\n",
       "  1,\n",
       "  529,\n",
       "  677],\n",
       " [8,\n",
       "  24,\n",
       "  1123,\n",
       "  678,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  907,\n",
       "  169,\n",
       "  908,\n",
       "  8,\n",
       "  202,\n",
       "  1428,\n",
       "  4,\n",
       "  1429,\n",
       "  40,\n",
       "  8,\n",
       "  24,\n",
       "  1123,\n",
       "  678,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  907,\n",
       "  169,\n",
       "  81,\n",
       "  5,\n",
       "  99,\n",
       "  8,\n",
       "  202,\n",
       "  1428,\n",
       "  4,\n",
       "  1429,\n",
       "  40],\n",
       " [327,\n",
       "  439,\n",
       "  1430,\n",
       "  4,\n",
       "  1431,\n",
       "  117,\n",
       "  770,\n",
       "  34,\n",
       "  1,\n",
       "  1432,\n",
       "  5,\n",
       "  1,\n",
       "  1124,\n",
       "  1433,\n",
       "  26,\n",
       "  2166,\n",
       "  530,\n",
       "  1125,\n",
       "  1,\n",
       "  1434,\n",
       "  5,\n",
       "  7,\n",
       "  1435,\n",
       "  322,\n",
       "  2167,\n",
       "  1430,\n",
       "  4,\n",
       "  1431,\n",
       "  117,\n",
       "  770,\n",
       "  34,\n",
       "  1,\n",
       "  1432,\n",
       "  5,\n",
       "  1,\n",
       "  1124,\n",
       "  1433,\n",
       "  26,\n",
       "  2168,\n",
       "  530,\n",
       "  1125,\n",
       "  1,\n",
       "  1434,\n",
       "  5,\n",
       "  7,\n",
       "  1435,\n",
       "  322,\n",
       "  128],\n",
       " [328,\n",
       "  32,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  904,\n",
       "  4,\n",
       "  26,\n",
       "  1436,\n",
       "  190,\n",
       "  7,\n",
       "  75,\n",
       "  5,\n",
       "  477,\n",
       "  1437,\n",
       "  1438,\n",
       "  478,\n",
       "  1439,\n",
       "  95,\n",
       "  7,\n",
       "  72,\n",
       "  377,\n",
       "  328,\n",
       "  1440,\n",
       "  9,\n",
       "  909,\n",
       "  1441,\n",
       "  2169,\n",
       "  381,\n",
       "  910,\n",
       "  23,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  904,\n",
       "  4,\n",
       "  26,\n",
       "  1436,\n",
       "  190,\n",
       "  7,\n",
       "  75,\n",
       "  5,\n",
       "  477,\n",
       "  1437,\n",
       "  1438,\n",
       "  478,\n",
       "  1439,\n",
       "  95,\n",
       "  7,\n",
       "  72,\n",
       "  377,\n",
       "  1440,\n",
       "  9,\n",
       "  909,\n",
       "  1441,\n",
       "  477],\n",
       " [30,\n",
       "  73,\n",
       "  13,\n",
       "  109,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  30,\n",
       "  73,\n",
       "  13,\n",
       "  233,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [30,\n",
       "  73,\n",
       "  13,\n",
       "  109,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  30,\n",
       "  73,\n",
       "  13,\n",
       "  233,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [6, 15, 382, 108, 383, 87, 11, 531, 610, 15, 382, 108, 383, 87, 11, 2170, 58],\n",
       " [1,\n",
       "  2171,\n",
       "  351,\n",
       "  5,\n",
       "  7,\n",
       "  1126,\n",
       "  5,\n",
       "  1442,\n",
       "  130,\n",
       "  1127,\n",
       "  524,\n",
       "  11,\n",
       "  206,\n",
       "  1443,\n",
       "  611,\n",
       "  14,\n",
       "  7,\n",
       "  532,\n",
       "  911,\n",
       "  5,\n",
       "  1444,\n",
       "  1445,\n",
       "  2,\n",
       "  3,\n",
       "  329,\n",
       "  1,\n",
       "  2172,\n",
       "  2173,\n",
       "  351,\n",
       "  5,\n",
       "  7,\n",
       "  1126,\n",
       "  5,\n",
       "  1442,\n",
       "  130,\n",
       "  1127,\n",
       "  524,\n",
       "  11,\n",
       "  206,\n",
       "  1443,\n",
       "  611,\n",
       "  14,\n",
       "  7,\n",
       "  532,\n",
       "  911,\n",
       "  5,\n",
       "  533,\n",
       "  1445,\n",
       "  2,\n",
       "  3,\n",
       "  329],\n",
       " [1,\n",
       "  612,\n",
       "  1446,\n",
       "  13,\n",
       "  176,\n",
       "  8,\n",
       "  1,\n",
       "  1128,\n",
       "  213,\n",
       "  5,\n",
       "  1,\n",
       "  263,\n",
       "  278,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  7,\n",
       "  612,\n",
       "  1446,\n",
       "  23,\n",
       "  1,\n",
       "  188,\n",
       "  13,\n",
       "  176,\n",
       "  8,\n",
       "  1,\n",
       "  1128,\n",
       "  213,\n",
       "  5,\n",
       "  1,\n",
       "  263,\n",
       "  278,\n",
       "  2,\n",
       "  3,\n",
       "  88],\n",
       " [1,\n",
       "  118,\n",
       "  145,\n",
       "  13,\n",
       "  1,\n",
       "  2174,\n",
       "  756,\n",
       "  131,\n",
       "  183,\n",
       "  889,\n",
       "  2,\n",
       "  3,\n",
       "  156,\n",
       "  890,\n",
       "  756,\n",
       "  218,\n",
       "  5,\n",
       "  44,\n",
       "  63,\n",
       "  34,\n",
       "  95,\n",
       "  123,\n",
       "  757,\n",
       "  142,\n",
       "  891,\n",
       "  26,\n",
       "  1407,\n",
       "  118,\n",
       "  145,\n",
       "  13,\n",
       "  1,\n",
       "  1117,\n",
       "  183,\n",
       "  889,\n",
       "  2,\n",
       "  3,\n",
       "  156,\n",
       "  42,\n",
       "  240,\n",
       "  756,\n",
       "  218,\n",
       "  5,\n",
       "  44,\n",
       "  63,\n",
       "  34,\n",
       "  95,\n",
       "  123,\n",
       "  757,\n",
       "  142,\n",
       "  891,\n",
       "  26,\n",
       "  1408],\n",
       " [7,\n",
       "  330,\n",
       "  11,\n",
       "  352,\n",
       "  534,\n",
       "  115,\n",
       "  4,\n",
       "  534,\n",
       "  215,\n",
       "  129,\n",
       "  123,\n",
       "  97,\n",
       "  8,\n",
       "  912,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  241,\n",
       "  252,\n",
       "  352,\n",
       "  143,\n",
       "  48,\n",
       "  13,\n",
       "  76,\n",
       "  7,\n",
       "  479,\n",
       "  1129,\n",
       "  2175,\n",
       "  330,\n",
       "  11,\n",
       "  352,\n",
       "  534,\n",
       "  115,\n",
       "  129,\n",
       "  123,\n",
       "  97,\n",
       "  8,\n",
       "  912,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  241,\n",
       "  31,\n",
       "  613,\n",
       "  352,\n",
       "  143,\n",
       "  48,\n",
       "  13,\n",
       "  76,\n",
       "  7,\n",
       "  479,\n",
       "  1129,\n",
       "  62],\n",
       " [913,\n",
       "  184,\n",
       "  13,\n",
       "  7,\n",
       "  914,\n",
       "  231,\n",
       "  5,\n",
       "  48,\n",
       "  1130,\n",
       "  915,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  913,\n",
       "  184,\n",
       "  13,\n",
       "  68,\n",
       "  231,\n",
       "  5,\n",
       "  48,\n",
       "  1130,\n",
       "  915,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [771,\n",
       "  172,\n",
       "  2176,\n",
       "  1447,\n",
       "  13,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  1448,\n",
       "  4,\n",
       "  1449,\n",
       "  1450,\n",
       "  5,\n",
       "  133,\n",
       "  61,\n",
       "  1451,\n",
       "  1452,\n",
       "  8,\n",
       "  1,\n",
       "  207,\n",
       "  772,\n",
       "  134,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1447,\n",
       "  13,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  2177,\n",
       "  1448,\n",
       "  4,\n",
       "  1449,\n",
       "  1450,\n",
       "  5,\n",
       "  133,\n",
       "  61,\n",
       "  1451,\n",
       "  1452,\n",
       "  8,\n",
       "  1,\n",
       "  207,\n",
       "  772,\n",
       "  134,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  1,\n",
       "  886,\n",
       "  599,\n",
       "  4,\n",
       "  51,\n",
       "  3433,\n",
       "  302,\n",
       "  6,\n",
       "  751,\n",
       "  1,\n",
       "  3434,\n",
       "  9,\n",
       "  1,\n",
       "  159,\n",
       "  62,\n",
       "  2127,\n",
       "  263,\n",
       "  3435,\n",
       "  2,\n",
       "  3,\n",
       "  88],\n",
       " [11,\n",
       "  599,\n",
       "  6,\n",
       "  15,\n",
       "  1131,\n",
       "  279,\n",
       "  115,\n",
       "  4,\n",
       "  221,\n",
       "  11,\n",
       "  292,\n",
       "  916,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  42,\n",
       "  13,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  255,\n",
       "  1132,\n",
       "  292,\n",
       "  599,\n",
       "  2178,\n",
       "  48,\n",
       "  265,\n",
       "  6,\n",
       "  18,\n",
       "  1131,\n",
       "  279,\n",
       "  115,\n",
       "  4,\n",
       "  221,\n",
       "  11,\n",
       "  292,\n",
       "  916,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  42,\n",
       "  13,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  255,\n",
       "  1132,\n",
       "  292,\n",
       "  599,\n",
       "  2179],\n",
       " [6,\n",
       "  50,\n",
       "  7,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  23,\n",
       "  1,\n",
       "  917,\n",
       "  185,\n",
       "  5,\n",
       "  1,\n",
       "  44,\n",
       "  353,\n",
       "  22,\n",
       "  1133,\n",
       "  480,\n",
       "  63,\n",
       "  10,\n",
       "  1,\n",
       "  614,\n",
       "  20,\n",
       "  615,\n",
       "  87,\n",
       "  14,\n",
       "  1,\n",
       "  155,\n",
       "  116,\n",
       "  2180,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  33,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  917,\n",
       "  185,\n",
       "  5,\n",
       "  1,\n",
       "  44,\n",
       "  353,\n",
       "  22,\n",
       "  1133,\n",
       "  480,\n",
       "  63,\n",
       "  10,\n",
       "  1,\n",
       "  614,\n",
       "  20,\n",
       "  615,\n",
       "  87,\n",
       "  14,\n",
       "  155,\n",
       "  116,\n",
       "  173],\n",
       " [9,\n",
       "  535,\n",
       "  130,\n",
       "  384,\n",
       "  4,\n",
       "  2181,\n",
       "  6,\n",
       "  69,\n",
       "  112,\n",
       "  99,\n",
       "  918,\n",
       "  163,\n",
       "  10,\n",
       "  99,\n",
       "  918,\n",
       "  1453,\n",
       "  2,\n",
       "  3,\n",
       "  36,\n",
       "  9,\n",
       "  535,\n",
       "  130,\n",
       "  354,\n",
       "  4,\n",
       "  2182,\n",
       "  6,\n",
       "  69,\n",
       "  112,\n",
       "  99,\n",
       "  918,\n",
       "  163,\n",
       "  10,\n",
       "  99,\n",
       "  918,\n",
       "  1453,\n",
       "  2,\n",
       "  3,\n",
       "  36],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  445,\n",
       "  81,\n",
       "  5,\n",
       "  256,\n",
       "  416,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  445,\n",
       "  81,\n",
       "  5,\n",
       "  99,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [111,\n",
       "  446,\n",
       "  8,\n",
       "  1,\n",
       "  679,\n",
       "  133,\n",
       "  385,\n",
       "  104,\n",
       "  919,\n",
       "  26,\n",
       "  124,\n",
       "  920,\n",
       "  4,\n",
       "  119,\n",
       "  680,\n",
       "  8,\n",
       "  1,\n",
       "  280,\n",
       "  773,\n",
       "  2183,\n",
       "  10,\n",
       "  536,\n",
       "  774,\n",
       "  536,\n",
       "  87,\n",
       "  111,\n",
       "  446,\n",
       "  8,\n",
       "  1,\n",
       "  679,\n",
       "  133,\n",
       "  13,\n",
       "  919,\n",
       "  26,\n",
       "  124,\n",
       "  920,\n",
       "  4,\n",
       "  119,\n",
       "  680,\n",
       "  10,\n",
       "  536,\n",
       "  774,\n",
       "  536,\n",
       "  87],\n",
       " [68,\n",
       "  103,\n",
       "  1,\n",
       "  1454,\n",
       "  82,\n",
       "  1455,\n",
       "  1456,\n",
       "  154,\n",
       "  681,\n",
       "  1457,\n",
       "  9,\n",
       "  1,\n",
       "  1458,\n",
       "  1134,\n",
       "  304,\n",
       "  8,\n",
       "  921,\n",
       "  2184,\n",
       "  103,\n",
       "  1,\n",
       "  1454,\n",
       "  82,\n",
       "  1455,\n",
       "  1456,\n",
       "  129,\n",
       "  123,\n",
       "  97,\n",
       "  34,\n",
       "  154,\n",
       "  681,\n",
       "  7,\n",
       "  1459,\n",
       "  9,\n",
       "  1,\n",
       "  1458,\n",
       "  1134,\n",
       "  304,\n",
       "  8,\n",
       "  921,\n",
       "  479],\n",
       " [6,\n",
       "  18,\n",
       "  537,\n",
       "  600,\n",
       "  378,\n",
       "  922,\n",
       "  4,\n",
       "  923,\n",
       "  36,\n",
       "  9,\n",
       "  1460,\n",
       "  1,\n",
       "  106,\n",
       "  5,\n",
       "  1,\n",
       "  19,\n",
       "  4,\n",
       "  1,\n",
       "  441,\n",
       "  2185,\n",
       "  1135,\n",
       "  4,\n",
       "  775,\n",
       "  2186,\n",
       "  11,\n",
       "  616,\n",
       "  617,\n",
       "  1461,\n",
       "  28,\n",
       "  924,\n",
       "  2187,\n",
       "  18,\n",
       "  537,\n",
       "  600,\n",
       "  378,\n",
       "  922,\n",
       "  4,\n",
       "  923,\n",
       "  36,\n",
       "  9,\n",
       "  1460,\n",
       "  1,\n",
       "  106,\n",
       "  5,\n",
       "  1,\n",
       "  19,\n",
       "  617,\n",
       "  1461,\n",
       "  28,\n",
       "  924,\n",
       "  600,\n",
       "  378,\n",
       "  13,\n",
       "  7,\n",
       "  526,\n",
       "  2188,\n",
       "  2189,\n",
       "  2190,\n",
       "  82,\n",
       "  42,\n",
       "  1462,\n",
       "  2191,\n",
       "  2192],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  14,\n",
       "  39,\n",
       "  191,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [776,\n",
       "  1136,\n",
       "  32,\n",
       "  1463,\n",
       "  10,\n",
       "  1464,\n",
       "  1137,\n",
       "  1465,\n",
       "  4,\n",
       "  1466,\n",
       "  117,\n",
       "  1136,\n",
       "  32,\n",
       "  1463,\n",
       "  10,\n",
       "  1464,\n",
       "  1137,\n",
       "  1465,\n",
       "  4,\n",
       "  1466,\n",
       "  117],\n",
       " [355,\n",
       "  136,\n",
       "  73,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  33,\n",
       "  176,\n",
       "  26,\n",
       "  1467,\n",
       "  305,\n",
       "  4,\n",
       "  1,\n",
       "  777,\n",
       "  132,\n",
       "  5,\n",
       "  1,\n",
       "  63,\n",
       "  8,\n",
       "  1,\n",
       "  147,\n",
       "  28,\n",
       "  67,\n",
       "  417,\n",
       "  26,\n",
       "  246,\n",
       "  47,\n",
       "  4,\n",
       "  1468,\n",
       "  45,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  355,\n",
       "  136,\n",
       "  73,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  33,\n",
       "  176,\n",
       "  26,\n",
       "  1467,\n",
       "  305,\n",
       "  4,\n",
       "  1,\n",
       "  777,\n",
       "  132,\n",
       "  5,\n",
       "  1,\n",
       "  63,\n",
       "  8,\n",
       "  1,\n",
       "  2193,\n",
       "  4,\n",
       "  147,\n",
       "  28,\n",
       "  67,\n",
       "  417,\n",
       "  26,\n",
       "  246,\n",
       "  47,\n",
       "  4,\n",
       "  1468,\n",
       "  45,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  150,\n",
       "  527,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  14,\n",
       "  331,\n",
       "  78,\n",
       "  166,\n",
       "  75,\n",
       "  9,\n",
       "  1469,\n",
       "  18,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  14,\n",
       "  1,\n",
       "  331,\n",
       "  78,\n",
       "  166,\n",
       "  75,\n",
       "  9,\n",
       "  778],\n",
       " [192,\n",
       "  6,\n",
       "  1138,\n",
       "  30,\n",
       "  73,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  14,\n",
       "  1,\n",
       "  107,\n",
       "  356,\n",
       "  73,\n",
       "  538,\n",
       "  1139,\n",
       "  109,\n",
       "  30,\n",
       "  73,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  14,\n",
       "  1,\n",
       "  107,\n",
       "  356,\n",
       "  73,\n",
       "  538,\n",
       "  82],\n",
       " [11,\n",
       "  231,\n",
       "  1470,\n",
       "  134,\n",
       "  4,\n",
       "  1471,\n",
       "  156,\n",
       "  779,\n",
       "  9,\n",
       "  1472,\n",
       "  332,\n",
       "  418,\n",
       "  5,\n",
       "  1,\n",
       "  139,\n",
       "  130,\n",
       "  293,\n",
       "  386,\n",
       "  72,\n",
       "  55,\n",
       "  2194,\n",
       "  231,\n",
       "  1470,\n",
       "  134,\n",
       "  4,\n",
       "  1471,\n",
       "  156,\n",
       "  779,\n",
       "  9,\n",
       "  1472,\n",
       "  332,\n",
       "  418,\n",
       "  5,\n",
       "  1,\n",
       "  139,\n",
       "  130,\n",
       "  293,\n",
       "  10,\n",
       "  91,\n",
       "  142,\n",
       "  5,\n",
       "  55,\n",
       "  2195],\n",
       " [1,\n",
       "  164,\n",
       "  33,\n",
       "  109,\n",
       "  447,\n",
       "  10,\n",
       "  1,\n",
       "  266,\n",
       "  164,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  1,\n",
       "  164,\n",
       "  33,\n",
       "  109,\n",
       "  10,\n",
       "  1,\n",
       "  266,\n",
       "  94,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [29,\n",
       "  97,\n",
       "  26,\n",
       "  448,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  481,\n",
       "  7,\n",
       "  1473,\n",
       "  128,\n",
       "  9,\n",
       "  216,\n",
       "  136,\n",
       "  2196,\n",
       "  72,\n",
       "  130,\n",
       "  142,\n",
       "  216,\n",
       "  97,\n",
       "  26,\n",
       "  448,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  481,\n",
       "  7,\n",
       "  1473,\n",
       "  128,\n",
       "  9,\n",
       "  216,\n",
       "  136,\n",
       "  142],\n",
       " [48,\n",
       "  22,\n",
       "  240,\n",
       "  925,\n",
       "  1474,\n",
       "  926,\n",
       "  247,\n",
       "  11,\n",
       "  333,\n",
       "  1475,\n",
       "  539,\n",
       "  1476,\n",
       "  1140,\n",
       "  926,\n",
       "  4,\n",
       "  248,\n",
       "  2197,\n",
       "  8,\n",
       "  326,\n",
       "  306,\n",
       "  419,\n",
       "  21,\n",
       "  1,\n",
       "  780,\n",
       "  540,\n",
       "  5,\n",
       "  1,\n",
       "  357,\n",
       "  22,\n",
       "  927,\n",
       "  2,\n",
       "  3,\n",
       "  2198,\n",
       "  351,\n",
       "  5,\n",
       "  326,\n",
       "  306,\n",
       "  419,\n",
       "  21,\n",
       "  1,\n",
       "  780,\n",
       "  540,\n",
       "  5,\n",
       "  1,\n",
       "  357,\n",
       "  22,\n",
       "  927,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  14,\n",
       "  1141,\n",
       "  1474,\n",
       "  926,\n",
       "  247,\n",
       "  11,\n",
       "  333,\n",
       "  1475,\n",
       "  539,\n",
       "  1476,\n",
       "  1140,\n",
       "  926,\n",
       "  4,\n",
       "  248,\n",
       "  1477],\n",
       " [51,\n",
       "  1478,\n",
       "  1142,\n",
       "  31,\n",
       "  679,\n",
       "  1,\n",
       "  22,\n",
       "  358,\n",
       "  442,\n",
       "  14,\n",
       "  7,\n",
       "  186,\n",
       "  171,\n",
       "  253,\n",
       "  4,\n",
       "  928,\n",
       "  52,\n",
       "  170,\n",
       "  1479,\n",
       "  4,\n",
       "  781,\n",
       "  156,\n",
       "  51,\n",
       "  682,\n",
       "  1142,\n",
       "  31,\n",
       "  679,\n",
       "  1,\n",
       "  22,\n",
       "  358,\n",
       "  442,\n",
       "  14,\n",
       "  7,\n",
       "  186,\n",
       "  171,\n",
       "  94,\n",
       "  4,\n",
       "  928,\n",
       "  52,\n",
       "  131,\n",
       "  1479,\n",
       "  4,\n",
       "  781,\n",
       "  156],\n",
       " [21,\n",
       "  1,\n",
       "  1480,\n",
       "  167,\n",
       "  5,\n",
       "  1481,\n",
       "  165,\n",
       "  683,\n",
       "  142,\n",
       "  129,\n",
       "  123,\n",
       "  15,\n",
       "  11,\n",
       "  1143,\n",
       "  11,\n",
       "  7,\n",
       "  684,\n",
       "  2199,\n",
       "  1,\n",
       "  1480,\n",
       "  167,\n",
       "  5,\n",
       "  1481,\n",
       "  165,\n",
       "  1143,\n",
       "  21,\n",
       "  1109,\n",
       "  144,\n",
       "  129,\n",
       "  123,\n",
       "  1482,\n",
       "  11,\n",
       "  7,\n",
       "  684,\n",
       "  479],\n",
       " [54,\n",
       "  12,\n",
       "  40,\n",
       "  13,\n",
       "  7,\n",
       "  307,\n",
       "  85,\n",
       "  22,\n",
       "  174,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  541,\n",
       "  232,\n",
       "  24,\n",
       "  58,\n",
       "  23,\n",
       "  54,\n",
       "  12,\n",
       "  40,\n",
       "  7,\n",
       "  307,\n",
       "  85,\n",
       "  22,\n",
       "  174,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  449],\n",
       " [1144,\n",
       "  35,\n",
       "  143,\n",
       "  5,\n",
       "  125,\n",
       "  14,\n",
       "  1483,\n",
       "  115,\n",
       "  33,\n",
       "  118,\n",
       "  97,\n",
       "  8,\n",
       "  482,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  1484,\n",
       "  33,\n",
       "  618,\n",
       "  1144,\n",
       "  8,\n",
       "  482,\n",
       "  2,\n",
       "  3,\n",
       "  117],\n",
       " [11,\n",
       "  359,\n",
       "  1,\n",
       "  101,\n",
       "  38,\n",
       "  29,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  177,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  8,\n",
       "  124,\n",
       "  60,\n",
       "  929,\n",
       "  359,\n",
       "  24,\n",
       "  38,\n",
       "  49,\n",
       "  1,\n",
       "  177,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  33,\n",
       "  15,\n",
       "  8,\n",
       "  124,\n",
       "  60,\n",
       "  930],\n",
       " [542,\n",
       "  334,\n",
       "  234,\n",
       "  931,\n",
       "  543,\n",
       "  4,\n",
       "  544,\n",
       "  387,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  255,\n",
       "  1145,\n",
       "  1485,\n",
       "  5,\n",
       "  137,\n",
       "  887,\n",
       "  306,\n",
       "  26,\n",
       "  420,\n",
       "  685,\n",
       "  782,\n",
       "  618,\n",
       "  137,\n",
       "  200,\n",
       "  2200,\n",
       "  334,\n",
       "  234,\n",
       "  931,\n",
       "  543,\n",
       "  4,\n",
       "  544,\n",
       "  387,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  255,\n",
       "  1145,\n",
       "  1485,\n",
       "  5,\n",
       "  137,\n",
       "  2201,\n",
       "  7,\n",
       "  189,\n",
       "  287,\n",
       "  5,\n",
       "  7,\n",
       "  137,\n",
       "  609,\n",
       "  31,\n",
       "  7,\n",
       "  137,\n",
       "  189,\n",
       "  1486],\n",
       " [8,\n",
       "  335,\n",
       "  1,\n",
       "  1487,\n",
       "  82,\n",
       "  8,\n",
       "  932,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  11,\n",
       "  126,\n",
       "  193,\n",
       "  173,\n",
       "  13,\n",
       "  76,\n",
       "  2202,\n",
       "  335,\n",
       "  1,\n",
       "  1487,\n",
       "  82,\n",
       "  932,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  11,\n",
       "  126,\n",
       "  193,\n",
       "  173,\n",
       "  33,\n",
       "  76,\n",
       "  15],\n",
       " [2203,\n",
       "  41,\n",
       "  686,\n",
       "  1488,\n",
       "  4,\n",
       "  1489,\n",
       "  1490,\n",
       "  1491,\n",
       "  40,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  666,\n",
       "  34,\n",
       "  483,\n",
       "  783,\n",
       "  154,\n",
       "  104,\n",
       "  933,\n",
       "  26,\n",
       "  1492,\n",
       "  5,\n",
       "  933,\n",
       "  934,\n",
       "  5,\n",
       "  1,\n",
       "  304,\n",
       "  1493,\n",
       "  8,\n",
       "  2204,\n",
       "  41,\n",
       "  686,\n",
       "  1488,\n",
       "  4,\n",
       "  1489,\n",
       "  1490,\n",
       "  1491,\n",
       "  40,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  666,\n",
       "  34,\n",
       "  483,\n",
       "  783,\n",
       "  154,\n",
       "  104,\n",
       "  933,\n",
       "  26,\n",
       "  1492,\n",
       "  5,\n",
       "  933,\n",
       "  934,\n",
       "  5,\n",
       "  1,\n",
       "  304,\n",
       "  8,\n",
       "  687],\n",
       " [6,\n",
       "  688,\n",
       "  70,\n",
       "  89,\n",
       "  5,\n",
       "  286,\n",
       "  667,\n",
       "  10,\n",
       "  1146,\n",
       "  1147,\n",
       "  935,\n",
       "  84,\n",
       "  6,\n",
       "  484,\n",
       "  1,\n",
       "  89,\n",
       "  5,\n",
       "  667,\n",
       "  10,\n",
       "  1146,\n",
       "  1147,\n",
       "  935,\n",
       "  84],\n",
       " [11,\n",
       "  1148,\n",
       "  56,\n",
       "  35,\n",
       "  235,\n",
       "  49,\n",
       "  154,\n",
       "  936,\n",
       "  21,\n",
       "  46,\n",
       "  23,\n",
       "  125,\n",
       "  174,\n",
       "  21,\n",
       "  85,\n",
       "  294,\n",
       "  1109,\n",
       "  358,\n",
       "  1494,\n",
       "  21,\n",
       "  1,\n",
       "  207,\n",
       "  1495,\n",
       "  4,\n",
       "  1496,\n",
       "  40,\n",
       "  8,\n",
       "  335,\n",
       "  56,\n",
       "  35,\n",
       "  235,\n",
       "  49,\n",
       "  154,\n",
       "  104,\n",
       "  1149,\n",
       "  26,\n",
       "  46,\n",
       "  23,\n",
       "  125,\n",
       "  174,\n",
       "  21,\n",
       "  85,\n",
       "  294,\n",
       "  1109,\n",
       "  358,\n",
       "  1497,\n",
       "  21,\n",
       "  1,\n",
       "  207,\n",
       "  1495,\n",
       "  4,\n",
       "  1496,\n",
       "  40],\n",
       " [83, 14, 94, 349, 10, 150, 151, 2, 3, 25, 83, 349, 10, 150, 151, 2, 3, 25],\n",
       " [2205,\n",
       "  937,\n",
       "  1,\n",
       "  2206,\n",
       "  19,\n",
       "  2207,\n",
       "  2208,\n",
       "  1150,\n",
       "  1,\n",
       "  1498,\n",
       "  1499,\n",
       "  2,\n",
       "  3,\n",
       "  165,\n",
       "  222,\n",
       "  13,\n",
       "  1,\n",
       "  525,\n",
       "  212,\n",
       "  5,\n",
       "  1500,\n",
       "  211,\n",
       "  689,\n",
       "  9,\n",
       "  104,\n",
       "  1501,\n",
       "  294,\n",
       "  1502,\n",
       "  545,\n",
       "  938,\n",
       "  1,\n",
       "  619,\n",
       "  9,\n",
       "  7,\n",
       "  886,\n",
       "  2209,\n",
       "  1498,\n",
       "  222,\n",
       "  1499,\n",
       "  2,\n",
       "  3,\n",
       "  165,\n",
       "  450,\n",
       "  1,\n",
       "  525,\n",
       "  212,\n",
       "  5,\n",
       "  1500,\n",
       "  211,\n",
       "  689,\n",
       "  9,\n",
       "  104,\n",
       "  1501,\n",
       "  294,\n",
       "  1502,\n",
       "  545,\n",
       "  938,\n",
       "  7,\n",
       "  29,\n",
       "  2210,\n",
       "  9,\n",
       "  7,\n",
       "  886,\n",
       "  60,\n",
       "  2211,\n",
       "  75],\n",
       " [8,\n",
       "  1,\n",
       "  308,\n",
       "  5,\n",
       "  48,\n",
       "  263,\n",
       "  6,\n",
       "  385,\n",
       "  104,\n",
       "  2212,\n",
       "  23,\n",
       "  1,\n",
       "  388,\n",
       "  189,\n",
       "  2213,\n",
       "  162,\n",
       "  138,\n",
       "  8,\n",
       "  451,\n",
       "  4,\n",
       "  690,\n",
       "  87,\n",
       "  42,\n",
       "  546,\n",
       "  23,\n",
       "  7,\n",
       "  1503,\n",
       "  784,\n",
       "  34,\n",
       "  937,\n",
       "  318,\n",
       "  1151,\n",
       "  9,\n",
       "  1504,\n",
       "  2214,\n",
       "  385,\n",
       "  939,\n",
       "  23,\n",
       "  1,\n",
       "  389,\n",
       "  189,\n",
       "  162,\n",
       "  138,\n",
       "  8,\n",
       "  451,\n",
       "  4,\n",
       "  690,\n",
       "  87,\n",
       "  42,\n",
       "  546,\n",
       "  23,\n",
       "  7,\n",
       "  1503,\n",
       "  784,\n",
       "  34,\n",
       "  937,\n",
       "  318,\n",
       "  1151,\n",
       "  9,\n",
       "  1504,\n",
       "  2215,\n",
       "  940],\n",
       " [54,\n",
       "  12,\n",
       "  40,\n",
       "  13,\n",
       "  7,\n",
       "  307,\n",
       "  85,\n",
       "  22,\n",
       "  174,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  2216,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  13,\n",
       "  66,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  449],\n",
       " [6,\n",
       "  95,\n",
       "  15,\n",
       "  1505,\n",
       "  7,\n",
       "  1152,\n",
       "  108,\n",
       "  122,\n",
       "  9,\n",
       "  547,\n",
       "  1506,\n",
       "  1507,\n",
       "  4,\n",
       "  548,\n",
       "  1508,\n",
       "  57,\n",
       "  1,\n",
       "  452,\n",
       "  33,\n",
       "  175,\n",
       "  10,\n",
       "  1505,\n",
       "  7,\n",
       "  1152,\n",
       "  108,\n",
       "  122,\n",
       "  9,\n",
       "  547,\n",
       "  1506,\n",
       "  1507,\n",
       "  4,\n",
       "  548,\n",
       "  1508,\n",
       "  57],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  139,\n",
       "  147,\n",
       "  75,\n",
       "  15,\n",
       "  8,\n",
       "  448,\n",
       "  2,\n",
       "  3,\n",
       "  36,\n",
       "  11,\n",
       "  24,\n",
       "  281,\n",
       "  170,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  139,\n",
       "  147,\n",
       "  28,\n",
       "  31,\n",
       "  8,\n",
       "  448,\n",
       "  2,\n",
       "  3,\n",
       "  36],\n",
       " [1,\n",
       "  217,\n",
       "  137,\n",
       "  187,\n",
       "  485,\n",
       "  336,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  13,\n",
       "  7,\n",
       "  390,\n",
       "  22,\n",
       "  247,\n",
       "  14,\n",
       "  137,\n",
       "  242,\n",
       "  1509,\n",
       "  1,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  236,\n",
       "  5,\n",
       "  1,\n",
       "  217,\n",
       "  2217,\n",
       "  137,\n",
       "  187,\n",
       "  1,\n",
       "  217,\n",
       "  137,\n",
       "  187,\n",
       "  485,\n",
       "  13,\n",
       "  7,\n",
       "  22,\n",
       "  5,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  2218,\n",
       "  247,\n",
       "  14,\n",
       "  137,\n",
       "  242,\n",
       "  336,\n",
       "  2,\n",
       "  3,\n",
       "  90],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  92,\n",
       "  94,\n",
       "  452,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  15,\n",
       "  7,\n",
       "  785,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  92,\n",
       "  152,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [786,\n",
       "  1,\n",
       "  1153,\n",
       "  15,\n",
       "  30,\n",
       "  142,\n",
       "  691,\n",
       "  1154,\n",
       "  7,\n",
       "  941,\n",
       "  212,\n",
       "  5,\n",
       "  30,\n",
       "  218,\n",
       "  6,\n",
       "  76,\n",
       "  18,\n",
       "  1,\n",
       "  1510,\n",
       "  183,\n",
       "  1511,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  5,\n",
       "  1512,\n",
       "  30,\n",
       "  218,\n",
       "  1155,\n",
       "  21,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  190,\n",
       "  1156,\n",
       "  1513,\n",
       "  620,\n",
       "  8,\n",
       "  7,\n",
       "  390,\n",
       "  207,\n",
       "  1514,\n",
       "  391,\n",
       "  13,\n",
       "  1,\n",
       "  1510,\n",
       "  183,\n",
       "  1511,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  5,\n",
       "  1512,\n",
       "  63,\n",
       "  218,\n",
       "  1155,\n",
       "  21,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  190,\n",
       "  1156,\n",
       "  1513,\n",
       "  620,\n",
       "  8,\n",
       "  7,\n",
       "  390,\n",
       "  207,\n",
       "  22],\n",
       " [1,\n",
       "  46,\n",
       "  28,\n",
       "  5,\n",
       "  1,\n",
       "  159,\n",
       "  62,\n",
       "  13,\n",
       "  1,\n",
       "  552,\n",
       "  22,\n",
       "  392,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  42,\n",
       "  240,\n",
       "  621,\n",
       "  453,\n",
       "  26,\n",
       "  692,\n",
       "  5,\n",
       "  44,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  787,\n",
       "  26,\n",
       "  1,\n",
       "  62,\n",
       "  942,\n",
       "  622,\n",
       "  21,\n",
       "  1,\n",
       "  552,\n",
       "  22,\n",
       "  392,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  42,\n",
       "  240,\n",
       "  621,\n",
       "  453,\n",
       "  26,\n",
       "  692,\n",
       "  5,\n",
       "  44,\n",
       "  31,\n",
       "  7,\n",
       "  1515,\n",
       "  41],\n",
       " [51,\n",
       "  29,\n",
       "  81,\n",
       "  33,\n",
       "  233,\n",
       "  10,\n",
       "  553,\n",
       "  4,\n",
       "  1,\n",
       "  177,\n",
       "  56,\n",
       "  78,\n",
       "  20,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  51,\n",
       "  5,\n",
       "  1,\n",
       "  56,\n",
       "  78,\n",
       "  33,\n",
       "  233,\n",
       "  10,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [98,\n",
       "  129,\n",
       "  123,\n",
       "  268,\n",
       "  34,\n",
       "  7,\n",
       "  1516,\n",
       "  75,\n",
       "  5,\n",
       "  2219,\n",
       "  154,\n",
       "  104,\n",
       "  15,\n",
       "  9,\n",
       "  1157,\n",
       "  554,\n",
       "  161,\n",
       "  623,\n",
       "  11,\n",
       "  413,\n",
       "  783,\n",
       "  8,\n",
       "  483,\n",
       "  906,\n",
       "  4,\n",
       "  943,\n",
       "  288,\n",
       "  98,\n",
       "  129,\n",
       "  123,\n",
       "  684,\n",
       "  1158,\n",
       "  8,\n",
       "  483,\n",
       "  34,\n",
       "  7,\n",
       "  1516,\n",
       "  75,\n",
       "  5,\n",
       "  1457,\n",
       "  21,\n",
       "  7,\n",
       "  161,\n",
       "  154,\n",
       "  104,\n",
       "  2220,\n",
       "  294,\n",
       "  2221,\n",
       "  8,\n",
       "  269,\n",
       "  9,\n",
       "  554,\n",
       "  1,\n",
       "  623,\n",
       "  8,\n",
       "  413,\n",
       "  783,\n",
       "  906,\n",
       "  4,\n",
       "  943,\n",
       "  288],\n",
       " [8,\n",
       "  1,\n",
       "  88,\n",
       "  29,\n",
       "  6,\n",
       "  944,\n",
       "  15,\n",
       "  1159,\n",
       "  486,\n",
       "  1517,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  8,\n",
       "  24,\n",
       "  29,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  195,\n",
       "  486,\n",
       "  176,\n",
       "  26,\n",
       "  1159,\n",
       "  1517,\n",
       "  2,\n",
       "  3,\n",
       "  117],\n",
       " [393,\n",
       "  6,\n",
       "  76,\n",
       "  225,\n",
       "  1,\n",
       "  693,\n",
       "  5,\n",
       "  1,\n",
       "  624,\n",
       "  126,\n",
       "  178,\n",
       "  14,\n",
       "  30,\n",
       "  178,\n",
       "  788,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  26,\n",
       "  1160,\n",
       "  789,\n",
       "  31,\n",
       "  93,\n",
       "  8,\n",
       "  7,\n",
       "  531,\n",
       "  61,\n",
       "  790,\n",
       "  2222,\n",
       "  76,\n",
       "  555,\n",
       "  1,\n",
       "  693,\n",
       "  5,\n",
       "  1,\n",
       "  624,\n",
       "  126,\n",
       "  178,\n",
       "  14,\n",
       "  1,\n",
       "  2223,\n",
       "  178,\n",
       "  26,\n",
       "  1160,\n",
       "  789,\n",
       "  31,\n",
       "  93,\n",
       "  788,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  1161,\n",
       "  14,\n",
       "  1,\n",
       "  101,\n",
       "  93,\n",
       "  8,\n",
       "  1,\n",
       "  531,\n",
       "  171],\n",
       " [172,\n",
       "  381,\n",
       "  32,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  63,\n",
       "  945,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  157,\n",
       "  114,\n",
       "  127,\n",
       "  625,\n",
       "  1518,\n",
       "  360,\n",
       "  1,\n",
       "  60,\n",
       "  91,\n",
       "  113,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  63,\n",
       "  945,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  157,\n",
       "  114,\n",
       "  127],\n",
       " [51,\n",
       "  361,\n",
       "  67,\n",
       "  233,\n",
       "  10,\n",
       "  1,\n",
       "  266,\n",
       "  556,\n",
       "  164,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  1,\n",
       "  361,\n",
       "  67,\n",
       "  414,\n",
       "  10,\n",
       "  1,\n",
       "  266,\n",
       "  556,\n",
       "  164,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [555,\n",
       "  9,\n",
       "  249,\n",
       "  1162,\n",
       "  626,\n",
       "  523,\n",
       "  32,\n",
       "  946,\n",
       "  241,\n",
       "  76,\n",
       "  454,\n",
       "  2224,\n",
       "  9,\n",
       "  249,\n",
       "  1162,\n",
       "  626,\n",
       "  523,\n",
       "  32,\n",
       "  946,\n",
       "  31,\n",
       "  350,\n",
       "  31,\n",
       "  2225,\n",
       "  667],\n",
       " [11,\n",
       "  46,\n",
       "  6,\n",
       "  18,\n",
       "  243,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  201,\n",
       "  11,\n",
       "  487,\n",
       "  14,\n",
       "  68,\n",
       "  331,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  1519,\n",
       "  18,\n",
       "  243,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  201,\n",
       "  11,\n",
       "  487,\n",
       "  14,\n",
       "  331,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  488],\n",
       " [11,\n",
       "  24,\n",
       "  184,\n",
       "  6,\n",
       "  18,\n",
       "  489,\n",
       "  791,\n",
       "  1,\n",
       "  490,\n",
       "  99,\n",
       "  108,\n",
       "  152,\n",
       "  557,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  42,\n",
       "  13,\n",
       "  1163,\n",
       "  9,\n",
       "  133,\n",
       "  215,\n",
       "  558,\n",
       "  14,\n",
       "  390,\n",
       "  1164,\n",
       "  5,\n",
       "  93,\n",
       "  4,\n",
       "  390,\n",
       "  2226,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  490,\n",
       "  99,\n",
       "  152,\n",
       "  557,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  31,\n",
       "  98,\n",
       "  13,\n",
       "  1163,\n",
       "  9,\n",
       "  133,\n",
       "  215,\n",
       "  558,\n",
       "  14,\n",
       "  390,\n",
       "  1164,\n",
       "  5,\n",
       "  93,\n",
       "  4,\n",
       "  306],\n",
       " [1,\n",
       "  118,\n",
       "  112,\n",
       "  58,\n",
       "  2227,\n",
       "  1,\n",
       "  792,\n",
       "  5,\n",
       "  1,\n",
       "  195,\n",
       "  5,\n",
       "  694,\n",
       "  695,\n",
       "  8,\n",
       "  1,\n",
       "  39,\n",
       "  195,\n",
       "  187,\n",
       "  362,\n",
       "  2,\n",
       "  3,\n",
       "  2228,\n",
       "  118,\n",
       "  112,\n",
       "  58,\n",
       "  2229,\n",
       "  2230,\n",
       "  1,\n",
       "  195,\n",
       "  5,\n",
       "  694,\n",
       "  695,\n",
       "  362,\n",
       "  2,\n",
       "  3,\n",
       "  88],\n",
       " [6,\n",
       "  15,\n",
       "  254,\n",
       "  1,\n",
       "  947,\n",
       "  948,\n",
       "  232,\n",
       "  386,\n",
       "  559,\n",
       "  1520,\n",
       "  1521,\n",
       "  31,\n",
       "  233,\n",
       "  26,\n",
       "  455,\n",
       "  2,\n",
       "  3,\n",
       "  560,\n",
       "  14,\n",
       "  696,\n",
       "  415,\n",
       "  5,\n",
       "  170,\n",
       "  170,\n",
       "  1165,\n",
       "  1522,\n",
       "  1,\n",
       "  1523,\n",
       "  1524,\n",
       "  15,\n",
       "  254,\n",
       "  1,\n",
       "  947,\n",
       "  948,\n",
       "  232,\n",
       "  386,\n",
       "  559,\n",
       "  1520,\n",
       "  1521,\n",
       "  31,\n",
       "  233,\n",
       "  26,\n",
       "  455,\n",
       "  2,\n",
       "  3,\n",
       "  560,\n",
       "  14,\n",
       "  696,\n",
       "  415,\n",
       "  5,\n",
       "  533,\n",
       "  107,\n",
       "  793],\n",
       " [6,\n",
       "  15,\n",
       "  254,\n",
       "  1,\n",
       "  947,\n",
       "  948,\n",
       "  232,\n",
       "  455,\n",
       "  2,\n",
       "  3,\n",
       "  560,\n",
       "  14,\n",
       "  696,\n",
       "  415,\n",
       "  5,\n",
       "  170,\n",
       "  170,\n",
       "  1165,\n",
       "  1522,\n",
       "  1,\n",
       "  1523,\n",
       "  1524,\n",
       "  15,\n",
       "  254,\n",
       "  1,\n",
       "  947,\n",
       "  948,\n",
       "  232,\n",
       "  455,\n",
       "  2,\n",
       "  3,\n",
       "  560,\n",
       "  14,\n",
       "  696,\n",
       "  415,\n",
       "  5,\n",
       "  533,\n",
       "  107,\n",
       "  793],\n",
       " [1,\n",
       "  888,\n",
       "  279,\n",
       "  755,\n",
       "  29,\n",
       "  33,\n",
       "  1166,\n",
       "  11,\n",
       "  7,\n",
       "  2231,\n",
       "  5,\n",
       "  394,\n",
       "  539,\n",
       "  1525,\n",
       "  1526,\n",
       "  36,\n",
       "  1,\n",
       "  888,\n",
       "  279,\n",
       "  755,\n",
       "  29,\n",
       "  13,\n",
       "  7,\n",
       "  2232,\n",
       "  755,\n",
       "  29,\n",
       "  1166,\n",
       "  11,\n",
       "  697,\n",
       "  394,\n",
       "  1526,\n",
       "  36],\n",
       " [1,\n",
       "  126,\n",
       "  1167,\n",
       "  67,\n",
       "  421,\n",
       "  26,\n",
       "  949,\n",
       "  5,\n",
       "  1168,\n",
       "  30,\n",
       "  132,\n",
       "  153,\n",
       "  14,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  1,\n",
       "  126,\n",
       "  193,\n",
       "  33,\n",
       "  421,\n",
       "  2233,\n",
       "  1168,\n",
       "  30,\n",
       "  132,\n",
       "  153,\n",
       "  14,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [30,\n",
       "  73,\n",
       "  13,\n",
       "  109,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  30,\n",
       "  73,\n",
       "  13,\n",
       "  109,\n",
       "  14,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [6,\n",
       "  794,\n",
       "  14,\n",
       "  697,\n",
       "  950,\n",
       "  5,\n",
       "  1169,\n",
       "  1170,\n",
       "  10,\n",
       "  422,\n",
       "  28,\n",
       "  4,\n",
       "  280,\n",
       "  561,\n",
       "  2,\n",
       "  3,\n",
       "  2234,\n",
       "  561,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  6,\n",
       "  76,\n",
       "  794,\n",
       "  14,\n",
       "  10,\n",
       "  112,\n",
       "  456,\n",
       "  5,\n",
       "  1169,\n",
       "  795,\n",
       "  14,\n",
       "  332,\n",
       "  950,\n",
       "  5,\n",
       "  1170],\n",
       " [1171,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  216,\n",
       "  1,\n",
       "  936,\n",
       "  21,\n",
       "  250,\n",
       "  1527,\n",
       "  31,\n",
       "  1,\n",
       "  1172,\n",
       "  8,\n",
       "  1,\n",
       "  78,\n",
       "  1528,\n",
       "  14,\n",
       "  250,\n",
       "  2235,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  337,\n",
       "  936,\n",
       "  21,\n",
       "  250,\n",
       "  1527,\n",
       "  8,\n",
       "  363,\n",
       "  5,\n",
       "  1,\n",
       "  1172,\n",
       "  8,\n",
       "  78,\n",
       "  1528],\n",
       " [6,\n",
       "  66,\n",
       "  7,\n",
       "  155,\n",
       "  116,\n",
       "  291,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  10,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  4,\n",
       "  109,\n",
       "  796,\n",
       "  14,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  46,\n",
       "  4,\n",
       "  796,\n",
       "  5,\n",
       "  7,\n",
       "  155,\n",
       "  116,\n",
       "  291,\n",
       "  170,\n",
       "  951,\n",
       "  41,\n",
       "  19,\n",
       "  32,\n",
       "  233,\n",
       "  23,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25],\n",
       " [7,\n",
       "  1173,\n",
       "  2236,\n",
       "  115,\n",
       "  154,\n",
       "  104,\n",
       "  289,\n",
       "  8,\n",
       "  1529,\n",
       "  4,\n",
       "  627,\n",
       "  36,\n",
       "  1,\n",
       "  1173,\n",
       "  1174,\n",
       "  154,\n",
       "  104,\n",
       "  289,\n",
       "  8,\n",
       "  1529,\n",
       "  4,\n",
       "  627,\n",
       "  36],\n",
       " [1,\n",
       "  118,\n",
       "  19,\n",
       "  6,\n",
       "  952,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  953,\n",
       "  238,\n",
       "  309,\n",
       "  41,\n",
       "  19,\n",
       "  5,\n",
       "  457,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  1,\n",
       "  19,\n",
       "  31,\n",
       "  138,\n",
       "  1175,\n",
       "  2237,\n",
       "  13,\n",
       "  1530,\n",
       "  9,\n",
       "  1,\n",
       "  953,\n",
       "  238,\n",
       "  309,\n",
       "  41,\n",
       "  19,\n",
       "  1176,\n",
       "  5,\n",
       "  457,\n",
       "  2,\n",
       "  3,\n",
       "  117],\n",
       " [1,\n",
       "  38,\n",
       "  49,\n",
       "  67,\n",
       "  66,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  101,\n",
       "  49,\n",
       "  32,\n",
       "  66,\n",
       "  14,\n",
       "  1,\n",
       "  177,\n",
       "  27,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  184,\n",
       "  58,\n",
       "  67,\n",
       "  196,\n",
       "  179,\n",
       "  10,\n",
       "  1,\n",
       "  1531,\n",
       "  108,\n",
       "  295,\n",
       "  288,\n",
       "  169,\n",
       "  190,\n",
       "  1532,\n",
       "  14,\n",
       "  7,\n",
       "  921,\n",
       "  162,\n",
       "  94,\n",
       "  2238,\n",
       "  184,\n",
       "  2239,\n",
       "  67,\n",
       "  196,\n",
       "  179,\n",
       "  10,\n",
       "  1,\n",
       "  1531,\n",
       "  108,\n",
       "  295,\n",
       "  288,\n",
       "  169,\n",
       "  190,\n",
       "  1532,\n",
       "  14,\n",
       "  1,\n",
       "  107,\n",
       "  226,\n",
       "  162,\n",
       "  11,\n",
       "  1,\n",
       "  60,\n",
       "  250,\n",
       "  143],\n",
       " [1,\n",
       "  46,\n",
       "  28,\n",
       "  5,\n",
       "  1,\n",
       "  159,\n",
       "  62,\n",
       "  13,\n",
       "  1,\n",
       "  552,\n",
       "  22,\n",
       "  392,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  42,\n",
       "  240,\n",
       "  621,\n",
       "  453,\n",
       "  26,\n",
       "  692,\n",
       "  5,\n",
       "  1533,\n",
       "  46,\n",
       "  28,\n",
       "  11,\n",
       "  1,\n",
       "  62,\n",
       "  13,\n",
       "  21,\n",
       "  1,\n",
       "  552,\n",
       "  22,\n",
       "  392,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  68,\n",
       "  2240,\n",
       "  310,\n",
       "  5,\n",
       "  621,\n",
       "  453,\n",
       "  26,\n",
       "  2241,\n",
       "  692,\n",
       "  5,\n",
       "  44],\n",
       " [2242,\n",
       "  1534,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  13,\n",
       "  1,\n",
       "  1535,\n",
       "  907,\n",
       "  169,\n",
       "  257,\n",
       "  251,\n",
       "  11,\n",
       "  2243,\n",
       "  1534,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  13,\n",
       "  1,\n",
       "  255,\n",
       "  2244,\n",
       "  220,\n",
       "  257,\n",
       "  251,\n",
       "  11,\n",
       "  954],\n",
       " [11,\n",
       "  1,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  6,\n",
       "  458,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  24,\n",
       "  38,\n",
       "  58,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [955,\n",
       "  797,\n",
       "  1177,\n",
       "  49,\n",
       "  232,\n",
       "  1536,\n",
       "  8,\n",
       "  1537,\n",
       "  479,\n",
       "  956,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  955,\n",
       "  290,\n",
       "  1177,\n",
       "  49,\n",
       "  232,\n",
       "  1536,\n",
       "  8,\n",
       "  1537,\n",
       "  479,\n",
       "  956,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [1,\n",
       "  100,\n",
       "  209,\n",
       "  41,\n",
       "  19,\n",
       "  33,\n",
       "  50,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  6,\n",
       "  50,\n",
       "  68,\n",
       "  44,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25],\n",
       " [11,\n",
       "  51,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  38,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  24,\n",
       "  38,\n",
       "  58,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  337,\n",
       "  24,\n",
       "  82,\n",
       "  23,\n",
       "  1,\n",
       "  280,\n",
       "  28,\n",
       "  456,\n",
       "  423,\n",
       "  2245,\n",
       "  422,\n",
       "  75,\n",
       "  5,\n",
       "  1,\n",
       "  357,\n",
       "  28,\n",
       "  176,\n",
       "  26,\n",
       "  1,\n",
       "  1538,\n",
       "  159,\n",
       "  62,\n",
       "  1539,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  423,\n",
       "  2246,\n",
       "  147,\n",
       "  75,\n",
       "  5,\n",
       "  1,\n",
       "  357,\n",
       "  28,\n",
       "  176,\n",
       "  26,\n",
       "  1,\n",
       "  1538,\n",
       "  159,\n",
       "  62,\n",
       "  1539,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  35,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  424,\n",
       "  24,\n",
       "  2247,\n",
       "  18,\n",
       "  1,\n",
       "  258,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  29,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  262,\n",
       "  24,\n",
       "  56,\n",
       "  35,\n",
       "  58],\n",
       " [241,\n",
       "  6,\n",
       "  1178,\n",
       "  459,\n",
       "  1540,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  15,\n",
       "  8,\n",
       "  448,\n",
       "  2,\n",
       "  3,\n",
       "  36,\n",
       "  31,\n",
       "  24,\n",
       "  46,\n",
       "  28,\n",
       "  4,\n",
       "  1,\n",
       "  1541,\n",
       "  31,\n",
       "  1,\n",
       "  422,\n",
       "  28,\n",
       "  31,\n",
       "  268,\n",
       "  8,\n",
       "  193,\n",
       "  170,\n",
       "  786,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  15,\n",
       "  8,\n",
       "  448,\n",
       "  2,\n",
       "  3,\n",
       "  36,\n",
       "  13,\n",
       "  1530,\n",
       "  31,\n",
       "  1,\n",
       "  2248,\n",
       "  5,\n",
       "  24,\n",
       "  46,\n",
       "  4,\n",
       "  422,\n",
       "  28,\n",
       "  6,\n",
       "  1542,\n",
       "  98,\n",
       "  31,\n",
       "  2249,\n",
       "  8,\n",
       "  193,\n",
       "  425],\n",
       " [1,\n",
       "  259,\n",
       "  222,\n",
       "  562,\n",
       "  1,\n",
       "  1543,\n",
       "  5,\n",
       "  698,\n",
       "  206,\n",
       "  51,\n",
       "  699,\n",
       "  9,\n",
       "  253,\n",
       "  8,\n",
       "  24,\n",
       "  957,\n",
       "  14,\n",
       "  1179,\n",
       "  9,\n",
       "  7,\n",
       "  296,\n",
       "  35,\n",
       "  14,\n",
       "  7,\n",
       "  798,\n",
       "  11,\n",
       "  799,\n",
       "  491,\n",
       "  563,\n",
       "  2,\n",
       "  3,\n",
       "  156,\n",
       "  259,\n",
       "  222,\n",
       "  48,\n",
       "  222,\n",
       "  562,\n",
       "  1,\n",
       "  1543,\n",
       "  5,\n",
       "  800,\n",
       "  2250,\n",
       "  2251,\n",
       "  4,\n",
       "  2252,\n",
       "  14,\n",
       "  1179,\n",
       "  9,\n",
       "  7,\n",
       "  958,\n",
       "  75,\n",
       "  5,\n",
       "  296,\n",
       "  491,\n",
       "  14,\n",
       "  7,\n",
       "  798,\n",
       "  11,\n",
       "  2253,\n",
       "  799,\n",
       "  125,\n",
       "  563,\n",
       "  2,\n",
       "  3,\n",
       "  156],\n",
       " [1,\n",
       "  46,\n",
       "  28,\n",
       "  5,\n",
       "  1,\n",
       "  159,\n",
       "  62,\n",
       "  13,\n",
       "  1,\n",
       "  552,\n",
       "  22,\n",
       "  392,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  42,\n",
       "  240,\n",
       "  621,\n",
       "  453,\n",
       "  26,\n",
       "  692,\n",
       "  5,\n",
       "  1533,\n",
       "  46,\n",
       "  28,\n",
       "  176,\n",
       "  11,\n",
       "  1,\n",
       "  62,\n",
       "  13,\n",
       "  7,\n",
       "  388,\n",
       "  5,\n",
       "  1,\n",
       "  552,\n",
       "  2254,\n",
       "  22,\n",
       "  392,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  42,\n",
       "  2255,\n",
       "  621,\n",
       "  453,\n",
       "  8,\n",
       "  44,\n",
       "  26,\n",
       "  2256,\n",
       "  190,\n",
       "  1,\n",
       "  801,\n",
       "  2257,\n",
       "  5,\n",
       "  2258],\n",
       " [147,\n",
       "  28,\n",
       "  33,\n",
       "  564,\n",
       "  21,\n",
       "  1,\n",
       "  220,\n",
       "  1544,\n",
       "  801,\n",
       "  22,\n",
       "  1545,\n",
       "  4,\n",
       "  1546,\n",
       "  36,\n",
       "  1547,\n",
       "  959,\n",
       "  7,\n",
       "  1180,\n",
       "  5,\n",
       "  1548,\n",
       "  4,\n",
       "  21,\n",
       "  105,\n",
       "  1,\n",
       "  960,\n",
       "  4,\n",
       "  453,\n",
       "  1549,\n",
       "  5,\n",
       "  1,\n",
       "  1181,\n",
       "  459,\n",
       "  1,\n",
       "  183,\n",
       "  5,\n",
       "  2259,\n",
       "  4,\n",
       "  2260,\n",
       "  88,\n",
       "  42,\n",
       "  33,\n",
       "  564,\n",
       "  21,\n",
       "  1,\n",
       "  220,\n",
       "  1544,\n",
       "  801,\n",
       "  22,\n",
       "  1547,\n",
       "  1545,\n",
       "  4,\n",
       "  1546,\n",
       "  36,\n",
       "  959,\n",
       "  7,\n",
       "  1180,\n",
       "  5,\n",
       "  1548,\n",
       "  4,\n",
       "  21,\n",
       "  105,\n",
       "  1,\n",
       "  960,\n",
       "  4,\n",
       "  453,\n",
       "  1549,\n",
       "  5,\n",
       "  1,\n",
       "  22],\n",
       " [6,\n",
       "  216,\n",
       "  70,\n",
       "  89,\n",
       "  10,\n",
       "  364,\n",
       "  311,\n",
       "  297,\n",
       "  260,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  1,\n",
       "  70,\n",
       "  89,\n",
       "  395,\n",
       "  10,\n",
       "  364,\n",
       "  311,\n",
       "  802,\n",
       "  32,\n",
       "  460,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  44,\n",
       "  540,\n",
       "  5,\n",
       "  1,\n",
       "  1550,\n",
       "  40,\n",
       "  293,\n",
       "  303,\n",
       "  183,\n",
       "  961,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  6,\n",
       "  337,\n",
       "  24,\n",
       "  293,\n",
       "  303,\n",
       "  29,\n",
       "  23,\n",
       "  1,\n",
       "  44,\n",
       "  540,\n",
       "  5,\n",
       "  1,\n",
       "  1550,\n",
       "  40,\n",
       "  22,\n",
       "  961,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [48,\n",
       "  28,\n",
       "  33,\n",
       "  803,\n",
       "  11,\n",
       "  1,\n",
       "  53,\n",
       "  565,\n",
       "  1551,\n",
       "  962,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  4,\n",
       "  351,\n",
       "  5,\n",
       "  1552,\n",
       "  136,\n",
       "  218,\n",
       "  14,\n",
       "  1553,\n",
       "  11,\n",
       "  46,\n",
       "  963,\n",
       "  31,\n",
       "  7,\n",
       "  422,\n",
       "  2261,\n",
       "  2262,\n",
       "  2263,\n",
       "  804,\n",
       "  1182,\n",
       "  13,\n",
       "  21,\n",
       "  62,\n",
       "  83,\n",
       "  5,\n",
       "  1,\n",
       "  53,\n",
       "  565,\n",
       "  1551,\n",
       "  962,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  4,\n",
       "  351,\n",
       "  5,\n",
       "  1552,\n",
       "  247,\n",
       "  136,\n",
       "  218,\n",
       "  14,\n",
       "  1553,\n",
       "  11,\n",
       "  46,\n",
       "  963,\n",
       "  31,\n",
       "  7,\n",
       "  422,\n",
       "  75],\n",
       " [1,\n",
       "  227,\n",
       "  19,\n",
       "  15,\n",
       "  11,\n",
       "  2264,\n",
       "  227,\n",
       "  13,\n",
       "  7,\n",
       "  964,\n",
       "  805,\n",
       "  256,\n",
       "  1183,\n",
       "  1554,\n",
       "  1555,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  268,\n",
       "  8,\n",
       "  426,\n",
       "  325,\n",
       "  24,\n",
       "  700,\n",
       "  227,\n",
       "  19,\n",
       "  13,\n",
       "  1,\n",
       "  964,\n",
       "  805,\n",
       "  256,\n",
       "  1183,\n",
       "  1554,\n",
       "  1555,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  268,\n",
       "  8,\n",
       "  426,\n",
       "  94],\n",
       " [441,\n",
       "  1556,\n",
       "  1557,\n",
       "  701,\n",
       "  13,\n",
       "  7,\n",
       "  806,\n",
       "  19,\n",
       "  42,\n",
       "  1558,\n",
       "  7,\n",
       "  566,\n",
       "  19,\n",
       "  702,\n",
       "  965,\n",
       "  31,\n",
       "  7,\n",
       "  1184,\n",
       "  528,\n",
       "  5,\n",
       "  441,\n",
       "  775,\n",
       "  966,\n",
       "  5,\n",
       "  441,\n",
       "  775,\n",
       "  2265,\n",
       "  1556,\n",
       "  1557,\n",
       "  701,\n",
       "  13,\n",
       "  7,\n",
       "  806,\n",
       "  19,\n",
       "  42,\n",
       "  1558,\n",
       "  7,\n",
       "  566,\n",
       "  19,\n",
       "  2266,\n",
       "  31,\n",
       "  7,\n",
       "  1559,\n",
       "  5,\n",
       "  63,\n",
       "  702,\n",
       "  965,\n",
       "  31,\n",
       "  7,\n",
       "  1184,\n",
       "  528,\n",
       "  5,\n",
       "  441,\n",
       "  775],\n",
       " [91,\n",
       "  113,\n",
       "  234,\n",
       "  114,\n",
       "  127,\n",
       "  967,\n",
       "  34,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  308,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  1560,\n",
       "  492,\n",
       "  461,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  365],\n",
       " [91,\n",
       "  113,\n",
       "  234,\n",
       "  114,\n",
       "  127,\n",
       "  967,\n",
       "  34,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  308,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  1560,\n",
       "  492,\n",
       "  461,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  365],\n",
       " [8,\n",
       "  57,\n",
       "  1561,\n",
       "  968,\n",
       "  968,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  15,\n",
       "  1562,\n",
       "  163,\n",
       "  23,\n",
       "  447,\n",
       "  247,\n",
       "  28,\n",
       "  42,\n",
       "  2267,\n",
       "  2268,\n",
       "  5,\n",
       "  2269,\n",
       "  57,\n",
       "  1561,\n",
       "  968,\n",
       "  968,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  15,\n",
       "  1562,\n",
       "  163,\n",
       "  23,\n",
       "  447,\n",
       "  247,\n",
       "  28,\n",
       "  4,\n",
       "  807,\n",
       "  925,\n",
       "  2270,\n",
       "  623],\n",
       " [6,\n",
       "  66,\n",
       "  7,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  23,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  703,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  6,\n",
       "  66,\n",
       "  7,\n",
       "  155,\n",
       "  116,\n",
       "  291,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  10,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  4,\n",
       "  109,\n",
       "  796,\n",
       "  14,\n",
       "  102,\n",
       "  96,\n",
       "  25],\n",
       " [1,\n",
       "  1185,\n",
       "  493,\n",
       "  65,\n",
       "  32,\n",
       "  51,\n",
       "  366,\n",
       "  312,\n",
       "  163,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  367,\n",
       "  207,\n",
       "  313,\n",
       "  100,\n",
       "  22,\n",
       "  969,\n",
       "  367,\n",
       "  22,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  1,\n",
       "  205,\n",
       "  65,\n",
       "  32,\n",
       "  50,\n",
       "  23,\n",
       "  1563,\n",
       "  44,\n",
       "  28,\n",
       "  1,\n",
       "  367,\n",
       "  207,\n",
       "  313,\n",
       "  100,\n",
       "  22,\n",
       "  969,\n",
       "  367,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  14,\n",
       "  1,\n",
       "  366,\n",
       "  312,\n",
       "  1186,\n",
       "  103],\n",
       " [6,\n",
       "  396,\n",
       "  704,\n",
       "  1564,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  23,\n",
       "  1,\n",
       "  30,\n",
       "  1565,\n",
       "  26,\n",
       "  447,\n",
       "  970,\n",
       "  494,\n",
       "  1187,\n",
       "  11,\n",
       "  2271,\n",
       "  4,\n",
       "  892,\n",
       "  63,\n",
       "  462,\n",
       "  193,\n",
       "  494,\n",
       "  6,\n",
       "  192,\n",
       "  396,\n",
       "  704,\n",
       "  1564,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  23,\n",
       "  1,\n",
       "  2272,\n",
       "  4,\n",
       "  2273,\n",
       "  1565,\n",
       "  26,\n",
       "  970,\n",
       "  494,\n",
       "  1187,\n",
       "  11,\n",
       "  2274,\n",
       "  4,\n",
       "  2275,\n",
       "  611,\n",
       "  4,\n",
       "  2276,\n",
       "  462,\n",
       "  193,\n",
       "  83],\n",
       " [172,\n",
       "  381,\n",
       "  32,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  63,\n",
       "  945,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  157,\n",
       "  114,\n",
       "  127,\n",
       "  98,\n",
       "  492,\n",
       "  461,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  365],\n",
       " [172,\n",
       "  188,\n",
       "  2277,\n",
       "  1,\n",
       "  936,\n",
       "  5,\n",
       "  10,\n",
       "  971,\n",
       "  61,\n",
       "  93,\n",
       "  4,\n",
       "  2278,\n",
       "  1,\n",
       "  113,\n",
       "  34,\n",
       "  971,\n",
       "  61,\n",
       "  93,\n",
       "  808,\n",
       "  7,\n",
       "  700,\n",
       "  75,\n",
       "  5,\n",
       "  892,\n",
       "  93,\n",
       "  34,\n",
       "  1566,\n",
       "  1,\n",
       "  777,\n",
       "  459,\n",
       "  93,\n",
       "  451,\n",
       "  84,\n",
       "  24,\n",
       "  113,\n",
       "  13,\n",
       "  34,\n",
       "  1,\n",
       "  971,\n",
       "  61,\n",
       "  93,\n",
       "  808,\n",
       "  7,\n",
       "  700,\n",
       "  75,\n",
       "  5,\n",
       "  892,\n",
       "  93,\n",
       "  34,\n",
       "  1566,\n",
       "  1,\n",
       "  777,\n",
       "  459,\n",
       "  93,\n",
       "  21,\n",
       "  451,\n",
       "  84,\n",
       "  4,\n",
       "  1,\n",
       "  971,\n",
       "  61,\n",
       "  93,\n",
       "  385,\n",
       "  1157,\n",
       "  554],\n",
       " [1567,\n",
       "  1568,\n",
       "  4,\n",
       "  1569,\n",
       "  1570,\n",
       "  129,\n",
       "  1571,\n",
       "  30,\n",
       "  2279,\n",
       "  1568,\n",
       "  4,\n",
       "  1569,\n",
       "  1570,\n",
       "  13,\n",
       "  75,\n",
       "  5,\n",
       "  1571,\n",
       "  30,\n",
       "  218],\n",
       " [1,\n",
       "  89,\n",
       "  395,\n",
       "  67,\n",
       "  109,\n",
       "  10,\n",
       "  1,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  12,\n",
       "  36,\n",
       "  70,\n",
       "  89,\n",
       "  395,\n",
       "  32,\n",
       "  109,\n",
       "  10,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36],\n",
       " [1,\n",
       "  107,\n",
       "  1572,\n",
       "  495,\n",
       "  103,\n",
       "  13,\n",
       "  1188,\n",
       "  567,\n",
       "  956,\n",
       "  4,\n",
       "  1573,\n",
       "  16,\n",
       "  1,\n",
       "  495,\n",
       "  13,\n",
       "  809,\n",
       "  196,\n",
       "  179,\n",
       "  10,\n",
       "  1,\n",
       "  1188,\n",
       "  567,\n",
       "  103,\n",
       "  956,\n",
       "  4,\n",
       "  1573,\n",
       "  16],\n",
       " [368,\n",
       "  28,\n",
       "  13,\n",
       "  1189,\n",
       "  15,\n",
       "  9,\n",
       "  972,\n",
       "  1,\n",
       "  304,\n",
       "  5,\n",
       "  28,\n",
       "  1574,\n",
       "  1575,\n",
       "  4,\n",
       "  1576,\n",
       "  25,\n",
       "  368,\n",
       "  28,\n",
       "  254,\n",
       "  2280,\n",
       "  1,\n",
       "  304,\n",
       "  5,\n",
       "  28,\n",
       "  1574,\n",
       "  1575,\n",
       "  4,\n",
       "  1576,\n",
       "  25],\n",
       " [51,\n",
       "  58,\n",
       "  67,\n",
       "  196,\n",
       "  179,\n",
       "  10,\n",
       "  1,\n",
       "  177,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  51,\n",
       "  1,\n",
       "  58,\n",
       "  32,\n",
       "  196,\n",
       "  179,\n",
       "  8,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [118,\n",
       "  6,\n",
       "  396,\n",
       "  705,\n",
       "  9,\n",
       "  535,\n",
       "  212,\n",
       "  4,\n",
       "  1577,\n",
       "  61,\n",
       "  23,\n",
       "  30,\n",
       "  1190,\n",
       "  249,\n",
       "  338,\n",
       "  339,\n",
       "  4,\n",
       "  186,\n",
       "  2281,\n",
       "  192,\n",
       "  396,\n",
       "  705,\n",
       "  9,\n",
       "  535,\n",
       "  212,\n",
       "  4,\n",
       "  1577,\n",
       "  11,\n",
       "  1,\n",
       "  1191,\n",
       "  61,\n",
       "  23,\n",
       "  30,\n",
       "  1190,\n",
       "  30,\n",
       "  2282,\n",
       "  338,\n",
       "  339,\n",
       "  4,\n",
       "  119,\n",
       "  568],\n",
       " [496,\n",
       "  99,\n",
       "  295,\n",
       "  87,\n",
       "  13,\n",
       "  7,\n",
       "  82,\n",
       "  61,\n",
       "  23,\n",
       "  158,\n",
       "  168,\n",
       "  668,\n",
       "  489,\n",
       "  11,\n",
       "  42,\n",
       "  6,\n",
       "  18,\n",
       "  254,\n",
       "  226,\n",
       "  669,\n",
       "  9,\n",
       "  2283,\n",
       "  973,\n",
       "  2284,\n",
       "  48,\n",
       "  62,\n",
       "  6,\n",
       "  18,\n",
       "  2285,\n",
       "  295,\n",
       "  87,\n",
       "  42,\n",
       "  13,\n",
       "  7,\n",
       "  82,\n",
       "  61,\n",
       "  23,\n",
       "  158,\n",
       "  168,\n",
       "  668,\n",
       "  489],\n",
       " [7,\n",
       "  327,\n",
       "  612,\n",
       "  497,\n",
       "  5,\n",
       "  1,\n",
       "  62,\n",
       "  154,\n",
       "  104,\n",
       "  289,\n",
       "  8,\n",
       "  1192,\n",
       "  2,\n",
       "  3,\n",
       "  974,\n",
       "  7,\n",
       "  2286,\n",
       "  497,\n",
       "  5,\n",
       "  1,\n",
       "  22,\n",
       "  4,\n",
       "  975,\n",
       "  154,\n",
       "  104,\n",
       "  289,\n",
       "  8,\n",
       "  2287,\n",
       "  497,\n",
       "  263,\n",
       "  1192,\n",
       "  2,\n",
       "  3,\n",
       "  974],\n",
       " [7,\n",
       "  189,\n",
       "  162,\n",
       "  369,\n",
       "  13,\n",
       "  7,\n",
       "  1578,\n",
       "  162,\n",
       "  1579,\n",
       "  288,\n",
       "  524,\n",
       "  206,\n",
       "  218,\n",
       "  5,\n",
       "  2288,\n",
       "  162,\n",
       "  2289,\n",
       "  1120,\n",
       "  32,\n",
       "  1578,\n",
       "  669,\n",
       "  1579,\n",
       "  288,\n",
       "  524,\n",
       "  206,\n",
       "  218,\n",
       "  5,\n",
       "  200],\n",
       " [6,\n",
       "  121,\n",
       "  672,\n",
       "  24,\n",
       "  323,\n",
       "  128,\n",
       "  11,\n",
       "  412,\n",
       "  324,\n",
       "  221,\n",
       "  4,\n",
       "  322,\n",
       "  443,\n",
       "  4,\n",
       "  438,\n",
       "  64,\n",
       "  11,\n",
       "  221,\n",
       "  4,\n",
       "  322,\n",
       "  6,\n",
       "  121,\n",
       "  672,\n",
       "  24,\n",
       "  323,\n",
       "  167,\n",
       "  443,\n",
       "  4,\n",
       "  438,\n",
       "  64],\n",
       " [1580, 314, 134, 36, 13, 15, 11, 1, 143, 6, 15, 1, 2290, 143, 314, 134, 36],\n",
       " [6, 15, 382, 108, 383, 87, 11, 531, 610, 15, 382, 20, 383, 87, 11, 531, 81],\n",
       " [6,\n",
       "  976,\n",
       "  7,\n",
       "  1193,\n",
       "  330,\n",
       "  14,\n",
       "  1194,\n",
       "  78,\n",
       "  4,\n",
       "  1581,\n",
       "  282,\n",
       "  9,\n",
       "  424,\n",
       "  1,\n",
       "  412,\n",
       "  19,\n",
       "  627,\n",
       "  4,\n",
       "  498,\n",
       "  25,\n",
       "  24,\n",
       "  412,\n",
       "  227,\n",
       "  19,\n",
       "  2291,\n",
       "  7,\n",
       "  1193,\n",
       "  330,\n",
       "  14,\n",
       "  1194,\n",
       "  78,\n",
       "  4,\n",
       "  1581,\n",
       "  282,\n",
       "  627,\n",
       "  4,\n",
       "  498,\n",
       "  25,\n",
       "  1121,\n",
       "  21,\n",
       "  7,\n",
       "  1195,\n",
       "  1193,\n",
       "  227,\n",
       "  19],\n",
       " [1,\n",
       "  164,\n",
       "  33,\n",
       "  109,\n",
       "  10,\n",
       "  1,\n",
       "  266,\n",
       "  94,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  1,\n",
       "  361,\n",
       "  67,\n",
       "  414,\n",
       "  10,\n",
       "  1,\n",
       "  266,\n",
       "  556,\n",
       "  164,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [11,\n",
       "  359,\n",
       "  1,\n",
       "  30,\n",
       "  73,\n",
       "  65,\n",
       "  6,\n",
       "  18,\n",
       "  977,\n",
       "  674,\n",
       "  4,\n",
       "  499,\n",
       "  90,\n",
       "  9,\n",
       "  121,\n",
       "  1,\n",
       "  30,\n",
       "  73,\n",
       "  65,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  977,\n",
       "  152,\n",
       "  674,\n",
       "  4,\n",
       "  499,\n",
       "  90],\n",
       " [1,\n",
       "  1196,\n",
       "  5,\n",
       "  1,\n",
       "  164,\n",
       "  33,\n",
       "  397,\n",
       "  10,\n",
       "  1,\n",
       "  1582,\n",
       "  978,\n",
       "  1583,\n",
       "  197,\n",
       "  6,\n",
       "  397,\n",
       "  164,\n",
       "  1196,\n",
       "  26,\n",
       "  10,\n",
       "  1,\n",
       "  1582,\n",
       "  978,\n",
       "  1583,\n",
       "  197],\n",
       " [98,\n",
       "  492,\n",
       "  461,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  979,\n",
       "  65,\n",
       "  5,\n",
       "  157,\n",
       "  360,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  112,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  95,\n",
       "  72,\n",
       "  365],\n",
       " [48,\n",
       "  183,\n",
       "  13,\n",
       "  1584,\n",
       "  5,\n",
       "  2292,\n",
       "  2293,\n",
       "  5,\n",
       "  125,\n",
       "  21,\n",
       "  1,\n",
       "  1585,\n",
       "  340,\n",
       "  340,\n",
       "  83,\n",
       "  8,\n",
       "  1586,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  11,\n",
       "  7,\n",
       "  1197,\n",
       "  5,\n",
       "  2294,\n",
       "  2295,\n",
       "  183,\n",
       "  13,\n",
       "  1584,\n",
       "  5,\n",
       "  1587,\n",
       "  136,\n",
       "  2296,\n",
       "  21,\n",
       "  58,\n",
       "  94,\n",
       "  1585,\n",
       "  4,\n",
       "  131,\n",
       "  2297,\n",
       "  2298,\n",
       "  8,\n",
       "  1586,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  11,\n",
       "  7,\n",
       "  1197,\n",
       "  5,\n",
       "  2299,\n",
       "  125],\n",
       " [1,\n",
       "  89,\n",
       "  395,\n",
       "  67,\n",
       "  109,\n",
       "  10,\n",
       "  1,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  12,\n",
       "  36,\n",
       "  7,\n",
       "  70,\n",
       "  89,\n",
       "  147,\n",
       "  33,\n",
       "  109,\n",
       "  10,\n",
       "  1,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  12,\n",
       "  36],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  60,\n",
       "  73,\n",
       "  122,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  30,\n",
       "  341,\n",
       "  1,\n",
       "  85,\n",
       "  500,\n",
       "  18,\n",
       "  1,\n",
       "  59,\n",
       "  122,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  341,\n",
       "  63,\n",
       "  8,\n",
       "  24,\n",
       "  85,\n",
       "  144],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  60,\n",
       "  73,\n",
       "  122,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  30,\n",
       "  341,\n",
       "  1,\n",
       "  85,\n",
       "  500,\n",
       "  18,\n",
       "  1,\n",
       "  59,\n",
       "  122,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  341,\n",
       "  63,\n",
       "  8,\n",
       "  24,\n",
       "  85,\n",
       "  144],\n",
       " [1,\n",
       "  669,\n",
       "  32,\n",
       "  1588,\n",
       "  10,\n",
       "  1198,\n",
       "  980,\n",
       "  199,\n",
       "  1589,\n",
       "  1590,\n",
       "  4,\n",
       "  1591,\n",
       "  37,\n",
       "  1198,\n",
       "  980,\n",
       "  199,\n",
       "  1589,\n",
       "  1590,\n",
       "  4,\n",
       "  1591,\n",
       "  37],\n",
       " [9,\n",
       "  2300,\n",
       "  48,\n",
       "  569,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  15,\n",
       "  1592,\n",
       "  1593,\n",
       "  14,\n",
       "  84,\n",
       "  1594,\n",
       "  9,\n",
       "  981,\n",
       "  1595,\n",
       "  946,\n",
       "  5,\n",
       "  63,\n",
       "  23,\n",
       "  7,\n",
       "  22,\n",
       "  5,\n",
       "  1199,\n",
       "  1596,\n",
       "  2301,\n",
       "  810,\n",
       "  167,\n",
       "  7,\n",
       "  22,\n",
       "  5,\n",
       "  1199,\n",
       "  1596,\n",
       "  2302,\n",
       "  33,\n",
       "  15,\n",
       "  26,\n",
       "  569,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  9,\n",
       "  981,\n",
       "  1595,\n",
       "  946,\n",
       "  5,\n",
       "  1,\n",
       "  63,\n",
       "  8,\n",
       "  1,\n",
       "  147,\n",
       "  456,\n",
       "  10,\n",
       "  1,\n",
       "  1592,\n",
       "  1593,\n",
       "  23,\n",
       "  84,\n",
       "  1594],\n",
       " [6,\n",
       "  216,\n",
       "  70,\n",
       "  89,\n",
       "  10,\n",
       "  364,\n",
       "  311,\n",
       "  297,\n",
       "  260,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  6,\n",
       "  752,\n",
       "  89,\n",
       "  10,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  342,\n",
       "  4,\n",
       "  86,\n",
       "  53,\n",
       "  190,\n",
       "  48,\n",
       "  811,\n",
       "  4,\n",
       "  95,\n",
       "  318,\n",
       "  794,\n",
       "  14,\n",
       "  2303,\n",
       "  8,\n",
       "  48,\n",
       "  167,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  238,\n",
       "  55,\n",
       "  52,\n",
       "  342,\n",
       "  4,\n",
       "  86,\n",
       "  53],\n",
       " [982,\n",
       "  7,\n",
       "  1597,\n",
       "  33,\n",
       "  181,\n",
       "  4,\n",
       "  983,\n",
       "  283,\n",
       "  706,\n",
       "  570,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  7,\n",
       "  279,\n",
       "  115,\n",
       "  122,\n",
       "  11,\n",
       "  292,\n",
       "  2304,\n",
       "  570,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  13,\n",
       "  7,\n",
       "  279,\n",
       "  115,\n",
       "  4,\n",
       "  221,\n",
       "  122,\n",
       "  5,\n",
       "  292],\n",
       " [1598,\n",
       "  165,\n",
       "  129,\n",
       "  97,\n",
       "  7,\n",
       "  704,\n",
       "  82,\n",
       "  11,\n",
       "  30,\n",
       "  812,\n",
       "  221,\n",
       "  1598,\n",
       "  165,\n",
       "  97,\n",
       "  290,\n",
       "  7,\n",
       "  82,\n",
       "  11,\n",
       "  30,\n",
       "  812,\n",
       "  221,\n",
       "  42,\n",
       "  6,\n",
       "  751,\n",
       "  9,\n",
       "  31,\n",
       "  427,\n",
       "  704],\n",
       " [6,\n",
       "  76,\n",
       "  501,\n",
       "  1,\n",
       "  323,\n",
       "  258,\n",
       "  286,\n",
       "  21,\n",
       "  7,\n",
       "  1599,\n",
       "  38,\n",
       "  29,\n",
       "  1200,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  14,\n",
       "  1,\n",
       "  259,\n",
       "  5,\n",
       "  2305,\n",
       "  76,\n",
       "  501,\n",
       "  1,\n",
       "  188,\n",
       "  21,\n",
       "  38,\n",
       "  19,\n",
       "  1200,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  31,\n",
       "  7,\n",
       "  2306],\n",
       " [6,\n",
       "  15,\n",
       "  243,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  53,\n",
       "  14,\n",
       "  7,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  1600,\n",
       "  18,\n",
       "  1,\n",
       "  243,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  53,\n",
       "  103,\n",
       "  9,\n",
       "  1201,\n",
       "  1,\n",
       "  1601,\n",
       "  5,\n",
       "  1,\n",
       "  315,\n",
       "  11,\n",
       "  228,\n",
       "  2307,\n",
       "  4,\n",
       "  228,\n",
       "  2308,\n",
       "  14,\n",
       "  7,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  494],\n",
       " [91,\n",
       "  379,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  666,\n",
       "  34,\n",
       "  813,\n",
       "  814,\n",
       "  8,\n",
       "  205,\n",
       "  63,\n",
       "  1,\n",
       "  157,\n",
       "  5,\n",
       "  7,\n",
       "  30,\n",
       "  13,\n",
       "  601,\n",
       "  9,\n",
       "  1,\n",
       "  149,\n",
       "  98,\n",
       "  2128,\n",
       "  2129,\n",
       "  2130,\n",
       "  1396,\n",
       "  2131,\n",
       "  1397,\n",
       "  1398,\n",
       "  7,\n",
       "  30,\n",
       "  26,\n",
       "  1,\n",
       "  1110,\n",
       "  98,\n",
       "  1111,\n",
       "  813,\n",
       "  814],\n",
       " [8,\n",
       "  269,\n",
       "  9,\n",
       "  707,\n",
       "  1,\n",
       "  628,\n",
       "  257,\n",
       "  142,\n",
       "  369,\n",
       "  984,\n",
       "  8,\n",
       "  1,\n",
       "  1601,\n",
       "  2309,\n",
       "  4,\n",
       "  768,\n",
       "  1602,\n",
       "  7,\n",
       "  815,\n",
       "  30,\n",
       "  816,\n",
       "  13,\n",
       "  1603,\n",
       "  571,\n",
       "  1,\n",
       "  91,\n",
       "  115,\n",
       "  5,\n",
       "  1,\n",
       "  1202,\n",
       "  22,\n",
       "  463,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  1,\n",
       "  815,\n",
       "  30,\n",
       "  816,\n",
       "  13,\n",
       "  1603,\n",
       "  571,\n",
       "  1,\n",
       "  91,\n",
       "  115,\n",
       "  5,\n",
       "  1,\n",
       "  1202,\n",
       "  22,\n",
       "  463,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  92,\n",
       "  94,\n",
       "  452,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  92,\n",
       "  20,\n",
       "  9,\n",
       "  69,\n",
       "  24,\n",
       "  163,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [1,\n",
       "  35,\n",
       "  19,\n",
       "  33,\n",
       "  50,\n",
       "  26,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  4,\n",
       "  1,\n",
       "  629,\n",
       "  33,\n",
       "  50,\n",
       "  26,\n",
       "  1,\n",
       "  2310,\n",
       "  70,\n",
       "  41,\n",
       "  343,\n",
       "  20,\n",
       "  2311,\n",
       "  1604,\n",
       "  4,\n",
       "  1605,\n",
       "  329,\n",
       "  1,\n",
       "  528,\n",
       "  2312,\n",
       "  13,\n",
       "  260,\n",
       "  10,\n",
       "  7,\n",
       "  708,\n",
       "  629,\n",
       "  41,\n",
       "  19,\n",
       "  34,\n",
       "  33,\n",
       "  50,\n",
       "  10,\n",
       "  1,\n",
       "  2313,\n",
       "  41,\n",
       "  343,\n",
       "  20,\n",
       "  1604,\n",
       "  4,\n",
       "  1605,\n",
       "  329],\n",
       " [48,\n",
       "  13,\n",
       "  7,\n",
       "  2314,\n",
       "  5,\n",
       "  1,\n",
       "  1606,\n",
       "  2315,\n",
       "  8,\n",
       "  1607,\n",
       "  4,\n",
       "  1608,\n",
       "  110,\n",
       "  48,\n",
       "  13,\n",
       "  72,\n",
       "  9,\n",
       "  1,\n",
       "  1606,\n",
       "  2316,\n",
       "  8,\n",
       "  1607,\n",
       "  4,\n",
       "  1608,\n",
       "  110],\n",
       " [6,\n",
       "  15,\n",
       "  60,\n",
       "  163,\n",
       "  169,\n",
       "  8,\n",
       "  92,\n",
       "  152,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  15,\n",
       "  7,\n",
       "  785,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  92,\n",
       "  152,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  92,\n",
       "  94,\n",
       "  452,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  11,\n",
       "  1,\n",
       "  226,\n",
       "  271,\n",
       "  199,\n",
       "  81,\n",
       "  6,\n",
       "  15,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [11,\n",
       "  1,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  6,\n",
       "  458,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  393,\n",
       "  6,\n",
       "  15,\n",
       "  17,\n",
       "  20,\n",
       "  31,\n",
       "  27,\n",
       "  296,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [253,\n",
       "  30,\n",
       "  132,\n",
       "  32,\n",
       "  417,\n",
       "  26,\n",
       "  1203,\n",
       "  1,\n",
       "  28,\n",
       "  8,\n",
       "  105,\n",
       "  398,\n",
       "  14,\n",
       "  59,\n",
       "  170,\n",
       "  4,\n",
       "  817,\n",
       "  1,\n",
       "  112,\n",
       "  50,\n",
       "  132,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  101,\n",
       "  30,\n",
       "  132,\n",
       "  67,\n",
       "  153,\n",
       "  26,\n",
       "  464,\n",
       "  59,\n",
       "  8,\n",
       "  105,\n",
       "  398,\n",
       "  4,\n",
       "  817,\n",
       "  10,\n",
       "  1,\n",
       "  356,\n",
       "  370,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [1,\n",
       "  818,\n",
       "  807,\n",
       "  26,\n",
       "  1,\n",
       "  425,\n",
       "  951,\n",
       "  1609,\n",
       "  371,\n",
       "  8,\n",
       "  1,\n",
       "  572,\n",
       "  2317,\n",
       "  422,\n",
       "  75,\n",
       "  33,\n",
       "  2318,\n",
       "  1610,\n",
       "  2319,\n",
       "  153,\n",
       "  14,\n",
       "  7,\n",
       "  60,\n",
       "  985,\n",
       "  41,\n",
       "  19,\n",
       "  14,\n",
       "  819,\n",
       "  4,\n",
       "  116,\n",
       "  173,\n",
       "  630,\n",
       "  4,\n",
       "  45,\n",
       "  165,\n",
       "  1,\n",
       "  41,\n",
       "  19,\n",
       "  13,\n",
       "  7,\n",
       "  100,\n",
       "  14,\n",
       "  819,\n",
       "  4,\n",
       "  116,\n",
       "  173,\n",
       "  630,\n",
       "  4,\n",
       "  45,\n",
       "  165],\n",
       " [6,\n",
       "  15,\n",
       "  60,\n",
       "  163,\n",
       "  169,\n",
       "  8,\n",
       "  92,\n",
       "  152,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  92,\n",
       "  20,\n",
       "  9,\n",
       "  69,\n",
       "  24,\n",
       "  163,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [70,\n",
       "  56,\n",
       "  35,\n",
       "  13,\n",
       "  809,\n",
       "  109,\n",
       "  10,\n",
       "  27,\n",
       "  49,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  98,\n",
       "  13,\n",
       "  7,\n",
       "  60,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [249,\n",
       "  338,\n",
       "  2,\n",
       "  3,\n",
       "  339,\n",
       "  13,\n",
       "  68,\n",
       "  428,\n",
       "  685,\n",
       "  257,\n",
       "  399,\n",
       "  42,\n",
       "  240,\n",
       "  130,\n",
       "  333,\n",
       "  1119,\n",
       "  44,\n",
       "  2320,\n",
       "  249,\n",
       "  428,\n",
       "  257,\n",
       "  399,\n",
       "  338,\n",
       "  2,\n",
       "  3,\n",
       "  339],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  445,\n",
       "  81,\n",
       "  5,\n",
       "  256,\n",
       "  416,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  15,\n",
       "  7,\n",
       "  785,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  92,\n",
       "  152,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [6,\n",
       "  986,\n",
       "  1,\n",
       "  1204,\n",
       "  5,\n",
       "  111,\n",
       "  522,\n",
       "  14,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  1205,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  9,\n",
       "  986,\n",
       "  522,\n",
       "  1611],\n",
       " [24,\n",
       "  133,\n",
       "  686,\n",
       "  481,\n",
       "  1,\n",
       "  372,\n",
       "  41,\n",
       "  20,\n",
       "  272,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  186,\n",
       "  198,\n",
       "  33,\n",
       "  2321,\n",
       "  10,\n",
       "  1,\n",
       "  372,\n",
       "  41,\n",
       "  20,\n",
       "  272,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [328,\n",
       "  15,\n",
       "  1,\n",
       "  1206,\n",
       "  164,\n",
       "  122,\n",
       "  266,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  11,\n",
       "  1,\n",
       "  164,\n",
       "  1,\n",
       "  164,\n",
       "  33,\n",
       "  109,\n",
       "  447,\n",
       "  10,\n",
       "  1,\n",
       "  266,\n",
       "  164,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [24,\n",
       "  29,\n",
       "  631,\n",
       "  8,\n",
       "  987,\n",
       "  62,\n",
       "  94,\n",
       "  195,\n",
       "  115,\n",
       "  8,\n",
       "  465,\n",
       "  988,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  6,\n",
       "  631,\n",
       "  8,\n",
       "  105,\n",
       "  573,\n",
       "  7,\n",
       "  4,\n",
       "  989,\n",
       "  5,\n",
       "  987,\n",
       "  62,\n",
       "  94,\n",
       "  195,\n",
       "  115,\n",
       "  8,\n",
       "  465,\n",
       "  988,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  14,\n",
       "  68,\n",
       "  990,\n",
       "  5,\n",
       "  24,\n",
       "  991,\n",
       "  29],\n",
       " [1,\n",
       "  44,\n",
       "  133,\n",
       "  33,\n",
       "  181,\n",
       "  10,\n",
       "  1,\n",
       "  30,\n",
       "  274,\n",
       "  1612,\n",
       "  21,\n",
       "  272,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  6,\n",
       "  1613,\n",
       "  1,\n",
       "  133,\n",
       "  10,\n",
       "  1,\n",
       "  107,\n",
       "  1614,\n",
       "  21,\n",
       "  272,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [1,\n",
       "  1615,\n",
       "  67,\n",
       "  373,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  120,\n",
       "  108,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  8,\n",
       "  335,\n",
       "  1,\n",
       "  28,\n",
       "  33,\n",
       "  181,\n",
       "  182,\n",
       "  4,\n",
       "  373,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [70,\n",
       "  56,\n",
       "  35,\n",
       "  13,\n",
       "  809,\n",
       "  109,\n",
       "  10,\n",
       "  27,\n",
       "  49,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  44,\n",
       "  140,\n",
       "  33,\n",
       "  181,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  144,\n",
       "  32,\n",
       "  400,\n",
       "  4,\n",
       "  466,\n",
       "  10,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  50,\n",
       "  68,\n",
       "  44,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  6,\n",
       "  66,\n",
       "  7,\n",
       "  629,\n",
       "  41,\n",
       "  19,\n",
       "  14,\n",
       "  116,\n",
       "  173,\n",
       "  10,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  25],\n",
       " [51,\n",
       "  5,\n",
       "  1,\n",
       "  133,\n",
       "  28,\n",
       "  21,\n",
       "  2322,\n",
       "  33,\n",
       "  181,\n",
       "  10,\n",
       "  1,\n",
       "  272,\n",
       "  632,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  6,\n",
       "  1613,\n",
       "  1,\n",
       "  133,\n",
       "  10,\n",
       "  1,\n",
       "  107,\n",
       "  1614,\n",
       "  21,\n",
       "  272,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [24,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  32,\n",
       "  50,\n",
       "  10,\n",
       "  17,\n",
       "  131,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  1,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  6,\n",
       "  458,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  46,\n",
       "  1,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  11,\n",
       "  282,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  121,\n",
       "  672,\n",
       "  24,\n",
       "  323,\n",
       "  128,\n",
       "  11,\n",
       "  412,\n",
       "  324,\n",
       "  221,\n",
       "  4,\n",
       "  322,\n",
       "  443,\n",
       "  4,\n",
       "  438,\n",
       "  64,\n",
       "  1207,\n",
       "  1616,\n",
       "  24,\n",
       "  323,\n",
       "  1208,\n",
       "  128,\n",
       "  11,\n",
       "  324,\n",
       "  221,\n",
       "  443,\n",
       "  4,\n",
       "  438,\n",
       "  64],\n",
       " [6,\n",
       "  15,\n",
       "  243,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  53,\n",
       "  14,\n",
       "  7,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  2323,\n",
       "  6,\n",
       "  18,\n",
       "  243,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  53,\n",
       "  14,\n",
       "  7,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  2324,\n",
       "  4,\n",
       "  7,\n",
       "  676,\n",
       "  415,\n",
       "  5,\n",
       "  1587],\n",
       " [11,\n",
       "  51,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  38,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  46,\n",
       "  1,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  11,\n",
       "  282,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  38,\n",
       "  49,\n",
       "  67,\n",
       "  66,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  144,\n",
       "  32,\n",
       "  400,\n",
       "  4,\n",
       "  466,\n",
       "  10,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  46,\n",
       "  1,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  11,\n",
       "  282,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  24,\n",
       "  38,\n",
       "  58,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  46,\n",
       "  1,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  11,\n",
       "  282,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  118,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  70,\n",
       "  56,\n",
       "  35],\n",
       " [11,\n",
       "  1,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  6,\n",
       "  458,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  101,\n",
       "  49,\n",
       "  32,\n",
       "  66,\n",
       "  14,\n",
       "  1,\n",
       "  177,\n",
       "  27,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  820,\n",
       "  35,\n",
       "  65,\n",
       "  10,\n",
       "  1,\n",
       "  27,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  38,\n",
       "  1209,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  15,\n",
       "  429,\n",
       "  180,\n",
       "  84,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  992,\n",
       "  46,\n",
       "  1617,\n",
       "  984,\n",
       "  1,\n",
       "  429,\n",
       "  171,\n",
       "  180,\n",
       "  84,\n",
       "  42,\n",
       "  33,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  2325,\n",
       "  502,\n",
       "  46,\n",
       "  28],\n",
       " [1,\n",
       "  1615,\n",
       "  67,\n",
       "  373,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  120,\n",
       "  108,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  8,\n",
       "  335,\n",
       "  1,\n",
       "  28,\n",
       "  33,\n",
       "  181,\n",
       "  182,\n",
       "  4,\n",
       "  373,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [6,\n",
       "  820,\n",
       "  35,\n",
       "  65,\n",
       "  10,\n",
       "  1,\n",
       "  27,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  38,\n",
       "  1209,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  446,\n",
       "  467,\n",
       "  993,\n",
       "  13,\n",
       "  2326,\n",
       "  14,\n",
       "  1,\n",
       "  1210,\n",
       "  566,\n",
       "  467,\n",
       "  8,\n",
       "  1,\n",
       "  147,\n",
       "  310,\n",
       "  702,\n",
       "  4,\n",
       "  1618,\n",
       "  387,\n",
       "  1619,\n",
       "  13,\n",
       "  7,\n",
       "  60,\n",
       "  70,\n",
       "  82,\n",
       "  34,\n",
       "  2327,\n",
       "  1,\n",
       "  467,\n",
       "  5,\n",
       "  7,\n",
       "  446,\n",
       "  8,\n",
       "  7,\n",
       "  709,\n",
       "  566,\n",
       "  14,\n",
       "  124,\n",
       "  1210,\n",
       "  566,\n",
       "  467,\n",
       "  8,\n",
       "  710,\n",
       "  18,\n",
       "  702,\n",
       "  4,\n",
       "  1618,\n",
       "  387],\n",
       " [6,\n",
       "  2328,\n",
       "  1,\n",
       "  70,\n",
       "  89,\n",
       "  5,\n",
       "  667,\n",
       "  8,\n",
       "  222,\n",
       "  14,\n",
       "  68,\n",
       "  821,\n",
       "  1211,\n",
       "  147,\n",
       "  344,\n",
       "  1620,\n",
       "  965,\n",
       "  2329,\n",
       "  7,\n",
       "  454,\n",
       "  2330,\n",
       "  8,\n",
       "  2331,\n",
       "  2332,\n",
       "  1621,\n",
       "  70,\n",
       "  89,\n",
       "  5,\n",
       "  1,\n",
       "  711,\n",
       "  8,\n",
       "  633,\n",
       "  83,\n",
       "  222,\n",
       "  11,\n",
       "  112,\n",
       "  994,\n",
       "  574,\n",
       "  68,\n",
       "  821,\n",
       "  1211,\n",
       "  147,\n",
       "  1620,\n",
       "  965],\n",
       " [7,\n",
       "  330,\n",
       "  11,\n",
       "  352,\n",
       "  534,\n",
       "  115,\n",
       "  4,\n",
       "  534,\n",
       "  215,\n",
       "  129,\n",
       "  123,\n",
       "  97,\n",
       "  8,\n",
       "  912,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  4,\n",
       "  7,\n",
       "  612,\n",
       "  115,\n",
       "  5,\n",
       "  1,\n",
       "  153,\n",
       "  188,\n",
       "  129,\n",
       "  123,\n",
       "  196,\n",
       "  2333,\n",
       "  330,\n",
       "  11,\n",
       "  352,\n",
       "  534,\n",
       "  115,\n",
       "  129,\n",
       "  123,\n",
       "  97,\n",
       "  8,\n",
       "  912,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  241,\n",
       "  31,\n",
       "  613,\n",
       "  352,\n",
       "  143,\n",
       "  48,\n",
       "  13,\n",
       "  76,\n",
       "  7,\n",
       "  479,\n",
       "  1129,\n",
       "  62],\n",
       " [11,\n",
       "  231,\n",
       "  316,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  289,\n",
       "  34,\n",
       "  1,\n",
       "  528,\n",
       "  5,\n",
       "  2334,\n",
       "  358,\n",
       "  13,\n",
       "  318,\n",
       "  1622,\n",
       "  7,\n",
       "  914,\n",
       "  2335,\n",
       "  5,\n",
       "  352,\n",
       "  2336,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  2337,\n",
       "  34,\n",
       "  145,\n",
       "  1623,\n",
       "  13,\n",
       "  34,\n",
       "  1,\n",
       "  995,\n",
       "  369,\n",
       "  5,\n",
       "  996,\n",
       "  65,\n",
       "  937,\n",
       "  318,\n",
       "  1622,\n",
       "  1624,\n",
       "  350,\n",
       "  14,\n",
       "  352,\n",
       "  1212],\n",
       " [23,\n",
       "  1,\n",
       "  298,\n",
       "  140,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  279,\n",
       "  2338,\n",
       "  138,\n",
       "  8,\n",
       "  1625,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  5,\n",
       "  1626,\n",
       "  2339,\n",
       "  9,\n",
       "  262,\n",
       "  30,\n",
       "  575,\n",
       "  4,\n",
       "  119,\n",
       "  198,\n",
       "  4,\n",
       "  15,\n",
       "  1,\n",
       "  2340,\n",
       "  2341,\n",
       "  29,\n",
       "  1625,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  33,\n",
       "  15,\n",
       "  9,\n",
       "  262,\n",
       "  30,\n",
       "  575,\n",
       "  4,\n",
       "  198,\n",
       "  4,\n",
       "  1,\n",
       "  101,\n",
       "  52,\n",
       "  33,\n",
       "  15,\n",
       "  9,\n",
       "  160,\n",
       "  1,\n",
       "  125,\n",
       "  8,\n",
       "  1,\n",
       "  353,\n",
       "  22],\n",
       " [223,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  161,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [223,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  161,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  223,\n",
       "  101,\n",
       "  58,\n",
       "  11,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  223,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  161,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [192,\n",
       "  1,\n",
       "  1213,\n",
       "  28,\n",
       "  33,\n",
       "  109,\n",
       "  11,\n",
       "  1214,\n",
       "  119,\n",
       "  198,\n",
       "  227,\n",
       "  1215,\n",
       "  4,\n",
       "  1627,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  8,\n",
       "  335,\n",
       "  1,\n",
       "  28,\n",
       "  33,\n",
       "  181,\n",
       "  182,\n",
       "  4,\n",
       "  373,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [6,\n",
       "  214,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  9,\n",
       "  216,\n",
       "  70,\n",
       "  89,\n",
       "  228,\n",
       "  410,\n",
       "  5,\n",
       "  24,\n",
       "  65,\n",
       "  555,\n",
       "  9,\n",
       "  7,\n",
       "  1628,\n",
       "  216,\n",
       "  89,\n",
       "  5,\n",
       "  188,\n",
       "  10,\n",
       "  77,\n",
       "  80,\n",
       "  190,\n",
       "  228,\n",
       "  410,\n",
       "  12,\n",
       "  36],\n",
       " [48,\n",
       "  29,\n",
       "  481,\n",
       "  1,\n",
       "  2342,\n",
       "  1629,\n",
       "  401,\n",
       "  138,\n",
       "  26,\n",
       "  1630,\n",
       "  2,\n",
       "  3,\n",
       "  201,\n",
       "  359,\n",
       "  23,\n",
       "  167,\n",
       "  26,\n",
       "  2343,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  6,\n",
       "  1631,\n",
       "  1,\n",
       "  1629,\n",
       "  401,\n",
       "  14,\n",
       "  1632,\n",
       "  97,\n",
       "  26,\n",
       "  1630,\n",
       "  2,\n",
       "  3,\n",
       "  201],\n",
       " [1,\n",
       "  41,\n",
       "  19,\n",
       "  13,\n",
       "  7,\n",
       "  100,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  19,\n",
       "  50,\n",
       "  10,\n",
       "  822,\n",
       "  14,\n",
       "  155,\n",
       "  116,\n",
       "  173,\n",
       "  4,\n",
       "  559,\n",
       "  1633,\n",
       "  66,\n",
       "  7,\n",
       "  629,\n",
       "  41,\n",
       "  19,\n",
       "  14,\n",
       "  116,\n",
       "  173,\n",
       "  10,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  25],\n",
       " [402,\n",
       "  712,\n",
       "  230,\n",
       "  548,\n",
       "  1634,\n",
       "  15,\n",
       "  8,\n",
       "  24,\n",
       "  19,\n",
       "  32,\n",
       "  823,\n",
       "  14,\n",
       "  503,\n",
       "  468,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  161,\n",
       "  13,\n",
       "  175,\n",
       "  14,\n",
       "  402,\n",
       "  712,\n",
       "  230,\n",
       "  548,\n",
       "  997,\n",
       "  10,\n",
       "  60,\n",
       "  476,\n",
       "  169,\n",
       "  8,\n",
       "  1,\n",
       "  503,\n",
       "  998,\n",
       "  468,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [402,\n",
       "  712,\n",
       "  230,\n",
       "  548,\n",
       "  1634,\n",
       "  15,\n",
       "  8,\n",
       "  24,\n",
       "  19,\n",
       "  32,\n",
       "  823,\n",
       "  14,\n",
       "  503,\n",
       "  468,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  161,\n",
       "  13,\n",
       "  175,\n",
       "  14,\n",
       "  402,\n",
       "  712,\n",
       "  230,\n",
       "  548,\n",
       "  997,\n",
       "  10,\n",
       "  60,\n",
       "  476,\n",
       "  169,\n",
       "  8,\n",
       "  1,\n",
       "  503,\n",
       "  998,\n",
       "  468,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [192,\n",
       "  1,\n",
       "  1213,\n",
       "  28,\n",
       "  33,\n",
       "  109,\n",
       "  11,\n",
       "  1214,\n",
       "  119,\n",
       "  198,\n",
       "  227,\n",
       "  1215,\n",
       "  4,\n",
       "  1627,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  8,\n",
       "  335,\n",
       "  1,\n",
       "  28,\n",
       "  33,\n",
       "  181,\n",
       "  182,\n",
       "  4,\n",
       "  373,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [98,\n",
       "  13,\n",
       "  7,\n",
       "  2344,\n",
       "  5,\n",
       "  1,\n",
       "  19,\n",
       "  97,\n",
       "  26,\n",
       "  1635,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  24,\n",
       "  118,\n",
       "  101,\n",
       "  13,\n",
       "  2345,\n",
       "  7,\n",
       "  2346,\n",
       "  576,\n",
       "  184,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  167,\n",
       "  5,\n",
       "  1635,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [6,\n",
       "  18,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  1,\n",
       "  56,\n",
       "  78,\n",
       "  403,\n",
       "  11,\n",
       "  553,\n",
       "  11,\n",
       "  1216,\n",
       "  1,\n",
       "  332,\n",
       "  2347,\n",
       "  15,\n",
       "  1,\n",
       "  92,\n",
       "  56,\n",
       "  78,\n",
       "  403,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  11,\n",
       "  105,\n",
       "  1216,\n",
       "  24,\n",
       "  215,\n",
       "  65,\n",
       "  4,\n",
       "  1636,\n",
       "  70,\n",
       "  250,\n",
       "  824],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  577,\n",
       "  119,\n",
       "  578,\n",
       "  1217,\n",
       "  5,\n",
       "  579,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  48,\n",
       "  8,\n",
       "  1637,\n",
       "  546,\n",
       "  23,\n",
       "  1,\n",
       "  139,\n",
       "  1218,\n",
       "  250,\n",
       "  19,\n",
       "  809,\n",
       "  1638,\n",
       "  21,\n",
       "  7,\n",
       "  159,\n",
       "  186,\n",
       "  119,\n",
       "  287,\n",
       "  290,\n",
       "  31,\n",
       "  1,\n",
       "  577,\n",
       "  119,\n",
       "  578,\n",
       "  5,\n",
       "  579,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  577,\n",
       "  119,\n",
       "  578,\n",
       "  1217,\n",
       "  5,\n",
       "  579,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  48,\n",
       "  8,\n",
       "  1637,\n",
       "  546,\n",
       "  23,\n",
       "  1,\n",
       "  139,\n",
       "  1218,\n",
       "  250,\n",
       "  19,\n",
       "  809,\n",
       "  1638,\n",
       "  21,\n",
       "  7,\n",
       "  159,\n",
       "  186,\n",
       "  119,\n",
       "  287,\n",
       "  290,\n",
       "  31,\n",
       "  1,\n",
       "  577,\n",
       "  119,\n",
       "  578,\n",
       "  5,\n",
       "  579,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [1,\n",
       "  531,\n",
       "  13,\n",
       "  50,\n",
       "  10,\n",
       "  2348,\n",
       "  21,\n",
       "  1,\n",
       "  280,\n",
       "  1218,\n",
       "  682,\n",
       "  423,\n",
       "  706,\n",
       "  13,\n",
       "  7,\n",
       "  1219,\n",
       "  169,\n",
       "  122,\n",
       "  11,\n",
       "  279,\n",
       "  115,\n",
       "  4,\n",
       "  221,\n",
       "  5,\n",
       "  2349,\n",
       "  4,\n",
       "  2350,\n",
       "  133,\n",
       "  570,\n",
       "  2,\n",
       "  3,\n",
       "  2351,\n",
       "  570,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  13,\n",
       "  7,\n",
       "  279,\n",
       "  115,\n",
       "  4,\n",
       "  221,\n",
       "  122,\n",
       "  5,\n",
       "  292],\n",
       " [11,\n",
       "  41,\n",
       "  343,\n",
       "  6,\n",
       "  50,\n",
       "  7,\n",
       "  1220,\n",
       "  100,\n",
       "  116,\n",
       "  291,\n",
       "  371,\n",
       "  19,\n",
       "  23,\n",
       "  1,\n",
       "  209,\n",
       "  386,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  825,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  6,\n",
       "  66,\n",
       "  7,\n",
       "  155,\n",
       "  630,\n",
       "  45,\n",
       "  291,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  10,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  4,\n",
       "  109,\n",
       "  796,\n",
       "  14,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  325],\n",
       " [14,\n",
       "  1,\n",
       "  46,\n",
       "  999,\n",
       "  5,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  442,\n",
       "  1,\n",
       "  46,\n",
       "  144,\n",
       "  14,\n",
       "  229,\n",
       "  1221,\n",
       "  8,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  30,\n",
       "  73,\n",
       "  33,\n",
       "  50,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  14,\n",
       "  1,\n",
       "  580,\n",
       "  356,\n",
       "  73,\n",
       "  538,\n",
       "  1139,\n",
       "  109,\n",
       "  30,\n",
       "  73,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  14,\n",
       "  1,\n",
       "  107,\n",
       "  356,\n",
       "  73,\n",
       "  538,\n",
       "  82],\n",
       " [6,\n",
       "  18,\n",
       "  226,\n",
       "  489,\n",
       "  21,\n",
       "  490,\n",
       "  4,\n",
       "  489,\n",
       "  14,\n",
       "  753,\n",
       "  162,\n",
       "  21,\n",
       "  404,\n",
       "  316,\n",
       "  4,\n",
       "  134,\n",
       "  25,\n",
       "  31,\n",
       "  24,\n",
       "  1222,\n",
       "  6,\n",
       "  18,\n",
       "  404,\n",
       "  14,\n",
       "  7,\n",
       "  226,\n",
       "  162,\n",
       "  316,\n",
       "  4,\n",
       "  134,\n",
       "  25],\n",
       " [6,\n",
       "  547,\n",
       "  1,\n",
       "  685,\n",
       "  2352,\n",
       "  8,\n",
       "  363,\n",
       "  5,\n",
       "  7,\n",
       "  1639,\n",
       "  29,\n",
       "  1640,\n",
       "  2,\n",
       "  3,\n",
       "  165,\n",
       "  6,\n",
       "  547,\n",
       "  24,\n",
       "  964,\n",
       "  1223,\n",
       "  103,\n",
       "  31,\n",
       "  7,\n",
       "  1639,\n",
       "  29,\n",
       "  1640,\n",
       "  2,\n",
       "  3,\n",
       "  165],\n",
       " [6,\n",
       "  18,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  160,\n",
       "  200,\n",
       "  4,\n",
       "  55,\n",
       "  761,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  11,\n",
       "  430,\n",
       "  55,\n",
       "  431,\n",
       "  4,\n",
       "  186,\n",
       "  93],\n",
       " [8,\n",
       "  48,\n",
       "  167,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  238,\n",
       "  55,\n",
       "  52,\n",
       "  342,\n",
       "  4,\n",
       "  86,\n",
       "  53,\n",
       "  8,\n",
       "  269,\n",
       "  9,\n",
       "  1000,\n",
       "  1,\n",
       "  2353,\n",
       "  2354,\n",
       "  6,\n",
       "  1224,\n",
       "  39,\n",
       "  52,\n",
       "  342,\n",
       "  4,\n",
       "  86,\n",
       "  53],\n",
       " [6,\n",
       "  18,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  160,\n",
       "  200,\n",
       "  4,\n",
       "  55,\n",
       "  761,\n",
       "  76,\n",
       "  210,\n",
       "  1,\n",
       "  55,\n",
       "  160,\n",
       "  5,\n",
       "  1,\n",
       "  125,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [1,\n",
       "  78,\n",
       "  103,\n",
       "  15,\n",
       "  8,\n",
       "  24,\n",
       "  826,\n",
       "  1001,\n",
       "  13,\n",
       "  1225,\n",
       "  1641,\n",
       "  581,\n",
       "  1,\n",
       "  1226,\n",
       "  184,\n",
       "  15,\n",
       "  8,\n",
       "  1,\n",
       "  58,\n",
       "  13,\n",
       "  1,\n",
       "  1225,\n",
       "  405,\n",
       "  189,\n",
       "  184,\n",
       "  1641,\n",
       "  581],\n",
       " [6,\n",
       "  18,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  160,\n",
       "  200,\n",
       "  4,\n",
       "  55,\n",
       "  761,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  11,\n",
       "  430,\n",
       "  55,\n",
       "  431,\n",
       "  4,\n",
       "  186,\n",
       "  93],\n",
       " [91,\n",
       "  379,\n",
       "  462,\n",
       "  504,\n",
       "  4,\n",
       "  2355,\n",
       "  57,\n",
       "  11,\n",
       "  68,\n",
       "  2127,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  1642,\n",
       "  34,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  104,\n",
       "  766,\n",
       "  601,\n",
       "  114,\n",
       "  127,\n",
       "  1,\n",
       "  1002,\n",
       "  34,\n",
       "  13,\n",
       "  1,\n",
       "  255,\n",
       "  432,\n",
       "  546,\n",
       "  23,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  34,\n",
       "  2356,\n",
       "  1227,\n",
       "  1,\n",
       "  666,\n",
       "  34,\n",
       "  63,\n",
       "  14,\n",
       "  72,\n",
       "  157,\n",
       "  237,\n",
       "  9,\n",
       "  194,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  114,\n",
       "  127],\n",
       " [248,\n",
       "  167,\n",
       "  13,\n",
       "  236,\n",
       "  5,\n",
       "  1,\n",
       "  258,\n",
       "  292,\n",
       "  279,\n",
       "  171,\n",
       "  706,\n",
       "  570,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  706,\n",
       "  570,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  13,\n",
       "  7,\n",
       "  279,\n",
       "  115,\n",
       "  4,\n",
       "  221,\n",
       "  122,\n",
       "  5,\n",
       "  292],\n",
       " [6,\n",
       "  18,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  160,\n",
       "  200,\n",
       "  4,\n",
       "  55,\n",
       "  761,\n",
       "  76,\n",
       "  210,\n",
       "  1,\n",
       "  55,\n",
       "  160,\n",
       "  5,\n",
       "  1,\n",
       "  125,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [1572,\n",
       "  461,\n",
       "  1,\n",
       "  634,\n",
       "  128,\n",
       "  9,\n",
       "  27,\n",
       "  35,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  36,\n",
       "  8,\n",
       "  42,\n",
       "  1,\n",
       "  405,\n",
       "  469,\n",
       "  129,\n",
       "  1,\n",
       "  2357,\n",
       "  226,\n",
       "  827,\n",
       "  1643,\n",
       "  1644,\n",
       "  505,\n",
       "  713,\n",
       "  505,\n",
       "  633,\n",
       "  2358,\n",
       "  634,\n",
       "  128,\n",
       "  9,\n",
       "  27,\n",
       "  35,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  36,\n",
       "  635,\n",
       "  65,\n",
       "  1,\n",
       "  1645,\n",
       "  35,\n",
       "  377,\n",
       "  1646,\n",
       "  713,\n",
       "  83,\n",
       "  1647,\n",
       "  1648,\n",
       "  713,\n",
       "  505,\n",
       "  633,\n",
       "  83,\n",
       "  319,\n",
       "  505,\n",
       "  13,\n",
       "  1,\n",
       "  209,\n",
       "  1228],\n",
       " [46,\n",
       "  28,\n",
       "  32,\n",
       "  61,\n",
       "  23,\n",
       "  7,\n",
       "  1649,\n",
       "  5,\n",
       "  1229,\n",
       "  828,\n",
       "  44,\n",
       "  144,\n",
       "  94,\n",
       "  21,\n",
       "  1,\n",
       "  1003,\n",
       "  399,\n",
       "  1004,\n",
       "  84,\n",
       "  105,\n",
       "  2359,\n",
       "  144,\n",
       "  32,\n",
       "  169,\n",
       "  21,\n",
       "  1,\n",
       "  1003,\n",
       "  399,\n",
       "  1004,\n",
       "  84],\n",
       " [368,\n",
       "  38,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  85,\n",
       "  22,\n",
       "  185,\n",
       "  131,\n",
       "  9,\n",
       "  69,\n",
       "  68,\n",
       "  506,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  1,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  6,\n",
       "  458,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [100,\n",
       "  41,\n",
       "  65,\n",
       "  5,\n",
       "  1650,\n",
       "  4,\n",
       "  44,\n",
       "  67,\n",
       "  50,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  6,\n",
       "  66,\n",
       "  7,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  23,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  703,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  293,\n",
       "  215,\n",
       "  183,\n",
       "  5,\n",
       "  1,\n",
       "  565,\n",
       "  117,\n",
       "  62,\n",
       "  344,\n",
       "  1005,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  6,\n",
       "  397,\n",
       "  24,\n",
       "  19,\n",
       "  23,\n",
       "  7,\n",
       "  130,\n",
       "  293,\n",
       "  215,\n",
       "  62,\n",
       "  565,\n",
       "  117,\n",
       "  62,\n",
       "  344,\n",
       "  1005,\n",
       "  2,\n",
       "  3,\n",
       "  117],\n",
       " [6,\n",
       "  192,\n",
       "  232,\n",
       "  30,\n",
       "  73,\n",
       "  14,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  8,\n",
       "  105,\n",
       "  398,\n",
       "  14,\n",
       "  1,\n",
       "  107,\n",
       "  106,\n",
       "  15,\n",
       "  8,\n",
       "  1651,\n",
       "  109,\n",
       "  30,\n",
       "  73,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  14,\n",
       "  1,\n",
       "  107,\n",
       "  356,\n",
       "  73,\n",
       "  538,\n",
       "  82],\n",
       " [368,\n",
       "  38,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  85,\n",
       "  22,\n",
       "  185,\n",
       "  131,\n",
       "  9,\n",
       "  69,\n",
       "  68,\n",
       "  506,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  51,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  38,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  89,\n",
       "  395,\n",
       "  67,\n",
       "  109,\n",
       "  10,\n",
       "  1,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  12,\n",
       "  36,\n",
       "  1,\n",
       "  70,\n",
       "  89,\n",
       "  395,\n",
       "  10,\n",
       "  364,\n",
       "  311,\n",
       "  802,\n",
       "  32,\n",
       "  460,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36],\n",
       " [51,\n",
       "  58,\n",
       "  67,\n",
       "  196,\n",
       "  179,\n",
       "  10,\n",
       "  1,\n",
       "  177,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  8,\n",
       "  124,\n",
       "  60,\n",
       "  1652,\n",
       "  580,\n",
       "  1,\n",
       "  38,\n",
       "  49,\n",
       "  67,\n",
       "  66,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [172,\n",
       "  125,\n",
       "  95,\n",
       "  192,\n",
       "  104,\n",
       "  983,\n",
       "  283,\n",
       "  68,\n",
       "  2360,\n",
       "  264,\n",
       "  52,\n",
       "  1230,\n",
       "  1653,\n",
       "  84,\n",
       "  14,\n",
       "  380,\n",
       "  2361,\n",
       "  125,\n",
       "  32,\n",
       "  983,\n",
       "  283,\n",
       "  1,\n",
       "  1230,\n",
       "  264,\n",
       "  52,\n",
       "  1653,\n",
       "  84,\n",
       "  14,\n",
       "  1,\n",
       "  2362,\n",
       "  2363],\n",
       " [1,\n",
       "  41,\n",
       "  19,\n",
       "  13,\n",
       "  7,\n",
       "  100,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  19,\n",
       "  50,\n",
       "  10,\n",
       "  822,\n",
       "  14,\n",
       "  155,\n",
       "  116,\n",
       "  173,\n",
       "  4,\n",
       "  559,\n",
       "  2364,\n",
       "  100,\n",
       "  209,\n",
       "  41,\n",
       "  19,\n",
       "  33,\n",
       "  50,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25],\n",
       " [504,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  714,\n",
       "  7,\n",
       "  1231,\n",
       "  103,\n",
       "  11,\n",
       "  1654,\n",
       "  411,\n",
       "  241,\n",
       "  1,\n",
       "  2365,\n",
       "  5,\n",
       "  1,\n",
       "  1654,\n",
       "  19,\n",
       "  4,\n",
       "  124,\n",
       "  1231,\n",
       "  103,\n",
       "  9,\n",
       "  2366,\n",
       "  13,\n",
       "  2367,\n",
       "  4,\n",
       "  2368,\n",
       "  6,\n",
       "  714,\n",
       "  68,\n",
       "  103,\n",
       "  11,\n",
       "  1655,\n",
       "  2369,\n",
       "  2370,\n",
       "  9,\n",
       "  1,\n",
       "  1231,\n",
       "  103,\n",
       "  5,\n",
       "  504,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  241,\n",
       "  1232,\n",
       "  9,\n",
       "  2371,\n",
       "  4,\n",
       "  424],\n",
       " [91,\n",
       "  113,\n",
       "  234,\n",
       "  114,\n",
       "  127,\n",
       "  967,\n",
       "  34,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  308,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  1656,\n",
       "  1657,\n",
       "  1658,\n",
       "  8,\n",
       "  1006,\n",
       "  1659,\n",
       "  72,\n",
       "  308,\n",
       "  5,\n",
       "  112,\n",
       "  363,\n",
       "  8,\n",
       "  1,\n",
       "  470,\n",
       "  4,\n",
       "  363,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  365,\n",
       "  114,\n",
       "  127],\n",
       " [91,\n",
       "  113,\n",
       "  234,\n",
       "  114,\n",
       "  127,\n",
       "  967,\n",
       "  34,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  308,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  1656,\n",
       "  1657,\n",
       "  1658,\n",
       "  8,\n",
       "  1006,\n",
       "  1659,\n",
       "  72,\n",
       "  308,\n",
       "  5,\n",
       "  112,\n",
       "  363,\n",
       "  8,\n",
       "  1,\n",
       "  470,\n",
       "  4,\n",
       "  363,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  365,\n",
       "  114,\n",
       "  127],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  6,\n",
       "  118,\n",
       "  18,\n",
       "  7,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  160,\n",
       "  111,\n",
       "  136,\n",
       "  4,\n",
       "  219,\n",
       "  1,\n",
       "  75,\n",
       "  5,\n",
       "  55,\n",
       "  242,\n",
       "  762,\n",
       "  14,\n",
       "  1,\n",
       "  136],\n",
       " [6,\n",
       "  76,\n",
       "  2372,\n",
       "  1,\n",
       "  340,\n",
       "  5,\n",
       "  829,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  23,\n",
       "  48,\n",
       "  2373,\n",
       "  76,\n",
       "  196,\n",
       "  179,\n",
       "  7,\n",
       "  1660,\n",
       "  433,\n",
       "  340,\n",
       "  319,\n",
       "  1,\n",
       "  1007,\n",
       "  32,\n",
       "  830,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  1233,\n",
       "  132,\n",
       "  153,\n",
       "  26,\n",
       "  1661,\n",
       "  340,\n",
       "  5,\n",
       "  829,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [6,\n",
       "  820,\n",
       "  35,\n",
       "  65,\n",
       "  10,\n",
       "  1,\n",
       "  27,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  38,\n",
       "  2374,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  161,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [119,\n",
       "  198,\n",
       "  33,\n",
       "  109,\n",
       "  14,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  1,\n",
       "  147,\n",
       "  75,\n",
       "  33,\n",
       "  224,\n",
       "  14,\n",
       "  1,\n",
       "  275,\n",
       "  148,\n",
       "  135,\n",
       "  110],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  1662,\n",
       "  715,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  227,\n",
       "  2375,\n",
       "  1,\n",
       "  227,\n",
       "  2376,\n",
       "  1008,\n",
       "  15,\n",
       "  1662,\n",
       "  715,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  213,\n",
       "  2377],\n",
       " [215,\n",
       "  481,\n",
       "  1,\n",
       "  92,\n",
       "  553,\n",
       "  152,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  15,\n",
       "  7,\n",
       "  785,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  92,\n",
       "  152,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  172,\n",
       "  93,\n",
       "  67,\n",
       "  153,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  94,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [6,\n",
       "  15,\n",
       "  243,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  53,\n",
       "  14,\n",
       "  7,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  1600,\n",
       "  15,\n",
       "  243,\n",
       "  31,\n",
       "  1,\n",
       "  527,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  53],\n",
       " [30,\n",
       "  73,\n",
       "  13,\n",
       "  109,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  48,\n",
       "  13,\n",
       "  233,\n",
       "  10,\n",
       "  716,\n",
       "  19,\n",
       "  83,\n",
       "  471,\n",
       "  2,\n",
       "  3,\n",
       "  581,\n",
       "  4,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [1,\n",
       "  147,\n",
       "  75,\n",
       "  33,\n",
       "  224,\n",
       "  14,\n",
       "  1,\n",
       "  275,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  11,\n",
       "  1,\n",
       "  275,\n",
       "  140,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  33,\n",
       "  15],\n",
       " [11,\n",
       "  1,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  6,\n",
       "  458,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  223,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  161,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [119,\n",
       "  198,\n",
       "  33,\n",
       "  109,\n",
       "  14,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  119,\n",
       "  198,\n",
       "  13,\n",
       "  109,\n",
       "  10,\n",
       "  1,\n",
       "  1663,\n",
       "  189,\n",
       "  171,\n",
       "  135,\n",
       "  110],\n",
       " [11,\n",
       "  359,\n",
       "  1,\n",
       "  30,\n",
       "  73,\n",
       "  65,\n",
       "  6,\n",
       "  18,\n",
       "  977,\n",
       "  674,\n",
       "  4,\n",
       "  499,\n",
       "  90,\n",
       "  11,\n",
       "  30,\n",
       "  132,\n",
       "  6,\n",
       "  15,\n",
       "  977,\n",
       "  674,\n",
       "  4,\n",
       "  499,\n",
       "  90],\n",
       " [1, 831, 28, 13, 419, 21, 1, 54, 22, 12, 40, 51, 1, 28, 1009, 21, 54, 12, 40],\n",
       " [1,\n",
       "  2378,\n",
       "  22,\n",
       "  13,\n",
       "  54,\n",
       "  717,\n",
       "  12,\n",
       "  40,\n",
       "  1,\n",
       "  831,\n",
       "  28,\n",
       "  13,\n",
       "  419,\n",
       "  21,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [91,\n",
       "  65,\n",
       "  5,\n",
       "  157,\n",
       "  360,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  112,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  95,\n",
       "  72,\n",
       "  979,\n",
       "  142,\n",
       "  546,\n",
       "  23,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  34,\n",
       "  72,\n",
       "  363,\n",
       "  1010,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  114,\n",
       "  127],\n",
       " [223,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  161,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  38,\n",
       "  49,\n",
       "  67,\n",
       "  66,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [54,\n",
       "  94,\n",
       "  12,\n",
       "  40,\n",
       "  98,\n",
       "  13,\n",
       "  7,\n",
       "  22,\n",
       "  5,\n",
       "  85,\n",
       "  306,\n",
       "  8,\n",
       "  1011,\n",
       "  394,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  449,\n",
       "  8,\n",
       "  24,\n",
       "  235,\n",
       "  58,\n",
       "  6,\n",
       "  2379,\n",
       "  275,\n",
       "  283,\n",
       "  572,\n",
       "  4,\n",
       "  18,\n",
       "  1,\n",
       "  280,\n",
       "  144,\n",
       "  9,\n",
       "  607,\n",
       "  24,\n",
       "  35,\n",
       "  49,\n",
       "  423,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  1,\n",
       "  54,\n",
       "  85,\n",
       "  22,\n",
       "  13,\n",
       "  219],\n",
       " [6,\n",
       "  223,\n",
       "  70,\n",
       "  89,\n",
       "  395,\n",
       "  11,\n",
       "  259,\n",
       "  211,\n",
       "  24,\n",
       "  606,\n",
       "  1664,\n",
       "  29,\n",
       "  1,\n",
       "  101,\n",
       "  4,\n",
       "  1,\n",
       "  493,\n",
       "  1665,\n",
       "  49,\n",
       "  10,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  14,\n",
       "  507,\n",
       "  70,\n",
       "  89,\n",
       "  13,\n",
       "  484,\n",
       "  23,\n",
       "  1,\n",
       "  259,\n",
       "  314,\n",
       "  10,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  14,\n",
       "  699,\n",
       "  507,\n",
       "  4,\n",
       "  228,\n",
       "  410],\n",
       " [1,\n",
       "  44,\n",
       "  140,\n",
       "  33,\n",
       "  181,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  101,\n",
       "  385,\n",
       "  104,\n",
       "  417,\n",
       "  26,\n",
       "  1,\n",
       "  17,\n",
       "  38,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  820,\n",
       "  35,\n",
       "  65,\n",
       "  10,\n",
       "  1,\n",
       "  27,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  38,\n",
       "  1209,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  35,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  424,\n",
       "  24,\n",
       "  65],\n",
       " [1,\n",
       "  101,\n",
       "  385,\n",
       "  104,\n",
       "  417,\n",
       "  26,\n",
       "  1,\n",
       "  17,\n",
       "  38,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  24,\n",
       "  38,\n",
       "  58,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  1399,\n",
       "  636,\n",
       "  13,\n",
       "  402,\n",
       "  10,\n",
       "  1012,\n",
       "  1013,\n",
       "  333,\n",
       "  718,\n",
       "  4,\n",
       "  1014,\n",
       "  339,\n",
       "  7,\n",
       "  432,\n",
       "  216,\n",
       "  5,\n",
       "  48,\n",
       "  2380,\n",
       "  13,\n",
       "  1012,\n",
       "  1013,\n",
       "  333,\n",
       "  2381,\n",
       "  718,\n",
       "  4,\n",
       "  1014,\n",
       "  339],\n",
       " [1,\n",
       "  38,\n",
       "  49,\n",
       "  67,\n",
       "  66,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  1015,\n",
       "  32,\n",
       "  637,\n",
       "  10,\n",
       "  7,\n",
       "  27,\n",
       "  29,\n",
       "  66,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  1234,\n",
       "  75],\n",
       " [1,\n",
       "  2382,\n",
       "  198,\n",
       "  129,\n",
       "  123,\n",
       "  414,\n",
       "  14,\n",
       "  1,\n",
       "  189,\n",
       "  171,\n",
       "  135,\n",
       "  110,\n",
       "  119,\n",
       "  198,\n",
       "  13,\n",
       "  109,\n",
       "  10,\n",
       "  1,\n",
       "  1663,\n",
       "  189,\n",
       "  171,\n",
       "  135,\n",
       "  110],\n",
       " [11,\n",
       "  1,\n",
       "  2383,\n",
       "  5,\n",
       "  119,\n",
       "  568,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  1205,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  11,\n",
       "  105,\n",
       "  6,\n",
       "  18,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  14,\n",
       "  41,\n",
       "  1666,\n",
       "  65,\n",
       "  386,\n",
       "  275,\n",
       "  19,\n",
       "  11,\n",
       "  275,\n",
       "  306,\n",
       "  44,\n",
       "  11,\n",
       "  44,\n",
       "  306],\n",
       " [1,\n",
       "  831,\n",
       "  28,\n",
       "  13,\n",
       "  419,\n",
       "  21,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  13,\n",
       "  66,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  449],\n",
       " [51,\n",
       "  361,\n",
       "  67,\n",
       "  233,\n",
       "  10,\n",
       "  1,\n",
       "  266,\n",
       "  556,\n",
       "  164,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  1,\n",
       "  164,\n",
       "  33,\n",
       "  109,\n",
       "  10,\n",
       "  1,\n",
       "  266,\n",
       "  94,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [1,\n",
       "  582,\n",
       "  832,\n",
       "  46,\n",
       "  22,\n",
       "  33,\n",
       "  564,\n",
       "  21,\n",
       "  1,\n",
       "  54,\n",
       "  310,\n",
       "  12,\n",
       "  40,\n",
       "  1,\n",
       "  831,\n",
       "  28,\n",
       "  13,\n",
       "  419,\n",
       "  21,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [6,\n",
       "  15,\n",
       "  429,\n",
       "  180,\n",
       "  84,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  992,\n",
       "  46,\n",
       "  2384,\n",
       "  13,\n",
       "  224,\n",
       "  14,\n",
       "  429,\n",
       "  180,\n",
       "  84,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  583,\n",
       "  1235],\n",
       " [11,\n",
       "  51,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  38,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  2385,\n",
       "  6,\n",
       "  76,\n",
       "  95,\n",
       "  7,\n",
       "  29,\n",
       "  61,\n",
       "  23,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [112,\n",
       "  1236,\n",
       "  28,\n",
       "  456,\n",
       "  95,\n",
       "  123,\n",
       "  1667,\n",
       "  1227,\n",
       "  26,\n",
       "  1016,\n",
       "  4,\n",
       "  638,\n",
       "  165,\n",
       "  68,\n",
       "  1237,\n",
       "  287,\n",
       "  11,\n",
       "  2386,\n",
       "  129,\n",
       "  123,\n",
       "  1667,\n",
       "  1227,\n",
       "  26,\n",
       "  1016,\n",
       "  4,\n",
       "  638,\n",
       "  165],\n",
       " [105,\n",
       "  5,\n",
       "  24,\n",
       "  49,\n",
       "  67,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  17,\n",
       "  161,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  38,\n",
       "  49,\n",
       "  67,\n",
       "  66,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  1,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  6,\n",
       "  458,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  101,\n",
       "  385,\n",
       "  104,\n",
       "  417,\n",
       "  26,\n",
       "  1,\n",
       "  17,\n",
       "  38,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [625,\n",
       "  157,\n",
       "  418,\n",
       "  910,\n",
       "  23,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  42,\n",
       "  1668,\n",
       "  34,\n",
       "  63,\n",
       "  1017,\n",
       "  8,\n",
       "  7,\n",
       "  72,\n",
       "  75,\n",
       "  5,\n",
       "  149,\n",
       "  32,\n",
       "  76,\n",
       "  72,\n",
       "  8,\n",
       "  157,\n",
       "  114,\n",
       "  127,\n",
       "  91,\n",
       "  65,\n",
       "  5,\n",
       "  157,\n",
       "  360,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  112,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  95,\n",
       "  72,\n",
       "  365],\n",
       " [439,\n",
       "  1669,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  97,\n",
       "  68,\n",
       "  345,\n",
       "  78,\n",
       "  103,\n",
       "  11,\n",
       "  374,\n",
       "  1203,\n",
       "  125,\n",
       "  8,\n",
       "  7,\n",
       "  566,\n",
       "  14,\n",
       "  2387,\n",
       "  2388,\n",
       "  2389,\n",
       "  1669,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  97,\n",
       "  7,\n",
       "  1670,\n",
       "  345,\n",
       "  128,\n",
       "  11,\n",
       "  1203,\n",
       "  2390,\n",
       "  340,\n",
       "  2391,\n",
       "  14,\n",
       "  762,\n",
       "  133,\n",
       "  1671,\n",
       "  1018,\n",
       "  833,\n",
       "  903,\n",
       "  2392],\n",
       " [7,\n",
       "  91,\n",
       "  142,\n",
       "  19,\n",
       "  13,\n",
       "  823,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  2393,\n",
       "  72,\n",
       "  979,\n",
       "  65,\n",
       "  5,\n",
       "  157,\n",
       "  360,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  112,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  95,\n",
       "  72,\n",
       "  365],\n",
       " [48,\n",
       "  263,\n",
       "  1238,\n",
       "  1,\n",
       "  673,\n",
       "  5,\n",
       "  24,\n",
       "  29,\n",
       "  34,\n",
       "  631,\n",
       "  8,\n",
       "  1,\n",
       "  573,\n",
       "  7,\n",
       "  5,\n",
       "  834,\n",
       "  62,\n",
       "  835,\n",
       "  195,\n",
       "  115,\n",
       "  8,\n",
       "  465,\n",
       "  639,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  8,\n",
       "  1,\n",
       "  280,\n",
       "  6,\n",
       "  385,\n",
       "  719,\n",
       "  1,\n",
       "  29,\n",
       "  14,\n",
       "  42,\n",
       "  6,\n",
       "  631,\n",
       "  8,\n",
       "  1,\n",
       "  1239,\n",
       "  2394,\n",
       "  215,\n",
       "  573,\n",
       "  5,\n",
       "  195,\n",
       "  115,\n",
       "  8,\n",
       "  465,\n",
       "  62,\n",
       "  835,\n",
       "  5,\n",
       "  565,\n",
       "  53,\n",
       "  639,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  27,\n",
       "  19,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  1,\n",
       "  58,\n",
       "  14,\n",
       "  51,\n",
       "  1,\n",
       "  60,\n",
       "  302,\n",
       "  539,\n",
       "  7,\n",
       "  375,\n",
       "  299,\n",
       "  19,\n",
       "  4,\n",
       "  7,\n",
       "  100,\n",
       "  41,\n",
       "  2395,\n",
       "  33,\n",
       "  66,\n",
       "  14,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  10,\n",
       "  1,\n",
       "  2396,\n",
       "  60,\n",
       "  720,\n",
       "  93,\n",
       "  539,\n",
       "  7,\n",
       "  100,\n",
       "  41,\n",
       "  19],\n",
       " [1240,\n",
       "  1241,\n",
       "  146,\n",
       "  584,\n",
       "  13,\n",
       "  7,\n",
       "  603,\n",
       "  671,\n",
       "  34,\n",
       "  887,\n",
       "  105,\n",
       "  1,\n",
       "  1019,\n",
       "  4,\n",
       "  379,\n",
       "  5,\n",
       "  41,\n",
       "  1020,\n",
       "  197,\n",
       "  1240,\n",
       "  1241,\n",
       "  146,\n",
       "  584,\n",
       "  1020,\n",
       "  197,\n",
       "  13,\n",
       "  7,\n",
       "  375,\n",
       "  146,\n",
       "  671,\n",
       "  34,\n",
       "  129,\n",
       "  123,\n",
       "  15,\n",
       "  11,\n",
       "  105,\n",
       "  2397,\n",
       "  2398,\n",
       "  389,\n",
       "  227,\n",
       "  4,\n",
       "  130,\n",
       "  227],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  256,\n",
       "  1672,\n",
       "  81,\n",
       "  5,\n",
       "  92,\n",
       "  20,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  14,\n",
       "  326,\n",
       "  2399,\n",
       "  15,\n",
       "  1,\n",
       "  226,\n",
       "  99,\n",
       "  81,\n",
       "  14,\n",
       "  107,\n",
       "  106,\n",
       "  4,\n",
       "  256,\n",
       "  416,\n",
       "  14,\n",
       "  326,\n",
       "  200,\n",
       "  105,\n",
       "  169,\n",
       "  190,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [1,\n",
       "  97,\n",
       "  19,\n",
       "  1673,\n",
       "  1,\n",
       "  701,\n",
       "  330,\n",
       "  5,\n",
       "  1674,\n",
       "  2,\n",
       "  3,\n",
       "  43,\n",
       "  1,\n",
       "  1675,\n",
       "  5,\n",
       "  1674,\n",
       "  2,\n",
       "  3,\n",
       "  43,\n",
       "  1021,\n",
       "  701,\n",
       "  14,\n",
       "  2400,\n",
       "  4,\n",
       "  1184,\n",
       "  836,\n",
       "  65,\n",
       "  10,\n",
       "  1,\n",
       "  818,\n",
       "  5,\n",
       "  1,\n",
       "  19],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  46,\n",
       "  185,\n",
       "  5,\n",
       "  1,\n",
       "  183,\n",
       "  21,\n",
       "  1676,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  1676,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  176,\n",
       "  7,\n",
       "  183,\n",
       "  5,\n",
       "  828,\n",
       "  616,\n",
       "  1242,\n",
       "  1141,\n",
       "  2401,\n",
       "  5,\n",
       "  616,\n",
       "  1155,\n",
       "  21,\n",
       "  145,\n",
       "  709,\n",
       "  2402,\n",
       "  2403,\n",
       "  2404,\n",
       "  117],\n",
       " [100,\n",
       "  41,\n",
       "  65,\n",
       "  5,\n",
       "  1650,\n",
       "  4,\n",
       "  44,\n",
       "  67,\n",
       "  50,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  8,\n",
       "  269,\n",
       "  9,\n",
       "  337,\n",
       "  1,\n",
       "  1677,\n",
       "  5,\n",
       "  111,\n",
       "  29,\n",
       "  6,\n",
       "  69,\n",
       "  100,\n",
       "  41,\n",
       "  65,\n",
       "  11,\n",
       "  111,\n",
       "  41,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25],\n",
       " [1,\n",
       "  144,\n",
       "  32,\n",
       "  118,\n",
       "  181,\n",
       "  4,\n",
       "  721,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  229,\n",
       "  192,\n",
       "  182,\n",
       "  4,\n",
       "  224,\n",
       "  26,\n",
       "  186,\n",
       "  119,\n",
       "  10,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  1,\n",
       "  22,\n",
       "  33,\n",
       "  192,\n",
       "  374,\n",
       "  224,\n",
       "  14,\n",
       "  186,\n",
       "  333,\n",
       "  10,\n",
       "  148,\n",
       "  135,\n",
       "  110],\n",
       " [1,\n",
       "  70,\n",
       "  89,\n",
       "  147,\n",
       "  13,\n",
       "  76,\n",
       "  196,\n",
       "  179,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  14,\n",
       "  228,\n",
       "  488,\n",
       "  297,\n",
       "  12,\n",
       "  36,\n",
       "  1,\n",
       "  722,\n",
       "  13,\n",
       "  640,\n",
       "  454,\n",
       "  670,\n",
       "  9,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  147,\n",
       "  12,\n",
       "  36],\n",
       " [1,\n",
       "  1678,\n",
       "  434,\n",
       "  837,\n",
       "  13,\n",
       "  68,\n",
       "  99,\n",
       "  434,\n",
       "  19,\n",
       "  175,\n",
       "  14,\n",
       "  1243,\n",
       "  7,\n",
       "  158,\n",
       "  168,\n",
       "  1679,\n",
       "  78,\n",
       "  122,\n",
       "  295,\n",
       "  37,\n",
       "  1,\n",
       "  1222,\n",
       "  13,\n",
       "  175,\n",
       "  31,\n",
       "  7,\n",
       "  434,\n",
       "  837,\n",
       "  50,\n",
       "  14,\n",
       "  1243,\n",
       "  295,\n",
       "  37],\n",
       " [1,\n",
       "  70,\n",
       "  89,\n",
       "  147,\n",
       "  13,\n",
       "  76,\n",
       "  196,\n",
       "  179,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  14,\n",
       "  228,\n",
       "  488,\n",
       "  297,\n",
       "  12,\n",
       "  36,\n",
       "  7,\n",
       "  70,\n",
       "  89,\n",
       "  147,\n",
       "  33,\n",
       "  109,\n",
       "  10,\n",
       "  1,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  12,\n",
       "  36],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  272,\n",
       "  20,\n",
       "  1680,\n",
       "  4,\n",
       "  273,\n",
       "  87,\n",
       "  9,\n",
       "  219,\n",
       "  1,\n",
       "  1244,\n",
       "  2405,\n",
       "  21,\n",
       "  111,\n",
       "  1681,\n",
       "  18,\n",
       "  1,\n",
       "  2406,\n",
       "  136,\n",
       "  2407,\n",
       "  21,\n",
       "  272,\n",
       "  1680,\n",
       "  4,\n",
       "  273,\n",
       "  87,\n",
       "  9,\n",
       "  262,\n",
       "  105,\n",
       "  136,\n",
       "  4,\n",
       "  30,\n",
       "  575,\n",
       "  23,\n",
       "  111,\n",
       "  133,\n",
       "  1245],\n",
       " [6,\n",
       "  2408,\n",
       "  1,\n",
       "  261,\n",
       "  1246,\n",
       "  9,\n",
       "  42,\n",
       "  111,\n",
       "  209,\n",
       "  2409,\n",
       "  2410,\n",
       "  13,\n",
       "  255,\n",
       "  585,\n",
       "  14,\n",
       "  1,\n",
       "  82,\n",
       "  5,\n",
       "  1682,\n",
       "  2,\n",
       "  3,\n",
       "  201,\n",
       "  6,\n",
       "  681,\n",
       "  48,\n",
       "  82,\n",
       "  586,\n",
       "  68,\n",
       "  1022,\n",
       "  1023,\n",
       "  259,\n",
       "  2132,\n",
       "  42,\n",
       "  13,\n",
       "  641,\n",
       "  14,\n",
       "  1,\n",
       "  2411,\n",
       "  8,\n",
       "  1682,\n",
       "  2,\n",
       "  3,\n",
       "  201],\n",
       " [6,\n",
       "  910,\n",
       "  23,\n",
       "  1,\n",
       "  1247,\n",
       "  585,\n",
       "  257,\n",
       "  130,\n",
       "  251,\n",
       "  97,\n",
       "  26,\n",
       "  1683,\n",
       "  2,\n",
       "  3,\n",
       "  1684,\n",
       "  9,\n",
       "  262,\n",
       "  2412,\n",
       "  709,\n",
       "  1,\n",
       "  2413,\n",
       "  5,\n",
       "  48,\n",
       "  263,\n",
       "  13,\n",
       "  7,\n",
       "  300,\n",
       "  345,\n",
       "  2414,\n",
       "  128,\n",
       "  9,\n",
       "  1685,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  1247,\n",
       "  585,\n",
       "  251,\n",
       "  2415,\n",
       "  587,\n",
       "  26,\n",
       "  1683,\n",
       "  2,\n",
       "  3,\n",
       "  1684],\n",
       " [9,\n",
       "  1157,\n",
       "  554,\n",
       "  1,\n",
       "  333,\n",
       "  303,\n",
       "  1248,\n",
       "  7,\n",
       "  22,\n",
       "  618,\n",
       "  1686,\n",
       "  129,\n",
       "  123,\n",
       "  247,\n",
       "  11,\n",
       "  1024,\n",
       "  1249,\n",
       "  4,\n",
       "  124,\n",
       "  603,\n",
       "  1687,\n",
       "  8,\n",
       "  1250,\n",
       "  306,\n",
       "  1688,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  1,\n",
       "  1686,\n",
       "  22,\n",
       "  13,\n",
       "  7,\n",
       "  447,\n",
       "  247,\n",
       "  22,\n",
       "  11,\n",
       "  1024,\n",
       "  4,\n",
       "  1249,\n",
       "  1689,\n",
       "  838,\n",
       "  472,\n",
       "  4,\n",
       "  248,\n",
       "  603,\n",
       "  1687,\n",
       "  136,\n",
       "  472,\n",
       "  1688,\n",
       "  2,\n",
       "  3,\n",
       "  90],\n",
       " [1,\n",
       "  452,\n",
       "  5,\n",
       "  826,\n",
       "  1251,\n",
       "  1221,\n",
       "  8,\n",
       "  1,\n",
       "  2416,\n",
       "  1252,\n",
       "  13,\n",
       "  2417,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  39,\n",
       "  2418,\n",
       "  2419,\n",
       "  2420,\n",
       "  29,\n",
       "  588,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  6,\n",
       "  396,\n",
       "  1,\n",
       "  39,\n",
       "  826,\n",
       "  1251,\n",
       "  29,\n",
       "  588,\n",
       "  2,\n",
       "  3,\n",
       "  88],\n",
       " [6,\n",
       "  232,\n",
       "  24,\n",
       "  58,\n",
       "  23,\n",
       "  54,\n",
       "  12,\n",
       "  40,\n",
       "  7,\n",
       "  307,\n",
       "  85,\n",
       "  22,\n",
       "  174,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  2421,\n",
       "  94,\n",
       "  12,\n",
       "  40,\n",
       "  98,\n",
       "  13,\n",
       "  7,\n",
       "  22,\n",
       "  5,\n",
       "  85,\n",
       "  306,\n",
       "  8,\n",
       "  1011,\n",
       "  394,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  449],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  81,\n",
       "  5,\n",
       "  172,\n",
       "  163,\n",
       "  14,\n",
       "  248,\n",
       "  107,\n",
       "  435,\n",
       "  302,\n",
       "  11,\n",
       "  24,\n",
       "  610,\n",
       "  15,\n",
       "  7,\n",
       "  785,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  92,\n",
       "  152,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  81,\n",
       "  5,\n",
       "  172,\n",
       "  163,\n",
       "  14,\n",
       "  248,\n",
       "  107,\n",
       "  435,\n",
       "  302,\n",
       "  11,\n",
       "  24,\n",
       "  610,\n",
       "  15,\n",
       "  1,\n",
       "  92,\n",
       "  20,\n",
       "  9,\n",
       "  69,\n",
       "  24,\n",
       "  163,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [355,\n",
       "  1253,\n",
       "  436,\n",
       "  1690,\n",
       "  779,\n",
       "  190,\n",
       "  970,\n",
       "  1,\n",
       "  1691,\n",
       "  333,\n",
       "  21,\n",
       "  1025,\n",
       "  358,\n",
       "  23,\n",
       "  1,\n",
       "  139,\n",
       "  996,\n",
       "  9,\n",
       "  723,\n",
       "  7,\n",
       "  1692,\n",
       "  915,\n",
       "  156,\n",
       "  355,\n",
       "  133,\n",
       "  436,\n",
       "  779,\n",
       "  9,\n",
       "  374,\n",
       "  723,\n",
       "  7,\n",
       "  799,\n",
       "  4,\n",
       "  2422,\n",
       "  1692,\n",
       "  5,\n",
       "  1254,\n",
       "  294,\n",
       "  1025,\n",
       "  358,\n",
       "  915,\n",
       "  156],\n",
       " [589,\n",
       "  6,\n",
       "  69,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  65,\n",
       "  14,\n",
       "  24,\n",
       "  132,\n",
       "  10,\n",
       "  1,\n",
       "  432,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  24,\n",
       "  38,\n",
       "  58,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  81,\n",
       "  5,\n",
       "  172,\n",
       "  163,\n",
       "  14,\n",
       "  248,\n",
       "  107,\n",
       "  435,\n",
       "  302,\n",
       "  11,\n",
       "  24,\n",
       "  610,\n",
       "  15,\n",
       "  1,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  81,\n",
       "  5,\n",
       "  1693,\n",
       "  4,\n",
       "  1,\n",
       "  1694,\n",
       "  20],\n",
       " [589,\n",
       "  6,\n",
       "  69,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  65,\n",
       "  14,\n",
       "  24,\n",
       "  132,\n",
       "  10,\n",
       "  1,\n",
       "  432,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  1,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  6,\n",
       "  458,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  69,\n",
       "  163,\n",
       "  11,\n",
       "  111,\n",
       "  5,\n",
       "  1,\n",
       "  642,\n",
       "  250,\n",
       "  354,\n",
       "  4,\n",
       "  11,\n",
       "  1,\n",
       "  583,\n",
       "  250,\n",
       "  75,\n",
       "  23,\n",
       "  1,\n",
       "  46,\n",
       "  75,\n",
       "  5,\n",
       "  111,\n",
       "  22,\n",
       "  10,\n",
       "  1,\n",
       "  107,\n",
       "  580,\n",
       "  5,\n",
       "  1,\n",
       "  366,\n",
       "  312,\n",
       "  81,\n",
       "  5,\n",
       "  202,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  6,\n",
       "  69,\n",
       "  256,\n",
       "  416,\n",
       "  163,\n",
       "  1255,\n",
       "  156,\n",
       "  10,\n",
       "  202,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  11,\n",
       "  111,\n",
       "  1695,\n",
       "  4,\n",
       "  76,\n",
       "  11,\n",
       "  1,\n",
       "  412,\n",
       "  19],\n",
       " [8,\n",
       "  335,\n",
       "  1,\n",
       "  22,\n",
       "  33,\n",
       "  473,\n",
       "  10,\n",
       "  1,\n",
       "  148,\n",
       "  1256,\n",
       "  135,\n",
       "  110,\n",
       "  83,\n",
       "  1,\n",
       "  22,\n",
       "  13,\n",
       "  473,\n",
       "  4,\n",
       "  224,\n",
       "  26,\n",
       "  186,\n",
       "  23,\n",
       "  105,\n",
       "  1026,\n",
       "  10,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110],\n",
       " [328,\n",
       "  32,\n",
       "  61,\n",
       "  23,\n",
       "  91,\n",
       "  113,\n",
       "  42,\n",
       "  1257,\n",
       "  643,\n",
       "  1,\n",
       "  1258,\n",
       "  34,\n",
       "  72,\n",
       "  63,\n",
       "  194,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  114,\n",
       "  127,\n",
       "  91,\n",
       "  65,\n",
       "  5,\n",
       "  157,\n",
       "  360,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  112,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  95,\n",
       "  72,\n",
       "  365],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  139,\n",
       "  147,\n",
       "  75,\n",
       "  15,\n",
       "  8,\n",
       "  448,\n",
       "  2,\n",
       "  3,\n",
       "  36,\n",
       "  11,\n",
       "  24,\n",
       "  281,\n",
       "  170,\n",
       "  241,\n",
       "  6,\n",
       "  1178,\n",
       "  459,\n",
       "  1540,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  15,\n",
       "  8,\n",
       "  448,\n",
       "  2,\n",
       "  3,\n",
       "  36,\n",
       "  31,\n",
       "  24,\n",
       "  46,\n",
       "  28,\n",
       "  4,\n",
       "  1,\n",
       "  1541,\n",
       "  31,\n",
       "  1,\n",
       "  422,\n",
       "  28,\n",
       "  31,\n",
       "  268,\n",
       "  8,\n",
       "  193,\n",
       "  170],\n",
       " [6,\n",
       "  820,\n",
       "  35,\n",
       "  65,\n",
       "  10,\n",
       "  1,\n",
       "  27,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  38,\n",
       "  2423,\n",
       "  6,\n",
       "  69,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  65,\n",
       "  14,\n",
       "  24,\n",
       "  132,\n",
       "  10,\n",
       "  1,\n",
       "  432,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  15,\n",
       "  1027,\n",
       "  93,\n",
       "  61,\n",
       "  23,\n",
       "  1028,\n",
       "  2,\n",
       "  3,\n",
       "  40,\n",
       "  11,\n",
       "  231,\n",
       "  1028,\n",
       "  2,\n",
       "  3,\n",
       "  40,\n",
       "  2133,\n",
       "  1,\n",
       "  18,\n",
       "  5,\n",
       "  1027,\n",
       "  93,\n",
       "  26,\n",
       "  10,\n",
       "  600,\n",
       "  378],\n",
       " [11,\n",
       "  359,\n",
       "  1,\n",
       "  101,\n",
       "  38,\n",
       "  29,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  177,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  8,\n",
       "  124,\n",
       "  60,\n",
       "  929,\n",
       "  51,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  38,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  35,\n",
       "  6,\n",
       "  18,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  14,\n",
       "  375,\n",
       "  299,\n",
       "  1695,\n",
       "  4,\n",
       "  1,\n",
       "  97,\n",
       "  19,\n",
       "  14,\n",
       "  441,\n",
       "  1259,\n",
       "  2424,\n",
       "  24,\n",
       "  38,\n",
       "  58,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [9,\n",
       "  909,\n",
       "  2425,\n",
       "  724,\n",
       "  6,\n",
       "  644,\n",
       "  7,\n",
       "  501,\n",
       "  5,\n",
       "  991,\n",
       "  724,\n",
       "  420,\n",
       "  8,\n",
       "  1,\n",
       "  217,\n",
       "  137,\n",
       "  187,\n",
       "  336,\n",
       "  2,\n",
       "  3,\n",
       "  2426,\n",
       "  8,\n",
       "  335,\n",
       "  6,\n",
       "  508,\n",
       "  34,\n",
       "  1,\n",
       "  441,\n",
       "  287,\n",
       "  2427,\n",
       "  350,\n",
       "  14,\n",
       "  1,\n",
       "  2428,\n",
       "  5,\n",
       "  137,\n",
       "  724,\n",
       "  8,\n",
       "  1,\n",
       "  217,\n",
       "  137,\n",
       "  187,\n",
       "  336,\n",
       "  2,\n",
       "  3,\n",
       "  90],\n",
       " [9,\n",
       "  644,\n",
       "  41,\n",
       "  65,\n",
       "  4,\n",
       "  216,\n",
       "  818,\n",
       "  6,\n",
       "  18,\n",
       "  614,\n",
       "  615,\n",
       "  87,\n",
       "  14,\n",
       "  1696,\n",
       "  155,\n",
       "  116,\n",
       "  839,\n",
       "  342,\n",
       "  4,\n",
       "  474,\n",
       "  197,\n",
       "  4,\n",
       "  14,\n",
       "  7,\n",
       "  725,\n",
       "  1029,\n",
       "  105,\n",
       "  41,\n",
       "  65,\n",
       "  18,\n",
       "  155,\n",
       "  116,\n",
       "  173,\n",
       "  342,\n",
       "  4,\n",
       "  474,\n",
       "  197],\n",
       " [6,\n",
       "  18,\n",
       "  600,\n",
       "  378,\n",
       "  9,\n",
       "  707,\n",
       "  1,\n",
       "  1697,\n",
       "  5,\n",
       "  699,\n",
       "  4,\n",
       "  840,\n",
       "  2429,\n",
       "  179,\n",
       "  1,\n",
       "  1260,\n",
       "  106,\n",
       "  922,\n",
       "  4,\n",
       "  923,\n",
       "  36,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  600,\n",
       "  378,\n",
       "  61,\n",
       "  701,\n",
       "  922,\n",
       "  4,\n",
       "  923,\n",
       "  36],\n",
       " [2430,\n",
       "  13,\n",
       "  318,\n",
       "  7,\n",
       "  2431,\n",
       "  774,\n",
       "  21,\n",
       "  1,\n",
       "  292,\n",
       "  999,\n",
       "  31,\n",
       "  13,\n",
       "  11,\n",
       "  231,\n",
       "  1,\n",
       "  536,\n",
       "  774,\n",
       "  536,\n",
       "  36,\n",
       "  83,\n",
       "  292,\n",
       "  774,\n",
       "  13,\n",
       "  509,\n",
       "  8,\n",
       "  1,\n",
       "  536,\n",
       "  1030,\n",
       "  536,\n",
       "  36],\n",
       " [9,\n",
       "  644,\n",
       "  41,\n",
       "  65,\n",
       "  4,\n",
       "  216,\n",
       "  818,\n",
       "  6,\n",
       "  18,\n",
       "  614,\n",
       "  615,\n",
       "  87,\n",
       "  14,\n",
       "  1696,\n",
       "  155,\n",
       "  116,\n",
       "  839,\n",
       "  342,\n",
       "  4,\n",
       "  474,\n",
       "  197,\n",
       "  4,\n",
       "  14,\n",
       "  7,\n",
       "  725,\n",
       "  1029,\n",
       "  105,\n",
       "  41,\n",
       "  65,\n",
       "  18,\n",
       "  155,\n",
       "  116,\n",
       "  173,\n",
       "  342,\n",
       "  4,\n",
       "  474,\n",
       "  197],\n",
       " [6,\n",
       "  121,\n",
       "  672,\n",
       "  24,\n",
       "  323,\n",
       "  526,\n",
       "  897,\n",
       "  61,\n",
       "  128,\n",
       "  11,\n",
       "  412,\n",
       "  324,\n",
       "  221,\n",
       "  4,\n",
       "  322,\n",
       "  443,\n",
       "  4,\n",
       "  438,\n",
       "  64,\n",
       "  1207,\n",
       "  1616,\n",
       "  24,\n",
       "  323,\n",
       "  1208,\n",
       "  128,\n",
       "  11,\n",
       "  324,\n",
       "  221,\n",
       "  443,\n",
       "  4,\n",
       "  438,\n",
       "  64],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  2432,\n",
       "  238,\n",
       "  894,\n",
       "  41,\n",
       "  19,\n",
       "  401,\n",
       "  5,\n",
       "  1698,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  31,\n",
       "  268,\n",
       "  8,\n",
       "  426,\n",
       "  253,\n",
       "  6,\n",
       "  360,\n",
       "  1,\n",
       "  238,\n",
       "  309,\n",
       "  401,\n",
       "  5,\n",
       "  1698,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  10,\n",
       "  112,\n",
       "  529,\n",
       "  2433,\n",
       "  5,\n",
       "  2434,\n",
       "  226,\n",
       "  2435],\n",
       " [1,\n",
       "  38,\n",
       "  49,\n",
       "  67,\n",
       "  66,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  50,\n",
       "  7,\n",
       "  212,\n",
       "  5,\n",
       "  841,\n",
       "  38,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  8,\n",
       "  124,\n",
       "  107,\n",
       "  726],\n",
       " [11,\n",
       "  359,\n",
       "  1,\n",
       "  101,\n",
       "  38,\n",
       "  29,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  177,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  8,\n",
       "  124,\n",
       "  60,\n",
       "  929,\n",
       "  24,\n",
       "  38,\n",
       "  58,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [368,\n",
       "  38,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  85,\n",
       "  22,\n",
       "  185,\n",
       "  131,\n",
       "  9,\n",
       "  69,\n",
       "  68,\n",
       "  506,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  38,\n",
       "  491,\n",
       "  7,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  66,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  4,\n",
       "  1,\n",
       "  958,\n",
       "  582,\n",
       "  183,\n",
       "  590,\n",
       "  1,\n",
       "  125,\n",
       "  8,\n",
       "  1,\n",
       "  147,\n",
       "  75,\n",
       "  33,\n",
       "  15,\n",
       "  9,\n",
       "  637,\n",
       "  1],\n",
       " [280,\n",
       "  251,\n",
       "  310,\n",
       "  4,\n",
       "  842,\n",
       "  7,\n",
       "  38,\n",
       "  19,\n",
       "  11,\n",
       "  2436,\n",
       "  33,\n",
       "  50,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  10,\n",
       "  124,\n",
       "  101,\n",
       "  1699,\n",
       "  50,\n",
       "  7,\n",
       "  19,\n",
       "  10,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  23,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  31,\n",
       "  24,\n",
       "  101,\n",
       "  29],\n",
       " [145,\n",
       "  1261,\n",
       "  128,\n",
       "  9,\n",
       "  143,\n",
       "  13,\n",
       "  317,\n",
       "  134,\n",
       "  4,\n",
       "  510,\n",
       "  43,\n",
       "  42,\n",
       "  13,\n",
       "  1700,\n",
       "  61,\n",
       "  23,\n",
       "  1031,\n",
       "  815,\n",
       "  211,\n",
       "  355,\n",
       "  4,\n",
       "  352,\n",
       "  2437,\n",
       "  82,\n",
       "  317,\n",
       "  134,\n",
       "  4,\n",
       "  510,\n",
       "  43,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  1031,\n",
       "  2438,\n",
       "  211,\n",
       "  1,\n",
       "  2439,\n",
       "  4,\n",
       "  2440,\n",
       "  1032],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  207,\n",
       "  313,\n",
       "  100,\n",
       "  22,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  9,\n",
       "  981,\n",
       "  1,\n",
       "  41,\n",
       "  19,\n",
       "  222,\n",
       "  11,\n",
       "  7,\n",
       "  1681,\n",
       "  15,\n",
       "  1,\n",
       "  836,\n",
       "  450,\n",
       "  21,\n",
       "  1,\n",
       "  207,\n",
       "  313,\n",
       "  100,\n",
       "  22,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  9,\n",
       "  535,\n",
       "  1,\n",
       "  467,\n",
       "  5,\n",
       "  18,\n",
       "  5,\n",
       "  111,\n",
       "  624,\n",
       "  843],\n",
       " [1,\n",
       "  2441,\n",
       "  1701,\n",
       "  61,\n",
       "  195,\n",
       "  115,\n",
       "  62,\n",
       "  13,\n",
       "  7,\n",
       "  2442,\n",
       "  5,\n",
       "  834,\n",
       "  62,\n",
       "  253,\n",
       "  1702,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  48,\n",
       "  263,\n",
       "  1238,\n",
       "  1,\n",
       "  128,\n",
       "  5,\n",
       "  1,\n",
       "  130,\n",
       "  2443,\n",
       "  511,\n",
       "  2444,\n",
       "  9,\n",
       "  62,\n",
       "  253,\n",
       "  5,\n",
       "  565,\n",
       "  53,\n",
       "  1701,\n",
       "  61,\n",
       "  195,\n",
       "  115,\n",
       "  1702,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [1,\n",
       "  57,\n",
       "  2445,\n",
       "  483,\n",
       "  159,\n",
       "  62,\n",
       "  278,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  2446,\n",
       "  190,\n",
       "  430,\n",
       "  2447,\n",
       "  319,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  477,\n",
       "  354,\n",
       "  33,\n",
       "  2448,\n",
       "  2449,\n",
       "  1262,\n",
       "  2450,\n",
       "  57,\n",
       "  159,\n",
       "  62,\n",
       "  2451,\n",
       "  23,\n",
       "  477,\n",
       "  303,\n",
       "  278,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  1263,\n",
       "  23,\n",
       "  477,\n",
       "  354,\n",
       "  5,\n",
       "  2452,\n",
       "  973],\n",
       " [1,\n",
       "  522,\n",
       "  308,\n",
       "  19,\n",
       "  1033,\n",
       "  11,\n",
       "  1264,\n",
       "  1265,\n",
       "  276,\n",
       "  1034,\n",
       "  4,\n",
       "  86,\n",
       "  87,\n",
       "  33,\n",
       "  1,\n",
       "  118,\n",
       "  345,\n",
       "  128,\n",
       "  9,\n",
       "  2453,\n",
       "  7,\n",
       "  2454,\n",
       "  101,\n",
       "  1,\n",
       "  1033,\n",
       "  13,\n",
       "  7,\n",
       "  806,\n",
       "  19,\n",
       "  11,\n",
       "  1,\n",
       "  345,\n",
       "  1703,\n",
       "  5,\n",
       "  1266,\n",
       "  1265,\n",
       "  276,\n",
       "  206,\n",
       "  645,\n",
       "  5,\n",
       "  186,\n",
       "  119,\n",
       "  568,\n",
       "  1034,\n",
       "  4,\n",
       "  86,\n",
       "  87],\n",
       " [1,\n",
       "  1033,\n",
       "  13,\n",
       "  7,\n",
       "  806,\n",
       "  19,\n",
       "  11,\n",
       "  1,\n",
       "  345,\n",
       "  1703,\n",
       "  5,\n",
       "  1266,\n",
       "  1265,\n",
       "  276,\n",
       "  206,\n",
       "  645,\n",
       "  5,\n",
       "  186,\n",
       "  119,\n",
       "  568,\n",
       "  1034,\n",
       "  4,\n",
       "  86,\n",
       "  87,\n",
       "  1,\n",
       "  666,\n",
       "  5,\n",
       "  1704,\n",
       "  7,\n",
       "  522,\n",
       "  26,\n",
       "  124,\n",
       "  2455,\n",
       "  4,\n",
       "  7,\n",
       "  332,\n",
       "  784,\n",
       "  5,\n",
       "  308,\n",
       "  13,\n",
       "  15,\n",
       "  26,\n",
       "  1,\n",
       "  1033,\n",
       "  345,\n",
       "  227,\n",
       "  19,\n",
       "  1034,\n",
       "  4,\n",
       "  86,\n",
       "  87],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  81,\n",
       "  176,\n",
       "  26,\n",
       "  1705,\n",
       "  2,\n",
       "  3,\n",
       "  201,\n",
       "  2456,\n",
       "  254,\n",
       "  1,\n",
       "  55,\n",
       "  276,\n",
       "  34,\n",
       "  32,\n",
       "  983,\n",
       "  9,\n",
       "  248,\n",
       "  1706,\n",
       "  18,\n",
       "  1,\n",
       "  55,\n",
       "  189,\n",
       "  684,\n",
       "  2457,\n",
       "  687,\n",
       "  309,\n",
       "  2458,\n",
       "  97,\n",
       "  26,\n",
       "  1705,\n",
       "  2,\n",
       "  3,\n",
       "  201,\n",
       "  2459,\n",
       "  2460,\n",
       "  248,\n",
       "  107,\n",
       "  55,\n",
       "  52,\n",
       "  14,\n",
       "  24,\n",
       "  213,\n",
       "  34,\n",
       "  1707,\n",
       "  1708,\n",
       "  63],\n",
       " [54,\n",
       "  12,\n",
       "  40,\n",
       "  13,\n",
       "  7,\n",
       "  307,\n",
       "  85,\n",
       "  22,\n",
       "  174,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  541,\n",
       "  18,\n",
       "  1,\n",
       "  275,\n",
       "  44,\n",
       "  85,\n",
       "  22,\n",
       "  844,\n",
       "  533,\n",
       "  480,\n",
       "  125,\n",
       "  21,\n",
       "  1,\n",
       "  22,\n",
       "  5,\n",
       "  203,\n",
       "  845,\n",
       "  208,\n",
       "  12,\n",
       "  40,\n",
       "  31,\n",
       "  1,\n",
       "  28,\n",
       "  23,\n",
       "  42,\n",
       "  1267,\n",
       "  13,\n",
       "  109,\n",
       "  9,\n",
       "  133],\n",
       " [1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  430,\n",
       "  93,\n",
       "  21,\n",
       "  1,\n",
       "  55,\n",
       "  160,\n",
       "  727,\n",
       "  15,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  160,\n",
       "  1,\n",
       "  125,\n",
       "  34,\n",
       "  1154,\n",
       "  7,\n",
       "  624,\n",
       "  1024,\n",
       "  1709,\n",
       "  4,\n",
       "  174,\n",
       "  1,\n",
       "  280,\n",
       "  93,\n",
       "  21,\n",
       "  1,\n",
       "  55,\n",
       "  160,\n",
       "  200],\n",
       " [1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  430,\n",
       "  93,\n",
       "  21,\n",
       "  1,\n",
       "  55,\n",
       "  160,\n",
       "  727,\n",
       "  15,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  160,\n",
       "  1,\n",
       "  125,\n",
       "  34,\n",
       "  1154,\n",
       "  7,\n",
       "  624,\n",
       "  1024,\n",
       "  1709,\n",
       "  4,\n",
       "  174,\n",
       "  1,\n",
       "  280,\n",
       "  93,\n",
       "  21,\n",
       "  1,\n",
       "  55,\n",
       "  160,\n",
       "  200],\n",
       " [1,\n",
       "  1268,\n",
       "  236,\n",
       "  5,\n",
       "  28,\n",
       "  622,\n",
       "  21,\n",
       "  846,\n",
       "  4,\n",
       "  1269,\n",
       "  583,\n",
       "  1270,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  28,\n",
       "  75,\n",
       "  12,\n",
       "  40,\n",
       "  1,\n",
       "  28,\n",
       "  15,\n",
       "  11,\n",
       "  1,\n",
       "  58,\n",
       "  138,\n",
       "  8,\n",
       "  48,\n",
       "  263,\n",
       "  622,\n",
       "  1710,\n",
       "  21,\n",
       "  1711,\n",
       "  491,\n",
       "  512,\n",
       "  4,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  5,\n",
       "  203,\n",
       "  845,\n",
       "  208,\n",
       "  12,\n",
       "  40],\n",
       " [6,\n",
       "  225,\n",
       "  1,\n",
       "  97,\n",
       "  19,\n",
       "  9,\n",
       "  24,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  1712,\n",
       "  19,\n",
       "  138,\n",
       "  8,\n",
       "  646,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  214,\n",
       "  9,\n",
       "  1713,\n",
       "  198,\n",
       "  1,\n",
       "  19,\n",
       "  401,\n",
       "  268,\n",
       "  8,\n",
       "  426,\n",
       "  83,\n",
       "  13,\n",
       "  7,\n",
       "  2461,\n",
       "  647,\n",
       "  5,\n",
       "  1,\n",
       "  1271,\n",
       "  401,\n",
       "  5,\n",
       "  646,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [98,\n",
       "  13,\n",
       "  7,\n",
       "  27,\n",
       "  29,\n",
       "  66,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  4,\n",
       "  2462,\n",
       "  10,\n",
       "  254,\n",
       "  1,\n",
       "  442,\n",
       "  400,\n",
       "  721,\n",
       "  85,\n",
       "  28,\n",
       "  176,\n",
       "  11,\n",
       "  1,\n",
       "  159,\n",
       "  2463,\n",
       "  491,\n",
       "  7,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  66,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  4,\n",
       "  1,\n",
       "  958,\n",
       "  582,\n",
       "  183,\n",
       "  590,\n",
       "  1,\n",
       "  125,\n",
       "  8,\n",
       "  1,\n",
       "  147,\n",
       "  75,\n",
       "  33,\n",
       "  15,\n",
       "  9,\n",
       "  637,\n",
       "  1,\n",
       "  159,\n",
       "  62],\n",
       " [8,\n",
       "  48,\n",
       "  185,\n",
       "  6,\n",
       "  118,\n",
       "  1714,\n",
       "  1,\n",
       "  1247,\n",
       "  189,\n",
       "  19,\n",
       "  5,\n",
       "  1715,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  4,\n",
       "  952,\n",
       "  7,\n",
       "  700,\n",
       "  2464,\n",
       "  1715,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  1,\n",
       "  2465,\n",
       "  19,\n",
       "  68,\n",
       "  819,\n",
       "  211,\n",
       "  1,\n",
       "  836,\n",
       "  19,\n",
       "  4,\n",
       "  1,\n",
       "  1716,\n",
       "  19,\n",
       "  33,\n",
       "  76,\n",
       "  728,\n",
       "  545,\n",
       "  227,\n",
       "  700,\n",
       "  125,\n",
       "  42,\n",
       "  2466,\n",
       "  7,\n",
       "  1717,\n",
       "  286],\n",
       " [1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  13,\n",
       "  66,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  541,\n",
       "  18,\n",
       "  1,\n",
       "  275,\n",
       "  44,\n",
       "  85,\n",
       "  22,\n",
       "  844,\n",
       "  533,\n",
       "  480,\n",
       "  125,\n",
       "  21,\n",
       "  1,\n",
       "  22,\n",
       "  5,\n",
       "  203,\n",
       "  845,\n",
       "  208,\n",
       "  12,\n",
       "  40,\n",
       "  31,\n",
       "  1,\n",
       "  28,\n",
       "  23,\n",
       "  42,\n",
       "  1267,\n",
       "  13,\n",
       "  109,\n",
       "  9,\n",
       "  133],\n",
       " [1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  13,\n",
       "  66,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  541,\n",
       "  18,\n",
       "  1,\n",
       "  275,\n",
       "  44,\n",
       "  85,\n",
       "  22,\n",
       "  844,\n",
       "  533,\n",
       "  480,\n",
       "  125,\n",
       "  21,\n",
       "  1,\n",
       "  22,\n",
       "  5,\n",
       "  203,\n",
       "  845,\n",
       "  208,\n",
       "  12,\n",
       "  40,\n",
       "  31,\n",
       "  1,\n",
       "  28,\n",
       "  23,\n",
       "  42,\n",
       "  1267,\n",
       "  13,\n",
       "  109,\n",
       "  9,\n",
       "  133],\n",
       " [6,\n",
       "  18,\n",
       "  243,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  201,\n",
       "  11,\n",
       "  1718,\n",
       "  14,\n",
       "  331,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  2467,\n",
       "  19,\n",
       "  33,\n",
       "  50,\n",
       "  648,\n",
       "  326,\n",
       "  1719,\n",
       "  10,\n",
       "  1,\n",
       "  2468,\n",
       "  647,\n",
       "  5,\n",
       "  729,\n",
       "  437,\n",
       "  847,\n",
       "  243,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  201,\n",
       "  14,\n",
       "  68,\n",
       "  331,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  488],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  177,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  121,\n",
       "  7,\n",
       "  60,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  42,\n",
       "  1720,\n",
       "  848,\n",
       "  9,\n",
       "  344,\n",
       "  63,\n",
       "  1272,\n",
       "  8,\n",
       "  1,\n",
       "  17,\n",
       "  126,\n",
       "  2469,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  35,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  424,\n",
       "  24,\n",
       "  65],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  2470,\n",
       "  122,\n",
       "  8,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  607,\n",
       "  1,\n",
       "  30,\n",
       "  2471,\n",
       "  76,\n",
       "  15,\n",
       "  59,\n",
       "  30,\n",
       "  73,\n",
       "  122,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  23,\n",
       "  1,\n",
       "  139,\n",
       "  1721,\n",
       "  4,\n",
       "  803,\n",
       "  2472,\n",
       "  2473,\n",
       "  9,\n",
       "  1,\n",
       "  73,\n",
       "  5,\n",
       "  2474,\n",
       "  2475,\n",
       "  8,\n",
       "  2476,\n",
       "  4,\n",
       "  44],\n",
       " [6,\n",
       "  192,\n",
       "  18,\n",
       "  1,\n",
       "  126,\n",
       "  303,\n",
       "  1722,\n",
       "  8,\n",
       "  1,\n",
       "  17,\n",
       "  70,\n",
       "  56,\n",
       "  35,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  219,\n",
       "  7,\n",
       "  126,\n",
       "  193,\n",
       "  42,\n",
       "  1723,\n",
       "  206,\n",
       "  1191,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  35,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  424,\n",
       "  24,\n",
       "  65],\n",
       " [126,\n",
       "  218,\n",
       "  32,\n",
       "  174,\n",
       "  21,\n",
       "  2477,\n",
       "  132,\n",
       "  153,\n",
       "  14,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  126,\n",
       "  218,\n",
       "  67,\n",
       "  174,\n",
       "  21,\n",
       "  1724,\n",
       "  30,\n",
       "  132,\n",
       "  4,\n",
       "  1725,\n",
       "  421,\n",
       "  26,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  10,\n",
       "  1,\n",
       "  966,\n",
       "  5,\n",
       "  705,\n",
       "  356,\n",
       "  4,\n",
       "  2478],\n",
       " [791,\n",
       "  6,\n",
       "  121,\n",
       "  2479,\n",
       "  1,\n",
       "  1726,\n",
       "  1727,\n",
       "  2480,\n",
       "  15,\n",
       "  26,\n",
       "  1728,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  11,\n",
       "  343,\n",
       "  465,\n",
       "  2481,\n",
       "  42,\n",
       "  385,\n",
       "  104,\n",
       "  24,\n",
       "  1035,\n",
       "  2482,\n",
       "  167,\n",
       "  13,\n",
       "  1273,\n",
       "  26,\n",
       "  1,\n",
       "  1726,\n",
       "  376,\n",
       "  128,\n",
       "  5,\n",
       "  1728,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  1274,\n",
       "  1,\n",
       "  19,\n",
       "  6,\n",
       "  751,\n",
       "  9,\n",
       "  31,\n",
       "  1,\n",
       "  1727,\n",
       "  376,\n",
       "  2483,\n",
       "  1274,\n",
       "  4,\n",
       "  6,\n",
       "  849,\n",
       "  48,\n",
       "  24,\n",
       "  1035,\n",
       "  101],\n",
       " [252,\n",
       "  346,\n",
       "  347,\n",
       "  84,\n",
       "  24,\n",
       "  115,\n",
       "  76,\n",
       "  586,\n",
       "  1,\n",
       "  139,\n",
       "  850,\n",
       "  11,\n",
       "  413,\n",
       "  730,\n",
       "  125,\n",
       "  290,\n",
       "  31,\n",
       "  1,\n",
       "  1729,\n",
       "  1730,\n",
       "  842,\n",
       "  1731,\n",
       "  8,\n",
       "  2484,\n",
       "  1,\n",
       "  301,\n",
       "  513,\n",
       "  8,\n",
       "  253,\n",
       "  325,\n",
       "  4,\n",
       "  344,\n",
       "  6,\n",
       "  154,\n",
       "  808,\n",
       "  7,\n",
       "  1232,\n",
       "  850,\n",
       "  9,\n",
       "  413,\n",
       "  730,\n",
       "  125,\n",
       "  539,\n",
       "  1,\n",
       "  1275,\n",
       "  1732,\n",
       "  9,\n",
       "  1,\n",
       "  115,\n",
       "  5,\n",
       "  346,\n",
       "  347,\n",
       "  84],\n",
       " [6,\n",
       "  750,\n",
       "  7,\n",
       "  2485,\n",
       "  708,\n",
       "  4,\n",
       "  2486,\n",
       "  345,\n",
       "  19,\n",
       "  1112,\n",
       "  26,\n",
       "  1,\n",
       "  427,\n",
       "  402,\n",
       "  636,\n",
       "  1733,\n",
       "  1734,\n",
       "  19,\n",
       "  97,\n",
       "  8,\n",
       "  1735,\n",
       "  4,\n",
       "  1736,\n",
       "  64,\n",
       "  42,\n",
       "  6,\n",
       "  1113,\n",
       "  9,\n",
       "  1,\n",
       "  2487,\n",
       "  1276,\n",
       "  642,\n",
       "  6,\n",
       "  1113,\n",
       "  1,\n",
       "  1734,\n",
       "  19,\n",
       "  97,\n",
       "  8,\n",
       "  1735,\n",
       "  4,\n",
       "  1736,\n",
       "  64,\n",
       "  9,\n",
       "  239,\n",
       "  4,\n",
       "  307,\n",
       "  302,\n",
       "  26,\n",
       "  2488,\n",
       "  1,\n",
       "  112,\n",
       "  427,\n",
       "  682,\n",
       "  9,\n",
       "  18,\n",
       "  7,\n",
       "  159,\n",
       "  1737],\n",
       " [24,\n",
       "  167,\n",
       "  13,\n",
       "  76,\n",
       "  601,\n",
       "  9,\n",
       "  1036,\n",
       "  4,\n",
       "  1037,\n",
       "  40,\n",
       "  319,\n",
       "  1,\n",
       "  142,\n",
       "  211,\n",
       "  1,\n",
       "  63,\n",
       "  23,\n",
       "  1,\n",
       "  431,\n",
       "  1738,\n",
       "  112,\n",
       "  731,\n",
       "  8,\n",
       "  1,\n",
       "  55,\n",
       "  758,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  1739,\n",
       "  7,\n",
       "  162,\n",
       "  2489,\n",
       "  55,\n",
       "  431,\n",
       "  13,\n",
       "  1,\n",
       "  1740,\n",
       "  431,\n",
       "  211,\n",
       "  1,\n",
       "  112,\n",
       "  731,\n",
       "  8,\n",
       "  7,\n",
       "  55,\n",
       "  160,\n",
       "  758,\n",
       "  4,\n",
       "  129,\n",
       "  123,\n",
       "  268,\n",
       "  9,\n",
       "  104,\n",
       "  1741,\n",
       "  11,\n",
       "  293,\n",
       "  303,\n",
       "  1036,\n",
       "  4,\n",
       "  1037,\n",
       "  40],\n",
       " [1273,\n",
       "  26,\n",
       "  323,\n",
       "  167,\n",
       "  6,\n",
       "  1742,\n",
       "  7,\n",
       "  467,\n",
       "  993,\n",
       "  5,\n",
       "  2490,\n",
       "  137,\n",
       "  1038,\n",
       "  42,\n",
       "  67,\n",
       "  289,\n",
       "  9,\n",
       "  104,\n",
       "  1,\n",
       "  255,\n",
       "  649,\n",
       "  959,\n",
       "  1,\n",
       "  2491,\n",
       "  22,\n",
       "  1743,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  2492,\n",
       "  5,\n",
       "  1,\n",
       "  542,\n",
       "  93,\n",
       "  1,\n",
       "  137,\n",
       "  1038,\n",
       "  42,\n",
       "  32,\n",
       "  289,\n",
       "  9,\n",
       "  104,\n",
       "  1,\n",
       "  255,\n",
       "  2493,\n",
       "  8,\n",
       "  24,\n",
       "  58,\n",
       "  2494,\n",
       "  14,\n",
       "  797,\n",
       "  289,\n",
       "  8,\n",
       "  1,\n",
       "  2495,\n",
       "  2496,\n",
       "  22,\n",
       "  1743,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [1,\n",
       "  2497,\n",
       "  286,\n",
       "  950,\n",
       "  67,\n",
       "  807,\n",
       "  10,\n",
       "  7,\n",
       "  1039,\n",
       "  406,\n",
       "  487,\n",
       "  103,\n",
       "  11,\n",
       "  46,\n",
       "  7,\n",
       "  158,\n",
       "  168,\n",
       "  184,\n",
       "  10,\n",
       "  921,\n",
       "  669,\n",
       "  1744,\n",
       "  626,\n",
       "  193,\n",
       "  344,\n",
       "  9,\n",
       "  193,\n",
       "  533,\n",
       "  508,\n",
       "  1,\n",
       "  2498,\n",
       "  633,\n",
       "  2499,\n",
       "  1745,\n",
       "  153,\n",
       "  1040,\n",
       "  494,\n",
       "  1041,\n",
       "  10,\n",
       "  1039,\n",
       "  406,\n",
       "  487,\n",
       "  103,\n",
       "  1277,\n",
       "  1744,\n",
       "  626,\n",
       "  11,\n",
       "  46,\n",
       "  7,\n",
       "  158,\n",
       "  168,\n",
       "  2500],\n",
       " [8,\n",
       "  48,\n",
       "  1746,\n",
       "  5,\n",
       "  48,\n",
       "  263,\n",
       "  6,\n",
       "  1714,\n",
       "  601,\n",
       "  167,\n",
       "  1,\n",
       "  381,\n",
       "  11,\n",
       "  111,\n",
       "  29,\n",
       "  4,\n",
       "  58,\n",
       "  4,\n",
       "  188,\n",
       "  11,\n",
       "  111,\n",
       "  573,\n",
       "  10,\n",
       "  1,\n",
       "  28,\n",
       "  176,\n",
       "  26,\n",
       "  834,\n",
       "  62,\n",
       "  835,\n",
       "  195,\n",
       "  115,\n",
       "  8,\n",
       "  2501,\n",
       "  263,\n",
       "  1238,\n",
       "  1,\n",
       "  673,\n",
       "  5,\n",
       "  24,\n",
       "  29,\n",
       "  34,\n",
       "  631,\n",
       "  8,\n",
       "  1,\n",
       "  573,\n",
       "  7,\n",
       "  5,\n",
       "  834,\n",
       "  62,\n",
       "  835,\n",
       "  195,\n",
       "  115,\n",
       "  8,\n",
       "  465,\n",
       "  639,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [8,\n",
       "  1747,\n",
       "  163,\n",
       "  50,\n",
       "  23,\n",
       "  485,\n",
       "  28,\n",
       "  154,\n",
       "  104,\n",
       "  214,\n",
       "  635,\n",
       "  9,\n",
       "  1042,\n",
       "  724,\n",
       "  206,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  15,\n",
       "  11,\n",
       "  46,\n",
       "  4,\n",
       "  281,\n",
       "  2502,\n",
       "  15,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [8,\n",
       "  1,\n",
       "  982,\n",
       "  185,\n",
       "  6,\n",
       "  1278,\n",
       "  732,\n",
       "  343,\n",
       "  5,\n",
       "  1748,\n",
       "  1749,\n",
       "  8,\n",
       "  7,\n",
       "  1599,\n",
       "  376,\n",
       "  73,\n",
       "  19,\n",
       "  499,\n",
       "  2,\n",
       "  3,\n",
       "  197,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  1750,\n",
       "  6,\n",
       "  1278,\n",
       "  732,\n",
       "  1,\n",
       "  376,\n",
       "  61,\n",
       "  30,\n",
       "  73,\n",
       "  65,\n",
       "  499,\n",
       "  197,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  1750],\n",
       " [48,\n",
       "  1279,\n",
       "  13,\n",
       "  640,\n",
       "  454,\n",
       "  190,\n",
       "  228,\n",
       "  410,\n",
       "  670,\n",
       "  9,\n",
       "  77,\n",
       "  80,\n",
       "  147,\n",
       "  12,\n",
       "  36,\n",
       "  1,\n",
       "  722,\n",
       "  13,\n",
       "  640,\n",
       "  454,\n",
       "  670,\n",
       "  9,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  147,\n",
       "  12,\n",
       "  36],\n",
       " [1751,\n",
       "  14,\n",
       "  2503,\n",
       "  1752,\n",
       "  4,\n",
       "  1753,\n",
       "  36,\n",
       "  1280,\n",
       "  434,\n",
       "  381,\n",
       "  32,\n",
       "  2504,\n",
       "  1,\n",
       "  255,\n",
       "  733,\n",
       "  15,\n",
       "  345,\n",
       "  128,\n",
       "  11,\n",
       "  2505,\n",
       "  2506,\n",
       "  1,\n",
       "  345,\n",
       "  128,\n",
       "  1280,\n",
       "  434,\n",
       "  381,\n",
       "  32,\n",
       "  258,\n",
       "  1752,\n",
       "  4,\n",
       "  1753,\n",
       "  36],\n",
       " [1281,\n",
       "  5,\n",
       "  24,\n",
       "  2507,\n",
       "  14,\n",
       "  1,\n",
       "  202,\n",
       "  152,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  6,\n",
       "  851,\n",
       "  48,\n",
       "  122,\n",
       "  11,\n",
       "  81,\n",
       "  2508,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  202,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  708,\n",
       "  2509,\n",
       "  11,\n",
       "  24,\n",
       "  58],\n",
       " [444,\n",
       "  1043,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  13,\n",
       "  175,\n",
       "  14,\n",
       "  7,\n",
       "  444,\n",
       "  166,\n",
       "  5,\n",
       "  769,\n",
       "  9,\n",
       "  1044,\n",
       "  1,\n",
       "  19,\n",
       "  21,\n",
       "  2510,\n",
       "  1043,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  13,\n",
       "  7,\n",
       "  650,\n",
       "  852,\n",
       "  349,\n",
       "  1754,\n",
       "  9,\n",
       "  1044,\n",
       "  2511,\n",
       "  5,\n",
       "  7,\n",
       "  309],\n",
       " [368,\n",
       "  38,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  85,\n",
       "  22,\n",
       "  185,\n",
       "  131,\n",
       "  9,\n",
       "  69,\n",
       "  68,\n",
       "  506,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  24,\n",
       "  38,\n",
       "  58,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [31,\n",
       "  1276,\n",
       "  8,\n",
       "  185,\n",
       "  131,\n",
       "  6,\n",
       "  153,\n",
       "  191,\n",
       "  21,\n",
       "  1,\n",
       "  619,\n",
       "  5,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  4,\n",
       "  86,\n",
       "  90,\n",
       "  6,\n",
       "  853,\n",
       "  18,\n",
       "  5,\n",
       "  55,\n",
       "  160,\n",
       "  333,\n",
       "  21,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  4,\n",
       "  86,\n",
       "  90],\n",
       " [6,\n",
       "  274,\n",
       "  4,\n",
       "  1282,\n",
       "  51,\n",
       "  5,\n",
       "  1,\n",
       "  144,\n",
       "  10,\n",
       "  1283,\n",
       "  787,\n",
       "  14,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  442,\n",
       "  1,\n",
       "  46,\n",
       "  144,\n",
       "  14,\n",
       "  229,\n",
       "  1221,\n",
       "  8,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [9,\n",
       "  210,\n",
       "  172,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  4,\n",
       "  1,\n",
       "  1755,\n",
       "  73,\n",
       "  21,\n",
       "  185,\n",
       "  1756,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  11,\n",
       "  430,\n",
       "  55,\n",
       "  431,\n",
       "  4,\n",
       "  186,\n",
       "  93],\n",
       " [1,\n",
       "  2512,\n",
       "  65,\n",
       "  32,\n",
       "  50,\n",
       "  10,\n",
       "  243,\n",
       "  527,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  53,\n",
       "  14,\n",
       "  68,\n",
       "  331,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  2513,\n",
       "  106,\n",
       "  32,\n",
       "  2514,\n",
       "  26,\n",
       "  243,\n",
       "  527,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  53,\n",
       "  14,\n",
       "  1,\n",
       "  78,\n",
       "  166,\n",
       "  488],\n",
       " [1,\n",
       "  1149,\n",
       "  132,\n",
       "  2515,\n",
       "  7,\n",
       "  1172,\n",
       "  5,\n",
       "  193,\n",
       "  344,\n",
       "  685,\n",
       "  375,\n",
       "  299,\n",
       "  19,\n",
       "  1757,\n",
       "  4,\n",
       "  86,\n",
       "  90,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  27,\n",
       "  49,\n",
       "  2516,\n",
       "  2517,\n",
       "  7,\n",
       "  375,\n",
       "  299,\n",
       "  19,\n",
       "  1757,\n",
       "  4,\n",
       "  86,\n",
       "  90],\n",
       " [1,\n",
       "  70,\n",
       "  89,\n",
       "  395,\n",
       "  10,\n",
       "  364,\n",
       "  311,\n",
       "  802,\n",
       "  32,\n",
       "  460,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  7,\n",
       "  70,\n",
       "  89,\n",
       "  147,\n",
       "  33,\n",
       "  109,\n",
       "  10,\n",
       "  1,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  12,\n",
       "  36],\n",
       " [9,\n",
       "  210,\n",
       "  172,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  4,\n",
       "  1,\n",
       "  1755,\n",
       "  73,\n",
       "  21,\n",
       "  185,\n",
       "  1756,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  11,\n",
       "  430,\n",
       "  55,\n",
       "  431,\n",
       "  4,\n",
       "  186,\n",
       "  93],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  150,\n",
       "  103,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  9,\n",
       "  651,\n",
       "  1,\n",
       "  805,\n",
       "  2518,\n",
       "  2519,\n",
       "  5,\n",
       "  1,\n",
       "  500,\n",
       "  18,\n",
       "  428,\n",
       "  78,\n",
       "  9,\n",
       "  69,\n",
       "  19,\n",
       "  106,\n",
       "  591,\n",
       "  1,\n",
       "  106,\n",
       "  10,\n",
       "  1,\n",
       "  150,\n",
       "  103,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [1,\n",
       "  205,\n",
       "  13,\n",
       "  21,\n",
       "  1758,\n",
       "  1759,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  1,\n",
       "  1284,\n",
       "  5,\n",
       "  30,\n",
       "  1045,\n",
       "  13,\n",
       "  2520,\n",
       "  18,\n",
       "  1,\n",
       "  2521,\n",
       "  2522,\n",
       "  30,\n",
       "  178,\n",
       "  21,\n",
       "  1758,\n",
       "  1759,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  150,\n",
       "  527,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  14,\n",
       "  331,\n",
       "  78,\n",
       "  166,\n",
       "  75,\n",
       "  9,\n",
       "  1469,\n",
       "  18,\n",
       "  428,\n",
       "  78,\n",
       "  9,\n",
       "  69,\n",
       "  19,\n",
       "  106,\n",
       "  591,\n",
       "  1,\n",
       "  106,\n",
       "  10,\n",
       "  1,\n",
       "  150,\n",
       "  103,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [6,\n",
       "  340,\n",
       "  14,\n",
       "  1,\n",
       "  27,\n",
       "  70,\n",
       "  56,\n",
       "  35,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  8,\n",
       "  269,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  1285,\n",
       "  44,\n",
       "  29,\n",
       "  4,\n",
       "  9,\n",
       "  508,\n",
       "  1,\n",
       "  1760,\n",
       "  5,\n",
       "  1,\n",
       "  1761,\n",
       "  1762,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  121,\n",
       "  7,\n",
       "  126,\n",
       "  61,\n",
       "  56,\n",
       "  35,\n",
       "  29,\n",
       "  14,\n",
       "  7,\n",
       "  1286,\n",
       "  100,\n",
       "  371,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  209,\n",
       "  140,\n",
       "  5,\n",
       "  24,\n",
       "  825],\n",
       " [368,\n",
       "  38,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  85,\n",
       "  22,\n",
       "  185,\n",
       "  131,\n",
       "  9,\n",
       "  69,\n",
       "  68,\n",
       "  506,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  38,\n",
       "  49,\n",
       "  67,\n",
       "  66,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  360,\n",
       "  1,\n",
       "  784,\n",
       "  8,\n",
       "  504,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  5,\n",
       "  2523,\n",
       "  76,\n",
       "  18,\n",
       "  172,\n",
       "  2524,\n",
       "  2525,\n",
       "  9,\n",
       "  348,\n",
       "  1025,\n",
       "  65,\n",
       "  11,\n",
       "  1,\n",
       "  103,\n",
       "  5,\n",
       "  504,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [1,\n",
       "  41,\n",
       "  19,\n",
       "  13,\n",
       "  7,\n",
       "  100,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  19,\n",
       "  50,\n",
       "  10,\n",
       "  822,\n",
       "  14,\n",
       "  155,\n",
       "  116,\n",
       "  173,\n",
       "  4,\n",
       "  559,\n",
       "  1633,\n",
       "  50,\n",
       "  68,\n",
       "  44,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25],\n",
       " [31,\n",
       "  1276,\n",
       "  8,\n",
       "  185,\n",
       "  131,\n",
       "  6,\n",
       "  153,\n",
       "  191,\n",
       "  21,\n",
       "  1,\n",
       "  619,\n",
       "  5,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  4,\n",
       "  86,\n",
       "  90,\n",
       "  6,\n",
       "  853,\n",
       "  18,\n",
       "  5,\n",
       "  55,\n",
       "  160,\n",
       "  333,\n",
       "  21,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  4,\n",
       "  86,\n",
       "  90],\n",
       " [76,\n",
       "  6,\n",
       "  337,\n",
       "  23,\n",
       "  1,\n",
       "  1046,\n",
       "  236,\n",
       "  5,\n",
       "  1,\n",
       "  1182,\n",
       "  183,\n",
       "  962,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  4,\n",
       "  508,\n",
       "  34,\n",
       "  24,\n",
       "  128,\n",
       "  2526,\n",
       "  9,\n",
       "  2527,\n",
       "  391,\n",
       "  13,\n",
       "  1,\n",
       "  1046,\n",
       "  236,\n",
       "  5,\n",
       "  1,\n",
       "  1182,\n",
       "  183,\n",
       "  962,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [24,\n",
       "  19,\n",
       "  129,\n",
       "  7,\n",
       "  1763,\n",
       "  334,\n",
       "  1764,\n",
       "  2,\n",
       "  3,\n",
       "  581,\n",
       "  14,\n",
       "  112,\n",
       "  2528,\n",
       "  111,\n",
       "  686,\n",
       "  7,\n",
       "  136,\n",
       "  8,\n",
       "  2529,\n",
       "  323,\n",
       "  167,\n",
       "  18,\n",
       "  136,\n",
       "  343,\n",
       "  14,\n",
       "  7,\n",
       "  1763,\n",
       "  334,\n",
       "  1764,\n",
       "  2,\n",
       "  3,\n",
       "  581],\n",
       " [1,\n",
       "  2530,\n",
       "  2531,\n",
       "  2532,\n",
       "  5,\n",
       "  1,\n",
       "  22,\n",
       "  461,\n",
       "  1,\n",
       "  854,\n",
       "  198,\n",
       "  1030,\n",
       "  961,\n",
       "  2,\n",
       "  3,\n",
       "  156,\n",
       "  1,\n",
       "  1127,\n",
       "  15,\n",
       "  9,\n",
       "  69,\n",
       "  1,\n",
       "  854,\n",
       "  171,\n",
       "  32,\n",
       "  855,\n",
       "  635,\n",
       "  21,\n",
       "  1,\n",
       "  854,\n",
       "  198,\n",
       "  1030,\n",
       "  961,\n",
       "  2,\n",
       "  3,\n",
       "  2533],\n",
       " [6,\n",
       "  76,\n",
       "  210,\n",
       "  1,\n",
       "  55,\n",
       "  160,\n",
       "  5,\n",
       "  1,\n",
       "  125,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  1765,\n",
       "  1,\n",
       "  1287,\n",
       "  5,\n",
       "  1288,\n",
       "  23,\n",
       "  55,\n",
       "  227,\n",
       "  6,\n",
       "  1224,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  131,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [1,\n",
       "  1399,\n",
       "  636,\n",
       "  13,\n",
       "  402,\n",
       "  10,\n",
       "  1012,\n",
       "  1013,\n",
       "  333,\n",
       "  718,\n",
       "  4,\n",
       "  1014,\n",
       "  339,\n",
       "  6,\n",
       "  644,\n",
       "  2534,\n",
       "  815,\n",
       "  636,\n",
       "  1766,\n",
       "  613,\n",
       "  2535,\n",
       "  8,\n",
       "  1,\n",
       "  636,\n",
       "  13,\n",
       "  1,\n",
       "  1012,\n",
       "  1013,\n",
       "  333,\n",
       "  211,\n",
       "  1,\n",
       "  112,\n",
       "  63,\n",
       "  718,\n",
       "  4,\n",
       "  1014,\n",
       "  339],\n",
       " [6,\n",
       "  219,\n",
       "  1289,\n",
       "  1290,\n",
       "  10,\n",
       "  112,\n",
       "  381,\n",
       "  1767,\n",
       "  1768,\n",
       "  1769,\n",
       "  4,\n",
       "  1770,\n",
       "  88,\n",
       "  4,\n",
       "  2536,\n",
       "  612,\n",
       "  1771,\n",
       "  8,\n",
       "  2537,\n",
       "  253,\n",
       "  76,\n",
       "  462,\n",
       "  2538,\n",
       "  2539,\n",
       "  219,\n",
       "  1290,\n",
       "  21,\n",
       "  2540,\n",
       "  10,\n",
       "  1767,\n",
       "  1768,\n",
       "  1769,\n",
       "  4,\n",
       "  1770,\n",
       "  88,\n",
       "  4,\n",
       "  24,\n",
       "  97,\n",
       "  2541,\n",
       "  29],\n",
       " [444,\n",
       "  1043,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  13,\n",
       "  175,\n",
       "  14,\n",
       "  7,\n",
       "  444,\n",
       "  166,\n",
       "  5,\n",
       "  769,\n",
       "  9,\n",
       "  1044,\n",
       "  1,\n",
       "  19,\n",
       "  21,\n",
       "  2542,\n",
       "  1,\n",
       "  619,\n",
       "  5,\n",
       "  1,\n",
       "  2543,\n",
       "  677,\n",
       "  13,\n",
       "  2544,\n",
       "  9,\n",
       "  7,\n",
       "  444,\n",
       "  677,\n",
       "  1043,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [1,\n",
       "  1772,\n",
       "  377,\n",
       "  33,\n",
       "  407,\n",
       "  26,\n",
       "  1,\n",
       "  77,\n",
       "  82,\n",
       "  504,\n",
       "  165,\n",
       "  1,\n",
       "  77,\n",
       "  378,\n",
       "  82,\n",
       "  586,\n",
       "  7,\n",
       "  592,\n",
       "  11,\n",
       "  1773,\n",
       "  1291,\n",
       "  7,\n",
       "  378,\n",
       "  377,\n",
       "  11,\n",
       "  7,\n",
       "  978,\n",
       "  545,\n",
       "  1,\n",
       "  377,\n",
       "  13,\n",
       "  318,\n",
       "  609,\n",
       "  504,\n",
       "  165],\n",
       " [1,\n",
       "  70,\n",
       "  89,\n",
       "  147,\n",
       "  13,\n",
       "  76,\n",
       "  196,\n",
       "  179,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  14,\n",
       "  228,\n",
       "  488,\n",
       "  297,\n",
       "  12,\n",
       "  36,\n",
       "  1,\n",
       "  89,\n",
       "  281,\n",
       "  13,\n",
       "  109,\n",
       "  26,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36],\n",
       " [1,\n",
       "  77,\n",
       "  378,\n",
       "  82,\n",
       "  586,\n",
       "  7,\n",
       "  592,\n",
       "  11,\n",
       "  1773,\n",
       "  1291,\n",
       "  7,\n",
       "  378,\n",
       "  377,\n",
       "  11,\n",
       "  7,\n",
       "  978,\n",
       "  545,\n",
       "  1,\n",
       "  377,\n",
       "  13,\n",
       "  318,\n",
       "  609,\n",
       "  504,\n",
       "  165,\n",
       "  1,\n",
       "  1772,\n",
       "  377,\n",
       "  33,\n",
       "  407,\n",
       "  26,\n",
       "  1,\n",
       "  77,\n",
       "  82,\n",
       "  504,\n",
       "  165],\n",
       " [11,\n",
       "  1292,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  1250,\n",
       "  28,\n",
       "  21,\n",
       "  1774,\n",
       "  652,\n",
       "  57,\n",
       "  11,\n",
       "  841,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  1774,\n",
       "  85,\n",
       "  22,\n",
       "  652,\n",
       "  57,\n",
       "  42,\n",
       "  32,\n",
       "  1292,\n",
       "  358,\n",
       "  21,\n",
       "  1,\n",
       "  203,\n",
       "  2545,\n",
       "  2546],\n",
       " [1,\n",
       "  235,\n",
       "  58,\n",
       "  67,\n",
       "  196,\n",
       "  179,\n",
       "  10,\n",
       "  1,\n",
       "  60,\n",
       "  634,\n",
       "  27,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  35,\n",
       "  29,\n",
       "  13,\n",
       "  50,\n",
       "  10,\n",
       "  1,\n",
       "  350,\n",
       "  609,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  1,\n",
       "  234,\n",
       "  5,\n",
       "  346,\n",
       "  347,\n",
       "  84,\n",
       "  9,\n",
       "  104,\n",
       "  514,\n",
       "  6,\n",
       "  95,\n",
       "  97,\n",
       "  7,\n",
       "  300,\n",
       "  384,\n",
       "  5,\n",
       "  734,\n",
       "  4,\n",
       "  1,\n",
       "  653,\n",
       "  301,\n",
       "  515,\n",
       "  8,\n",
       "  1047,\n",
       "  14,\n",
       "  248,\n",
       "  478,\n",
       "  301,\n",
       "  1775,\n",
       "  6,\n",
       "  1776,\n",
       "  1,\n",
       "  112,\n",
       "  301,\n",
       "  513,\n",
       "  5,\n",
       "  346,\n",
       "  347,\n",
       "  84,\n",
       "  4,\n",
       "  1777,\n",
       "  1,\n",
       "  653,\n",
       "  301,\n",
       "  515],\n",
       " [131,\n",
       "  14,\n",
       "  172,\n",
       "  200,\n",
       "  725,\n",
       "  1,\n",
       "  767,\n",
       "  1778,\n",
       "  14,\n",
       "  1179,\n",
       "  9,\n",
       "  106,\n",
       "  32,\n",
       "  260,\n",
       "  574,\n",
       "  1,\n",
       "  1779,\n",
       "  571,\n",
       "  782,\n",
       "  103,\n",
       "  1780,\n",
       "  4,\n",
       "  1781,\n",
       "  197,\n",
       "  1778,\n",
       "  32,\n",
       "  260,\n",
       "  1782,\n",
       "  574,\n",
       "  1779,\n",
       "  571,\n",
       "  334,\n",
       "  1780,\n",
       "  4,\n",
       "  1781,\n",
       "  197],\n",
       " [675,\n",
       "  1783,\n",
       "  95,\n",
       "  728,\n",
       "  1293,\n",
       "  1784,\n",
       "  26,\n",
       "  1785,\n",
       "  1,\n",
       "  207,\n",
       "  1786,\n",
       "  26,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  63,\n",
       "  1017,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  365,\n",
       "  114,\n",
       "  127,\n",
       "  91,\n",
       "  65,\n",
       "  5,\n",
       "  157,\n",
       "  360,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  112,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  95,\n",
       "  72,\n",
       "  365],\n",
       " [6,\n",
       "  15,\n",
       "  202,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  11,\n",
       "  51,\n",
       "  24,\n",
       "  215,\n",
       "  610,\n",
       "  18,\n",
       "  1,\n",
       "  202,\n",
       "  20,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  11,\n",
       "  24,\n",
       "  576,\n",
       "  78,\n",
       "  58],\n",
       " [239,\n",
       "  144,\n",
       "  1,\n",
       "  22,\n",
       "  15,\n",
       "  8,\n",
       "  1,\n",
       "  280,\n",
       "  58,\n",
       "  13,\n",
       "  1,\n",
       "  628,\n",
       "  1787,\n",
       "  1048,\n",
       "  22,\n",
       "  1788,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  1,\n",
       "  58,\n",
       "  32,\n",
       "  196,\n",
       "  179,\n",
       "  23,\n",
       "  7,\n",
       "  388,\n",
       "  5,\n",
       "  1,\n",
       "  628,\n",
       "  1787,\n",
       "  1048,\n",
       "  22,\n",
       "  2547,\n",
       "  1788,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  31,\n",
       "  98,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  1,\n",
       "  2548,\n",
       "  28,\n",
       "  2549,\n",
       "  1789,\n",
       "  5,\n",
       "  1,\n",
       "  2550,\n",
       "  143,\n",
       "  2551],\n",
       " [118,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  70,\n",
       "  56,\n",
       "  1790,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  126,\n",
       "  303],\n",
       " [810, 85, 22, 13, 1, 2552, 307, 85, 22, 1791, 2, 3, 37, 22, 1791, 2, 3, 37],\n",
       " [1,\n",
       "  85,\n",
       "  22,\n",
       "  13,\n",
       "  593,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  48,\n",
       "  13,\n",
       "  233,\n",
       "  10,\n",
       "  716,\n",
       "  19,\n",
       "  83,\n",
       "  471,\n",
       "  2,\n",
       "  3,\n",
       "  581,\n",
       "  4,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [6,\n",
       "  1175,\n",
       "  2553,\n",
       "  2554,\n",
       "  31,\n",
       "  7,\n",
       "  130,\n",
       "  136,\n",
       "  215,\n",
       "  62,\n",
       "  8,\n",
       "  7,\n",
       "  1271,\n",
       "  401,\n",
       "  2555,\n",
       "  1,\n",
       "  2556,\n",
       "  1271,\n",
       "  19,\n",
       "  5,\n",
       "  278,\n",
       "  53,\n",
       "  7,\n",
       "  647,\n",
       "  5,\n",
       "  646,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  225,\n",
       "  1,\n",
       "  97,\n",
       "  19,\n",
       "  9,\n",
       "  24,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  1712,\n",
       "  19,\n",
       "  138,\n",
       "  8,\n",
       "  646,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  214,\n",
       "  9,\n",
       "  1713,\n",
       "  198],\n",
       " [51,\n",
       "  172,\n",
       "  93,\n",
       "  32,\n",
       "  1792,\n",
       "  21,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  51,\n",
       "  1,\n",
       "  58,\n",
       "  32,\n",
       "  196,\n",
       "  179,\n",
       "  8,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  48,\n",
       "  265,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  11,\n",
       "  24,\n",
       "  58,\n",
       "  6,\n",
       "  18,\n",
       "  2557,\n",
       "  125,\n",
       "  21,\n",
       "  54,\n",
       "  12,\n",
       "  40,\n",
       "  11,\n",
       "  111,\n",
       "  41,\n",
       "  654,\n",
       "  280,\n",
       "  1,\n",
       "  628,\n",
       "  930,\n",
       "  5,\n",
       "  652,\n",
       "  53],\n",
       " [1,\n",
       "  306,\n",
       "  67,\n",
       "  118,\n",
       "  374,\n",
       "  1049,\n",
       "  4,\n",
       "  181,\n",
       "  494,\n",
       "  4,\n",
       "  192,\n",
       "  328,\n",
       "  67,\n",
       "  186,\n",
       "  224,\n",
       "  26,\n",
       "  429,\n",
       "  171,\n",
       "  180,\n",
       "  84,\n",
       "  42,\n",
       "  33,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  2558,\n",
       "  502,\n",
       "  46,\n",
       "  28,\n",
       "  1,\n",
       "  2559,\n",
       "  28,\n",
       "  33,\n",
       "  224,\n",
       "  10,\n",
       "  429,\n",
       "  180,\n",
       "  84,\n",
       "  10,\n",
       "  7,\n",
       "  19,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  549,\n",
       "  550,\n",
       "  551],\n",
       " [2560,\n",
       "  829,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  830,\n",
       "  261,\n",
       "  63,\n",
       "  61,\n",
       "  23,\n",
       "  30,\n",
       "  73,\n",
       "  1793,\n",
       "  6,\n",
       "  1125,\n",
       "  299,\n",
       "  261,\n",
       "  2561,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  509,\n",
       "  7,\n",
       "  82,\n",
       "  319,\n",
       "  261,\n",
       "  133,\n",
       "  13,\n",
       "  830,\n",
       "  9,\n",
       "  1794,\n",
       "  1,\n",
       "  209,\n",
       "  30,\n",
       "  269,\n",
       "  61,\n",
       "  23,\n",
       "  30,\n",
       "  73],\n",
       " [11,\n",
       "  41,\n",
       "  343,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  44,\n",
       "  353,\n",
       "  22,\n",
       "  14,\n",
       "  100,\n",
       "  371,\n",
       "  175,\n",
       "  14,\n",
       "  1,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  25,\n",
       "  41,\n",
       "  19,\n",
       "  11,\n",
       "  51,\n",
       "  1795,\n",
       "  49,\n",
       "  6,\n",
       "  18,\n",
       "  7,\n",
       "  1050,\n",
       "  116,\n",
       "  291,\n",
       "  41,\n",
       "  19,\n",
       "  50,\n",
       "  10,\n",
       "  1,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  25],\n",
       " [11,\n",
       "  24,\n",
       "  701,\n",
       "  908,\n",
       "  6,\n",
       "  18,\n",
       "  382,\n",
       "  383,\n",
       "  87,\n",
       "  11,\n",
       "  46,\n",
       "  239,\n",
       "  996,\n",
       "  65,\n",
       "  6,\n",
       "  18,\n",
       "  382,\n",
       "  20,\n",
       "  383,\n",
       "  87],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  60,\n",
       "  1796,\n",
       "  75,\n",
       "  5,\n",
       "  55,\n",
       "  795,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  14,\n",
       "  39,\n",
       "  191,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [982,\n",
       "  6,\n",
       "  337,\n",
       "  1797,\n",
       "  350,\n",
       "  1,\n",
       "  973,\n",
       "  562,\n",
       "  97,\n",
       "  8,\n",
       "  1171,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1624,\n",
       "  14,\n",
       "  722,\n",
       "  8,\n",
       "  286,\n",
       "  4,\n",
       "  722,\n",
       "  8,\n",
       "  78,\n",
       "  2562,\n",
       "  76,\n",
       "  225,\n",
       "  164,\n",
       "  2563,\n",
       "  8,\n",
       "  363,\n",
       "  5,\n",
       "  1,\n",
       "  78,\n",
       "  166,\n",
       "  72,\n",
       "  9,\n",
       "  1171,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  590,\n",
       "  34,\n",
       "  6,\n",
       "  707,\n",
       "  4,\n",
       "  225,\n",
       "  1,\n",
       "  532,\n",
       "  722,\n",
       "  8,\n",
       "  1,\n",
       "  78,\n",
       "  166],\n",
       " [1019,\n",
       "  8,\n",
       "  2564,\n",
       "  13,\n",
       "  247,\n",
       "  280,\n",
       "  1,\n",
       "  55,\n",
       "  61,\n",
       "  671,\n",
       "  15,\n",
       "  8,\n",
       "  1,\n",
       "  2565,\n",
       "  55,\n",
       "  187,\n",
       "  42,\n",
       "  33,\n",
       "  76,\n",
       "  15,\n",
       "  8,\n",
       "  1,\n",
       "  305,\n",
       "  992,\n",
       "  22,\n",
       "  1798,\n",
       "  2,\n",
       "  3,\n",
       "  329,\n",
       "  11,\n",
       "  1,\n",
       "  305,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  992,\n",
       "  22,\n",
       "  1798,\n",
       "  2,\n",
       "  3,\n",
       "  329],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  81,\n",
       "  5,\n",
       "  1693,\n",
       "  4,\n",
       "  1,\n",
       "  1694,\n",
       "  2566,\n",
       "  18,\n",
       "  1,\n",
       "  445,\n",
       "  81,\n",
       "  5,\n",
       "  99,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  92,\n",
       "  20,\n",
       "  9,\n",
       "  69,\n",
       "  24,\n",
       "  163,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  11,\n",
       "  1799,\n",
       "  6,\n",
       "  15,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  9,\n",
       "  69,\n",
       "  163],\n",
       " [1,\n",
       "  46,\n",
       "  75,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  69,\n",
       "  1,\n",
       "  27,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  41,\n",
       "  19,\n",
       "  11,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  50,\n",
       "  7,\n",
       "  60,\n",
       "  17,\n",
       "  101,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  23,\n",
       "  1,\n",
       "  139,\n",
       "  46,\n",
       "  28,\n",
       "  4,\n",
       "  15,\n",
       "  1,\n",
       "  139,\n",
       "  253,\n",
       "  951,\n",
       "  41,\n",
       "  19,\n",
       "  9,\n",
       "  1294,\n",
       "  2567],\n",
       " [172,\n",
       "  93,\n",
       "  67,\n",
       "  153,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  94,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  14,\n",
       "  39,\n",
       "  191,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [6,\n",
       "  66,\n",
       "  7,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  23,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  703,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  145,\n",
       "  13,\n",
       "  7,\n",
       "  1051,\n",
       "  41,\n",
       "  19,\n",
       "  66,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  4,\n",
       "  50,\n",
       "  206,\n",
       "  7,\n",
       "  155,\n",
       "  213,\n",
       "  5,\n",
       "  1,\n",
       "  247,\n",
       "  22,\n",
       "  8,\n",
       "  42,\n",
       "  613,\n",
       "  98,\n",
       "  13,\n",
       "  1295,\n",
       "  14,\n",
       "  124,\n",
       "  384,\n",
       "  856,\n",
       "  98,\n",
       "  477],\n",
       " [8,\n",
       "  1,\n",
       "  205,\n",
       "  140,\n",
       "  1,\n",
       "  275,\n",
       "  22,\n",
       "  13,\n",
       "  186,\n",
       "  119,\n",
       "  224,\n",
       "  26,\n",
       "  10,\n",
       "  148,\n",
       "  122,\n",
       "  135,\n",
       "  110,\n",
       "  11,\n",
       "  2568,\n",
       "  133,\n",
       "  14,\n",
       "  186,\n",
       "  4,\n",
       "  2569,\n",
       "  2570,\n",
       "  1,\n",
       "  22,\n",
       "  13,\n",
       "  473,\n",
       "  4,\n",
       "  224,\n",
       "  26,\n",
       "  186,\n",
       "  23,\n",
       "  105,\n",
       "  1026,\n",
       "  10,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110],\n",
       " [1,\n",
       "  1800,\n",
       "  1019,\n",
       "  857,\n",
       "  1801,\n",
       "  2,\n",
       "  3,\n",
       "  110,\n",
       "  1,\n",
       "  1800,\n",
       "  1019,\n",
       "  857,\n",
       "  1801,\n",
       "  2,\n",
       "  3,\n",
       "  110,\n",
       "  471,\n",
       "  22,\n",
       "  2571,\n",
       "  4,\n",
       "  1296,\n",
       "  1297,\n",
       "  129,\n",
       "  123,\n",
       "  15,\n",
       "  31,\n",
       "  7,\n",
       "  296,\n",
       "  22,\n",
       "  8,\n",
       "  675,\n",
       "  765,\n",
       "  764],\n",
       " [6,\n",
       "  986,\n",
       "  1,\n",
       "  1204,\n",
       "  5,\n",
       "  111,\n",
       "  522,\n",
       "  14,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  119,\n",
       "  198,\n",
       "  33,\n",
       "  109,\n",
       "  14,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110],\n",
       " [14,\n",
       "  1,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  25,\n",
       "  11,\n",
       "  41,\n",
       "  343,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  25,\n",
       "  11,\n",
       "  60,\n",
       "  1031,\n",
       "  343,\n",
       "  14,\n",
       "  68,\n",
       "  1031,\n",
       "  1298,\n",
       "  5,\n",
       "  170],\n",
       " [6,\n",
       "  1278,\n",
       "  732,\n",
       "  1,\n",
       "  376,\n",
       "  61,\n",
       "  30,\n",
       "  73,\n",
       "  65,\n",
       "  499,\n",
       "  197,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  84,\n",
       "  47,\n",
       "  13,\n",
       "  1,\n",
       "  376,\n",
       "  73,\n",
       "  19,\n",
       "  5,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  84],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  14,\n",
       "  39,\n",
       "  191,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  172,\n",
       "  93,\n",
       "  67,\n",
       "  153,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  94,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  14,\n",
       "  39,\n",
       "  191,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  60,\n",
       "  1796,\n",
       "  75,\n",
       "  5,\n",
       "  55,\n",
       "  795,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [51,\n",
       "  1,\n",
       "  1032,\n",
       "  32,\n",
       "  397,\n",
       "  10,\n",
       "  317,\n",
       "  134,\n",
       "  36,\n",
       "  11,\n",
       "  1,\n",
       "  436,\n",
       "  62,\n",
       "  6,\n",
       "  225,\n",
       "  188,\n",
       "  10,\n",
       "  317,\n",
       "  134,\n",
       "  36],\n",
       " [1,\n",
       "  516,\n",
       "  311,\n",
       "  297,\n",
       "  67,\n",
       "  407,\n",
       "  10,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  6,\n",
       "  752,\n",
       "  89,\n",
       "  10,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36],\n",
       " [6,\n",
       "  175,\n",
       "  2572,\n",
       "  10,\n",
       "  1,\n",
       "  1802,\n",
       "  403,\n",
       "  1803,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  1804,\n",
       "  260,\n",
       "  10,\n",
       "  1,\n",
       "  355,\n",
       "  2573,\n",
       "  2574,\n",
       "  5,\n",
       "  1802,\n",
       "  1803,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  42,\n",
       "  1299,\n",
       "  7,\n",
       "  1300,\n",
       "  2575],\n",
       " [11,\n",
       "  1,\n",
       "  226,\n",
       "  271,\n",
       "  199,\n",
       "  81,\n",
       "  6,\n",
       "  15,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  11,\n",
       "  1799,\n",
       "  6,\n",
       "  15,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  9,\n",
       "  69,\n",
       "  163],\n",
       " [1,\n",
       "  171,\n",
       "  6,\n",
       "  18,\n",
       "  13,\n",
       "  429,\n",
       "  180,\n",
       "  84,\n",
       "  7,\n",
       "  529,\n",
       "  526,\n",
       "  629,\n",
       "  171,\n",
       "  42,\n",
       "  33,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  960,\n",
       "  735,\n",
       "  22,\n",
       "  1805,\n",
       "  1806,\n",
       "  1807,\n",
       "  2576,\n",
       "  28,\n",
       "  33,\n",
       "  224,\n",
       "  10,\n",
       "  429,\n",
       "  180,\n",
       "  84,\n",
       "  10,\n",
       "  7,\n",
       "  19,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  549,\n",
       "  550,\n",
       "  551],\n",
       " [6,\n",
       "  95,\n",
       "  15,\n",
       "  1,\n",
       "  81,\n",
       "  138,\n",
       "  8,\n",
       "  1808,\n",
       "  4,\n",
       "  1809,\n",
       "  288,\n",
       "  14,\n",
       "  405,\n",
       "  200,\n",
       "  5,\n",
       "  911,\n",
       "  725,\n",
       "  9,\n",
       "  1810,\n",
       "  2577,\n",
       "  1,\n",
       "  78,\n",
       "  103,\n",
       "  6,\n",
       "  15,\n",
       "  1300,\n",
       "  2578,\n",
       "  14,\n",
       "  2579,\n",
       "  1811,\n",
       "  163,\n",
       "  42,\n",
       "  2580,\n",
       "  68,\n",
       "  1812,\n",
       "  5,\n",
       "  405,\n",
       "  200,\n",
       "  5,\n",
       "  725,\n",
       "  911,\n",
       "  1808,\n",
       "  4,\n",
       "  1809,\n",
       "  288],\n",
       " [6,\n",
       "  66,\n",
       "  7,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  23,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  703,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  6,\n",
       "  121,\n",
       "  24,\n",
       "  433,\n",
       "  49,\n",
       "  8,\n",
       "  7,\n",
       "  60,\n",
       "  592,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  29,\n",
       "  102,\n",
       "  11,\n",
       "  41,\n",
       "  1813,\n",
       "  96,\n",
       "  25,\n",
       "  4,\n",
       "  60,\n",
       "  257,\n",
       "  299,\n",
       "  19],\n",
       " [1814,\n",
       "  406,\n",
       "  594,\n",
       "  379,\n",
       "  2581,\n",
       "  31,\n",
       "  236,\n",
       "  5,\n",
       "  1,\n",
       "  583,\n",
       "  264,\n",
       "  2582,\n",
       "  1,\n",
       "  380,\n",
       "  76,\n",
       "  1815,\n",
       "  169,\n",
       "  7,\n",
       "  2583,\n",
       "  287,\n",
       "  5,\n",
       "  2584,\n",
       "  379,\n",
       "  8,\n",
       "  1,\n",
       "  773,\n",
       "  5,\n",
       "  406,\n",
       "  594,\n",
       "  2585,\n",
       "  619,\n",
       "  1,\n",
       "  146,\n",
       "  2586,\n",
       "  612,\n",
       "  130,\n",
       "  418,\n",
       "  8,\n",
       "  1,\n",
       "  827,\n",
       "  5,\n",
       "  406,\n",
       "  594,\n",
       "  379,\n",
       "  277,\n",
       "  2,\n",
       "  3,\n",
       "  40],\n",
       " [252,\n",
       "  1,\n",
       "  2587,\n",
       "  159,\n",
       "  62,\n",
       "  1,\n",
       "  16,\n",
       "  159,\n",
       "  62,\n",
       "  2588,\n",
       "  23,\n",
       "  55,\n",
       "  227,\n",
       "  4,\n",
       "  779,\n",
       "  190,\n",
       "  2589,\n",
       "  258,\n",
       "  56,\n",
       "  78,\n",
       "  530,\n",
       "  214,\n",
       "  9,\n",
       "  48,\n",
       "  62,\n",
       "  715,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  118,\n",
       "  2590,\n",
       "  190,\n",
       "  42,\n",
       "  470,\n",
       "  990,\n",
       "  33,\n",
       "  1816,\n",
       "  33,\n",
       "  1,\n",
       "  16,\n",
       "  502,\n",
       "  159,\n",
       "  62,\n",
       "  23,\n",
       "  55,\n",
       "  227,\n",
       "  715,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [252,\n",
       "  346,\n",
       "  347,\n",
       "  84,\n",
       "  24,\n",
       "  115,\n",
       "  76,\n",
       "  586,\n",
       "  1,\n",
       "  139,\n",
       "  850,\n",
       "  11,\n",
       "  413,\n",
       "  730,\n",
       "  125,\n",
       "  290,\n",
       "  31,\n",
       "  1,\n",
       "  1729,\n",
       "  1730,\n",
       "  842,\n",
       "  1731,\n",
       "  8,\n",
       "  2591,\n",
       "  252,\n",
       "  346,\n",
       "  347,\n",
       "  84,\n",
       "  24,\n",
       "  115,\n",
       "  154,\n",
       "  76,\n",
       "  1301,\n",
       "  34,\n",
       "  1,\n",
       "  730,\n",
       "  136,\n",
       "  2592,\n",
       "  13,\n",
       "  1817,\n",
       "  1793,\n",
       "  2593,\n",
       "  13,\n",
       "  1818],\n",
       " [1,\n",
       "  126,\n",
       "  193,\n",
       "  13,\n",
       "  174,\n",
       "  21,\n",
       "  7,\n",
       "  239,\n",
       "  133,\n",
       "  585,\n",
       "  23,\n",
       "  1,\n",
       "  30,\n",
       "  472,\n",
       "  10,\n",
       "  856,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  7,\n",
       "  720,\n",
       "  837,\n",
       "  5,\n",
       "  613,\n",
       "  433,\n",
       "  29,\n",
       "  13,\n",
       "  1,\n",
       "  126,\n",
       "  193,\n",
       "  42,\n",
       "  240,\n",
       "  239,\n",
       "  126,\n",
       "  218,\n",
       "  174,\n",
       "  21,\n",
       "  7,\n",
       "  239,\n",
       "  22,\n",
       "  1040,\n",
       "  30,\n",
       "  73,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [367,\n",
       "  207,\n",
       "  313,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  129,\n",
       "  123,\n",
       "  15,\n",
       "  9,\n",
       "  688,\n",
       "  446,\n",
       "  2594,\n",
       "  42,\n",
       "  13,\n",
       "  15,\n",
       "  31,\n",
       "  7,\n",
       "  216,\n",
       "  5,\n",
       "  1,\n",
       "  2595,\n",
       "  5,\n",
       "  1,\n",
       "  2596,\n",
       "  367,\n",
       "  207,\n",
       "  313,\n",
       "  28,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  129,\n",
       "  123,\n",
       "  268,\n",
       "  9,\n",
       "  2597,\n",
       "  7,\n",
       "  1400,\n",
       "  222,\n",
       "  955,\n",
       "  48,\n",
       "  28,\n",
       "  33,\n",
       "  318,\n",
       "  169,\n",
       "  648,\n",
       "  1,\n",
       "  2598,\n",
       "  5,\n",
       "  48,\n",
       "  655],\n",
       " [6,\n",
       "  118,\n",
       "  18,\n",
       "  7,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  160,\n",
       "  111,\n",
       "  136,\n",
       "  4,\n",
       "  219,\n",
       "  1,\n",
       "  75,\n",
       "  5,\n",
       "  55,\n",
       "  242,\n",
       "  762,\n",
       "  14,\n",
       "  1,\n",
       "  136,\n",
       "  7,\n",
       "  514,\n",
       "  75,\n",
       "  5,\n",
       "  52,\n",
       "  568,\n",
       "  4,\n",
       "  1,\n",
       "  82,\n",
       "  15,\n",
       "  9,\n",
       "  858,\n",
       "  21,\n",
       "  7,\n",
       "  522,\n",
       "  9,\n",
       "  7,\n",
       "  1819,\n",
       "  55,\n",
       "  146,\n",
       "  154,\n",
       "  104,\n",
       "  289,\n",
       "  8,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [6,\n",
       "  118,\n",
       "  18,\n",
       "  7,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  160,\n",
       "  111,\n",
       "  136,\n",
       "  4,\n",
       "  219,\n",
       "  1,\n",
       "  75,\n",
       "  5,\n",
       "  55,\n",
       "  242,\n",
       "  762,\n",
       "  14,\n",
       "  1,\n",
       "  136,\n",
       "  7,\n",
       "  514,\n",
       "  75,\n",
       "  5,\n",
       "  52,\n",
       "  568,\n",
       "  4,\n",
       "  1,\n",
       "  82,\n",
       "  15,\n",
       "  9,\n",
       "  858,\n",
       "  21,\n",
       "  7,\n",
       "  522,\n",
       "  9,\n",
       "  7,\n",
       "  1819,\n",
       "  55,\n",
       "  146,\n",
       "  154,\n",
       "  104,\n",
       "  289,\n",
       "  8,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [172,\n",
       "  320,\n",
       "  808,\n",
       "  68,\n",
       "  1237,\n",
       "  241,\n",
       "  1820,\n",
       "  327,\n",
       "  2599,\n",
       "  850,\n",
       "  9,\n",
       "  1,\n",
       "  2600,\n",
       "  5,\n",
       "  1302,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  2601,\n",
       "  328,\n",
       "  2602,\n",
       "  1821,\n",
       "  886,\n",
       "  1259,\n",
       "  11,\n",
       "  1,\n",
       "  2603,\n",
       "  5,\n",
       "  2604,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  2605,\n",
       "  34,\n",
       "  60,\n",
       "  285,\n",
       "  1822,\n",
       "  2606,\n",
       "  602,\n",
       "  2607,\n",
       "  285,\n",
       "  42,\n",
       "  328,\n",
       "  2608,\n",
       "  9,\n",
       "  1,\n",
       "  2609,\n",
       "  34,\n",
       "  1,\n",
       "  1002,\n",
       "  1823,\n",
       "  285,\n",
       "  2610,\n",
       "  7,\n",
       "  886,\n",
       "  1174,\n",
       "  414,\n",
       "  848,\n",
       "  5,\n",
       "  1821,\n",
       "  1824],\n",
       " [6,\n",
       "  340,\n",
       "  14,\n",
       "  1,\n",
       "  27,\n",
       "  70,\n",
       "  56,\n",
       "  35,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  8,\n",
       "  269,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  1285,\n",
       "  44,\n",
       "  29,\n",
       "  4,\n",
       "  9,\n",
       "  508,\n",
       "  1,\n",
       "  1760,\n",
       "  5,\n",
       "  1,\n",
       "  1761,\n",
       "  1762,\n",
       "  714,\n",
       "  7,\n",
       "  29,\n",
       "  34,\n",
       "  2611,\n",
       "  7,\n",
       "  710,\n",
       "  17,\n",
       "  70,\n",
       "  56,\n",
       "  35,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  4,\n",
       "  2612,\n",
       "  98,\n",
       "  9,\n",
       "  1,\n",
       "  1250,\n",
       "  470],\n",
       " [11,\n",
       "  24,\n",
       "  22,\n",
       "  6,\n",
       "  1178,\n",
       "  459,\n",
       "  358,\n",
       "  21,\n",
       "  1,\n",
       "  2613,\n",
       "  185,\n",
       "  5,\n",
       "  1,\n",
       "  300,\n",
       "  859,\n",
       "  620,\n",
       "  22,\n",
       "  1052,\n",
       "  90,\n",
       "  21,\n",
       "  1,\n",
       "  1303,\n",
       "  2614,\n",
       "  859,\n",
       "  620,\n",
       "  351,\n",
       "  5,\n",
       "  963,\n",
       "  256,\n",
       "  125,\n",
       "  21,\n",
       "  1,\n",
       "  300,\n",
       "  859,\n",
       "  620,\n",
       "  22,\n",
       "  1052,\n",
       "  90,\n",
       "  1303,\n",
       "  16,\n",
       "  6,\n",
       "  459,\n",
       "  254,\n",
       "  125,\n",
       "  34,\n",
       "  2615,\n",
       "  190,\n",
       "  1156,\n",
       "  145,\n",
       "  736,\n",
       "  656,\n",
       "  670,\n",
       "  9,\n",
       "  1,\n",
       "  2616],\n",
       " [328,\n",
       "  15,\n",
       "  1,\n",
       "  1206,\n",
       "  164,\n",
       "  122,\n",
       "  266,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  11,\n",
       "  1,\n",
       "  164,\n",
       "  6,\n",
       "  849,\n",
       "  1,\n",
       "  280,\n",
       "  354,\n",
       "  5,\n",
       "  2617,\n",
       "  2134,\n",
       "  1,\n",
       "  266,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  33,\n",
       "  15,\n",
       "  11,\n",
       "  164],\n",
       " [51,\n",
       "  144,\n",
       "  67,\n",
       "  419,\n",
       "  21,\n",
       "  1,\n",
       "  1003,\n",
       "  399,\n",
       "  1004,\n",
       "  84,\n",
       "  1,\n",
       "  143,\n",
       "  75,\n",
       "  33,\n",
       "  2618,\n",
       "  5,\n",
       "  2619,\n",
       "  2620,\n",
       "  21,\n",
       "  1,\n",
       "  471,\n",
       "  2621,\n",
       "  28,\n",
       "  5,\n",
       "  1,\n",
       "  1003,\n",
       "  399,\n",
       "  1004,\n",
       "  84],\n",
       " [1,\n",
       "  29,\n",
       "  421,\n",
       "  616,\n",
       "  67,\n",
       "  397,\n",
       "  10,\n",
       "  317,\n",
       "  562,\n",
       "  134,\n",
       "  36,\n",
       "  1,\n",
       "  1032,\n",
       "  21,\n",
       "  1,\n",
       "  642,\n",
       "  103,\n",
       "  11,\n",
       "  1,\n",
       "  2622,\n",
       "  67,\n",
       "  397,\n",
       "  61,\n",
       "  23,\n",
       "  317,\n",
       "  975,\n",
       "  134,\n",
       "  36],\n",
       " [1,\n",
       "  1825,\n",
       "  843,\n",
       "  824,\n",
       "  62,\n",
       "  13,\n",
       "  9,\n",
       "  860,\n",
       "  1,\n",
       "  766,\n",
       "  861,\n",
       "  30,\n",
       "  9,\n",
       "  7,\n",
       "  209,\n",
       "  21,\n",
       "  7,\n",
       "  501,\n",
       "  5,\n",
       "  1304,\n",
       "  2623,\n",
       "  1826,\n",
       "  4,\n",
       "  1827,\n",
       "  329,\n",
       "  1,\n",
       "  1825,\n",
       "  843,\n",
       "  183,\n",
       "  1826,\n",
       "  4,\n",
       "  1827,\n",
       "  329,\n",
       "  351,\n",
       "  5,\n",
       "  1828,\n",
       "  1226,\n",
       "  63,\n",
       "  11,\n",
       "  111,\n",
       "  5,\n",
       "  42,\n",
       "  253,\n",
       "  1829,\n",
       "  63,\n",
       "  32,\n",
       "  617,\n",
       "  4,\n",
       "  1,\n",
       "  62,\n",
       "  13,\n",
       "  9,\n",
       "  860,\n",
       "  1,\n",
       "  1829,\n",
       "  30,\n",
       "  255,\n",
       "  72,\n",
       "  9,\n",
       "  1,\n",
       "  1226],\n",
       " [6,\n",
       "  688,\n",
       "  24,\n",
       "  93,\n",
       "  10,\n",
       "  1,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  25,\n",
       "  83,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  25,\n",
       "  9,\n",
       "  121,\n",
       "  51,\n",
       "  41,\n",
       "  65,\n",
       "  15,\n",
       "  8,\n",
       "  48,\n",
       "  167,\n",
       "  386,\n",
       "  105,\n",
       "  11,\n",
       "  28,\n",
       "  824,\n",
       "  4,\n",
       "  11,\n",
       "  1,\n",
       "  235,\n",
       "  49,\n",
       "  15,\n",
       "  11,\n",
       "  1830,\n",
       "  143],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  445,\n",
       "  81,\n",
       "  5,\n",
       "  256,\n",
       "  416,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  1,\n",
       "  1053,\n",
       "  15,\n",
       "  13,\n",
       "  7,\n",
       "  256,\n",
       "  416,\n",
       "  1053,\n",
       "  8,\n",
       "  1,\n",
       "  81,\n",
       "  176,\n",
       "  26,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [11,\n",
       "  48,\n",
       "  265,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  1,\n",
       "  70,\n",
       "  857,\n",
       "  11,\n",
       "  48,\n",
       "  62,\n",
       "  33,\n",
       "  174,\n",
       "  21,\n",
       "  1,\n",
       "  1305,\n",
       "  54,\n",
       "  325,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [1,\n",
       "  70,\n",
       "  89,\n",
       "  395,\n",
       "  10,\n",
       "  364,\n",
       "  311,\n",
       "  802,\n",
       "  32,\n",
       "  460,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  6,\n",
       "  15,\n",
       "  77,\n",
       "  80,\n",
       "  11,\n",
       "  281,\n",
       "  70,\n",
       "  89,\n",
       "  12,\n",
       "  36],\n",
       " [6,\n",
       "  192,\n",
       "  232,\n",
       "  30,\n",
       "  73,\n",
       "  14,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  8,\n",
       "  105,\n",
       "  398,\n",
       "  14,\n",
       "  1,\n",
       "  107,\n",
       "  106,\n",
       "  15,\n",
       "  8,\n",
       "  1651,\n",
       "  15,\n",
       "  1,\n",
       "  59,\n",
       "  108,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  657,\n",
       "  1,\n",
       "  30,\n",
       "  132],\n",
       " [6,\n",
       "  66,\n",
       "  7,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  23,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  703,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  6,\n",
       "  688,\n",
       "  24,\n",
       "  93,\n",
       "  10,\n",
       "  1,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  25],\n",
       " [6,\n",
       "  15,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  341,\n",
       "  1,\n",
       "  63,\n",
       "  8,\n",
       "  1,\n",
       "  1181,\n",
       "  192,\n",
       "  232,\n",
       "  30,\n",
       "  73,\n",
       "  14,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  8,\n",
       "  105,\n",
       "  398,\n",
       "  14,\n",
       "  1,\n",
       "  107,\n",
       "  106,\n",
       "  15,\n",
       "  8,\n",
       "  17],\n",
       " [1,\n",
       "  126,\n",
       "  1167,\n",
       "  67,\n",
       "  421,\n",
       "  26,\n",
       "  949,\n",
       "  5,\n",
       "  1168,\n",
       "  30,\n",
       "  132,\n",
       "  153,\n",
       "  14,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  1,\n",
       "  30,\n",
       "  73,\n",
       "  33,\n",
       "  153,\n",
       "  26,\n",
       "  464,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [1831,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  214,\n",
       "  1832,\n",
       "  776,\n",
       "  19,\n",
       "  9,\n",
       "  607,\n",
       "  4,\n",
       "  707,\n",
       "  2624,\n",
       "  2625,\n",
       "  8,\n",
       "  248,\n",
       "  1833,\n",
       "  1834,\n",
       "  30,\n",
       "  575,\n",
       "  2626,\n",
       "  1,\n",
       "  2627,\n",
       "  5,\n",
       "  1833,\n",
       "  1834,\n",
       "  298,\n",
       "  30,\n",
       "  575,\n",
       "  1,\n",
       "  1832,\n",
       "  776,\n",
       "  128,\n",
       "  97,\n",
       "  26,\n",
       "  1831,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  1,\n",
       "  2628,\n",
       "  793,\n",
       "  4,\n",
       "  1,\n",
       "  2629,\n",
       "  2630,\n",
       "  2631,\n",
       "  1,\n",
       "  1835,\n",
       "  5],\n",
       " [561,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  95,\n",
       "  97,\n",
       "  9,\n",
       "  18,\n",
       "  30,\n",
       "  658,\n",
       "  31,\n",
       "  93,\n",
       "  9,\n",
       "  554,\n",
       "  1280,\n",
       "  70,\n",
       "  55,\n",
       "  227,\n",
       "  11,\n",
       "  44,\n",
       "  4,\n",
       "  2632,\n",
       "  269,\n",
       "  9,\n",
       "  1836,\n",
       "  1,\n",
       "  1306,\n",
       "  5,\n",
       "  247,\n",
       "  28,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  55,\n",
       "  52,\n",
       "  561,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  15,\n",
       "  30,\n",
       "  658,\n",
       "  260,\n",
       "  21,\n",
       "  2633,\n",
       "  28,\n",
       "  31,\n",
       "  93,\n",
       "  11,\n",
       "  46,\n",
       "  7,\n",
       "  52],\n",
       " [327,\n",
       "  439,\n",
       "  570,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  417,\n",
       "  706,\n",
       "  7,\n",
       "  29,\n",
       "  11,\n",
       "  279,\n",
       "  115,\n",
       "  4,\n",
       "  221,\n",
       "  5,\n",
       "  292,\n",
       "  48,\n",
       "  29,\n",
       "  154,\n",
       "  104,\n",
       "  15,\n",
       "  9,\n",
       "  554,\n",
       "  1,\n",
       "  623,\n",
       "  5,\n",
       "  1837,\n",
       "  1838,\n",
       "  29,\n",
       "  2634,\n",
       "  1,\n",
       "  1721,\n",
       "  32,\n",
       "  1213,\n",
       "  14,\n",
       "  1,\n",
       "  279,\n",
       "  115,\n",
       "  4,\n",
       "  221,\n",
       "  29,\n",
       "  706,\n",
       "  570,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  34,\n",
       "  2635,\n",
       "  7,\n",
       "  649,\n",
       "  2636,\n",
       "  5,\n",
       "  1837,\n",
       "  1839],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  14,\n",
       "  124,\n",
       "  107,\n",
       "  1840,\n",
       "  359,\n",
       "  24,\n",
       "  38,\n",
       "  49,\n",
       "  1,\n",
       "  177,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  33,\n",
       "  15,\n",
       "  8,\n",
       "  124,\n",
       "  60,\n",
       "  930],\n",
       " [423,\n",
       "  366,\n",
       "  1841,\n",
       "  6,\n",
       "  18,\n",
       "  1842,\n",
       "  647,\n",
       "  14,\n",
       "  1843,\n",
       "  173,\n",
       "  435,\n",
       "  83,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  1844,\n",
       "  366,\n",
       "  312,\n",
       "  184,\n",
       "  8,\n",
       "  445,\n",
       "  14,\n",
       "  1,\n",
       "  107,\n",
       "  302,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [6,\n",
       "  18,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  14,\n",
       "  1,\n",
       "  331,\n",
       "  78,\n",
       "  166,\n",
       "  75,\n",
       "  9,\n",
       "  2637,\n",
       "  18,\n",
       "  428,\n",
       "  78,\n",
       "  9,\n",
       "  69,\n",
       "  19,\n",
       "  106,\n",
       "  591,\n",
       "  1,\n",
       "  106,\n",
       "  10,\n",
       "  1,\n",
       "  150,\n",
       "  103,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [1,\n",
       "  235,\n",
       "  58,\n",
       "  67,\n",
       "  196,\n",
       "  179,\n",
       "  10,\n",
       "  1,\n",
       "  60,\n",
       "  634,\n",
       "  27,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  101,\n",
       "  49,\n",
       "  32,\n",
       "  66,\n",
       "  14,\n",
       "  1,\n",
       "  177,\n",
       "  27,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  150,\n",
       "  82,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  9,\n",
       "  374,\n",
       "  285,\n",
       "  1,\n",
       "  78,\n",
       "  166,\n",
       "  11,\n",
       "  111,\n",
       "  1845,\n",
       "  18,\n",
       "  428,\n",
       "  78,\n",
       "  9,\n",
       "  69,\n",
       "  19,\n",
       "  106,\n",
       "  591,\n",
       "  1,\n",
       "  106,\n",
       "  10,\n",
       "  1,\n",
       "  150,\n",
       "  103,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [771,\n",
       "  1,\n",
       "  991,\n",
       "  1846,\n",
       "  144,\n",
       "  1,\n",
       "  1847,\n",
       "  22,\n",
       "  338,\n",
       "  2,\n",
       "  3,\n",
       "  110,\n",
       "  13,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  255,\n",
       "  733,\n",
       "  2638,\n",
       "  1847,\n",
       "  22,\n",
       "  338,\n",
       "  2,\n",
       "  3,\n",
       "  110,\n",
       "  13,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  2639,\n",
       "  2640,\n",
       "  169,\n",
       "  447,\n",
       "  1848,\n",
       "  144,\n",
       "  11,\n",
       "  1685],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  490,\n",
       "  158,\n",
       "  168,\n",
       "  56,\n",
       "  99,\n",
       "  316,\n",
       "  4,\n",
       "  134,\n",
       "  25,\n",
       "  184,\n",
       "  11,\n",
       "  46,\n",
       "  4,\n",
       "  232,\n",
       "  2641,\n",
       "  1041,\n",
       "  11,\n",
       "  2642,\n",
       "  2643,\n",
       "  158,\n",
       "  168,\n",
       "  56,\n",
       "  316,\n",
       "  4,\n",
       "  134,\n",
       "  25,\n",
       "  184,\n",
       "  13,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  169,\n",
       "  46,\n",
       "  28],\n",
       " [6,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  274,\n",
       "  4,\n",
       "  1282,\n",
       "  51,\n",
       "  5,\n",
       "  1,\n",
       "  144,\n",
       "  10,\n",
       "  1283,\n",
       "  787,\n",
       "  14,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  83],\n",
       " [6,\n",
       "  66,\n",
       "  7,\n",
       "  862,\n",
       "  433,\n",
       "  19,\n",
       "  21,\n",
       "  1,\n",
       "  239,\n",
       "  470,\n",
       "  22,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  2644,\n",
       "  32,\n",
       "  374,\n",
       "  421,\n",
       "  574,\n",
       "  30,\n",
       "  73,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  84,\n",
       "  23,\n",
       "  85,\n",
       "  2645,\n",
       "  192,\n",
       "  414,\n",
       "  18,\n",
       "  5,\n",
       "  1,\n",
       "  59,\n",
       "  108,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  84,\n",
       "  9,\n",
       "  262,\n",
       "  30,\n",
       "  73,\n",
       "  23,\n",
       "  1,\n",
       "  85,\n",
       "  144],\n",
       " [523,\n",
       "  32,\n",
       "  112,\n",
       "  1849,\n",
       "  994,\n",
       "  9,\n",
       "  686,\n",
       "  2646,\n",
       "  28,\n",
       "  1288,\n",
       "  4,\n",
       "  470,\n",
       "  990,\n",
       "  1850,\n",
       "  88,\n",
       "  523,\n",
       "  32,\n",
       "  112,\n",
       "  994,\n",
       "  97,\n",
       "  8,\n",
       "  1,\n",
       "  2647,\n",
       "  9,\n",
       "  2648,\n",
       "  48,\n",
       "  304,\n",
       "  1850,\n",
       "  88],\n",
       " [6,\n",
       "  223,\n",
       "  101,\n",
       "  58,\n",
       "  11,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  50,\n",
       "  7,\n",
       "  19,\n",
       "  10,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  23,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  31,\n",
       "  24,\n",
       "  101,\n",
       "  29],\n",
       " [1,\n",
       "  62,\n",
       "  5,\n",
       "  2649,\n",
       "  689,\n",
       "  9,\n",
       "  1292,\n",
       "  1307,\n",
       "  8,\n",
       "  1152,\n",
       "  133,\n",
       "  4,\n",
       "  938,\n",
       "  172,\n",
       "  689,\n",
       "  9,\n",
       "  7,\n",
       "  804,\n",
       "  863,\n",
       "  33,\n",
       "  439,\n",
       "  97,\n",
       "  8,\n",
       "  1851,\n",
       "  1852,\n",
       "  143,\n",
       "  1853,\n",
       "  88,\n",
       "  1854,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  6,\n",
       "  289,\n",
       "  1,\n",
       "  1535,\n",
       "  5,\n",
       "  290,\n",
       "  22,\n",
       "  9,\n",
       "  104,\n",
       "  1,\n",
       "  183,\n",
       "  21,\n",
       "  1,\n",
       "  62,\n",
       "  9,\n",
       "  909,\n",
       "  2650,\n",
       "  689,\n",
       "  8,\n",
       "  2651,\n",
       "  133,\n",
       "  2652,\n",
       "  2653,\n",
       "  26,\n",
       "  1851,\n",
       "  1852,\n",
       "  143,\n",
       "  1853,\n",
       "  2654,\n",
       "  8,\n",
       "  88,\n",
       "  1854,\n",
       "  2,\n",
       "  3,\n",
       "  88],\n",
       " [6,\n",
       "  66,\n",
       "  7,\n",
       "  862,\n",
       "  433,\n",
       "  19,\n",
       "  21,\n",
       "  1,\n",
       "  239,\n",
       "  470,\n",
       "  22,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  108,\n",
       "  152,\n",
       "  170,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  408,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  76,\n",
       "  18,\n",
       "  2655,\n",
       "  916,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  9,\n",
       "  2656,\n",
       "  4,\n",
       "  274,\n",
       "  1,\n",
       "  292,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  1181,\n",
       "  18,\n",
       "  1,\n",
       "  1131,\n",
       "  152,\n",
       "  916,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  9,\n",
       "  2657,\n",
       "  1,\n",
       "  920,\n",
       "  4,\n",
       "  1,\n",
       "  279,\n",
       "  93,\n",
       "  5,\n",
       "  1,\n",
       "  113,\n",
       "  4,\n",
       "  296,\n",
       "  35],\n",
       " [105,\n",
       "  46,\n",
       "  4,\n",
       "  281,\n",
       "  28,\n",
       "  2658,\n",
       "  5,\n",
       "  2659,\n",
       "  1308,\n",
       "  174,\n",
       "  21,\n",
       "  1,\n",
       "  1054,\n",
       "  22,\n",
       "  278,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  1,\n",
       "  28,\n",
       "  176,\n",
       "  11,\n",
       "  1,\n",
       "  159,\n",
       "  62,\n",
       "  13,\n",
       "  1855,\n",
       "  21,\n",
       "  1,\n",
       "  1054,\n",
       "  22,\n",
       "  278,\n",
       "  2,\n",
       "  3,\n",
       "  90],\n",
       " [6,\n",
       "  223,\n",
       "  101,\n",
       "  58,\n",
       "  11,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  66,\n",
       "  7,\n",
       "  862,\n",
       "  433,\n",
       "  19,\n",
       "  21,\n",
       "  1,\n",
       "  239,\n",
       "  470,\n",
       "  22,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [9,\n",
       "  219,\n",
       "  24,\n",
       "  186,\n",
       "  119,\n",
       "  93,\n",
       "  6,\n",
       "  118,\n",
       "  680,\n",
       "  1,\n",
       "  1856,\n",
       "  10,\n",
       "  1,\n",
       "  272,\n",
       "  119,\n",
       "  171,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  943,\n",
       "  171,\n",
       "  176,\n",
       "  26,\n",
       "  272,\n",
       "  11,\n",
       "  24,\n",
       "  119,\n",
       "  198,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [1,\n",
       "  144,\n",
       "  32,\n",
       "  400,\n",
       "  4,\n",
       "  466,\n",
       "  10,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  274,\n",
       "  4,\n",
       "  1282,\n",
       "  51,\n",
       "  5,\n",
       "  1,\n",
       "  144,\n",
       "  10,\n",
       "  1283,\n",
       "  787,\n",
       "  14,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  83],\n",
       " [6,\n",
       "  50,\n",
       "  7,\n",
       "  19,\n",
       "  10,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  23,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  31,\n",
       "  24,\n",
       "  101,\n",
       "  2660,\n",
       "  101,\n",
       "  13,\n",
       "  7,\n",
       "  27,\n",
       "  235,\n",
       "  29,\n",
       "  50,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  1309,\n",
       "  4,\n",
       "  147,\n",
       "  1055,\n",
       "  6,\n",
       "  15,\n",
       "  1857,\n",
       "  1858,\n",
       "  1055,\n",
       "  21,\n",
       "  1,\n",
       "  1310,\n",
       "  279,\n",
       "  399,\n",
       "  1056,\n",
       "  2,\n",
       "  3,\n",
       "  165,\n",
       "  11,\n",
       "  44,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  1310,\n",
       "  399,\n",
       "  1056,\n",
       "  2,\n",
       "  3,\n",
       "  165,\n",
       "  9,\n",
       "  864,\n",
       "  44,\n",
       "  63,\n",
       "  283,\n",
       "  2661],\n",
       " [192,\n",
       "  6,\n",
       "  1138,\n",
       "  30,\n",
       "  73,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  14,\n",
       "  1,\n",
       "  107,\n",
       "  356,\n",
       "  73,\n",
       "  538,\n",
       "  1139,\n",
       "  192,\n",
       "  232,\n",
       "  30,\n",
       "  73,\n",
       "  14,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  8,\n",
       "  105,\n",
       "  398,\n",
       "  14,\n",
       "  1,\n",
       "  107,\n",
       "  106,\n",
       "  15,\n",
       "  8,\n",
       "  17],\n",
       " [409,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  95,\n",
       "  516,\n",
       "  1,\n",
       "  2662,\n",
       "  5,\n",
       "  865,\n",
       "  56,\n",
       "  78,\n",
       "  1057,\n",
       "  9,\n",
       "  1,\n",
       "  2663,\n",
       "  2664,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  18,\n",
       "  56,\n",
       "  78,\n",
       "  381,\n",
       "  1186,\n",
       "  99,\n",
       "  4,\n",
       "  913,\n",
       "  9,\n",
       "  1000,\n",
       "  1311,\n",
       "  23,\n",
       "  694,\n",
       "  695],\n",
       " [6,\n",
       "  69,\n",
       "  1,\n",
       "  324,\n",
       "  1058,\n",
       "  811,\n",
       "  10,\n",
       "  1059,\n",
       "  1060,\n",
       "  315,\n",
       "  83,\n",
       "  14,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  18,\n",
       "  428,\n",
       "  78,\n",
       "  9,\n",
       "  69,\n",
       "  19,\n",
       "  106,\n",
       "  591,\n",
       "  1,\n",
       "  106,\n",
       "  10,\n",
       "  1,\n",
       "  150,\n",
       "  103,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [11,\n",
       "  1,\n",
       "  683,\n",
       "  1312,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  367,\n",
       "  207,\n",
       "  313,\n",
       "  100,\n",
       "  22,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  42,\n",
       "  240,\n",
       "  450,\n",
       "  11,\n",
       "  698,\n",
       "  21,\n",
       "  800,\n",
       "  571,\n",
       "  9,\n",
       "  1313,\n",
       "  153,\n",
       "  21,\n",
       "  206,\n",
       "  83,\n",
       "  1061,\n",
       "  30,\n",
       "  2665,\n",
       "  1062,\n",
       "  22,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  13,\n",
       "  7,\n",
       "  183,\n",
       "  1242,\n",
       "  5,\n",
       "  1,\n",
       "  450,\n",
       "  11,\n",
       "  698,\n",
       "  153,\n",
       "  21,\n",
       "  83,\n",
       "  1061,\n",
       "  494,\n",
       "  533,\n",
       "  63,\n",
       "  5,\n",
       "  44,\n",
       "  207,\n",
       "  133,\n",
       "  1859,\n",
       "  9,\n",
       "  7,\n",
       "  525,\n",
       "  1835,\n",
       "  1860,\n",
       "  772],\n",
       " [6,\n",
       "  66,\n",
       "  7,\n",
       "  862,\n",
       "  433,\n",
       "  19,\n",
       "  21,\n",
       "  1,\n",
       "  239,\n",
       "  470,\n",
       "  22,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  144,\n",
       "  32,\n",
       "  400,\n",
       "  4,\n",
       "  466,\n",
       "  10,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [423,\n",
       "  366,\n",
       "  1841,\n",
       "  6,\n",
       "  18,\n",
       "  1842,\n",
       "  647,\n",
       "  14,\n",
       "  1843,\n",
       "  173,\n",
       "  435,\n",
       "  83,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  423,\n",
       "  271,\n",
       "  1314,\n",
       "  6,\n",
       "  18,\n",
       "  271,\n",
       "  199,\n",
       "  14,\n",
       "  517,\n",
       "  315,\n",
       "  798,\n",
       "  4,\n",
       "  1315,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [6,\n",
       "  66,\n",
       "  7,\n",
       "  862,\n",
       "  433,\n",
       "  19,\n",
       "  21,\n",
       "  1,\n",
       "  239,\n",
       "  470,\n",
       "  22,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  274,\n",
       "  4,\n",
       "  737,\n",
       "  1,\n",
       "  28,\n",
       "  14,\n",
       "  1,\n",
       "  60,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [68,\n",
       "  2666,\n",
       "  155,\n",
       "  2667,\n",
       "  1050,\n",
       "  41,\n",
       "  19,\n",
       "  13,\n",
       "  407,\n",
       "  10,\n",
       "  1,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  1,\n",
       "  41,\n",
       "  19,\n",
       "  13,\n",
       "  7,\n",
       "  60,\n",
       "  100,\n",
       "  19,\n",
       "  407,\n",
       "  21,\n",
       "  1,\n",
       "  427,\n",
       "  28,\n",
       "  10,\n",
       "  155,\n",
       "  116,\n",
       "  173,\n",
       "  1018,\n",
       "  567,\n",
       "  865,\n",
       "  102,\n",
       "  1248,\n",
       "  96,\n",
       "  2,\n",
       "  3,\n",
       "  88],\n",
       " [11,\n",
       "  1563,\n",
       "  28,\n",
       "  697,\n",
       "  2668,\n",
       "  853,\n",
       "  18,\n",
       "  5,\n",
       "  1,\n",
       "  207,\n",
       "  313,\n",
       "  100,\n",
       "  22,\n",
       "  969,\n",
       "  1062,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  836,\n",
       "  450,\n",
       "  21,\n",
       "  1,\n",
       "  207,\n",
       "  313,\n",
       "  100,\n",
       "  22,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  9,\n",
       "  535,\n",
       "  1,\n",
       "  467,\n",
       "  5,\n",
       "  18,\n",
       "  5,\n",
       "  111,\n",
       "  624,\n",
       "  843],\n",
       " [6,\n",
       "  216,\n",
       "  70,\n",
       "  89,\n",
       "  10,\n",
       "  364,\n",
       "  311,\n",
       "  297,\n",
       "  260,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  1,\n",
       "  1861,\n",
       "  2669,\n",
       "  1862,\n",
       "  364,\n",
       "  311,\n",
       "  297,\n",
       "  2670,\n",
       "  9,\n",
       "  1,\n",
       "  1035,\n",
       "  29,\n",
       "  10,\n",
       "  1,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  12,\n",
       "  36],\n",
       " [83,\n",
       "  1,\n",
       "  65,\n",
       "  32,\n",
       "  823,\n",
       "  10,\n",
       "  1225,\n",
       "  405,\n",
       "  189,\n",
       "  163,\n",
       "  31,\n",
       "  175,\n",
       "  1316,\n",
       "  202,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  14,\n",
       "  107,\n",
       "  435,\n",
       "  1317,\n",
       "  202,\n",
       "  1318,\n",
       "  81,\n",
       "  5,\n",
       "  99,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  33,\n",
       "  15,\n",
       "  31,\n",
       "  184,\n",
       "  14,\n",
       "  107,\n",
       "  435,\n",
       "  302],\n",
       " [98,\n",
       "  492,\n",
       "  461,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  979,\n",
       "  142,\n",
       "  546,\n",
       "  23,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  34,\n",
       "  72,\n",
       "  363,\n",
       "  1010,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  114,\n",
       "  127],\n",
       " [849,\n",
       "  1,\n",
       "  112,\n",
       "  934,\n",
       "  2671,\n",
       "  564,\n",
       "  21,\n",
       "  1,\n",
       "  217,\n",
       "  137,\n",
       "  187,\n",
       "  485,\n",
       "  336,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  5,\n",
       "  7,\n",
       "  1863,\n",
       "  4,\n",
       "  7,\n",
       "  1319,\n",
       "  293,\n",
       "  1864,\n",
       "  2672,\n",
       "  4,\n",
       "  1865,\n",
       "  90,\n",
       "  15,\n",
       "  137,\n",
       "  242,\n",
       "  5,\n",
       "  1,\n",
       "  217,\n",
       "  137,\n",
       "  187,\n",
       "  336,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  31,\n",
       "  7,\n",
       "  250],\n",
       " [1143,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  2673,\n",
       "  34,\n",
       "  63,\n",
       "  14,\n",
       "  72,\n",
       "  157,\n",
       "  95,\n",
       "  72,\n",
       "  1697,\n",
       "  959,\n",
       "  394,\n",
       "  91,\n",
       "  142,\n",
       "  546,\n",
       "  23,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  34,\n",
       "  72,\n",
       "  363,\n",
       "  1010,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  114,\n",
       "  127],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  259,\n",
       "  222,\n",
       "  31,\n",
       "  1035,\n",
       "  2674,\n",
       "  42,\n",
       "  13,\n",
       "  2675,\n",
       "  23,\n",
       "  1,\n",
       "  422,\n",
       "  75,\n",
       "  10,\n",
       "  1,\n",
       "  1866,\n",
       "  1867,\n",
       "  103,\n",
       "  1868,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  1,\n",
       "  487,\n",
       "  13,\n",
       "  233,\n",
       "  10,\n",
       "  1,\n",
       "  1866,\n",
       "  1867,\n",
       "  103,\n",
       "  21,\n",
       "  1,\n",
       "  1244,\n",
       "  2676,\n",
       "  2677,\n",
       "  1868,\n",
       "  2,\n",
       "  3,\n",
       "  87],\n",
       " [1,\n",
       "  293,\n",
       "  792,\n",
       "  62,\n",
       "  5,\n",
       "  2678,\n",
       "  386,\n",
       "  13,\n",
       "  2679,\n",
       "  4,\n",
       "  1869,\n",
       "  332,\n",
       "  21,\n",
       "  205,\n",
       "  130,\n",
       "  293,\n",
       "  792,\n",
       "  62,\n",
       "  252,\n",
       "  1870,\n",
       "  62,\n",
       "  344,\n",
       "  1005,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  1,\n",
       "  1870,\n",
       "  62,\n",
       "  344,\n",
       "  183,\n",
       "  13,\n",
       "  7,\n",
       "  733,\n",
       "  15,\n",
       "  1871,\n",
       "  11,\n",
       "  293,\n",
       "  215,\n",
       "  1005,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [56,\n",
       "  35,\n",
       "  29,\n",
       "  302,\n",
       "  6,\n",
       "  15,\n",
       "  7,\n",
       "  27,\n",
       "  70,\n",
       "  56,\n",
       "  35,\n",
       "  19,\n",
       "  31,\n",
       "  175,\n",
       "  8,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  56,\n",
       "  1320,\n",
       "  225,\n",
       "  1,\n",
       "  19,\n",
       "  659,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  35,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  214,\n",
       "  9,\n",
       "  1063,\n",
       "  645],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  866,\n",
       "  320,\n",
       "  34,\n",
       "  32,\n",
       "  660,\n",
       "  26,\n",
       "  1,\n",
       "  264,\n",
       "  44,\n",
       "  251,\n",
       "  146,\n",
       "  380,\n",
       "  321,\n",
       "  84,\n",
       "  1,\n",
       "  58,\n",
       "  32,\n",
       "  196,\n",
       "  179,\n",
       "  23,\n",
       "  7,\n",
       "  2680,\n",
       "  2681,\n",
       "  264,\n",
       "  146,\n",
       "  11,\n",
       "  44,\n",
       "  1,\n",
       "  2682,\n",
       "  44,\n",
       "  251,\n",
       "  146,\n",
       "  380,\n",
       "  277,\n",
       "  4,\n",
       "  321,\n",
       "  84],\n",
       " [8,\n",
       "  111,\n",
       "  957,\n",
       "  1,\n",
       "  722,\n",
       "  5,\n",
       "  2683,\n",
       "  2684,\n",
       "  38,\n",
       "  206,\n",
       "  1,\n",
       "  101,\n",
       "  38,\n",
       "  13,\n",
       "  640,\n",
       "  454,\n",
       "  1196,\n",
       "  5,\n",
       "  2685,\n",
       "  10,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  8,\n",
       "  51,\n",
       "  1064,\n",
       "  188,\n",
       "  32,\n",
       "  640,\n",
       "  454,\n",
       "  2686,\n",
       "  280,\n",
       "  1,\n",
       "  654,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36],\n",
       " [9,\n",
       "  1872,\n",
       "  2687,\n",
       "  1321,\n",
       "  1140,\n",
       "  462,\n",
       "  231,\n",
       "  94,\n",
       "  6,\n",
       "  219,\n",
       "  7,\n",
       "  501,\n",
       "  5,\n",
       "  2688,\n",
       "  611,\n",
       "  42,\n",
       "  547,\n",
       "  359,\n",
       "  1065,\n",
       "  856,\n",
       "  2689,\n",
       "  294,\n",
       "  2690,\n",
       "  21,\n",
       "  1,\n",
       "  710,\n",
       "  1873,\n",
       "  486,\n",
       "  1874,\n",
       "  2,\n",
       "  3,\n",
       "  896,\n",
       "  6,\n",
       "  219,\n",
       "  7,\n",
       "  501,\n",
       "  890,\n",
       "  925,\n",
       "  2691,\n",
       "  1875,\n",
       "  611,\n",
       "  21,\n",
       "  249,\n",
       "  4,\n",
       "  7,\n",
       "  501,\n",
       "  890,\n",
       "  925,\n",
       "  963,\n",
       "  611,\n",
       "  34,\n",
       "  547,\n",
       "  2692,\n",
       "  1876,\n",
       "  21,\n",
       "  1,\n",
       "  710,\n",
       "  1873,\n",
       "  486,\n",
       "  1874,\n",
       "  2,\n",
       "  3,\n",
       "  896],\n",
       " [1,\n",
       "  2693,\n",
       "  287,\n",
       "  228,\n",
       "  33,\n",
       "  7,\n",
       "  60,\n",
       "  2694,\n",
       "  1877,\n",
       "  1733,\n",
       "  10,\n",
       "  1,\n",
       "  139,\n",
       "  93,\n",
       "  31,\n",
       "  661,\n",
       "  2,\n",
       "  3,\n",
       "  40,\n",
       "  1,\n",
       "  212,\n",
       "  5,\n",
       "  93,\n",
       "  174,\n",
       "  21,\n",
       "  1,\n",
       "  2695,\n",
       "  46,\n",
       "  75,\n",
       "  33,\n",
       "  1150,\n",
       "  2696,\n",
       "  2697,\n",
       "  10,\n",
       "  1,\n",
       "  250,\n",
       "  75,\n",
       "  1878,\n",
       "  26,\n",
       "  661,\n",
       "  2,\n",
       "  3,\n",
       "  40],\n",
       " [8,\n",
       "  24,\n",
       "  167,\n",
       "  252,\n",
       "  1879,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  6,\n",
       "  76,\n",
       "  849,\n",
       "  1,\n",
       "  137,\n",
       "  575,\n",
       "  62,\n",
       "  31,\n",
       "  7,\n",
       "  790,\n",
       "  1322,\n",
       "  2698,\n",
       "  9,\n",
       "  1,\n",
       "  167,\n",
       "  5,\n",
       "  1879,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  24,\n",
       "  863,\n",
       "  19,\n",
       "  481,\n",
       "  805,\n",
       "  256,\n",
       "  1880,\n",
       "  83,\n",
       "  9,\n",
       "  607,\n",
       "  7,\n",
       "  790,\n",
       "  1322,\n",
       "  19],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  28,\n",
       "  34,\n",
       "  67,\n",
       "  2699,\n",
       "  4,\n",
       "  442,\n",
       "  26,\n",
       "  1066,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  169,\n",
       "  11,\n",
       "  2700,\n",
       "  8,\n",
       "  248,\n",
       "  1881,\n",
       "  428,\n",
       "  2701,\n",
       "  583,\n",
       "  673,\n",
       "  5,\n",
       "  1,\n",
       "  1123,\n",
       "  2702,\n",
       "  28,\n",
       "  1414,\n",
       "  4,\n",
       "  599,\n",
       "  154,\n",
       "  104,\n",
       "  289,\n",
       "  8,\n",
       "  1066,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  4,\n",
       "  1,\n",
       "  1881,\n",
       "  2703],\n",
       " [6,\n",
       "  50,\n",
       "  1882,\n",
       "  55,\n",
       "  1067,\n",
       "  11,\n",
       "  425,\n",
       "  394,\n",
       "  10,\n",
       "  1,\n",
       "  1323,\n",
       "  159,\n",
       "  62,\n",
       "  691,\n",
       "  1068,\n",
       "  4,\n",
       "  1069,\n",
       "  37,\n",
       "  292,\n",
       "  867,\n",
       "  735,\n",
       "  1285,\n",
       "  1324,\n",
       "  4,\n",
       "  2704,\n",
       "  1882,\n",
       "  227,\n",
       "  58,\n",
       "  1304,\n",
       "  394,\n",
       "  21,\n",
       "  1,\n",
       "  502,\n",
       "  1766,\n",
       "  159,\n",
       "  62,\n",
       "  32,\n",
       "  15,\n",
       "  867,\n",
       "  735,\n",
       "  1324,\n",
       "  4,\n",
       "  954,\n",
       "  1068,\n",
       "  4,\n",
       "  1069,\n",
       "  37],\n",
       " [24,\n",
       "  301,\n",
       "  513,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  300,\n",
       "  384,\n",
       "  734,\n",
       "  32,\n",
       "  2705,\n",
       "  14,\n",
       "  797,\n",
       "  5,\n",
       "  346,\n",
       "  347,\n",
       "  84,\n",
       "  23,\n",
       "  1,\n",
       "  145,\n",
       "  777,\n",
       "  4,\n",
       "  32,\n",
       "  2706,\n",
       "  9,\n",
       "  2707,\n",
       "  14,\n",
       "  1,\n",
       "  1883,\n",
       "  211,\n",
       "  2708,\n",
       "  4,\n",
       "  868,\n",
       "  23,\n",
       "  1,\n",
       "  205,\n",
       "  2709,\n",
       "  1,\n",
       "  234,\n",
       "  5,\n",
       "  346,\n",
       "  347,\n",
       "  84,\n",
       "  9,\n",
       "  104,\n",
       "  514,\n",
       "  6,\n",
       "  95,\n",
       "  97,\n",
       "  7,\n",
       "  300,\n",
       "  384,\n",
       "  5,\n",
       "  734,\n",
       "  4,\n",
       "  1,\n",
       "  653,\n",
       "  301,\n",
       "  515,\n",
       "  8,\n",
       "  1047,\n",
       "  14,\n",
       "  248,\n",
       "  478,\n",
       "  301,\n",
       "  515],\n",
       " [1,\n",
       "  780,\n",
       "  146,\n",
       "  2710,\n",
       "  1,\n",
       "  1884,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  780,\n",
       "  187,\n",
       "  125,\n",
       "  638,\n",
       "  2,\n",
       "  3,\n",
       "  110,\n",
       "  1,\n",
       "  128,\n",
       "  509,\n",
       "  8,\n",
       "  48,\n",
       "  263,\n",
       "  13,\n",
       "  7,\n",
       "  118,\n",
       "  1116,\n",
       "  9,\n",
       "  1885,\n",
       "  848,\n",
       "  729,\n",
       "  227,\n",
       "  49,\n",
       "  61,\n",
       "  23,\n",
       "  2711,\n",
       "  2712,\n",
       "  2713,\n",
       "  902,\n",
       "  9,\n",
       "  1,\n",
       "  1884,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  969,\n",
       "  780],\n",
       " [640,\n",
       "  454,\n",
       "  188,\n",
       "  752,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  11,\n",
       "  259,\n",
       "  4,\n",
       "  1114,\n",
       "  32,\n",
       "  2135,\n",
       "  14,\n",
       "  2136,\n",
       "  228,\n",
       "  754,\n",
       "  4,\n",
       "  228,\n",
       "  2714,\n",
       "  752,\n",
       "  89,\n",
       "  10,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36],\n",
       " [8,\n",
       "  281,\n",
       "  6,\n",
       "  15,\n",
       "  525,\n",
       "  312,\n",
       "  1070,\n",
       "  282,\n",
       "  1071,\n",
       "  4,\n",
       "  1072,\n",
       "  36,\n",
       "  1188,\n",
       "  567,\n",
       "  4,\n",
       "  1,\n",
       "  1886,\n",
       "  790,\n",
       "  19,\n",
       "  1200,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  1325,\n",
       "  1,\n",
       "  35,\n",
       "  14,\n",
       "  525,\n",
       "  312,\n",
       "  1070,\n",
       "  1071,\n",
       "  4,\n",
       "  1072,\n",
       "  36],\n",
       " [1325,\n",
       "  1,\n",
       "  35,\n",
       "  14,\n",
       "  525,\n",
       "  312,\n",
       "  1070,\n",
       "  1071,\n",
       "  4,\n",
       "  1072,\n",
       "  36,\n",
       "  589,\n",
       "  6,\n",
       "  385,\n",
       "  225,\n",
       "  112,\n",
       "  405,\n",
       "  940,\n",
       "  1,\n",
       "  649,\n",
       "  532,\n",
       "  2715,\n",
       "  858,\n",
       "  405,\n",
       "  469,\n",
       "  4,\n",
       "  1,\n",
       "  525,\n",
       "  312,\n",
       "  1070,\n",
       "  2716,\n",
       "  405,\n",
       "  469,\n",
       "  1071,\n",
       "  4,\n",
       "  1072,\n",
       "  36],\n",
       " [6,\n",
       "  18,\n",
       "  7,\n",
       "  2717,\n",
       "  1118,\n",
       "  893,\n",
       "  19,\n",
       "  1887,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  23,\n",
       "  7,\n",
       "  1262,\n",
       "  1118,\n",
       "  893,\n",
       "  19,\n",
       "  1887,\n",
       "  16,\n",
       "  34,\n",
       "  1888,\n",
       "  124,\n",
       "  2718,\n",
       "  23,\n",
       "  30,\n",
       "  142,\n",
       "  8,\n",
       "  7,\n",
       "  168,\n",
       "  816],\n",
       " [6,\n",
       "  15,\n",
       "  1889,\n",
       "  1041,\n",
       "  75,\n",
       "  1,\n",
       "  311,\n",
       "  802,\n",
       "  9,\n",
       "  364,\n",
       "  4,\n",
       "  15,\n",
       "  1,\n",
       "  2719,\n",
       "  608,\n",
       "  11,\n",
       "  2720,\n",
       "  143,\n",
       "  134,\n",
       "  36,\n",
       "  1580,\n",
       "  314,\n",
       "  134,\n",
       "  36,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  1,\n",
       "  143],\n",
       " [1,\n",
       "  471,\n",
       "  22,\n",
       "  224,\n",
       "  14,\n",
       "  249,\n",
       "  1073,\n",
       "  338,\n",
       "  2,\n",
       "  3,\n",
       "  581,\n",
       "  1,\n",
       "  1073,\n",
       "  8,\n",
       "  249,\n",
       "  32,\n",
       "  2721,\n",
       "  670,\n",
       "  9,\n",
       "  1,\n",
       "  467,\n",
       "  28,\n",
       "  8,\n",
       "  1,\n",
       "  447,\n",
       "  224,\n",
       "  251,\n",
       "  2722,\n",
       "  1890,\n",
       "  338,\n",
       "  2,\n",
       "  3,\n",
       "  581],\n",
       " [31,\n",
       "  68,\n",
       "  81,\n",
       "  6,\n",
       "  18,\n",
       "  99,\n",
       "  1326,\n",
       "  295,\n",
       "  288,\n",
       "  31,\n",
       "  7,\n",
       "  576,\n",
       "  184,\n",
       "  11,\n",
       "  2723,\n",
       "  4,\n",
       "  2724,\n",
       "  6,\n",
       "  851,\n",
       "  158,\n",
       "  168,\n",
       "  668,\n",
       "  10,\n",
       "  99,\n",
       "  1326,\n",
       "  295,\n",
       "  288],\n",
       " [11,\n",
       "  35,\n",
       "  1167,\n",
       "  1,\n",
       "  17,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  31,\n",
       "  350,\n",
       "  31,\n",
       "  1891,\n",
       "  2725,\n",
       "  19,\n",
       "  2726,\n",
       "  17,\n",
       "  1892,\n",
       "  1891,\n",
       "  1892,\n",
       "  2727,\n",
       "  190,\n",
       "  2728,\n",
       "  2729,\n",
       "  225,\n",
       "  1,\n",
       "  19,\n",
       "  659,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  35,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  214,\n",
       "  9,\n",
       "  1063,\n",
       "  645],\n",
       " [1,\n",
       "  1893,\n",
       "  776,\n",
       "  5,\n",
       "  1894,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  2730,\n",
       "  206,\n",
       "  2731,\n",
       "  2732,\n",
       "  21,\n",
       "  1,\n",
       "  1895,\n",
       "  26,\n",
       "  2733,\n",
       "  51,\n",
       "  1896,\n",
       "  2734,\n",
       "  4,\n",
       "  2735,\n",
       "  1893,\n",
       "  776,\n",
       "  1482,\n",
       "  1897,\n",
       "  26,\n",
       "  1894,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  1493,\n",
       "  1,\n",
       "  2736,\n",
       "  1895,\n",
       "  2737,\n",
       "  226,\n",
       "  2738,\n",
       "  973,\n",
       "  8,\n",
       "  1,\n",
       "  415,\n",
       "  5,\n",
       "  1,\n",
       "  2739],\n",
       " [6,\n",
       "  719,\n",
       "  68,\n",
       "  1327,\n",
       "  9,\n",
       "  1,\n",
       "  259,\n",
       "  222,\n",
       "  563,\n",
       "  2,\n",
       "  3,\n",
       "  156,\n",
       "  34,\n",
       "  385,\n",
       "  2740,\n",
       "  172,\n",
       "  2741,\n",
       "  6,\n",
       "  662,\n",
       "  719,\n",
       "  7,\n",
       "  226,\n",
       "  1327,\n",
       "  9,\n",
       "  1,\n",
       "  2742,\n",
       "  222,\n",
       "  563,\n",
       "  2,\n",
       "  3,\n",
       "  156,\n",
       "  42,\n",
       "  1462,\n",
       "  290,\n",
       "  7,\n",
       "  2743],\n",
       " [6,\n",
       "  214,\n",
       "  1,\n",
       "  366,\n",
       "  312,\n",
       "  894,\n",
       "  576,\n",
       "  78,\n",
       "  103,\n",
       "  21,\n",
       "  1,\n",
       "  202,\n",
       "  56,\n",
       "  78,\n",
       "  403,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  6,\n",
       "  223,\n",
       "  58,\n",
       "  10,\n",
       "  1260,\n",
       "  366,\n",
       "  312,\n",
       "  184,\n",
       "  175,\n",
       "  8,\n",
       "  1,\n",
       "  202,\n",
       "  20,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [6,\n",
       "  1201,\n",
       "  1,\n",
       "  2744,\n",
       "  1328,\n",
       "  315,\n",
       "  10,\n",
       "  2745,\n",
       "  487,\n",
       "  4,\n",
       "  1,\n",
       "  243,\n",
       "  285,\n",
       "  469,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  53,\n",
       "  1,\n",
       "  309,\n",
       "  13,\n",
       "  50,\n",
       "  10,\n",
       "  1329,\n",
       "  14,\n",
       "  2746,\n",
       "  1898,\n",
       "  10,\n",
       "  1,\n",
       "  243,\n",
       "  285,\n",
       "  469,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  53],\n",
       " [70,\n",
       "  89,\n",
       "  8,\n",
       "  259,\n",
       "  222,\n",
       "  711,\n",
       "  33,\n",
       "  460,\n",
       "  26,\n",
       "  10,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  70,\n",
       "  89,\n",
       "  5,\n",
       "  1,\n",
       "  711,\n",
       "  211,\n",
       "  49,\n",
       "  13,\n",
       "  260,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  228,\n",
       "  410,\n",
       "  83,\n",
       "  2747,\n",
       "  1330],\n",
       " [6,\n",
       "  216,\n",
       "  70,\n",
       "  89,\n",
       "  10,\n",
       "  364,\n",
       "  311,\n",
       "  297,\n",
       "  260,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  11,\n",
       "  51,\n",
       "  188,\n",
       "  6,\n",
       "  260,\n",
       "  248,\n",
       "  311,\n",
       "  297,\n",
       "  228,\n",
       "  410,\n",
       "  26,\n",
       "  949,\n",
       "  5,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36],\n",
       " [131,\n",
       "  31,\n",
       "  1331,\n",
       "  6,\n",
       "  1142,\n",
       "  51,\n",
       "  568,\n",
       "  34,\n",
       "  858,\n",
       "  9,\n",
       "  1899,\n",
       "  8,\n",
       "  1,\n",
       "  577,\n",
       "  680,\n",
       "  2748,\n",
       "  21,\n",
       "  579,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  8,\n",
       "  1,\n",
       "  119,\n",
       "  680,\n",
       "  472,\n",
       "  6,\n",
       "  2749,\n",
       "  15,\n",
       "  1,\n",
       "  577,\n",
       "  578,\n",
       "  97,\n",
       "  26,\n",
       "  579,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  8,\n",
       "  938,\n",
       "  1332,\n",
       "  568,\n",
       "  283,\n",
       "  577,\n",
       "  2750],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  866,\n",
       "  320,\n",
       "  34,\n",
       "  32,\n",
       "  660,\n",
       "  26,\n",
       "  1,\n",
       "  264,\n",
       "  44,\n",
       "  251,\n",
       "  146,\n",
       "  380,\n",
       "  321,\n",
       "  84,\n",
       "  662,\n",
       "  6,\n",
       "  18,\n",
       "  83,\n",
       "  1,\n",
       "  2751,\n",
       "  146,\n",
       "  52,\n",
       "  344,\n",
       "  4,\n",
       "  94,\n",
       "  1,\n",
       "  264,\n",
       "  44,\n",
       "  251,\n",
       "  146,\n",
       "  277,\n",
       "  4,\n",
       "  321,\n",
       "  84,\n",
       "  4,\n",
       "  1230,\n",
       "  52],\n",
       " [6,\n",
       "  216,\n",
       "  70,\n",
       "  89,\n",
       "  10,\n",
       "  364,\n",
       "  311,\n",
       "  297,\n",
       "  260,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  11,\n",
       "  70,\n",
       "  89,\n",
       "  281,\n",
       "  6,\n",
       "  18,\n",
       "  7,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  97,\n",
       "  8,\n",
       "  12,\n",
       "  36],\n",
       " [23,\n",
       "  130,\n",
       "  2752,\n",
       "  1322,\n",
       "  2753,\n",
       "  4,\n",
       "  1900,\n",
       "  87,\n",
       "  118,\n",
       "  509,\n",
       "  7,\n",
       "  29,\n",
       "  61,\n",
       "  23,\n",
       "  7,\n",
       "  70,\n",
       "  184,\n",
       "  42,\n",
       "  13,\n",
       "  50,\n",
       "  23,\n",
       "  7,\n",
       "  2754,\n",
       "  144,\n",
       "  2755,\n",
       "  4,\n",
       "  1900,\n",
       "  87,\n",
       "  67,\n",
       "  1,\n",
       "  118,\n",
       "  9,\n",
       "  719,\n",
       "  7,\n",
       "  70,\n",
       "  29,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  28,\n",
       "  21,\n",
       "  1,\n",
       "  2756,\n",
       "  511,\n",
       "  9,\n",
       "  374,\n",
       "  1901,\n",
       "  130,\n",
       "  1876],\n",
       " [6,\n",
       "  396,\n",
       "  1,\n",
       "  729,\n",
       "  437,\n",
       "  847,\n",
       "  103,\n",
       "  14,\n",
       "  1898,\n",
       "  4,\n",
       "  1,\n",
       "  518,\n",
       "  285,\n",
       "  469,\n",
       "  519,\n",
       "  64,\n",
       "  6,\n",
       "  18,\n",
       "  7,\n",
       "  520,\n",
       "  415,\n",
       "  5,\n",
       "  869,\n",
       "  4,\n",
       "  18,\n",
       "  518,\n",
       "  519,\n",
       "  64,\n",
       "  31,\n",
       "  1,\n",
       "  437,\n",
       "  285,\n",
       "  82],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  226,\n",
       "  99,\n",
       "  81,\n",
       "  14,\n",
       "  107,\n",
       "  106,\n",
       "  4,\n",
       "  256,\n",
       "  416,\n",
       "  14,\n",
       "  326,\n",
       "  200,\n",
       "  105,\n",
       "  169,\n",
       "  190,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  445,\n",
       "  81,\n",
       "  5,\n",
       "  99,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [6,\n",
       "  147,\n",
       "  1,\n",
       "  70,\n",
       "  89,\n",
       "  5,\n",
       "  667,\n",
       "  211,\n",
       "  413,\n",
       "  235,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  12,\n",
       "  36,\n",
       "  70,\n",
       "  89,\n",
       "  395,\n",
       "  32,\n",
       "  109,\n",
       "  10,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36],\n",
       " [8,\n",
       "  709,\n",
       "  1,\n",
       "  52,\n",
       "  1299,\n",
       "  1,\n",
       "  1195,\n",
       "  227,\n",
       "  103,\n",
       "  715,\n",
       "  36,\n",
       "  83,\n",
       "  1,\n",
       "  52,\n",
       "  1299,\n",
       "  1,\n",
       "  1195,\n",
       "  103,\n",
       "  715,\n",
       "  36,\n",
       "  4,\n",
       "  98,\n",
       "  492,\n",
       "  1815,\n",
       "  18,\n",
       "  5,\n",
       "  7,\n",
       "  1333,\n",
       "  4,\n",
       "  7,\n",
       "  2757],\n",
       " [6,\n",
       "  18,\n",
       "  518,\n",
       "  519,\n",
       "  64,\n",
       "  9,\n",
       "  285,\n",
       "  1,\n",
       "  106,\n",
       "  648,\n",
       "  2758,\n",
       "  1,\n",
       "  46,\n",
       "  608,\n",
       "  6,\n",
       "  18,\n",
       "  518,\n",
       "  519,\n",
       "  64,\n",
       "  9,\n",
       "  285,\n",
       "  19,\n",
       "  106,\n",
       "  14,\n",
       "  7,\n",
       "  520,\n",
       "  415,\n",
       "  1828],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  1,\n",
       "  49,\n",
       "  11,\n",
       "  1,\n",
       "  44,\n",
       "  572,\n",
       "  35,\n",
       "  558,\n",
       "  67,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  1334,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [584,\n",
       "  13,\n",
       "  7,\n",
       "  375,\n",
       "  234,\n",
       "  5,\n",
       "  146,\n",
       "  1020,\n",
       "  156,\n",
       "  1,\n",
       "  679,\n",
       "  5,\n",
       "  1,\n",
       "  1335,\n",
       "  29,\n",
       "  13,\n",
       "  7,\n",
       "  389,\n",
       "  115,\n",
       "  8,\n",
       "  1,\n",
       "  827,\n",
       "  5,\n",
       "  7,\n",
       "  1174,\n",
       "  5,\n",
       "  2759,\n",
       "  1241,\n",
       "  146,\n",
       "  584,\n",
       "  1020,\n",
       "  156],\n",
       " [1,\n",
       "  188,\n",
       "  11,\n",
       "  2760,\n",
       "  4,\n",
       "  1484,\n",
       "  95,\n",
       "  870,\n",
       "  123,\n",
       "  516,\n",
       "  8,\n",
       "  482,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  131,\n",
       "  1144,\n",
       "  35,\n",
       "  143,\n",
       "  5,\n",
       "  125,\n",
       "  14,\n",
       "  1483,\n",
       "  115,\n",
       "  33,\n",
       "  118,\n",
       "  97,\n",
       "  8,\n",
       "  482,\n",
       "  2,\n",
       "  3,\n",
       "  117],\n",
       " [1,\n",
       "  130,\n",
       "  287,\n",
       "  13,\n",
       "  406,\n",
       "  594,\n",
       "  379,\n",
       "  277,\n",
       "  2,\n",
       "  3,\n",
       "  40,\n",
       "  1,\n",
       "  380,\n",
       "  1074,\n",
       "  406,\n",
       "  594,\n",
       "  379,\n",
       "  866,\n",
       "  277,\n",
       "  2,\n",
       "  3,\n",
       "  40,\n",
       "  320,\n",
       "  42,\n",
       "  32,\n",
       "  2761,\n",
       "  782,\n",
       "  34,\n",
       "  2762,\n",
       "  1902,\n",
       "  1903,\n",
       "  478,\n",
       "  242,\n",
       "  4,\n",
       "  205,\n",
       "  28],\n",
       " [1904,\n",
       "  2763,\n",
       "  13,\n",
       "  1,\n",
       "  305,\n",
       "  2764,\n",
       "  9,\n",
       "  2765,\n",
       "  1905,\n",
       "  4,\n",
       "  1906,\n",
       "  329,\n",
       "  1336,\n",
       "  167,\n",
       "  2766,\n",
       "  104,\n",
       "  9,\n",
       "  1113,\n",
       "  1,\n",
       "  305,\n",
       "  183,\n",
       "  26,\n",
       "  1160,\n",
       "  1022,\n",
       "  1337,\n",
       "  9,\n",
       "  1,\n",
       "  2767,\n",
       "  11,\n",
       "  1148,\n",
       "  1904,\n",
       "  1905,\n",
       "  4,\n",
       "  1906,\n",
       "  329],\n",
       " [463,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  871,\n",
       "  34,\n",
       "  1338,\n",
       "  5,\n",
       "  1,\n",
       "  1029,\n",
       "  354,\n",
       "  8,\n",
       "  1,\n",
       "  1339,\n",
       "  22,\n",
       "  94,\n",
       "  67,\n",
       "  663,\n",
       "  463,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  2768,\n",
       "  1,\n",
       "  1907,\n",
       "  480,\n",
       "  63,\n",
       "  305,\n",
       "  1339,\n",
       "  872,\n",
       "  22,\n",
       "  4,\n",
       "  2769,\n",
       "  34,\n",
       "  663,\n",
       "  1340,\n",
       "  11,\n",
       "  1338,\n",
       "  5,\n",
       "  1,\n",
       "  30,\n",
       "  354,\n",
       "  241,\n",
       "  254,\n",
       "  325,\n",
       "  5,\n",
       "  1,\n",
       "  1908,\n",
       "  838,\n",
       "  993,\n",
       "  14,\n",
       "  2770,\n",
       "  5,\n",
       "  663],\n",
       " [105,\n",
       "  41,\n",
       "  65,\n",
       "  18,\n",
       "  155,\n",
       "  116,\n",
       "  173,\n",
       "  342,\n",
       "  4,\n",
       "  474,\n",
       "  197,\n",
       "  100,\n",
       "  41,\n",
       "  65,\n",
       "  32,\n",
       "  50,\n",
       "  206,\n",
       "  1,\n",
       "  1909,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  10,\n",
       "  614,\n",
       "  615,\n",
       "  87,\n",
       "  14,\n",
       "  155,\n",
       "  116,\n",
       "  839,\n",
       "  342,\n",
       "  4,\n",
       "  474,\n",
       "  197],\n",
       " [6,\n",
       "  15,\n",
       "  382,\n",
       "  383,\n",
       "  87,\n",
       "  11,\n",
       "  48,\n",
       "  340,\n",
       "  4,\n",
       "  1,\n",
       "  139,\n",
       "  2771,\n",
       "  2772,\n",
       "  15,\n",
       "  8,\n",
       "  1,\n",
       "  340,\n",
       "  516,\n",
       "  8,\n",
       "  193,\n",
       "  131,\n",
       "  494,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  1910,\n",
       "  81,\n",
       "  8,\n",
       "  382,\n",
       "  383,\n",
       "  87],\n",
       " [6,\n",
       "  752,\n",
       "  89,\n",
       "  10,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  1,\n",
       "  70,\n",
       "  89,\n",
       "  147,\n",
       "  13,\n",
       "  76,\n",
       "  196,\n",
       "  179,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  14,\n",
       "  228,\n",
       "  488,\n",
       "  297,\n",
       "  12,\n",
       "  36],\n",
       " [589,\n",
       "  6,\n",
       "  69,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  65,\n",
       "  14,\n",
       "  24,\n",
       "  132,\n",
       "  10,\n",
       "  1,\n",
       "  432,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  393,\n",
       "  6,\n",
       "  15,\n",
       "  17,\n",
       "  20,\n",
       "  31,\n",
       "  27,\n",
       "  296,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  359,\n",
       "  1,\n",
       "  101,\n",
       "  38,\n",
       "  29,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  177,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  8,\n",
       "  124,\n",
       "  60,\n",
       "  2773,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  14,\n",
       "  124,\n",
       "  107,\n",
       "  302],\n",
       " [48,\n",
       "  188,\n",
       "  8,\n",
       "  1,\n",
       "  130,\n",
       "  2774,\n",
       "  2775,\n",
       "  182,\n",
       "  9,\n",
       "  2776,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  120,\n",
       "  1256,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  6,\n",
       "  76,\n",
       "  182,\n",
       "  51,\n",
       "  63,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [6,\n",
       "  15,\n",
       "  60,\n",
       "  163,\n",
       "  169,\n",
       "  8,\n",
       "  92,\n",
       "  152,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  131,\n",
       "  6,\n",
       "  15,\n",
       "  271,\n",
       "  199,\n",
       "  2777,\n",
       "  226,\n",
       "  99,\n",
       "  770,\n",
       "  1141,\n",
       "  1,\n",
       "  139,\n",
       "  188,\n",
       "  11,\n",
       "  1,\n",
       "  1233,\n",
       "  1341,\n",
       "  1,\n",
       "  81,\n",
       "  33,\n",
       "  419,\n",
       "  21,\n",
       "  92,\n",
       "  152,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [542,\n",
       "  334,\n",
       "  234,\n",
       "  543,\n",
       "  4,\n",
       "  544,\n",
       "  387,\n",
       "  2778,\n",
       "  9,\n",
       "  1,\n",
       "  118,\n",
       "  2779,\n",
       "  861,\n",
       "  1342,\n",
       "  9,\n",
       "  24,\n",
       "  167,\n",
       "  351,\n",
       "  5,\n",
       "  1911,\n",
       "  5,\n",
       "  137,\n",
       "  242,\n",
       "  8,\n",
       "  1,\n",
       "  308,\n",
       "  5,\n",
       "  542,\n",
       "  334,\n",
       "  234,\n",
       "  543,\n",
       "  4,\n",
       "  544,\n",
       "  387],\n",
       " [105,\n",
       "  41,\n",
       "  65,\n",
       "  18,\n",
       "  155,\n",
       "  116,\n",
       "  173,\n",
       "  342,\n",
       "  4,\n",
       "  474,\n",
       "  197,\n",
       "  100,\n",
       "  41,\n",
       "  65,\n",
       "  32,\n",
       "  50,\n",
       "  206,\n",
       "  1,\n",
       "  1909,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  10,\n",
       "  614,\n",
       "  615,\n",
       "  87,\n",
       "  14,\n",
       "  155,\n",
       "  116,\n",
       "  839,\n",
       "  342,\n",
       "  4,\n",
       "  474,\n",
       "  197],\n",
       " [6,\n",
       "  69,\n",
       "  1,\n",
       "  324,\n",
       "  1058,\n",
       "  811,\n",
       "  10,\n",
       "  1059,\n",
       "  1060,\n",
       "  315,\n",
       "  14,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  83,\n",
       "  10,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [11,\n",
       "  46,\n",
       "  1,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  11,\n",
       "  282,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  51,\n",
       "  24,\n",
       "  35,\n",
       "  49,\n",
       "  32,\n",
       "  61,\n",
       "  23,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  4,\n",
       "  60,\n",
       "  682,\n",
       "  11,\n",
       "  46,\n",
       "  4,\n",
       "  1343,\n",
       "  1,\n",
       "  65],\n",
       " [6,\n",
       "  18,\n",
       "  317,\n",
       "  222,\n",
       "  31,\n",
       "  24,\n",
       "  143,\n",
       "  314,\n",
       "  134,\n",
       "  36,\n",
       "  14,\n",
       "  60,\n",
       "  1912,\n",
       "  344,\n",
       "  6,\n",
       "  1075,\n",
       "  48,\n",
       "  873,\n",
       "  13,\n",
       "  327,\n",
       "  641,\n",
       "  14,\n",
       "  1,\n",
       "  317,\n",
       "  143,\n",
       "  314,\n",
       "  15,\n",
       "  11,\n",
       "  436,\n",
       "  134,\n",
       "  36],\n",
       " [11,\n",
       "  359,\n",
       "  1,\n",
       "  101,\n",
       "  38,\n",
       "  29,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  177,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  8,\n",
       "  124,\n",
       "  60,\n",
       "  929,\n",
       "  46,\n",
       "  1,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  11,\n",
       "  282,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  54,\n",
       "  85,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  31,\n",
       "  1,\n",
       "  825,\n",
       "  21,\n",
       "  42,\n",
       "  9,\n",
       "  219,\n",
       "  1,\n",
       "  2780,\n",
       "  2781,\n",
       "  239,\n",
       "  2782,\n",
       "  219,\n",
       "  24,\n",
       "  1344,\n",
       "  146,\n",
       "  21,\n",
       "  1,\n",
       "  1913,\n",
       "  540,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  213,\n",
       "  170,\n",
       "  12,\n",
       "  40],\n",
       " [6,\n",
       "  18,\n",
       "  1076,\n",
       "  199,\n",
       "  1914,\n",
       "  14,\n",
       "  1915,\n",
       "  349,\n",
       "  4,\n",
       "  158,\n",
       "  168,\n",
       "  199,\n",
       "  1115,\n",
       "  14,\n",
       "  68,\n",
       "  753,\n",
       "  162,\n",
       "  21,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  423,\n",
       "  271,\n",
       "  1314,\n",
       "  6,\n",
       "  18,\n",
       "  271,\n",
       "  199,\n",
       "  14,\n",
       "  517,\n",
       "  315,\n",
       "  798,\n",
       "  4,\n",
       "  1315,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [51,\n",
       "  28,\n",
       "  15,\n",
       "  8,\n",
       "  24,\n",
       "  58,\n",
       "  32,\n",
       "  874,\n",
       "  721,\n",
       "  4,\n",
       "  274,\n",
       "  10,\n",
       "  1,\n",
       "  120,\n",
       "  20,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  6,\n",
       "  274,\n",
       "  44,\n",
       "  28,\n",
       "  4,\n",
       "  864,\n",
       "  298,\n",
       "  28,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  120,\n",
       "  20,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [786,\n",
       "  1,\n",
       "  126,\n",
       "  193,\n",
       "  240,\n",
       "  1916,\n",
       "  1,\n",
       "  512,\n",
       "  22,\n",
       "  33,\n",
       "  473,\n",
       "  10,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  83,\n",
       "  1,\n",
       "  22,\n",
       "  13,\n",
       "  473,\n",
       "  4,\n",
       "  224,\n",
       "  26,\n",
       "  186,\n",
       "  23,\n",
       "  105,\n",
       "  1026,\n",
       "  10,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110],\n",
       " [11,\n",
       "  48,\n",
       "  265,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  271,\n",
       "  199,\n",
       "  184,\n",
       "  21,\n",
       "  92,\n",
       "  14,\n",
       "  517,\n",
       "  2783,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  83,\n",
       "  6,\n",
       "  18,\n",
       "  271,\n",
       "  199,\n",
       "  14,\n",
       "  517,\n",
       "  349,\n",
       "  175,\n",
       "  10,\n",
       "  1,\n",
       "  92,\n",
       "  20,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [51,\n",
       "  24,\n",
       "  49,\n",
       "  32,\n",
       "  664,\n",
       "  14,\n",
       "  7,\n",
       "  60,\n",
       "  27,\n",
       "  29,\n",
       "  66,\n",
       "  14,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  24,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  48,\n",
       "  1303,\n",
       "  32,\n",
       "  7,\n",
       "  2784,\n",
       "  21,\n",
       "  24,\n",
       "  323,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  61,\n",
       "  49,\n",
       "  21,\n",
       "  2785],\n",
       " [6,\n",
       "  147,\n",
       "  24,\n",
       "  975,\n",
       "  8,\n",
       "  1,\n",
       "  726,\n",
       "  5,\n",
       "  1,\n",
       "  1345,\n",
       "  57,\n",
       "  143,\n",
       "  62,\n",
       "  1917,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  1,\n",
       "  555,\n",
       "  49,\n",
       "  32,\n",
       "  397,\n",
       "  23,\n",
       "  1,\n",
       "  2786,\n",
       "  1150,\n",
       "  872,\n",
       "  35,\n",
       "  62,\n",
       "  5,\n",
       "  1345,\n",
       "  57,\n",
       "  1917,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [172,\n",
       "  163,\n",
       "  95,\n",
       "  123,\n",
       "  15,\n",
       "  8,\n",
       "  601,\n",
       "  167,\n",
       "  26,\n",
       "  409,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  6,\n",
       "  397,\n",
       "  24,\n",
       "  82,\n",
       "  14,\n",
       "  694,\n",
       "  732,\n",
       "  358,\n",
       "  34,\n",
       "  67,\n",
       "  15,\n",
       "  8,\n",
       "  409,\n",
       "  2,\n",
       "  3,\n",
       "  87],\n",
       " [145,\n",
       "  13,\n",
       "  21,\n",
       "  475,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  1,\n",
       "  1284,\n",
       "  5,\n",
       "  30,\n",
       "  1045,\n",
       "  13,\n",
       "  1918,\n",
       "  231,\n",
       "  475,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  770,\n",
       "  34,\n",
       "  1,\n",
       "  1919,\n",
       "  1920,\n",
       "  11,\n",
       "  30,\n",
       "  178,\n",
       "  13,\n",
       "  2787],\n",
       " [30,\n",
       "  73,\n",
       "  10,\n",
       "  59,\n",
       "  20,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  84,\n",
       "  1,\n",
       "  107,\n",
       "  580,\n",
       "  31,\n",
       "  169,\n",
       "  8,\n",
       "  46,\n",
       "  229,\n",
       "  11,\n",
       "  2788,\n",
       "  73,\n",
       "  13,\n",
       "  109,\n",
       "  26,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  84,\n",
       "  8,\n",
       "  105,\n",
       "  398,\n",
       "  14,\n",
       "  1,\n",
       "  107,\n",
       "  726],\n",
       " [6,\n",
       "  289,\n",
       "  34,\n",
       "  10,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  9,\n",
       "  285,\n",
       "  1,\n",
       "  106,\n",
       "  13,\n",
       "  650,\n",
       "  852,\n",
       "  83,\n",
       "  10,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [1,\n",
       "  217,\n",
       "  137,\n",
       "  187,\n",
       "  485,\n",
       "  336,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  13,\n",
       "  7,\n",
       "  390,\n",
       "  22,\n",
       "  247,\n",
       "  14,\n",
       "  137,\n",
       "  242,\n",
       "  1509,\n",
       "  1,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  236,\n",
       "  5,\n",
       "  1,\n",
       "  217,\n",
       "  2789,\n",
       "  5,\n",
       "  1,\n",
       "  255,\n",
       "  1741,\n",
       "  1337,\n",
       "  11,\n",
       "  137,\n",
       "  724,\n",
       "  8,\n",
       "  44,\n",
       "  13,\n",
       "  1,\n",
       "  217,\n",
       "  137,\n",
       "  187,\n",
       "  336,\n",
       "  2,\n",
       "  3,\n",
       "  90],\n",
       " [1,\n",
       "  29,\n",
       "  33,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  44,\n",
       "  4,\n",
       "  867,\n",
       "  236,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  213,\n",
       "  131,\n",
       "  12,\n",
       "  40,\n",
       "  1,\n",
       "  54,\n",
       "  28,\n",
       "  75,\n",
       "  351,\n",
       "  5,\n",
       "  1921,\n",
       "  125,\n",
       "  5,\n",
       "  1,\n",
       "  305,\n",
       "  236,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [11,\n",
       "  231,\n",
       "  475,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  555,\n",
       "  471,\n",
       "  658,\n",
       "  1346,\n",
       "  178,\n",
       "  4,\n",
       "  1347,\n",
       "  178,\n",
       "  8,\n",
       "  595,\n",
       "  4,\n",
       "  738,\n",
       "  2790,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  214,\n",
       "  30,\n",
       "  178,\n",
       "  9,\n",
       "  738,\n",
       "  4,\n",
       "  736,\n",
       "  656,\n",
       "  1077,\n",
       "  595],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  108,\n",
       "  152,\n",
       "  170,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  408,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  8,\n",
       "  709,\n",
       "  6,\n",
       "  18,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  427,\n",
       "  27,\n",
       "  235,\n",
       "  29,\n",
       "  23,\n",
       "  1,\n",
       "  2791,\n",
       "  22],\n",
       " [1896,\n",
       "  2792,\n",
       "  23,\n",
       "  48,\n",
       "  29,\n",
       "  95,\n",
       "  268,\n",
       "  34,\n",
       "  1,\n",
       "  256,\n",
       "  416,\n",
       "  1255,\n",
       "  156,\n",
       "  1078,\n",
       "  205,\n",
       "  199,\n",
       "  530,\n",
       "  252,\n",
       "  158,\n",
       "  168,\n",
       "  199,\n",
       "  294,\n",
       "  1922,\n",
       "  2793,\n",
       "  2794,\n",
       "  29,\n",
       "  11,\n",
       "  1,\n",
       "  391,\n",
       "  62,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  256,\n",
       "  416,\n",
       "  1255,\n",
       "  156],\n",
       " [6,\n",
       "  50,\n",
       "  7,\n",
       "  212,\n",
       "  5,\n",
       "  841,\n",
       "  38,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  8,\n",
       "  124,\n",
       "  107,\n",
       "  1923,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  69,\n",
       "  60,\n",
       "  27,\n",
       "  49,\n",
       "  14,\n",
       "  107,\n",
       "  739],\n",
       " [423,\n",
       "  2795,\n",
       "  627,\n",
       "  2,\n",
       "  3,\n",
       "  201,\n",
       "  97,\n",
       "  9,\n",
       "  18,\n",
       "  928,\n",
       "  1924,\n",
       "  238,\n",
       "  875,\n",
       "  9,\n",
       "  19,\n",
       "  112,\n",
       "  868,\n",
       "  2796,\n",
       "  2,\n",
       "  3,\n",
       "  201,\n",
       "  2797,\n",
       "  7,\n",
       "  928,\n",
       "  1924,\n",
       "  238,\n",
       "  309,\n",
       "  4,\n",
       "  2798,\n",
       "  1925,\n",
       "  286],\n",
       " [51,\n",
       "  28,\n",
       "  15,\n",
       "  8,\n",
       "  24,\n",
       "  58,\n",
       "  32,\n",
       "  874,\n",
       "  721,\n",
       "  4,\n",
       "  274,\n",
       "  10,\n",
       "  1,\n",
       "  120,\n",
       "  20,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  6,\n",
       "  274,\n",
       "  44,\n",
       "  28,\n",
       "  4,\n",
       "  864,\n",
       "  298,\n",
       "  28,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  120,\n",
       "  20,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [1,\n",
       "  1268,\n",
       "  236,\n",
       "  5,\n",
       "  28,\n",
       "  622,\n",
       "  21,\n",
       "  846,\n",
       "  4,\n",
       "  1269,\n",
       "  2799,\n",
       "  1270,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  183,\n",
       "  12,\n",
       "  40,\n",
       "  1,\n",
       "  261,\n",
       "  5,\n",
       "  239,\n",
       "  28,\n",
       "  15,\n",
       "  8,\n",
       "  1,\n",
       "  58,\n",
       "  13,\n",
       "  1,\n",
       "  54,\n",
       "  310,\n",
       "  12,\n",
       "  40],\n",
       " [6,\n",
       "  1075,\n",
       "  48,\n",
       "  873,\n",
       "  13,\n",
       "  327,\n",
       "  641,\n",
       "  14,\n",
       "  1,\n",
       "  317,\n",
       "  143,\n",
       "  314,\n",
       "  15,\n",
       "  11,\n",
       "  436,\n",
       "  134,\n",
       "  36,\n",
       "  317,\n",
       "  134,\n",
       "  36,\n",
       "  13,\n",
       "  1,\n",
       "  1670,\n",
       "  355,\n",
       "  314,\n",
       "  1153,\n",
       "  15,\n",
       "  9,\n",
       "  337,\n",
       "  1,\n",
       "  133,\n",
       "  436,\n",
       "  188],\n",
       " [589,\n",
       "  6,\n",
       "  69,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  65,\n",
       "  14,\n",
       "  24,\n",
       "  132,\n",
       "  10,\n",
       "  1,\n",
       "  432,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  69,\n",
       "  60,\n",
       "  27,\n",
       "  49,\n",
       "  14,\n",
       "  107,\n",
       "  739],\n",
       " [589,\n",
       "  6,\n",
       "  69,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  65,\n",
       "  14,\n",
       "  24,\n",
       "  132,\n",
       "  10,\n",
       "  1,\n",
       "  432,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  101,\n",
       "  49,\n",
       "  32,\n",
       "  66,\n",
       "  14,\n",
       "  1,\n",
       "  177,\n",
       "  27,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  161,\n",
       "  2800,\n",
       "  11,\n",
       "  1,\n",
       "  606,\n",
       "  35,\n",
       "  617,\n",
       "  7,\n",
       "  75,\n",
       "  5,\n",
       "  65,\n",
       "  2801,\n",
       "  840,\n",
       "  505,\n",
       "  1008,\n",
       "  83,\n",
       "  2802,\n",
       "  2803,\n",
       "  83,\n",
       "  633,\n",
       "  1277,\n",
       "  83,\n",
       "  26,\n",
       "  2804,\n",
       "  1,\n",
       "  634,\n",
       "  250,\n",
       "  222,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  36,\n",
       "  505,\n",
       "  1008,\n",
       "  83,\n",
       "  1643,\n",
       "  1644,\n",
       "  386,\n",
       "  1008,\n",
       "  83,\n",
       "  840,\n",
       "  2805,\n",
       "  634,\n",
       "  128,\n",
       "  9,\n",
       "  27,\n",
       "  35,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  36,\n",
       "  635,\n",
       "  65,\n",
       "  1,\n",
       "  1645,\n",
       "  35,\n",
       "  377,\n",
       "  1646,\n",
       "  713,\n",
       "  83,\n",
       "  1647,\n",
       "  1648,\n",
       "  713,\n",
       "  505,\n",
       "  633,\n",
       "  83,\n",
       "  319,\n",
       "  505,\n",
       "  13,\n",
       "  1,\n",
       "  209,\n",
       "  1228],\n",
       " [1,\n",
       "  46,\n",
       "  75,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  69,\n",
       "  1,\n",
       "  27,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  41,\n",
       "  19,\n",
       "  11,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  69,\n",
       "  60,\n",
       "  27,\n",
       "  49,\n",
       "  14,\n",
       "  107,\n",
       "  739],\n",
       " [1,\n",
       "  2806,\n",
       "  21,\n",
       "  1,\n",
       "  1202,\n",
       "  32,\n",
       "  400,\n",
       "  4,\n",
       "  473,\n",
       "  10,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  31,\n",
       "  176,\n",
       "  26,\n",
       "  1,\n",
       "  2807,\n",
       "  51,\n",
       "  44,\n",
       "  28,\n",
       "  32,\n",
       "  119,\n",
       "  224,\n",
       "  4,\n",
       "  473,\n",
       "  10,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110],\n",
       " [9,\n",
       "  2808,\n",
       "  1,\n",
       "  2809,\n",
       "  5,\n",
       "  782,\n",
       "  6,\n",
       "  1321,\n",
       "  876,\n",
       "  171,\n",
       "  1,\n",
       "  44,\n",
       "  353,\n",
       "  22,\n",
       "  1926,\n",
       "  4,\n",
       "  1927,\n",
       "  43,\n",
       "  1,\n",
       "  629,\n",
       "  209,\n",
       "  41,\n",
       "  19,\n",
       "  13,\n",
       "  50,\n",
       "  21,\n",
       "  1,\n",
       "  917,\n",
       "  540,\n",
       "  5,\n",
       "  44,\n",
       "  353,\n",
       "  22,\n",
       "  1926,\n",
       "  4,\n",
       "  1927,\n",
       "  43],\n",
       " [101,\n",
       "  30,\n",
       "  132,\n",
       "  67,\n",
       "  153,\n",
       "  26,\n",
       "  464,\n",
       "  59,\n",
       "  8,\n",
       "  105,\n",
       "  398,\n",
       "  4,\n",
       "  817,\n",
       "  10,\n",
       "  1,\n",
       "  356,\n",
       "  370,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  35,\n",
       "  65,\n",
       "  67,\n",
       "  50,\n",
       "  206,\n",
       "  1,\n",
       "  239,\n",
       "  28,\n",
       "  34,\n",
       "  33,\n",
       "  374,\n",
       "  593,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  8,\n",
       "  105,\n",
       "  398,\n",
       "  4,\n",
       "  1,\n",
       "  2810,\n",
       "  370,\n",
       "  33,\n",
       "  15,\n",
       "  9,\n",
       "  2811],\n",
       " [1,\n",
       "  188,\n",
       "  2812,\n",
       "  8,\n",
       "  193,\n",
       "  131,\n",
       "  32,\n",
       "  153,\n",
       "  14,\n",
       "  1,\n",
       "  1318,\n",
       "  184,\n",
       "  50,\n",
       "  10,\n",
       "  1,\n",
       "  202,\n",
       "  403,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  23,\n",
       "  24,\n",
       "  1928,\n",
       "  565,\n",
       "  88,\n",
       "  422,\n",
       "  4,\n",
       "  46,\n",
       "  144,\n",
       "  2813,\n",
       "  2814,\n",
       "  720,\n",
       "  19,\n",
       "  13,\n",
       "  7,\n",
       "  405,\n",
       "  189,\n",
       "  184,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  2815,\n",
       "  85,\n",
       "  46,\n",
       "  28,\n",
       "  10,\n",
       "  202,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [6,\n",
       "  50,\n",
       "  7,\n",
       "  212,\n",
       "  5,\n",
       "  841,\n",
       "  38,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  8,\n",
       "  124,\n",
       "  107,\n",
       "  1923,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  286,\n",
       "  5,\n",
       "  24,\n",
       "  103,\n",
       "  13,\n",
       "  555,\n",
       "  14,\n",
       "  1,\n",
       "  221,\n",
       "  623,\n",
       "  153,\n",
       "  14,\n",
       "  7,\n",
       "  1929,\n",
       "  5,\n",
       "  1,\n",
       "  665,\n",
       "  103,\n",
       "  131,\n",
       "  665,\n",
       "  740,\n",
       "  42,\n",
       "  1325,\n",
       "  1,\n",
       "  157,\n",
       "  5,\n",
       "  68,\n",
       "  2816,\n",
       "  30,\n",
       "  26,\n",
       "  2817,\n",
       "  34,\n",
       "  30,\n",
       "  812,\n",
       "  221,\n",
       "  13,\n",
       "  109,\n",
       "  10,\n",
       "  1,\n",
       "  665,\n",
       "  103,\n",
       "  665,\n",
       "  740],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  445,\n",
       "  81,\n",
       "  5,\n",
       "  99,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  18,\n",
       "  7,\n",
       "  2818,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  81,\n",
       "  5,\n",
       "  256,\n",
       "  1672,\n",
       "  4,\n",
       "  489,\n",
       "  8,\n",
       "  553,\n",
       "  31,\n",
       "  350,\n",
       "  31,\n",
       "  7,\n",
       "  650,\n",
       "  852,\n",
       "  437,\n",
       "  1930,\n",
       "  200,\n",
       "  81,\n",
       "  21,\n",
       "  1,\n",
       "  2819,\n",
       "  2820],\n",
       " [11,\n",
       "  231,\n",
       "  475,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  770,\n",
       "  34,\n",
       "  1,\n",
       "  1919,\n",
       "  1920,\n",
       "  11,\n",
       "  30,\n",
       "  178,\n",
       "  13,\n",
       "  62,\n",
       "  1348,\n",
       "  475,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  214,\n",
       "  30,\n",
       "  178,\n",
       "  9,\n",
       "  738,\n",
       "  4,\n",
       "  736,\n",
       "  656,\n",
       "  1077,\n",
       "  595],\n",
       " [2821,\n",
       "  481,\n",
       "  272,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  11,\n",
       "  119,\n",
       "  198,\n",
       "  4,\n",
       "  1215,\n",
       "  7,\n",
       "  1931,\n",
       "  11,\n",
       "  1931,\n",
       "  1838,\n",
       "  4,\n",
       "  249,\n",
       "  1162,\n",
       "  626,\n",
       "  9,\n",
       "  2822,\n",
       "  1,\n",
       "  2823,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  943,\n",
       "  171,\n",
       "  176,\n",
       "  26,\n",
       "  272,\n",
       "  11,\n",
       "  24,\n",
       "  119,\n",
       "  198,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [51,\n",
       "  24,\n",
       "  35,\n",
       "  49,\n",
       "  32,\n",
       "  61,\n",
       "  23,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  4,\n",
       "  60,\n",
       "  682,\n",
       "  11,\n",
       "  46,\n",
       "  4,\n",
       "  1343,\n",
       "  1,\n",
       "  2824,\n",
       "  24,\n",
       "  49,\n",
       "  32,\n",
       "  664,\n",
       "  14,\n",
       "  7,\n",
       "  60,\n",
       "  27,\n",
       "  29,\n",
       "  66,\n",
       "  14,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  46,\n",
       "  75,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  69,\n",
       "  1,\n",
       "  27,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  41,\n",
       "  19,\n",
       "  11,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  108,\n",
       "  152,\n",
       "  170,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  408,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [589,\n",
       "  6,\n",
       "  69,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  65,\n",
       "  14,\n",
       "  24,\n",
       "  132,\n",
       "  10,\n",
       "  1,\n",
       "  432,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  46,\n",
       "  1,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  11,\n",
       "  282,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  2825,\n",
       "  2826,\n",
       "  2827,\n",
       "  129,\n",
       "  655,\n",
       "  1932,\n",
       "  11,\n",
       "  905,\n",
       "  11,\n",
       "  1636,\n",
       "  1,\n",
       "  2828,\n",
       "  23,\n",
       "  1,\n",
       "  1602,\n",
       "  5,\n",
       "  1,\n",
       "  362,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  2829,\n",
       "  984,\n",
       "  1,\n",
       "  258,\n",
       "  195,\n",
       "  2830,\n",
       "  26,\n",
       "  362,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  11,\n",
       "  48,\n",
       "  265],\n",
       " [98,\n",
       "  129,\n",
       "  123,\n",
       "  268,\n",
       "  8,\n",
       "  323,\n",
       "  167,\n",
       "  23,\n",
       "  293,\n",
       "  303,\n",
       "  34,\n",
       "  1,\n",
       "  1740,\n",
       "  55,\n",
       "  431,\n",
       "  211,\n",
       "  833,\n",
       "  112,\n",
       "  731,\n",
       "  1933,\n",
       "  1,\n",
       "  333,\n",
       "  1934,\n",
       "  9,\n",
       "  2831,\n",
       "  7,\n",
       "  1935,\n",
       "  211,\n",
       "  789,\n",
       "  1036,\n",
       "  4,\n",
       "  1037,\n",
       "  40,\n",
       "  24,\n",
       "  167,\n",
       "  13,\n",
       "  76,\n",
       "  601,\n",
       "  9,\n",
       "  1036,\n",
       "  4,\n",
       "  1037,\n",
       "  40,\n",
       "  319,\n",
       "  1,\n",
       "  142,\n",
       "  211,\n",
       "  1,\n",
       "  63,\n",
       "  23,\n",
       "  1,\n",
       "  431,\n",
       "  1738,\n",
       "  112,\n",
       "  731,\n",
       "  8,\n",
       "  1,\n",
       "  55,\n",
       "  758,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  1739,\n",
       "  7,\n",
       "  162,\n",
       "  369],\n",
       " [172,\n",
       "  530,\n",
       "  67,\n",
       "  15,\n",
       "  9,\n",
       "  2832,\n",
       "  8,\n",
       "  1,\n",
       "  1,\n",
       "  1048,\n",
       "  472,\n",
       "  62,\n",
       "  573,\n",
       "  7,\n",
       "  4,\n",
       "  1239,\n",
       "  472,\n",
       "  62,\n",
       "  573,\n",
       "  989,\n",
       "  5,\n",
       "  1,\n",
       "  834,\n",
       "  62,\n",
       "  835,\n",
       "  195,\n",
       "  115,\n",
       "  8,\n",
       "  465,\n",
       "  639,\n",
       "  2,\n",
       "  3,\n",
       "  1,\n",
       "  29,\n",
       "  34,\n",
       "  153,\n",
       "  1,\n",
       "  606,\n",
       "  286,\n",
       "  8,\n",
       "  1,\n",
       "  195,\n",
       "  115,\n",
       "  190,\n",
       "  1239,\n",
       "  472,\n",
       "  62,\n",
       "  5,\n",
       "  565,\n",
       "  88,\n",
       "  1192,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  4,\n",
       "  53,\n",
       "  639,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  1497,\n",
       "  465,\n",
       "  9,\n",
       "  121,\n",
       "  2833,\n",
       "  2834],\n",
       " [27,\n",
       "  38,\n",
       "  29,\n",
       "  7,\n",
       "  60,\n",
       "  2835,\n",
       "  2836,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  33,\n",
       "  66,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  261,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  14,\n",
       "  106,\n",
       "  75,\n",
       "  72,\n",
       "  9,\n",
       "  797,\n",
       "  5,\n",
       "  2837,\n",
       "  2838,\n",
       "  491,\n",
       "  7,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  66,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  4,\n",
       "  1,\n",
       "  958,\n",
       "  582,\n",
       "  183,\n",
       "  590,\n",
       "  1,\n",
       "  125,\n",
       "  8,\n",
       "  1,\n",
       "  147,\n",
       "  75,\n",
       "  33,\n",
       "  15,\n",
       "  9,\n",
       "  637,\n",
       "  1],\n",
       " [11,\n",
       "  1,\n",
       "  234,\n",
       "  5,\n",
       "  346,\n",
       "  347,\n",
       "  84,\n",
       "  9,\n",
       "  104,\n",
       "  514,\n",
       "  6,\n",
       "  95,\n",
       "  97,\n",
       "  7,\n",
       "  300,\n",
       "  384,\n",
       "  5,\n",
       "  734,\n",
       "  4,\n",
       "  1,\n",
       "  653,\n",
       "  301,\n",
       "  515,\n",
       "  8,\n",
       "  1047,\n",
       "  14,\n",
       "  248,\n",
       "  478,\n",
       "  301,\n",
       "  2839,\n",
       "  1,\n",
       "  301,\n",
       "  513,\n",
       "  8,\n",
       "  253,\n",
       "  325,\n",
       "  4,\n",
       "  344,\n",
       "  6,\n",
       "  154,\n",
       "  808,\n",
       "  7,\n",
       "  1232,\n",
       "  850,\n",
       "  9,\n",
       "  413,\n",
       "  730,\n",
       "  125,\n",
       "  539,\n",
       "  1,\n",
       "  1275,\n",
       "  1732,\n",
       "  9,\n",
       "  1,\n",
       "  115,\n",
       "  5,\n",
       "  346,\n",
       "  347,\n",
       "  84],\n",
       " [51,\n",
       "  603,\n",
       "  361,\n",
       "  1349,\n",
       "  11,\n",
       "  93,\n",
       "  119,\n",
       "  1007,\n",
       "  325,\n",
       "  276,\n",
       "  32,\n",
       "  21,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  236,\n",
       "  5,\n",
       "  876,\n",
       "  198,\n",
       "  4,\n",
       "  736,\n",
       "  656,\n",
       "  1077,\n",
       "  11,\n",
       "  1936,\n",
       "  1937,\n",
       "  93,\n",
       "  32,\n",
       "  196,\n",
       "  179,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [103,\n",
       "  170,\n",
       "  1862,\n",
       "  7,\n",
       "  1938,\n",
       "  103,\n",
       "  11,\n",
       "  1,\n",
       "  1289,\n",
       "  619,\n",
       "  1939,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  121,\n",
       "  7,\n",
       "  52,\n",
       "  6,\n",
       "  18,\n",
       "  7,\n",
       "  1289,\n",
       "  184,\n",
       "  9,\n",
       "  821,\n",
       "  1,\n",
       "  2840,\n",
       "  4,\n",
       "  396,\n",
       "  1,\n",
       "  1938,\n",
       "  2841,\n",
       "  103,\n",
       "  1939,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  11,\n",
       "  435,\n",
       "  411],\n",
       " [1,\n",
       "  62,\n",
       "  13,\n",
       "  236,\n",
       "  5,\n",
       "  1,\n",
       "  130,\n",
       "  143,\n",
       "  64,\n",
       "  877,\n",
       "  569,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  130,\n",
       "  1350,\n",
       "  142,\n",
       "  2842,\n",
       "  13,\n",
       "  1,\n",
       "  62,\n",
       "  5,\n",
       "  2843,\n",
       "  1,\n",
       "  142,\n",
       "  5,\n",
       "  7,\n",
       "  654,\n",
       "  5,\n",
       "  125,\n",
       "  23,\n",
       "  7,\n",
       "  1885,\n",
       "  21,\n",
       "  83,\n",
       "  9,\n",
       "  170,\n",
       "  569,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  139,\n",
       "  164,\n",
       "  2844,\n",
       "  31,\n",
       "  1079,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  76,\n",
       "  18,\n",
       "  2845,\n",
       "  9,\n",
       "  225,\n",
       "  659,\n",
       "  1,\n",
       "  2846,\n",
       "  99,\n",
       "  82,\n",
       "  4,\n",
       "  99,\n",
       "  101,\n",
       "  5,\n",
       "  1079,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  39,\n",
       "  52,\n",
       "  83,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  33,\n",
       "  15,\n",
       "  9,\n",
       "  723,\n",
       "  51,\n",
       "  55,\n",
       "  2847,\n",
       "  139,\n",
       "  2848,\n",
       "  5,\n",
       "  980,\n",
       "  33,\n",
       "  214,\n",
       "  9,\n",
       "  1,\n",
       "  217,\n",
       "  187,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  2849,\n",
       "  29,\n",
       "  9,\n",
       "  723,\n",
       "  55,\n",
       "  361,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [83,\n",
       "  1040,\n",
       "  1214,\n",
       "  6,\n",
       "  2850,\n",
       "  4,\n",
       "  920,\n",
       "  616,\n",
       "  4,\n",
       "  1351,\n",
       "  1940,\n",
       "  21,\n",
       "  111,\n",
       "  1597,\n",
       "  10,\n",
       "  1,\n",
       "  272,\n",
       "  20,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  11,\n",
       "  1941,\n",
       "  6,\n",
       "  1049,\n",
       "  4,\n",
       "  181,\n",
       "  1,\n",
       "  28,\n",
       "  10,\n",
       "  272,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [56,\n",
       "  35,\n",
       "  29,\n",
       "  302,\n",
       "  6,\n",
       "  15,\n",
       "  7,\n",
       "  27,\n",
       "  70,\n",
       "  56,\n",
       "  35,\n",
       "  19,\n",
       "  31,\n",
       "  175,\n",
       "  8,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  56,\n",
       "  1320,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  35,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  424,\n",
       "  24,\n",
       "  65],\n",
       " [1,\n",
       "  126,\n",
       "  193,\n",
       "  13,\n",
       "  174,\n",
       "  21,\n",
       "  7,\n",
       "  239,\n",
       "  133,\n",
       "  585,\n",
       "  23,\n",
       "  1,\n",
       "  30,\n",
       "  472,\n",
       "  10,\n",
       "  856,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  1,\n",
       "  22,\n",
       "  13,\n",
       "  118,\n",
       "  593,\n",
       "  10,\n",
       "  7,\n",
       "  30,\n",
       "  73,\n",
       "  370,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [48,\n",
       "  401,\n",
       "  13,\n",
       "  650,\n",
       "  72,\n",
       "  9,\n",
       "  1,\n",
       "  330,\n",
       "  5,\n",
       "  1080,\n",
       "  1081,\n",
       "  4,\n",
       "  1082,\n",
       "  36,\n",
       "  131,\n",
       "  2851,\n",
       "  13,\n",
       "  7,\n",
       "  310,\n",
       "  5,\n",
       "  108,\n",
       "  682,\n",
       "  11,\n",
       "  372,\n",
       "  41,\n",
       "  686,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  2852,\n",
       "  1080,\n",
       "  330,\n",
       "  1081,\n",
       "  4,\n",
       "  1082,\n",
       "  36],\n",
       " [2853,\n",
       "  10,\n",
       "  1159,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  486,\n",
       "  128,\n",
       "  97,\n",
       "  26,\n",
       "  1942,\n",
       "  4,\n",
       "  482,\n",
       "  36,\n",
       "  8,\n",
       "  269,\n",
       "  9,\n",
       "  644,\n",
       "  1,\n",
       "  257,\n",
       "  1352,\n",
       "  804,\n",
       "  636,\n",
       "  2854,\n",
       "  1943,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  195,\n",
       "  486,\n",
       "  421,\n",
       "  26,\n",
       "  1942,\n",
       "  4,\n",
       "  482,\n",
       "  36],\n",
       " [1944,\n",
       "  6,\n",
       "  1945,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  120,\n",
       "  20,\n",
       "  131,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  9,\n",
       "  1946,\n",
       "  111,\n",
       "  838,\n",
       "  11,\n",
       "  1,\n",
       "  136,\n",
       "  218,\n",
       "  8,\n",
       "  1,\n",
       "  143,\n",
       "  500,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  120,\n",
       "  878,\n",
       "  171,\n",
       "  11,\n",
       "  186,\n",
       "  198,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [26,\n",
       "  726,\n",
       "  699,\n",
       "  2855,\n",
       "  4,\n",
       "  2856,\n",
       "  11,\n",
       "  51,\n",
       "  2857,\n",
       "  1,\n",
       "  1300,\n",
       "  162,\n",
       "  154,\n",
       "  104,\n",
       "  1947,\n",
       "  9,\n",
       "  1,\n",
       "  162,\n",
       "  97,\n",
       "  8,\n",
       "  451,\n",
       "  4,\n",
       "  690,\n",
       "  156,\n",
       "  1,\n",
       "  1083,\n",
       "  13,\n",
       "  72,\n",
       "  9,\n",
       "  1,\n",
       "  1083,\n",
       "  11,\n",
       "  1,\n",
       "  189,\n",
       "  162,\n",
       "  8,\n",
       "  451,\n",
       "  4,\n",
       "  690,\n",
       "  156],\n",
       " [94,\n",
       "  6,\n",
       "  484,\n",
       "  1,\n",
       "  711,\n",
       "  8,\n",
       "  286,\n",
       "  11,\n",
       "  70,\n",
       "  89,\n",
       "  10,\n",
       "  68,\n",
       "  821,\n",
       "  1211,\n",
       "  608,\n",
       "  935,\n",
       "  84,\n",
       "  14,\n",
       "  2858,\n",
       "  1330,\n",
       "  6,\n",
       "  688,\n",
       "  70,\n",
       "  89,\n",
       "  5,\n",
       "  286,\n",
       "  667,\n",
       "  10,\n",
       "  1146,\n",
       "  1147,\n",
       "  935,\n",
       "  84],\n",
       " [1944,\n",
       "  6,\n",
       "  1945,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  120,\n",
       "  20,\n",
       "  131,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  9,\n",
       "  1946,\n",
       "  111,\n",
       "  838,\n",
       "  11,\n",
       "  1,\n",
       "  136,\n",
       "  218,\n",
       "  8,\n",
       "  1,\n",
       "  143,\n",
       "  500,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  120,\n",
       "  878,\n",
       "  171,\n",
       "  11,\n",
       "  186,\n",
       "  198,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [51,\n",
       "  603,\n",
       "  361,\n",
       "  1349,\n",
       "  11,\n",
       "  93,\n",
       "  119,\n",
       "  1007,\n",
       "  325,\n",
       "  276,\n",
       "  32,\n",
       "  21,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  236,\n",
       "  5,\n",
       "  876,\n",
       "  198,\n",
       "  4,\n",
       "  736,\n",
       "  656,\n",
       "  1077,\n",
       "  11,\n",
       "  1936,\n",
       "  1937,\n",
       "  93,\n",
       "  32,\n",
       "  196,\n",
       "  179,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [8,\n",
       "  1948,\n",
       "  4,\n",
       "  1949,\n",
       "  36,\n",
       "  1,\n",
       "  324,\n",
       "  5,\n",
       "  758,\n",
       "  61,\n",
       "  2859,\n",
       "  33,\n",
       "  15,\n",
       "  9,\n",
       "  496,\n",
       "  7,\n",
       "  75,\n",
       "  5,\n",
       "  125,\n",
       "  8,\n",
       "  2860,\n",
       "  2861,\n",
       "  1253,\n",
       "  2862,\n",
       "  33,\n",
       "  76,\n",
       "  1,\n",
       "  19,\n",
       "  15,\n",
       "  9,\n",
       "  496,\n",
       "  125,\n",
       "  8,\n",
       "  1948,\n",
       "  4,\n",
       "  1949,\n",
       "  36],\n",
       " [89,\n",
       "  33,\n",
       "  484,\n",
       "  10,\n",
       "  7,\n",
       "  141,\n",
       "  77,\n",
       "  12,\n",
       "  36,\n",
       "  14,\n",
       "  507,\n",
       "  1353,\n",
       "  2863,\n",
       "  89,\n",
       "  13,\n",
       "  484,\n",
       "  23,\n",
       "  1,\n",
       "  259,\n",
       "  314,\n",
       "  10,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  14,\n",
       "  699,\n",
       "  507,\n",
       "  4,\n",
       "  228,\n",
       "  410],\n",
       " [11,\n",
       "  1309,\n",
       "  4,\n",
       "  147,\n",
       "  1055,\n",
       "  6,\n",
       "  15,\n",
       "  1857,\n",
       "  1858,\n",
       "  1055,\n",
       "  21,\n",
       "  1,\n",
       "  1310,\n",
       "  279,\n",
       "  399,\n",
       "  1056,\n",
       "  2,\n",
       "  3,\n",
       "  165,\n",
       "  9,\n",
       "  2864,\n",
       "  48,\n",
       "  216,\n",
       "  6,\n",
       "  260,\n",
       "  1,\n",
       "  2865,\n",
       "  142,\n",
       "  211,\n",
       "  63,\n",
       "  4,\n",
       "  248,\n",
       "  279,\n",
       "  2866,\n",
       "  21,\n",
       "  1,\n",
       "  2867,\n",
       "  399,\n",
       "  1056,\n",
       "  2,\n",
       "  3,\n",
       "  165],\n",
       " [6,\n",
       "  15,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  9,\n",
       "  348,\n",
       "  1051,\n",
       "  41,\n",
       "  65,\n",
       "  14,\n",
       "  116,\n",
       "  173,\n",
       "  23,\n",
       "  1,\n",
       "  209,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  239,\n",
       "  46,\n",
       "  2868,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  25,\n",
       "  9,\n",
       "  121,\n",
       "  51,\n",
       "  41,\n",
       "  65,\n",
       "  15,\n",
       "  8,\n",
       "  48,\n",
       "  167,\n",
       "  386,\n",
       "  105,\n",
       "  11,\n",
       "  28,\n",
       "  824,\n",
       "  4,\n",
       "  11,\n",
       "  1,\n",
       "  235,\n",
       "  49,\n",
       "  15,\n",
       "  11,\n",
       "  1830,\n",
       "  143],\n",
       " [6,\n",
       "  66,\n",
       "  7,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  23,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  703,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  6,\n",
       "  15,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  9,\n",
       "  348,\n",
       "  1051,\n",
       "  41,\n",
       "  65,\n",
       "  14,\n",
       "  116,\n",
       "  173,\n",
       "  23,\n",
       "  1,\n",
       "  209,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  239,\n",
       "  46,\n",
       "  144],\n",
       " [68,\n",
       "  707,\n",
       "  5,\n",
       "  1,\n",
       "  2869,\n",
       "  5,\n",
       "  7,\n",
       "  2870,\n",
       "  1165,\n",
       "  7,\n",
       "  477,\n",
       "  1859,\n",
       "  33,\n",
       "  260,\n",
       "  206,\n",
       "  1,\n",
       "  247,\n",
       "  44,\n",
       "  353,\n",
       "  2871,\n",
       "  22,\n",
       "  1950,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  8,\n",
       "  1,\n",
       "  872,\n",
       "  1861,\n",
       "  6,\n",
       "  508,\n",
       "  1,\n",
       "  1396,\n",
       "  5,\n",
       "  7,\n",
       "  388,\n",
       "  5,\n",
       "  247,\n",
       "  44,\n",
       "  353,\n",
       "  1950,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  207,\n",
       "  313,\n",
       "  100,\n",
       "  22,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  9,\n",
       "  981,\n",
       "  1,\n",
       "  41,\n",
       "  19,\n",
       "  222,\n",
       "  11,\n",
       "  7,\n",
       "  2872,\n",
       "  1,\n",
       "  2873,\n",
       "  13,\n",
       "  318,\n",
       "  169,\n",
       "  833,\n",
       "  327,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  207,\n",
       "  313,\n",
       "  100,\n",
       "  22,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  9,\n",
       "  219,\n",
       "  1,\n",
       "  367,\n",
       "  763,\n",
       "  250],\n",
       " [56,\n",
       "  35,\n",
       "  29,\n",
       "  302,\n",
       "  6,\n",
       "  15,\n",
       "  7,\n",
       "  27,\n",
       "  70,\n",
       "  56,\n",
       "  35,\n",
       "  19,\n",
       "  31,\n",
       "  175,\n",
       "  8,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  56,\n",
       "  1790,\n",
       "  17,\n",
       "  38,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  586,\n",
       "  7,\n",
       "  514,\n",
       "  70,\n",
       "  35,\n",
       "  29,\n",
       "  2874,\n",
       "  643,\n",
       "  1,\n",
       "  2875,\n",
       "  2876],\n",
       " [56,\n",
       "  35,\n",
       "  29,\n",
       "  302,\n",
       "  6,\n",
       "  15,\n",
       "  7,\n",
       "  27,\n",
       "  70,\n",
       "  56,\n",
       "  35,\n",
       "  19,\n",
       "  31,\n",
       "  175,\n",
       "  8,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  56,\n",
       "  1320,\n",
       "  50,\n",
       "  7,\n",
       "  19,\n",
       "  10,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  23,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  31,\n",
       "  24,\n",
       "  101,\n",
       "  29],\n",
       " [7,\n",
       "  1245,\n",
       "  13,\n",
       "  7,\n",
       "  406,\n",
       "  2877,\n",
       "  334,\n",
       "  1242,\n",
       "  5,\n",
       "  2878,\n",
       "  1951,\n",
       "  5,\n",
       "  63,\n",
       "  1952,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  1,\n",
       "  1245,\n",
       "  1042,\n",
       "  578,\n",
       "  13,\n",
       "  7,\n",
       "  2879,\n",
       "  213,\n",
       "  5,\n",
       "  2880,\n",
       "  578,\n",
       "  1952,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [2881,\n",
       "  7,\n",
       "  388,\n",
       "  5,\n",
       "  1567,\n",
       "  183,\n",
       "  14,\n",
       "  1953,\n",
       "  30,\n",
       "  218,\n",
       "  338,\n",
       "  4,\n",
       "  1954,\n",
       "  1084,\n",
       "  6,\n",
       "  18,\n",
       "  7,\n",
       "  75,\n",
       "  5,\n",
       "  1953,\n",
       "  30,\n",
       "  218,\n",
       "  21,\n",
       "  7,\n",
       "  678,\n",
       "  196,\n",
       "  179,\n",
       "  26,\n",
       "  338,\n",
       "  4,\n",
       "  1954,\n",
       "  1084],\n",
       " [6,\n",
       "  153,\n",
       "  1955,\n",
       "  1212,\n",
       "  10,\n",
       "  1,\n",
       "  266,\n",
       "  164,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  21,\n",
       "  112,\n",
       "  1956,\n",
       "  131,\n",
       "  51,\n",
       "  361,\n",
       "  67,\n",
       "  233,\n",
       "  10,\n",
       "  1,\n",
       "  266,\n",
       "  556,\n",
       "  164,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [6,\n",
       "  1354,\n",
       "  1,\n",
       "  160,\n",
       "  200,\n",
       "  8,\n",
       "  357,\n",
       "  283,\n",
       "  389,\n",
       "  191,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  120,\n",
       "  878,\n",
       "  171,\n",
       "  11,\n",
       "  186,\n",
       "  198,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [437,\n",
       "  1957,\n",
       "  370,\n",
       "  9,\n",
       "  1044,\n",
       "  1,\n",
       "  1958,\n",
       "  437,\n",
       "  304,\n",
       "  1959,\n",
       "  88,\n",
       "  11,\n",
       "  1,\n",
       "  1958,\n",
       "  437,\n",
       "  304,\n",
       "  1244,\n",
       "  2882,\n",
       "  154,\n",
       "  104,\n",
       "  807,\n",
       "  26,\n",
       "  1957,\n",
       "  1,\n",
       "  1804,\n",
       "  1959,\n",
       "  88],\n",
       " [6,\n",
       "  153,\n",
       "  1955,\n",
       "  1212,\n",
       "  10,\n",
       "  1,\n",
       "  266,\n",
       "  164,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  21,\n",
       "  112,\n",
       "  1956,\n",
       "  131,\n",
       "  1,\n",
       "  361,\n",
       "  67,\n",
       "  414,\n",
       "  10,\n",
       "  1,\n",
       "  266,\n",
       "  556,\n",
       "  164,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [6,\n",
       "  192,\n",
       "  719,\n",
       "  8,\n",
       "  327,\n",
       "  741,\n",
       "  7,\n",
       "  1960,\n",
       "  298,\n",
       "  22,\n",
       "  1,\n",
       "  217,\n",
       "  298,\n",
       "  187,\n",
       "  742,\n",
       "  2,\n",
       "  3,\n",
       "  40,\n",
       "  11,\n",
       "  298,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  217,\n",
       "  298,\n",
       "  187,\n",
       "  213,\n",
       "  2883,\n",
       "  1626,\n",
       "  742,\n",
       "  2,\n",
       "  3,\n",
       "  40],\n",
       " [8,\n",
       "  269,\n",
       "  9,\n",
       "  1621,\n",
       "  70,\n",
       "  89,\n",
       "  5,\n",
       "  1,\n",
       "  153,\n",
       "  188,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  12,\n",
       "  36,\n",
       "  42,\n",
       "  2884,\n",
       "  1,\n",
       "  528,\n",
       "  2885,\n",
       "  34,\n",
       "  7,\n",
       "  460,\n",
       "  711,\n",
       "  11,\n",
       "  70,\n",
       "  89,\n",
       "  281,\n",
       "  6,\n",
       "  18,\n",
       "  7,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  97,\n",
       "  8,\n",
       "  12,\n",
       "  36],\n",
       " [70,\n",
       "  89,\n",
       "  8,\n",
       "  259,\n",
       "  222,\n",
       "  711,\n",
       "  33,\n",
       "  460,\n",
       "  26,\n",
       "  10,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  89,\n",
       "  33,\n",
       "  484,\n",
       "  10,\n",
       "  7,\n",
       "  141,\n",
       "  77,\n",
       "  12,\n",
       "  36,\n",
       "  14,\n",
       "  507,\n",
       "  1353,\n",
       "  1961],\n",
       " [1,\n",
       "  1675,\n",
       "  29,\n",
       "  1085,\n",
       "  23,\n",
       "  1,\n",
       "  2886,\n",
       "  5,\n",
       "  1,\n",
       "  29,\n",
       "  138,\n",
       "  8,\n",
       "  1962,\n",
       "  4,\n",
       "  781,\n",
       "  2887,\n",
       "  1,\n",
       "  2888,\n",
       "  184,\n",
       "  481,\n",
       "  7,\n",
       "  1588,\n",
       "  29,\n",
       "  359,\n",
       "  23,\n",
       "  167,\n",
       "  138,\n",
       "  8,\n",
       "  4,\n",
       "  1962,\n",
       "  4,\n",
       "  781,\n",
       "  2889],\n",
       " [11,\n",
       "  2890,\n",
       "  1,\n",
       "  1934,\n",
       "  2891,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  1062,\n",
       "  22,\n",
       "  425,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  31,\n",
       "  7,\n",
       "  1963,\n",
       "  1327,\n",
       "  6,\n",
       "  18,\n",
       "  1716,\n",
       "  450,\n",
       "  21,\n",
       "  1,\n",
       "  207,\n",
       "  313,\n",
       "  22,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37],\n",
       " [126,\n",
       "  218,\n",
       "  67,\n",
       "  174,\n",
       "  21,\n",
       "  1724,\n",
       "  30,\n",
       "  132,\n",
       "  4,\n",
       "  1725,\n",
       "  421,\n",
       "  26,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  10,\n",
       "  1,\n",
       "  966,\n",
       "  5,\n",
       "  705,\n",
       "  356,\n",
       "  4,\n",
       "  2892,\n",
       "  132,\n",
       "  67,\n",
       "  417,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [6,\n",
       "  1964,\n",
       "  51,\n",
       "  5,\n",
       "  24,\n",
       "  58,\n",
       "  8,\n",
       "  202,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  10,\n",
       "  271,\n",
       "  2893,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  42,\n",
       "  240,\n",
       "  1,\n",
       "  81,\n",
       "  5,\n",
       "  51,\n",
       "  493,\n",
       "  530,\n",
       "  33,\n",
       "  15,\n",
       "  8,\n",
       "  24,\n",
       "  678],\n",
       " [105,\n",
       "  144,\n",
       "  67,\n",
       "  174,\n",
       "  21,\n",
       "  1,\n",
       "  220,\n",
       "  85,\n",
       "  22,\n",
       "  1355,\n",
       "  652,\n",
       "  64,\n",
       "  1,\n",
       "  85,\n",
       "  28,\n",
       "  67,\n",
       "  419,\n",
       "  21,\n",
       "  1355,\n",
       "  652,\n",
       "  64,\n",
       "  42,\n",
       "  586,\n",
       "  1334,\n",
       "  144,\n",
       "  14,\n",
       "  164],\n",
       " [6,\n",
       "  1964,\n",
       "  51,\n",
       "  5,\n",
       "  24,\n",
       "  58,\n",
       "  8,\n",
       "  202,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  10,\n",
       "  271,\n",
       "  2894,\n",
       "  223,\n",
       "  58,\n",
       "  10,\n",
       "  1260,\n",
       "  366,\n",
       "  312,\n",
       "  184,\n",
       "  175,\n",
       "  8,\n",
       "  1,\n",
       "  202,\n",
       "  20,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [1,\n",
       "  582,\n",
       "  832,\n",
       "  46,\n",
       "  22,\n",
       "  33,\n",
       "  564,\n",
       "  21,\n",
       "  1,\n",
       "  54,\n",
       "  310,\n",
       "  12,\n",
       "  40,\n",
       "  1,\n",
       "  58,\n",
       "  939,\n",
       "  23,\n",
       "  35,\n",
       "  21,\n",
       "  305,\n",
       "  9,\n",
       "  44,\n",
       "  10,\n",
       "  1,\n",
       "  54,\n",
       "  28,\n",
       "  12,\n",
       "  40],\n",
       " [384,\n",
       "  29,\n",
       "  1673,\n",
       "  1,\n",
       "  384,\n",
       "  29,\n",
       "  34,\n",
       "  13,\n",
       "  66,\n",
       "  283,\n",
       "  1,\n",
       "  1080,\n",
       "  425,\n",
       "  330,\n",
       "  1081,\n",
       "  4,\n",
       "  1082,\n",
       "  36,\n",
       "  48,\n",
       "  401,\n",
       "  13,\n",
       "  650,\n",
       "  72,\n",
       "  9,\n",
       "  1,\n",
       "  330,\n",
       "  5,\n",
       "  1080,\n",
       "  1081,\n",
       "  4,\n",
       "  1082,\n",
       "  36],\n",
       " [11,\n",
       "  205,\n",
       "  394,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  144,\n",
       "  414,\n",
       "  169,\n",
       "  11,\n",
       "  1,\n",
       "  1323,\n",
       "  159,\n",
       "  62,\n",
       "  1068,\n",
       "  4,\n",
       "  1069,\n",
       "  37,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  1323,\n",
       "  1068,\n",
       "  4,\n",
       "  1069,\n",
       "  37,\n",
       "  377,\n",
       "  28,\n",
       "  21,\n",
       "  2895,\n",
       "  332,\n",
       "  394,\n",
       "  292,\n",
       "  2896,\n",
       "  735,\n",
       "  2897,\n",
       "  1324,\n",
       "  572,\n",
       "  4,\n",
       "  954],\n",
       " [193,\n",
       "  170,\n",
       "  1021,\n",
       "  24,\n",
       "  299,\n",
       "  19,\n",
       "  14,\n",
       "  7,\n",
       "  1356,\n",
       "  5,\n",
       "  1,\n",
       "  299,\n",
       "  19,\n",
       "  97,\n",
       "  8,\n",
       "  1086,\n",
       "  4,\n",
       "  1087,\n",
       "  57,\n",
       "  6,\n",
       "  1965,\n",
       "  34,\n",
       "  24,\n",
       "  19,\n",
       "  1078,\n",
       "  1,\n",
       "  19,\n",
       "  97,\n",
       "  8,\n",
       "  1086,\n",
       "  4,\n",
       "  1087,\n",
       "  57,\n",
       "  8,\n",
       "  51,\n",
       "  1064],\n",
       " [193,\n",
       "  170,\n",
       "  1021,\n",
       "  24,\n",
       "  299,\n",
       "  19,\n",
       "  14,\n",
       "  7,\n",
       "  1356,\n",
       "  5,\n",
       "  1,\n",
       "  299,\n",
       "  19,\n",
       "  97,\n",
       "  8,\n",
       "  1086,\n",
       "  4,\n",
       "  1087,\n",
       "  57,\n",
       "  6,\n",
       "  1965,\n",
       "  34,\n",
       "  24,\n",
       "  19,\n",
       "  1078,\n",
       "  1,\n",
       "  19,\n",
       "  97,\n",
       "  8,\n",
       "  1086,\n",
       "  4,\n",
       "  1087,\n",
       "  57,\n",
       "  8,\n",
       "  51,\n",
       "  1064],\n",
       " [172,\n",
       "  188,\n",
       "  2898,\n",
       "  797,\n",
       "  617,\n",
       "  8,\n",
       "  2899,\n",
       "  2,\n",
       "  3,\n",
       "  43,\n",
       "  319,\n",
       "  1,\n",
       "  1357,\n",
       "  162,\n",
       "  2900,\n",
       "  2901,\n",
       "  1717,\n",
       "  1358,\n",
       "  286,\n",
       "  602,\n",
       "  1,\n",
       "  1966,\n",
       "  2902,\n",
       "  2,\n",
       "  3,\n",
       "  43,\n",
       "  95,\n",
       "  268,\n",
       "  1,\n",
       "  1966,\n",
       "  162,\n",
       "  9,\n",
       "  104,\n",
       "  2903,\n",
       "  8,\n",
       "  1967,\n",
       "  4,\n",
       "  1,\n",
       "  1357,\n",
       "  162,\n",
       "  8,\n",
       "  1967,\n",
       "  131,\n",
       "  319,\n",
       "  840,\n",
       "  4,\n",
       "  699,\n",
       "  32,\n",
       "  1,\n",
       "  212,\n",
       "  5,\n",
       "  2904,\n",
       "  8,\n",
       "  200,\n",
       "  1006,\n",
       "  83,\n",
       "  4,\n",
       "  1006,\n",
       "  94,\n",
       "  1864],\n",
       " [11,\n",
       "  231,\n",
       "  357,\n",
       "  510,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  7,\n",
       "  1359,\n",
       "  164,\n",
       "  511,\n",
       "  851,\n",
       "  48,\n",
       "  2905,\n",
       "  48,\n",
       "  1968,\n",
       "  7,\n",
       "  1262,\n",
       "  1359,\n",
       "  164,\n",
       "  2906,\n",
       "  618,\n",
       "  1,\n",
       "  357,\n",
       "  511,\n",
       "  510,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  33,\n",
       "  2907],\n",
       " [31,\n",
       "  11,\n",
       "  1969,\n",
       "  35,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  44,\n",
       "  1970,\n",
       "  727,\n",
       "  18,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  160,\n",
       "  200,\n",
       "  4,\n",
       "  55,\n",
       "  242],\n",
       " [11,\n",
       "  105,\n",
       "  44,\n",
       "  4,\n",
       "  305,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  1321,\n",
       "  876,\n",
       "  171,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  9,\n",
       "  210,\n",
       "  2908,\n",
       "  11,\n",
       "  305,\n",
       "  28,\n",
       "  1138,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  632],\n",
       " [6,\n",
       "  69,\n",
       "  24,\n",
       "  19,\n",
       "  23,\n",
       "  7,\n",
       "  388,\n",
       "  5,\n",
       "  1,\n",
       "  1971,\n",
       "  1972,\n",
       "  425,\n",
       "  22,\n",
       "  463,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  6,\n",
       "  66,\n",
       "  7,\n",
       "  804,\n",
       "  863,\n",
       "  1899,\n",
       "  94,\n",
       "  1824,\n",
       "  83,\n",
       "  10,\n",
       "  1,\n",
       "  2909,\n",
       "  22,\n",
       "  463,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [1,\n",
       "  1015,\n",
       "  32,\n",
       "  637,\n",
       "  10,\n",
       "  7,\n",
       "  27,\n",
       "  29,\n",
       "  66,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  1234,\n",
       "  1973,\n",
       "  118,\n",
       "  112,\n",
       "  1088,\n",
       "  32,\n",
       "  60,\n",
       "  49,\n",
       "  10,\n",
       "  408,\n",
       "  294,\n",
       "  1360,\n",
       "  50,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  1015,\n",
       "  32,\n",
       "  637,\n",
       "  10,\n",
       "  7,\n",
       "  27,\n",
       "  29,\n",
       "  66,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  1234,\n",
       "  1973,\n",
       "  161,\n",
       "  13,\n",
       "  66,\n",
       "  23,\n",
       "  1089,\n",
       "  5,\n",
       "  68,\n",
       "  177,\n",
       "  27,\n",
       "  38,\n",
       "  161,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  216,\n",
       "  70,\n",
       "  89,\n",
       "  10,\n",
       "  364,\n",
       "  311,\n",
       "  297,\n",
       "  260,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  89,\n",
       "  33,\n",
       "  484,\n",
       "  10,\n",
       "  7,\n",
       "  141,\n",
       "  77,\n",
       "  12,\n",
       "  36,\n",
       "  14,\n",
       "  507,\n",
       "  1353,\n",
       "  1961],\n",
       " [31,\n",
       "  11,\n",
       "  1969,\n",
       "  35,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  44,\n",
       "  1970,\n",
       "  727,\n",
       "  18,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  160,\n",
       "  200,\n",
       "  4,\n",
       "  55,\n",
       "  242],\n",
       " [63,\n",
       "  67,\n",
       "  2910,\n",
       "  4,\n",
       "  182,\n",
       "  10,\n",
       "  1,\n",
       "  249,\n",
       "  1256,\n",
       "  8,\n",
       "  1,\n",
       "  272,\n",
       "  94,\n",
       "  20,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  11,\n",
       "  1941,\n",
       "  6,\n",
       "  1049,\n",
       "  4,\n",
       "  181,\n",
       "  1,\n",
       "  28,\n",
       "  10,\n",
       "  272,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [44,\n",
       "  125,\n",
       "  32,\n",
       "  373,\n",
       "  283,\n",
       "  55,\n",
       "  782,\n",
       "  26,\n",
       "  39,\n",
       "  52,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  1,\n",
       "  1817,\n",
       "  242,\n",
       "  32,\n",
       "  51,\n",
       "  1,\n",
       "  537,\n",
       "  191,\n",
       "  660,\n",
       "  26,\n",
       "  1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [105,\n",
       "  5,\n",
       "  24,\n",
       "  49,\n",
       "  67,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  17,\n",
       "  161,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  161,\n",
       "  13,\n",
       "  66,\n",
       "  23,\n",
       "  1089,\n",
       "  5,\n",
       "  68,\n",
       "  177,\n",
       "  27,\n",
       "  38,\n",
       "  161,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  76,\n",
       "  15,\n",
       "  1974,\n",
       "  1975,\n",
       "  4,\n",
       "  1976,\n",
       "  288,\n",
       "  11,\n",
       "  704,\n",
       "  1,\n",
       "  2911,\n",
       "  486,\n",
       "  1977,\n",
       "  2912,\n",
       "  1,\n",
       "  1974,\n",
       "  486,\n",
       "  1975,\n",
       "  4,\n",
       "  1976,\n",
       "  288,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  970,\n",
       "  1,\n",
       "  331,\n",
       "  75,\n",
       "  5,\n",
       "  1309,\n",
       "  63,\n",
       "  5,\n",
       "  83],\n",
       " [35,\n",
       "  190,\n",
       "  1,\n",
       "  1978,\n",
       "  201,\n",
       "  877,\n",
       "  1979,\n",
       "  2,\n",
       "  3,\n",
       "  201,\n",
       "  1,\n",
       "  483,\n",
       "  1130,\n",
       "  5,\n",
       "  1,\n",
       "  2913,\n",
       "  655,\n",
       "  1932,\n",
       "  631,\n",
       "  8,\n",
       "  105,\n",
       "  2914,\n",
       "  5,\n",
       "  1,\n",
       "  1978,\n",
       "  201,\n",
       "  159,\n",
       "  62,\n",
       "  2915,\n",
       "  35,\n",
       "  4,\n",
       "  2916,\n",
       "  792,\n",
       "  1979,\n",
       "  2,\n",
       "  3,\n",
       "  201],\n",
       " [1291,\n",
       "  4,\n",
       "  2917,\n",
       "  649,\n",
       "  1980,\n",
       "  13,\n",
       "  7,\n",
       "  2918,\n",
       "  980,\n",
       "  2919,\n",
       "  11,\n",
       "  352,\n",
       "  2920,\n",
       "  498,\n",
       "  197,\n",
       "  68,\n",
       "  231,\n",
       "  5,\n",
       "  290,\n",
       "  7,\n",
       "  2921,\n",
       "  1737,\n",
       "  13,\n",
       "  649,\n",
       "  1980,\n",
       "  498,\n",
       "  197],\n",
       " [1981,\n",
       "  222,\n",
       "  1982,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  1982,\n",
       "  249,\n",
       "  1983,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  257,\n",
       "  251,\n",
       "  9,\n",
       "  1901,\n",
       "  1745,\n",
       "  11,\n",
       "  111,\n",
       "  30,\n",
       "  61,\n",
       "  23,\n",
       "  493,\n",
       "  1311,\n",
       "  386,\n",
       "  1984,\n",
       "  1985,\n",
       "  4,\n",
       "  995,\n",
       "  2922,\n",
       "  192,\n",
       "  95,\n",
       "  757,\n",
       "  7,\n",
       "  195,\n",
       "  222,\n",
       "  10,\n",
       "  1,\n",
       "  1981,\n",
       "  1983,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  257,\n",
       "  251,\n",
       "  9,\n",
       "  111,\n",
       "  30,\n",
       "  8,\n",
       "  1,\n",
       "  75,\n",
       "  5,\n",
       "  1494,\n",
       "  2923],\n",
       " [54,\n",
       "  94,\n",
       "  12,\n",
       "  40,\n",
       "  98,\n",
       "  13,\n",
       "  7,\n",
       "  22,\n",
       "  5,\n",
       "  85,\n",
       "  306,\n",
       "  8,\n",
       "  1011,\n",
       "  394,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  449,\n",
       "  6,\n",
       "  15,\n",
       "  7,\n",
       "  388,\n",
       "  5,\n",
       "  1,\n",
       "  28,\n",
       "  176,\n",
       "  11,\n",
       "  1,\n",
       "  391,\n",
       "  877,\n",
       "  23,\n",
       "  70,\n",
       "  56,\n",
       "  35,\n",
       "  94,\n",
       "  42,\n",
       "  351,\n",
       "  1189,\n",
       "  5,\n",
       "  306,\n",
       "  21,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [6,\n",
       "  76,\n",
       "  196,\n",
       "  179,\n",
       "  7,\n",
       "  1660,\n",
       "  433,\n",
       "  340,\n",
       "  319,\n",
       "  1,\n",
       "  1007,\n",
       "  32,\n",
       "  830,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  1233,\n",
       "  132,\n",
       "  153,\n",
       "  26,\n",
       "  1661,\n",
       "  340,\n",
       "  5,\n",
       "  829,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  829,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  509,\n",
       "  7,\n",
       "  82,\n",
       "  319,\n",
       "  261,\n",
       "  133,\n",
       "  13,\n",
       "  830,\n",
       "  9,\n",
       "  1794,\n",
       "  1,\n",
       "  209,\n",
       "  30,\n",
       "  269,\n",
       "  61,\n",
       "  23,\n",
       "  30,\n",
       "  73],\n",
       " [1986,\n",
       "  13,\n",
       "  1987,\n",
       "  728,\n",
       "  31,\n",
       "  68,\n",
       "  345,\n",
       "  322,\n",
       "  62,\n",
       "  643,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  34,\n",
       "  1,\n",
       "  30,\n",
       "  157,\n",
       "  13,\n",
       "  1988,\n",
       "  26,\n",
       "  1,\n",
       "  75,\n",
       "  5,\n",
       "  149,\n",
       "  8,\n",
       "  42,\n",
       "  98,\n",
       "  2924,\n",
       "  65,\n",
       "  5,\n",
       "  157,\n",
       "  360,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  112,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  95,\n",
       "  72,\n",
       "  365],\n",
       " [1,\n",
       "  28,\n",
       "  15,\n",
       "  11,\n",
       "  1,\n",
       "  58,\n",
       "  138,\n",
       "  8,\n",
       "  48,\n",
       "  263,\n",
       "  622,\n",
       "  1710,\n",
       "  21,\n",
       "  1711,\n",
       "  491,\n",
       "  512,\n",
       "  4,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  5,\n",
       "  203,\n",
       "  845,\n",
       "  208,\n",
       "  12,\n",
       "  40,\n",
       "  1274,\n",
       "  54,\n",
       "  94,\n",
       "  12,\n",
       "  40,\n",
       "  98,\n",
       "  13,\n",
       "  7,\n",
       "  22,\n",
       "  5,\n",
       "  85,\n",
       "  306,\n",
       "  8,\n",
       "  1011,\n",
       "  394,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  449],\n",
       " [11,\n",
       "  1,\n",
       "  41,\n",
       "  19,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  25,\n",
       "  9,\n",
       "  348,\n",
       "  7,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  23,\n",
       "  1,\n",
       "  209,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  717,\n",
       "  14,\n",
       "  844,\n",
       "  1989,\n",
       "  1246,\n",
       "  14,\n",
       "  116,\n",
       "  2925,\n",
       "  19,\n",
       "  11,\n",
       "  51,\n",
       "  1795,\n",
       "  49,\n",
       "  6,\n",
       "  18,\n",
       "  7,\n",
       "  1050,\n",
       "  116,\n",
       "  291,\n",
       "  41,\n",
       "  19,\n",
       "  50,\n",
       "  10,\n",
       "  1,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  25],\n",
       " [118,\n",
       "  15,\n",
       "  26,\n",
       "  1990,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  1690,\n",
       "  183,\n",
       "  240,\n",
       "  253,\n",
       "  332,\n",
       "  354,\n",
       "  5,\n",
       "  1361,\n",
       "  695,\n",
       "  419,\n",
       "  21,\n",
       "  2926,\n",
       "  539,\n",
       "  1090,\n",
       "  1991,\n",
       "  2927,\n",
       "  4,\n",
       "  2928,\n",
       "  2929,\n",
       "  14,\n",
       "  507,\n",
       "  1984,\n",
       "  9,\n",
       "  337,\n",
       "  2930,\n",
       "  11,\n",
       "  195,\n",
       "  215,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  2931,\n",
       "  1361,\n",
       "  695,\n",
       "  803,\n",
       "  26,\n",
       "  1990,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  1304,\n",
       "  332,\n",
       "  1361,\n",
       "  1135,\n",
       "  1090,\n",
       "  989,\n",
       "  1991,\n",
       "  924,\n",
       "  2932,\n",
       "  2933,\n",
       "  505,\n",
       "  4],\n",
       " [1,\n",
       "  255,\n",
       "  1362,\n",
       "  231,\n",
       "  1363,\n",
       "  1364,\n",
       "  104,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  11,\n",
       "  48,\n",
       "  265,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [11,\n",
       "  41,\n",
       "  343,\n",
       "  6,\n",
       "  50,\n",
       "  7,\n",
       "  1220,\n",
       "  100,\n",
       "  116,\n",
       "  291,\n",
       "  371,\n",
       "  19,\n",
       "  23,\n",
       "  1,\n",
       "  209,\n",
       "  386,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  825,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  8,\n",
       "  269,\n",
       "  9,\n",
       "  337,\n",
       "  1,\n",
       "  1677,\n",
       "  5,\n",
       "  111,\n",
       "  29,\n",
       "  6,\n",
       "  69,\n",
       "  100,\n",
       "  41,\n",
       "  65,\n",
       "  11,\n",
       "  111,\n",
       "  41,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25],\n",
       " [328,\n",
       "  15,\n",
       "  1,\n",
       "  1206,\n",
       "  164,\n",
       "  122,\n",
       "  266,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  11,\n",
       "  1,\n",
       "  164,\n",
       "  1,\n",
       "  361,\n",
       "  67,\n",
       "  414,\n",
       "  10,\n",
       "  1,\n",
       "  266,\n",
       "  556,\n",
       "  164,\n",
       "  122,\n",
       "  267,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [1,\n",
       "  215,\n",
       "  33,\n",
       "  223,\n",
       "  10,\n",
       "  332,\n",
       "  92,\n",
       "  530,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  1,\n",
       "  99,\n",
       "  65,\n",
       "  67,\n",
       "  50,\n",
       "  10,\n",
       "  1,\n",
       "  92,\n",
       "  20,\n",
       "  253,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  13,\n",
       "  66,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  541,\n",
       "  15,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [11,\n",
       "  41,\n",
       "  343,\n",
       "  6,\n",
       "  50,\n",
       "  7,\n",
       "  1220,\n",
       "  100,\n",
       "  116,\n",
       "  291,\n",
       "  371,\n",
       "  19,\n",
       "  23,\n",
       "  1,\n",
       "  209,\n",
       "  386,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  825,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  1,\n",
       "  41,\n",
       "  19,\n",
       "  13,\n",
       "  7,\n",
       "  100,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  19,\n",
       "  50,\n",
       "  10,\n",
       "  822,\n",
       "  14,\n",
       "  155,\n",
       "  116,\n",
       "  173,\n",
       "  4,\n",
       "  559,\n",
       "  567],\n",
       " [1,\n",
       "  582,\n",
       "  832,\n",
       "  46,\n",
       "  22,\n",
       "  33,\n",
       "  564,\n",
       "  21,\n",
       "  1,\n",
       "  54,\n",
       "  310,\n",
       "  12,\n",
       "  40,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  120,\n",
       "  878,\n",
       "  171,\n",
       "  11,\n",
       "  186,\n",
       "  198,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  6,\n",
       "  76,\n",
       "  182,\n",
       "  51,\n",
       "  63,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [51,\n",
       "  5,\n",
       "  24,\n",
       "  65,\n",
       "  32,\n",
       "  50,\n",
       "  10,\n",
       "  1992,\n",
       "  455,\n",
       "  2,\n",
       "  3,\n",
       "  974,\n",
       "  6,\n",
       "  50,\n",
       "  24,\n",
       "  628,\n",
       "  238,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  420,\n",
       "  863,\n",
       "  8,\n",
       "  193,\n",
       "  131,\n",
       "  14,\n",
       "  1992,\n",
       "  455,\n",
       "  2,\n",
       "  3,\n",
       "  974],\n",
       " [6,\n",
       "  18,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  14,\n",
       "  124,\n",
       "  107,\n",
       "  106,\n",
       "  9,\n",
       "  723,\n",
       "  126,\n",
       "  1993,\n",
       "  15,\n",
       "  1,\n",
       "  59,\n",
       "  108,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  657,\n",
       "  1,\n",
       "  30,\n",
       "  132],\n",
       " [6,\n",
       "  32,\n",
       "  2934,\n",
       "  14,\n",
       "  60,\n",
       "  1248,\n",
       "  31,\n",
       "  1994,\n",
       "  1995,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  1,\n",
       "  636,\n",
       "  13,\n",
       "  402,\n",
       "  14,\n",
       "  2935,\n",
       "  31,\n",
       "  175,\n",
       "  8,\n",
       "  1994,\n",
       "  1995,\n",
       "  2,\n",
       "  3,\n",
       "  88],\n",
       " [328,\n",
       "  32,\n",
       "  61,\n",
       "  23,\n",
       "  91,\n",
       "  113,\n",
       "  42,\n",
       "  1257,\n",
       "  643,\n",
       "  1,\n",
       "  1258,\n",
       "  34,\n",
       "  72,\n",
       "  63,\n",
       "  194,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  114,\n",
       "  127,\n",
       "  625,\n",
       "  1518,\n",
       "  360,\n",
       "  1,\n",
       "  60,\n",
       "  91,\n",
       "  113,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  63,\n",
       "  945,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  157,\n",
       "  114,\n",
       "  127],\n",
       " [124,\n",
       "  575,\n",
       "  19,\n",
       "  13,\n",
       "  7,\n",
       "  2936,\n",
       "  529,\n",
       "  526,\n",
       "  19,\n",
       "  376,\n",
       "  19,\n",
       "  627,\n",
       "  2,\n",
       "  3,\n",
       "  43,\n",
       "  11,\n",
       "  298,\n",
       "  7,\n",
       "  575,\n",
       "  19,\n",
       "  627,\n",
       "  2,\n",
       "  3,\n",
       "  43,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  2937,\n",
       "  30,\n",
       "  2938],\n",
       " [1,\n",
       "  1091,\n",
       "  763,\n",
       "  162,\n",
       "  33,\n",
       "  50,\n",
       "  14,\n",
       "  404,\n",
       "  316,\n",
       "  4,\n",
       "  134,\n",
       "  25,\n",
       "  31,\n",
       "  24,\n",
       "  1222,\n",
       "  6,\n",
       "  18,\n",
       "  404,\n",
       "  14,\n",
       "  7,\n",
       "  226,\n",
       "  162,\n",
       "  316,\n",
       "  4,\n",
       "  134,\n",
       "  25],\n",
       " [1,\n",
       "  144,\n",
       "  32,\n",
       "  118,\n",
       "  181,\n",
       "  4,\n",
       "  721,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  229,\n",
       "  192,\n",
       "  182,\n",
       "  4,\n",
       "  224,\n",
       "  26,\n",
       "  186,\n",
       "  119,\n",
       "  10,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  1,\n",
       "  22,\n",
       "  33,\n",
       "  1947,\n",
       "  21,\n",
       "  2939,\n",
       "  9,\n",
       "  2940,\n",
       "  133,\n",
       "  413,\n",
       "  1228,\n",
       "  1288,\n",
       "  476,\n",
       "  67,\n",
       "  192,\n",
       "  214,\n",
       "  4,\n",
       "  1,\n",
       "  22,\n",
       "  33,\n",
       "  182,\n",
       "  10,\n",
       "  148,\n",
       "  135,\n",
       "  110],\n",
       " [11,\n",
       "  231,\n",
       "  357,\n",
       "  510,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  7,\n",
       "  1359,\n",
       "  164,\n",
       "  511,\n",
       "  851,\n",
       "  48,\n",
       "  2941,\n",
       "  231,\n",
       "  1,\n",
       "  357,\n",
       "  510,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  511,\n",
       "  2942,\n",
       "  11,\n",
       "  48,\n",
       "  128],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  177,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  121,\n",
       "  7,\n",
       "  60,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  42,\n",
       "  1720,\n",
       "  848,\n",
       "  9,\n",
       "  344,\n",
       "  63,\n",
       "  1272,\n",
       "  8,\n",
       "  1,\n",
       "  17,\n",
       "  126,\n",
       "  2943,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  126,\n",
       "  61,\n",
       "  38,\n",
       "  29,\n",
       "  14,\n",
       "  1,\n",
       "  85,\n",
       "  28,\n",
       "  870,\n",
       "  587],\n",
       " [8,\n",
       "  24,\n",
       "  58,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  490,\n",
       "  152,\n",
       "  557,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  9,\n",
       "  972,\n",
       "  1,\n",
       "  2944,\n",
       "  304,\n",
       "  14,\n",
       "  900,\n",
       "  94,\n",
       "  349,\n",
       "  4,\n",
       "  900,\n",
       "  94,\n",
       "  2945,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  490,\n",
       "  99,\n",
       "  152,\n",
       "  557,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  31,\n",
       "  98,\n",
       "  13,\n",
       "  1163,\n",
       "  9,\n",
       "  133,\n",
       "  215,\n",
       "  558,\n",
       "  14,\n",
       "  390,\n",
       "  1164,\n",
       "  5,\n",
       "  93,\n",
       "  4,\n",
       "  306],\n",
       " [1,\n",
       "  582,\n",
       "  832,\n",
       "  46,\n",
       "  22,\n",
       "  33,\n",
       "  564,\n",
       "  21,\n",
       "  1,\n",
       "  54,\n",
       "  310,\n",
       "  12,\n",
       "  40,\n",
       "  11,\n",
       "  48,\n",
       "  265,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [1,\n",
       "  1365,\n",
       "  1366,\n",
       "  22,\n",
       "  1996,\n",
       "  2,\n",
       "  3,\n",
       "  43,\n",
       "  13,\n",
       "  7,\n",
       "  22,\n",
       "  5,\n",
       "  133,\n",
       "  1856,\n",
       "  5,\n",
       "  655,\n",
       "  2946,\n",
       "  1365,\n",
       "  1366,\n",
       "  22,\n",
       "  1,\n",
       "  1365,\n",
       "  1366,\n",
       "  22,\n",
       "  1996,\n",
       "  2,\n",
       "  3,\n",
       "  43,\n",
       "  13,\n",
       "  2947,\n",
       "  2948,\n",
       "  2949],\n",
       " [6,\n",
       "  18,\n",
       "  317,\n",
       "  134,\n",
       "  36,\n",
       "  11,\n",
       "  1997,\n",
       "  1,\n",
       "  879,\n",
       "  5,\n",
       "  1998,\n",
       "  18,\n",
       "  317,\n",
       "  314,\n",
       "  134,\n",
       "  36,\n",
       "  9,\n",
       "  337,\n",
       "  421,\n",
       "  2950,\n",
       "  659,\n",
       "  296,\n",
       "  1032],\n",
       " [1,\n",
       "  216,\n",
       "  459,\n",
       "  13,\n",
       "  1,\n",
       "  2951,\n",
       "  2952,\n",
       "  1999,\n",
       "  569,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  1,\n",
       "  62,\n",
       "  13,\n",
       "  236,\n",
       "  5,\n",
       "  1,\n",
       "  130,\n",
       "  143,\n",
       "  64,\n",
       "  877,\n",
       "  569,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [1,\n",
       "  1849,\n",
       "  144,\n",
       "  6,\n",
       "  18,\n",
       "  32,\n",
       "  54,\n",
       "  12,\n",
       "  40,\n",
       "  4,\n",
       "  1,\n",
       "  2953,\n",
       "  2954,\n",
       "  48,\n",
       "  265,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [6,\n",
       "  976,\n",
       "  48,\n",
       "  427,\n",
       "  28,\n",
       "  11,\n",
       "  46,\n",
       "  31,\n",
       "  138,\n",
       "  8,\n",
       "  455,\n",
       "  2,\n",
       "  3,\n",
       "  560,\n",
       "  6,\n",
       "  76,\n",
       "  15,\n",
       "  374,\n",
       "  2000,\n",
       "  368,\n",
       "  427,\n",
       "  28,\n",
       "  455,\n",
       "  2,\n",
       "  3,\n",
       "  560],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  120,\n",
       "  878,\n",
       "  171,\n",
       "  11,\n",
       "  186,\n",
       "  198,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  6,\n",
       "  76,\n",
       "  182,\n",
       "  51,\n",
       "  63,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [48,\n",
       "  13,\n",
       "  7,\n",
       "  625,\n",
       "  314,\n",
       "  2001,\n",
       "  23,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  5,\n",
       "  157,\n",
       "  2002,\n",
       "  34,\n",
       "  142,\n",
       "  5,\n",
       "  308,\n",
       "  2003,\n",
       "  142,\n",
       "  5,\n",
       "  157,\n",
       "  1092,\n",
       "  114,\n",
       "  127,\n",
       "  625,\n",
       "  157,\n",
       "  418,\n",
       "  910,\n",
       "  23,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  42,\n",
       "  1668,\n",
       "  34,\n",
       "  63,\n",
       "  1017,\n",
       "  8,\n",
       "  7,\n",
       "  72,\n",
       "  75,\n",
       "  5,\n",
       "  149,\n",
       "  32,\n",
       "  76,\n",
       "  72,\n",
       "  8,\n",
       "  157,\n",
       "  114,\n",
       "  127],\n",
       " [2004,\n",
       "  475,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  397,\n",
       "  493,\n",
       "  332,\n",
       "  30,\n",
       "  418,\n",
       "  23,\n",
       "  595,\n",
       "  4,\n",
       "  738,\n",
       "  4,\n",
       "  2955,\n",
       "  34,\n",
       "  345,\n",
       "  30,\n",
       "  418,\n",
       "  1149,\n",
       "  595,\n",
       "  4,\n",
       "  2956,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  337,\n",
       "  332,\n",
       "  1057,\n",
       "  11,\n",
       "  1264,\n",
       "  30,\n",
       "  418,\n",
       "  4,\n",
       "  741,\n",
       "  454,\n",
       "  2957,\n",
       "  11,\n",
       "  576,\n",
       "  595,\n",
       "  4,\n",
       "  738,\n",
       "  49,\n",
       "  545,\n",
       "  76,\n",
       "  2005,\n",
       "  30,\n",
       "  1045],\n",
       " [1367,\n",
       "  296,\n",
       "  29,\n",
       "  6,\n",
       "  225,\n",
       "  7,\n",
       "  212,\n",
       "  5,\n",
       "  1368,\n",
       "  1369,\n",
       "  9,\n",
       "  1,\n",
       "  258,\n",
       "  70,\n",
       "  35,\n",
       "  1001,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  192,\n",
       "  18,\n",
       "  1,\n",
       "  126,\n",
       "  303,\n",
       "  1722,\n",
       "  8,\n",
       "  1,\n",
       "  17,\n",
       "  70,\n",
       "  56,\n",
       "  35,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  219,\n",
       "  7,\n",
       "  126,\n",
       "  193,\n",
       "  42,\n",
       "  1723,\n",
       "  206,\n",
       "  1191],\n",
       " [11,\n",
       "  46,\n",
       "  99,\n",
       "  163,\n",
       "  6,\n",
       "  15,\n",
       "  404,\n",
       "  152,\n",
       "  316,\n",
       "  4,\n",
       "  134,\n",
       "  156,\n",
       "  6,\n",
       "  15,\n",
       "  404,\n",
       "  9,\n",
       "  424,\n",
       "  24,\n",
       "  2958,\n",
       "  99,\n",
       "  11,\n",
       "  199,\n",
       "  316,\n",
       "  4,\n",
       "  134,\n",
       "  156],\n",
       " [1,\n",
       "  30,\n",
       "  73,\n",
       "  33,\n",
       "  50,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  14,\n",
       "  1,\n",
       "  580,\n",
       "  2959,\n",
       "  30,\n",
       "  73,\n",
       "  33,\n",
       "  153,\n",
       "  26,\n",
       "  464,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [1,\n",
       "  22,\n",
       "  13,\n",
       "  118,\n",
       "  593,\n",
       "  10,\n",
       "  7,\n",
       "  30,\n",
       "  73,\n",
       "  370,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  1,\n",
       "  30,\n",
       "  73,\n",
       "  33,\n",
       "  153,\n",
       "  26,\n",
       "  464,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [6,\n",
       "  262,\n",
       "  77,\n",
       "  80,\n",
       "  14,\n",
       "  604,\n",
       "  411,\n",
       "  31,\n",
       "  138,\n",
       "  8,\n",
       "  12,\n",
       "  36,\n",
       "  6,\n",
       "  15,\n",
       "  77,\n",
       "  80,\n",
       "  11,\n",
       "  281,\n",
       "  70,\n",
       "  89,\n",
       "  12,\n",
       "  36],\n",
       " [6,\n",
       "  262,\n",
       "  77,\n",
       "  80,\n",
       "  14,\n",
       "  604,\n",
       "  411,\n",
       "  31,\n",
       "  138,\n",
       "  26,\n",
       "  12,\n",
       "  36,\n",
       "  6,\n",
       "  15,\n",
       "  77,\n",
       "  80,\n",
       "  11,\n",
       "  281,\n",
       "  70,\n",
       "  89,\n",
       "  12,\n",
       "  36],\n",
       " [526,\n",
       "  897,\n",
       "  875,\n",
       "  2006,\n",
       "  2007,\n",
       "  4,\n",
       "  2008,\n",
       "  37,\n",
       "  13,\n",
       "  458,\n",
       "  11,\n",
       "  78,\n",
       "  4,\n",
       "  2960,\n",
       "  897,\n",
       "  875,\n",
       "  2006,\n",
       "  2007,\n",
       "  4,\n",
       "  2008,\n",
       "  37,\n",
       "  32,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  70,\n",
       "  1875,\n",
       "  78,\n",
       "  2961],\n",
       " [14,\n",
       "  1,\n",
       "  46,\n",
       "  999,\n",
       "  5,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  14,\n",
       "  124,\n",
       "  107,\n",
       "  302],\n",
       " [6,\n",
       "  69,\n",
       "  11,\n",
       "  1023,\n",
       "  1719,\n",
       "  10,\n",
       "  520,\n",
       "  729,\n",
       "  437,\n",
       "  847,\n",
       "  1,\n",
       "  518,\n",
       "  285,\n",
       "  469,\n",
       "  519,\n",
       "  64,\n",
       "  4,\n",
       "  2962,\n",
       "  2963,\n",
       "  2009,\n",
       "  106,\n",
       "  2964,\n",
       "  10,\n",
       "  1,\n",
       "  2965,\n",
       "  410,\n",
       "  410,\n",
       "  11,\n",
       "  677,\n",
       "  106,\n",
       "  4,\n",
       "  754,\n",
       "  754,\n",
       "  11,\n",
       "  178,\n",
       "  4,\n",
       "  69,\n",
       "  1,\n",
       "  19,\n",
       "  10,\n",
       "  729,\n",
       "  437,\n",
       "  847,\n",
       "  1329,\n",
       "  14,\n",
       "  78,\n",
       "  2010],\n",
       " [48,\n",
       "  2966,\n",
       "  1,\n",
       "  2967,\n",
       "  5,\n",
       "  482,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  34,\n",
       "  41,\n",
       "  19,\n",
       "  4,\n",
       "  375,\n",
       "  299,\n",
       "  65,\n",
       "  254,\n",
       "  95,\n",
       "  2968,\n",
       "  2969,\n",
       "  23,\n",
       "  35,\n",
       "  2970,\n",
       "  439,\n",
       "  482,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  95,\n",
       "  97,\n",
       "  7,\n",
       "  300,\n",
       "  35,\n",
       "  2971,\n",
       "  401,\n",
       "  34,\n",
       "  2972,\n",
       "  254,\n",
       "  23,\n",
       "  427,\n",
       "  144],\n",
       " [11,\n",
       "  48,\n",
       "  265,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  6,\n",
       "  337,\n",
       "  24,\n",
       "  82,\n",
       "  26,\n",
       "  949,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [6,\n",
       "  976,\n",
       "  48,\n",
       "  427,\n",
       "  28,\n",
       "  11,\n",
       "  46,\n",
       "  31,\n",
       "  138,\n",
       "  8,\n",
       "  455,\n",
       "  2,\n",
       "  3,\n",
       "  560,\n",
       "  6,\n",
       "  76,\n",
       "  15,\n",
       "  374,\n",
       "  2000,\n",
       "  368,\n",
       "  427,\n",
       "  28,\n",
       "  455,\n",
       "  2,\n",
       "  3,\n",
       "  560],\n",
       " [24,\n",
       "  161,\n",
       "  13,\n",
       "  7,\n",
       "  1333,\n",
       "  161,\n",
       "  72,\n",
       "  9,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  43,\n",
       "  126,\n",
       "  303,\n",
       "  33,\n",
       "  109,\n",
       "  280,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  43],\n",
       " [11,\n",
       "  1,\n",
       "  44,\n",
       "  572,\n",
       "  4,\n",
       "  841,\n",
       "  49,\n",
       "  6,\n",
       "  15,\n",
       "  85,\n",
       "  46,\n",
       "  28,\n",
       "  21,\n",
       "  1,\n",
       "  54,\n",
       "  4,\n",
       "  872,\n",
       "  2011,\n",
       "  144,\n",
       "  31,\n",
       "  350,\n",
       "  31,\n",
       "  1,\n",
       "  2012,\n",
       "  22,\n",
       "  2013,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  8,\n",
       "  24,\n",
       "  2014,\n",
       "  1789,\n",
       "  6,\n",
       "  50,\n",
       "  68,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  254,\n",
       "  46,\n",
       "  28,\n",
       "  21,\n",
       "  1,\n",
       "  2012,\n",
       "  22,\n",
       "  2013,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [1,\n",
       "  30,\n",
       "  73,\n",
       "  33,\n",
       "  50,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  14,\n",
       "  1,\n",
       "  580,\n",
       "  2973,\n",
       "  73,\n",
       "  33,\n",
       "  233,\n",
       "  14,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  11,\n",
       "  105,\n",
       "  49],\n",
       " [542,\n",
       "  334,\n",
       "  234,\n",
       "  931,\n",
       "  543,\n",
       "  4,\n",
       "  544,\n",
       "  387,\n",
       "  887,\n",
       "  1,\n",
       "  2015,\n",
       "  334,\n",
       "  5,\n",
       "  7,\n",
       "  133,\n",
       "  26,\n",
       "  7,\n",
       "  420,\n",
       "  189,\n",
       "  618,\n",
       "  137,\n",
       "  189,\n",
       "  1486,\n",
       "  31,\n",
       "  268,\n",
       "  8,\n",
       "  426,\n",
       "  83,\n",
       "  1,\n",
       "  861,\n",
       "  1342,\n",
       "  9,\n",
       "  24,\n",
       "  167,\n",
       "  351,\n",
       "  5,\n",
       "  1911,\n",
       "  5,\n",
       "  137,\n",
       "  242,\n",
       "  8,\n",
       "  1,\n",
       "  308,\n",
       "  5,\n",
       "  542,\n",
       "  334,\n",
       "  234,\n",
       "  543,\n",
       "  4,\n",
       "  544,\n",
       "  387],\n",
       " [1,\n",
       "  183,\n",
       "  15,\n",
       "  11,\n",
       "  1,\n",
       "  58,\n",
       "  516,\n",
       "  8,\n",
       "  48,\n",
       "  263,\n",
       "  129,\n",
       "  123,\n",
       "  1855,\n",
       "  26,\n",
       "  2016,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  1,\n",
       "  205,\n",
       "  1951,\n",
       "  95,\n",
       "  123,\n",
       "  15,\n",
       "  2974,\n",
       "  11,\n",
       "  231,\n",
       "  26,\n",
       "  2975,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  294,\n",
       "  2016,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  2976,\n",
       "  24,\n",
       "  81,\n",
       "  13,\n",
       "  332,\n",
       "  8,\n",
       "  255,\n",
       "  5,\n",
       "  1,\n",
       "  1064],\n",
       " [7,\n",
       "  914,\n",
       "  28,\n",
       "  261,\n",
       "  11,\n",
       "  48,\n",
       "  13,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  11,\n",
       "  48,\n",
       "  265,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [1,\n",
       "  255,\n",
       "  1362,\n",
       "  231,\n",
       "  1363,\n",
       "  1364,\n",
       "  104,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [870,\n",
       "  362,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  15,\n",
       "  7,\n",
       "  880,\n",
       "  2977,\n",
       "  9,\n",
       "  2004,\n",
       "  210,\n",
       "  7,\n",
       "  168,\n",
       "  287,\n",
       "  5,\n",
       "  111,\n",
       "  136,\n",
       "  2978,\n",
       "  2979,\n",
       "  205,\n",
       "  257,\n",
       "  142,\n",
       "  93,\n",
       "  9,\n",
       "  554,\n",
       "  1,\n",
       "  2980,\n",
       "  1,\n",
       "  2981,\n",
       "  62,\n",
       "  362,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  15,\n",
       "  7,\n",
       "  880,\n",
       "  238,\n",
       "  309,\n",
       "  9,\n",
       "  19,\n",
       "  111,\n",
       "  136,\n",
       "  2982,\n",
       "  1093,\n",
       "  1,\n",
       "  287,\n",
       "  11,\n",
       "  1,\n",
       "  136,\n",
       "  21,\n",
       "  1,\n",
       "  418,\n",
       "  5,\n",
       "  124,\n",
       "  2983],\n",
       " [1,\n",
       "  118,\n",
       "  261,\n",
       "  13,\n",
       "  1,\n",
       "  502,\n",
       "  43,\n",
       "  159,\n",
       "  62,\n",
       "  854,\n",
       "  596,\n",
       "  278,\n",
       "  597,\n",
       "  4,\n",
       "  74,\n",
       "  598,\n",
       "  43,\n",
       "  4,\n",
       "  1,\n",
       "  391,\n",
       "  261,\n",
       "  13,\n",
       "  1,\n",
       "  36,\n",
       "  1114,\n",
       "  355,\n",
       "  879,\n",
       "  303,\n",
       "  927,\n",
       "  36,\n",
       "  2017,\n",
       "  13,\n",
       "  855,\n",
       "  21,\n",
       "  1,\n",
       "  743,\n",
       "  159,\n",
       "  62,\n",
       "  596,\n",
       "  278,\n",
       "  597,\n",
       "  4,\n",
       "  74,\n",
       "  598,\n",
       "  43],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  121,\n",
       "  7,\n",
       "  126,\n",
       "  61,\n",
       "  56,\n",
       "  35,\n",
       "  29,\n",
       "  14,\n",
       "  7,\n",
       "  1286,\n",
       "  100,\n",
       "  371,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  209,\n",
       "  140,\n",
       "  5,\n",
       "  24,\n",
       "  2984,\n",
       "  48,\n",
       "  265,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  9,\n",
       "  121,\n",
       "  7,\n",
       "  27,\n",
       "  70,\n",
       "  235,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  14,\n",
       "  46,\n",
       "  28,\n",
       "  21,\n",
       "  1,\n",
       "  35,\n",
       "  62,\n",
       "  5,\n",
       "  1,\n",
       "  1345,\n",
       "  88,\n",
       "  877,\n",
       "  2985,\n",
       "  2,\n",
       "  3,\n",
       "  94],\n",
       " [6,\n",
       "  492,\n",
       "  750,\n",
       "  68,\n",
       "  1237,\n",
       "  82,\n",
       "  61,\n",
       "  23,\n",
       "  1999,\n",
       "  2986,\n",
       "  260,\n",
       "  21,\n",
       "  1,\n",
       "  259,\n",
       "  286,\n",
       "  216,\n",
       "  563,\n",
       "  2,\n",
       "  3,\n",
       "  156,\n",
       "  6,\n",
       "  216,\n",
       "  35,\n",
       "  693,\n",
       "  574,\n",
       "  1,\n",
       "  259,\n",
       "  222,\n",
       "  563,\n",
       "  2,\n",
       "  3,\n",
       "  156],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  92,\n",
       "  94,\n",
       "  452,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  226,\n",
       "  99,\n",
       "  81,\n",
       "  14,\n",
       "  107,\n",
       "  106,\n",
       "  4,\n",
       "  256,\n",
       "  416,\n",
       "  14,\n",
       "  326,\n",
       "  200,\n",
       "  105,\n",
       "  169,\n",
       "  190,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [393,\n",
       "  1,\n",
       "  743,\n",
       "  159,\n",
       "  62,\n",
       "  596,\n",
       "  278,\n",
       "  597,\n",
       "  4,\n",
       "  74,\n",
       "  598,\n",
       "  43,\n",
       "  13,\n",
       "  68,\n",
       "  426,\n",
       "  83,\n",
       "  495,\n",
       "  188,\n",
       "  11,\n",
       "  1,\n",
       "  2018,\n",
       "  1094,\n",
       "  1095,\n",
       "  4,\n",
       "  1094,\n",
       "  1095,\n",
       "  2019,\n",
       "  13,\n",
       "  855,\n",
       "  21,\n",
       "  1,\n",
       "  743,\n",
       "  159,\n",
       "  62,\n",
       "  596,\n",
       "  278,\n",
       "  597,\n",
       "  4,\n",
       "  74,\n",
       "  598,\n",
       "  43],\n",
       " [881,\n",
       "  6,\n",
       "  874,\n",
       "  181,\n",
       "  4,\n",
       "  182,\n",
       "  1,\n",
       "  133,\n",
       "  8,\n",
       "  512,\n",
       "  4,\n",
       "  353,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  20,\n",
       "  1370,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  6,\n",
       "  76,\n",
       "  182,\n",
       "  51,\n",
       "  63,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [1,\n",
       "  144,\n",
       "  32,\n",
       "  118,\n",
       "  181,\n",
       "  4,\n",
       "  721,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  229,\n",
       "  192,\n",
       "  182,\n",
       "  4,\n",
       "  224,\n",
       "  26,\n",
       "  186,\n",
       "  119,\n",
       "  10,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  51,\n",
       "  2020,\n",
       "  67,\n",
       "  182,\n",
       "  4,\n",
       "  828,\n",
       "  10,\n",
       "  148,\n",
       "  135,\n",
       "  110],\n",
       " [6,\n",
       "  18,\n",
       "  518,\n",
       "  519,\n",
       "  64,\n",
       "  9,\n",
       "  285,\n",
       "  1,\n",
       "  106,\n",
       "  648,\n",
       "  2021,\n",
       "  76,\n",
       "  18,\n",
       "  518,\n",
       "  519,\n",
       "  64,\n",
       "  9,\n",
       "  651,\n",
       "  1,\n",
       "  78,\n",
       "  5,\n",
       "  42,\n",
       "  13,\n",
       "  7,\n",
       "  852,\n",
       "  82,\n",
       "  9,\n",
       "  69,\n",
       "  1,\n",
       "  238,\n",
       "  875],\n",
       " [1,\n",
       "  118,\n",
       "  261,\n",
       "  13,\n",
       "  1,\n",
       "  502,\n",
       "  43,\n",
       "  159,\n",
       "  62,\n",
       "  854,\n",
       "  596,\n",
       "  278,\n",
       "  597,\n",
       "  4,\n",
       "  74,\n",
       "  598,\n",
       "  43,\n",
       "  4,\n",
       "  1,\n",
       "  391,\n",
       "  261,\n",
       "  13,\n",
       "  1,\n",
       "  36,\n",
       "  1114,\n",
       "  355,\n",
       "  879,\n",
       "  303,\n",
       "  927,\n",
       "  36,\n",
       "  2017,\n",
       "  13,\n",
       "  855,\n",
       "  21,\n",
       "  1,\n",
       "  743,\n",
       "  159,\n",
       "  62,\n",
       "  596,\n",
       "  278,\n",
       "  597,\n",
       "  4,\n",
       "  74,\n",
       "  598,\n",
       "  43],\n",
       " [6,\n",
       "  15,\n",
       "  7,\n",
       "  57,\n",
       "  2022,\n",
       "  5,\n",
       "  512,\n",
       "  94,\n",
       "  42,\n",
       "  33,\n",
       "  119,\n",
       "  224,\n",
       "  4,\n",
       "  182,\n",
       "  10,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  51,\n",
       "  2020,\n",
       "  67,\n",
       "  182,\n",
       "  4,\n",
       "  828,\n",
       "  10,\n",
       "  148,\n",
       "  135,\n",
       "  110],\n",
       " [439,\n",
       "  510,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  2987,\n",
       "  30,\n",
       "  178,\n",
       "  26,\n",
       "  646,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  11,\n",
       "  2988,\n",
       "  2015,\n",
       "  4,\n",
       "  683,\n",
       "  93,\n",
       "  11,\n",
       "  576,\n",
       "  2023,\n",
       "  2989,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  214,\n",
       "  189,\n",
       "  669,\n",
       "  9,\n",
       "  2023,\n",
       "  1096],\n",
       " [393,\n",
       "  1,\n",
       "  743,\n",
       "  159,\n",
       "  62,\n",
       "  596,\n",
       "  278,\n",
       "  597,\n",
       "  4,\n",
       "  74,\n",
       "  598,\n",
       "  43,\n",
       "  13,\n",
       "  68,\n",
       "  426,\n",
       "  83,\n",
       "  495,\n",
       "  188,\n",
       "  11,\n",
       "  1,\n",
       "  2018,\n",
       "  1094,\n",
       "  1095,\n",
       "  4,\n",
       "  1094,\n",
       "  1095,\n",
       "  2019,\n",
       "  13,\n",
       "  855,\n",
       "  21,\n",
       "  1,\n",
       "  743,\n",
       "  159,\n",
       "  62,\n",
       "  596,\n",
       "  278,\n",
       "  597,\n",
       "  4,\n",
       "  74,\n",
       "  598,\n",
       "  43],\n",
       " [6,\n",
       "  232,\n",
       "  24,\n",
       "  58,\n",
       "  23,\n",
       "  54,\n",
       "  12,\n",
       "  40,\n",
       "  7,\n",
       "  307,\n",
       "  85,\n",
       "  22,\n",
       "  174,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  541,\n",
       "  15,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [6,\n",
       "  214,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  9,\n",
       "  216,\n",
       "  70,\n",
       "  89,\n",
       "  228,\n",
       "  410,\n",
       "  5,\n",
       "  24,\n",
       "  65,\n",
       "  555,\n",
       "  9,\n",
       "  7,\n",
       "  1628,\n",
       "  15,\n",
       "  77,\n",
       "  80,\n",
       "  11,\n",
       "  281,\n",
       "  70,\n",
       "  89,\n",
       "  12,\n",
       "  36],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  14,\n",
       "  124,\n",
       "  107,\n",
       "  1317,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  126,\n",
       "  61,\n",
       "  38,\n",
       "  29,\n",
       "  14,\n",
       "  1,\n",
       "  85,\n",
       "  28,\n",
       "  870,\n",
       "  587],\n",
       " [1,\n",
       "  70,\n",
       "  89,\n",
       "  147,\n",
       "  13,\n",
       "  76,\n",
       "  196,\n",
       "  179,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  14,\n",
       "  228,\n",
       "  488,\n",
       "  297,\n",
       "  12,\n",
       "  36,\n",
       "  6,\n",
       "  15,\n",
       "  77,\n",
       "  80,\n",
       "  11,\n",
       "  281,\n",
       "  70,\n",
       "  89,\n",
       "  12,\n",
       "  36],\n",
       " [48,\n",
       "  19,\n",
       "  13,\n",
       "  1273,\n",
       "  26,\n",
       "  168,\n",
       "  816,\n",
       "  19,\n",
       "  702,\n",
       "  2,\n",
       "  3,\n",
       "  2024,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  2990,\n",
       "  381,\n",
       "  5,\n",
       "  1704,\n",
       "  7,\n",
       "  566,\n",
       "  11,\n",
       "  142,\n",
       "  2991,\n",
       "  211,\n",
       "  358,\n",
       "  13,\n",
       "  1,\n",
       "  168,\n",
       "  816,\n",
       "  19,\n",
       "  702,\n",
       "  2,\n",
       "  3,\n",
       "  2024],\n",
       " [1,\n",
       "  28,\n",
       "  33,\n",
       "  1049,\n",
       "  283,\n",
       "  1236,\n",
       "  1065,\n",
       "  4,\n",
       "  2992,\n",
       "  1065,\n",
       "  8,\n",
       "  7,\n",
       "  72,\n",
       "  2993,\n",
       "  31,\n",
       "  1,\n",
       "  28,\n",
       "  15,\n",
       "  26,\n",
       "  1016,\n",
       "  4,\n",
       "  638,\n",
       "  165,\n",
       "  6,\n",
       "  95,\n",
       "  15,\n",
       "  1,\n",
       "  1236,\n",
       "  28,\n",
       "  509,\n",
       "  8,\n",
       "  1016,\n",
       "  4,\n",
       "  638,\n",
       "  165,\n",
       "  94],\n",
       " [1,\n",
       "  2994,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  2995,\n",
       "  284,\n",
       "  2996,\n",
       "  52,\n",
       "  661,\n",
       "  37,\n",
       "  1,\n",
       "  391,\n",
       "  103,\n",
       "  1542,\n",
       "  2997,\n",
       "  13,\n",
       "  1,\n",
       "  2998,\n",
       "  2999,\n",
       "  103,\n",
       "  11,\n",
       "  3000,\n",
       "  3001,\n",
       "  189,\n",
       "  175,\n",
       "  8,\n",
       "  1,\n",
       "  3002,\n",
       "  661,\n",
       "  37],\n",
       " [145,\n",
       "  13,\n",
       "  7,\n",
       "  1051,\n",
       "  41,\n",
       "  19,\n",
       "  66,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  4,\n",
       "  50,\n",
       "  206,\n",
       "  7,\n",
       "  155,\n",
       "  213,\n",
       "  5,\n",
       "  1,\n",
       "  247,\n",
       "  22,\n",
       "  8,\n",
       "  42,\n",
       "  613,\n",
       "  98,\n",
       "  13,\n",
       "  1295,\n",
       "  14,\n",
       "  124,\n",
       "  384,\n",
       "  856,\n",
       "  98,\n",
       "  3003,\n",
       "  4,\n",
       "  796,\n",
       "  5,\n",
       "  7,\n",
       "  155,\n",
       "  116,\n",
       "  291,\n",
       "  170,\n",
       "  951,\n",
       "  41,\n",
       "  19,\n",
       "  32,\n",
       "  233,\n",
       "  23,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  325],\n",
       " [1,\n",
       "  30,\n",
       "  73,\n",
       "  33,\n",
       "  153,\n",
       "  26,\n",
       "  464,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  101,\n",
       "  30,\n",
       "  132,\n",
       "  67,\n",
       "  153,\n",
       "  26,\n",
       "  464,\n",
       "  59,\n",
       "  8,\n",
       "  105,\n",
       "  398,\n",
       "  4,\n",
       "  817,\n",
       "  10,\n",
       "  1,\n",
       "  356,\n",
       "  370,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [491,\n",
       "  11,\n",
       "  44,\n",
       "  63,\n",
       "  8,\n",
       "  1,\n",
       "  257,\n",
       "  1371,\n",
       "  32,\n",
       "  174,\n",
       "  21,\n",
       "  7,\n",
       "  1261,\n",
       "  30,\n",
       "  73,\n",
       "  5,\n",
       "  125,\n",
       "  21,\n",
       "  1,\n",
       "  54,\n",
       "  85,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  8,\n",
       "  269,\n",
       "  9,\n",
       "  554,\n",
       "  1,\n",
       "  3004,\n",
       "  5,\n",
       "  1,\n",
       "  30,\n",
       "  132,\n",
       "  1,\n",
       "  358,\n",
       "  67,\n",
       "  1295,\n",
       "  283,\n",
       "  7,\n",
       "  1254,\n",
       "  3005,\n",
       "  3006,\n",
       "  14,\n",
       "  1305,\n",
       "  85,\n",
       "  28,\n",
       "  21,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [51,\n",
       "  1,\n",
       "  41,\n",
       "  65,\n",
       "  371,\n",
       "  15,\n",
       "  8,\n",
       "  24,\n",
       "  58,\n",
       "  32,\n",
       "  2025,\n",
       "  155,\n",
       "  116,\n",
       "  291,\n",
       "  1372,\n",
       "  50,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  1,\n",
       "  41,\n",
       "  65,\n",
       "  32,\n",
       "  407,\n",
       "  10,\n",
       "  1,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  14,\n",
       "  155,\n",
       "  116,\n",
       "  173],\n",
       " [8,\n",
       "  51,\n",
       "  58,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  99,\n",
       "  184,\n",
       "  14,\n",
       "  1039,\n",
       "  406,\n",
       "  487,\n",
       "  1318,\n",
       "  81,\n",
       "  169,\n",
       "  8,\n",
       "  1,\n",
       "  202,\n",
       "  152,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  11,\n",
       "  51,\n",
       "  58,\n",
       "  6,\n",
       "  18,\n",
       "  7,\n",
       "  3007,\n",
       "  184,\n",
       "  31,\n",
       "  175,\n",
       "  8,\n",
       "  202,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  20],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  9,\n",
       "  1294,\n",
       "  7,\n",
       "  2026,\n",
       "  11,\n",
       "  111,\n",
       "  136,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  375,\n",
       "  55,\n",
       "  52,\n",
       "  8,\n",
       "  1,\n",
       "  39,\n",
       "  70,\n",
       "  372,\n",
       "  41,\n",
       "  52,\n",
       "  744,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  276,\n",
       "  11,\n",
       "  1,\n",
       "  28,\n",
       "  456],\n",
       " [599,\n",
       "  6,\n",
       "  181,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  51,\n",
       "  1373,\n",
       "  11,\n",
       "  41,\n",
       "  343,\n",
       "  10,\n",
       "  1,\n",
       "  60,\n",
       "  632,\n",
       "  5,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  108,\n",
       "  152,\n",
       "  170,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  408,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  18,\n",
       "  17,\n",
       "  325,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  424,\n",
       "  48,\n",
       "  19,\n",
       "  26,\n",
       "  726,\n",
       "  1,\n",
       "  261,\n",
       "  4,\n",
       "  209,\n",
       "  1026,\n",
       "  9,\n",
       "  104,\n",
       "  3008,\n",
       "  645],\n",
       " [368,\n",
       "  38,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  85,\n",
       "  22,\n",
       "  185,\n",
       "  131,\n",
       "  9,\n",
       "  69,\n",
       "  68,\n",
       "  506,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  46,\n",
       "  1,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  11,\n",
       "  282,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [368,\n",
       "  38,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  85,\n",
       "  22,\n",
       "  185,\n",
       "  131,\n",
       "  9,\n",
       "  69,\n",
       "  68,\n",
       "  506,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  101,\n",
       "  49,\n",
       "  32,\n",
       "  66,\n",
       "  14,\n",
       "  1,\n",
       "  177,\n",
       "  27,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  118,\n",
       "  1925,\n",
       "  78,\n",
       "  61,\n",
       "  29,\n",
       "  13,\n",
       "  138,\n",
       "  8,\n",
       "  2027,\n",
       "  2,\n",
       "  3,\n",
       "  156,\n",
       "  9,\n",
       "  972,\n",
       "  826,\n",
       "  6,\n",
       "  15,\n",
       "  7,\n",
       "  1929,\n",
       "  5,\n",
       "  1,\n",
       "  861,\n",
       "  1477,\n",
       "  128,\n",
       "  138,\n",
       "  8,\n",
       "  2027,\n",
       "  2,\n",
       "  3,\n",
       "  156],\n",
       " [368,\n",
       "  38,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  85,\n",
       "  22,\n",
       "  185,\n",
       "  131,\n",
       "  9,\n",
       "  69,\n",
       "  68,\n",
       "  506,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  15,\n",
       "  2028,\n",
       "  537,\n",
       "  39,\n",
       "  191,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  219,\n",
       "  868,\n",
       "  5,\n",
       "  1,\n",
       "  1331,\n",
       "  4,\n",
       "  15,\n",
       "  254,\n",
       "  7,\n",
       "  388,\n",
       "  5,\n",
       "  2029,\n",
       "  5,\n",
       "  7,\n",
       "  2030,\n",
       "  219,\n",
       "  389,\n",
       "  191,\n",
       "  10,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  4,\n",
       "  18,\n",
       "  124,\n",
       "  537,\n",
       "  55,\n",
       "  773],\n",
       " [6,\n",
       "  860,\n",
       "  31,\n",
       "  7,\n",
       "  2031,\n",
       "  22,\n",
       "  54,\n",
       "  717,\n",
       "  12,\n",
       "  40,\n",
       "  14,\n",
       "  2032,\n",
       "  85,\n",
       "  2033,\n",
       "  232,\n",
       "  24,\n",
       "  58,\n",
       "  23,\n",
       "  54,\n",
       "  12,\n",
       "  40,\n",
       "  7,\n",
       "  307,\n",
       "  85,\n",
       "  22,\n",
       "  42,\n",
       "  13,\n",
       "  138,\n",
       "  8,\n",
       "  741,\n",
       "  8,\n",
       "  185,\n",
       "  1810],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  375,\n",
       "  55,\n",
       "  52,\n",
       "  8,\n",
       "  1,\n",
       "  39,\n",
       "  70,\n",
       "  372,\n",
       "  41,\n",
       "  52,\n",
       "  744,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  276,\n",
       "  11,\n",
       "  1,\n",
       "  28,\n",
       "  2034,\n",
       "  24,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  348,\n",
       "  55,\n",
       "  276],\n",
       " [1,\n",
       "  1279,\n",
       "  5,\n",
       "  1,\n",
       "  1886,\n",
       "  13,\n",
       "  1374,\n",
       "  9,\n",
       "  402,\n",
       "  1375,\n",
       "  5,\n",
       "  1,\n",
       "  486,\n",
       "  4,\n",
       "  1,\n",
       "  402,\n",
       "  1097,\n",
       "  5,\n",
       "  1,\n",
       "  940,\n",
       "  31,\n",
       "  524,\n",
       "  8,\n",
       "  468,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  503,\n",
       "  403,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  262,\n",
       "  51,\n",
       "  5,\n",
       "  1,\n",
       "  1376,\n",
       "  476,\n",
       "  468,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  91,\n",
       "  113,\n",
       "  5,\n",
       "  157,\n",
       "  114,\n",
       "  127,\n",
       "  13,\n",
       "  7,\n",
       "  3009,\n",
       "  128,\n",
       "  11,\n",
       "  1655,\n",
       "  446,\n",
       "  3010,\n",
       "  13,\n",
       "  7,\n",
       "  625,\n",
       "  314,\n",
       "  2001,\n",
       "  23,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  5,\n",
       "  157,\n",
       "  2002,\n",
       "  34,\n",
       "  142,\n",
       "  5,\n",
       "  308,\n",
       "  2003,\n",
       "  142,\n",
       "  5,\n",
       "  157,\n",
       "  1092,\n",
       "  114,\n",
       "  127],\n",
       " [599,\n",
       "  6,\n",
       "  181,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  51,\n",
       "  1373,\n",
       "  11,\n",
       "  41,\n",
       "  343,\n",
       "  10,\n",
       "  1,\n",
       "  60,\n",
       "  632,\n",
       "  5,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [145,\n",
       "  13,\n",
       "  21,\n",
       "  475,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  1,\n",
       "  1284,\n",
       "  5,\n",
       "  30,\n",
       "  1045,\n",
       "  13,\n",
       "  1918,\n",
       "  471,\n",
       "  1,\n",
       "  93,\n",
       "  32,\n",
       "  1,\n",
       "  3011,\n",
       "  93,\n",
       "  174,\n",
       "  21,\n",
       "  30,\n",
       "  658,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  592,\n",
       "  31,\n",
       "  475,\n",
       "  2,\n",
       "  3,\n",
       "  117],\n",
       " [1,\n",
       "  3012,\n",
       "  67,\n",
       "  3013,\n",
       "  459,\n",
       "  3014,\n",
       "  23,\n",
       "  24,\n",
       "  115,\n",
       "  11,\n",
       "  1,\n",
       "  249,\n",
       "  1126,\n",
       "  4,\n",
       "  130,\n",
       "  142,\n",
       "  562,\n",
       "  745,\n",
       "  2,\n",
       "  3,\n",
       "  36,\n",
       "  11,\n",
       "  7,\n",
       "  654,\n",
       "  5,\n",
       "  63,\n",
       "  249,\n",
       "  586,\n",
       "  7,\n",
       "  3015,\n",
       "  5,\n",
       "  562,\n",
       "  5,\n",
       "  1,\n",
       "  130,\n",
       "  142,\n",
       "  745,\n",
       "  2,\n",
       "  3,\n",
       "  36],\n",
       " [368,\n",
       "  38,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  85,\n",
       "  22,\n",
       "  185,\n",
       "  131,\n",
       "  9,\n",
       "  69,\n",
       "  68,\n",
       "  506,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  24,\n",
       "  101,\n",
       "  13,\n",
       "  7,\n",
       "  27,\n",
       "  235,\n",
       "  29,\n",
       "  50,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  70,\n",
       "  89,\n",
       "  147,\n",
       "  13,\n",
       "  76,\n",
       "  196,\n",
       "  179,\n",
       "  14,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  14,\n",
       "  228,\n",
       "  488,\n",
       "  297,\n",
       "  12,\n",
       "  36,\n",
       "  6,\n",
       "  216,\n",
       "  89,\n",
       "  5,\n",
       "  188,\n",
       "  10,\n",
       "  77,\n",
       "  80,\n",
       "  190,\n",
       "  228,\n",
       "  410,\n",
       "  12,\n",
       "  36],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  108,\n",
       "  152,\n",
       "  170,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  408,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1367,\n",
       "  296,\n",
       "  29,\n",
       "  6,\n",
       "  225,\n",
       "  7,\n",
       "  212,\n",
       "  5,\n",
       "  1368,\n",
       "  1369,\n",
       "  9,\n",
       "  1,\n",
       "  258,\n",
       "  70,\n",
       "  35,\n",
       "  1001,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  15,\n",
       "  7,\n",
       "  57,\n",
       "  2022,\n",
       "  5,\n",
       "  512,\n",
       "  94,\n",
       "  42,\n",
       "  33,\n",
       "  119,\n",
       "  224,\n",
       "  4,\n",
       "  182,\n",
       "  10,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  131,\n",
       "  51,\n",
       "  44,\n",
       "  28,\n",
       "  32,\n",
       "  119,\n",
       "  224,\n",
       "  4,\n",
       "  473,\n",
       "  10,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110],\n",
       " [1,\n",
       "  133,\n",
       "  33,\n",
       "  442,\n",
       "  10,\n",
       "  2035,\n",
       "  425,\n",
       "  9,\n",
       "  1351,\n",
       "  2036,\n",
       "  4,\n",
       "  192,\n",
       "  181,\n",
       "  14,\n",
       "  1,\n",
       "  39,\n",
       "  632,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  8,\n",
       "  335,\n",
       "  1,\n",
       "  28,\n",
       "  33,\n",
       "  181,\n",
       "  182,\n",
       "  4,\n",
       "  373,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [6,\n",
       "  232,\n",
       "  24,\n",
       "  58,\n",
       "  23,\n",
       "  54,\n",
       "  12,\n",
       "  40,\n",
       "  7,\n",
       "  307,\n",
       "  85,\n",
       "  22,\n",
       "  42,\n",
       "  13,\n",
       "  138,\n",
       "  8,\n",
       "  741,\n",
       "  8,\n",
       "  185,\n",
       "  3016,\n",
       "  12,\n",
       "  40,\n",
       "  13,\n",
       "  7,\n",
       "  307,\n",
       "  85,\n",
       "  22,\n",
       "  174,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  449],\n",
       " [6,\n",
       "  18,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  1,\n",
       "  56,\n",
       "  78,\n",
       "  403,\n",
       "  11,\n",
       "  553,\n",
       "  11,\n",
       "  1216,\n",
       "  1,\n",
       "  332,\n",
       "  3017,\n",
       "  199,\n",
       "  175,\n",
       "  8,\n",
       "  553,\n",
       "  14,\n",
       "  1,\n",
       "  92,\n",
       "  56,\n",
       "  78,\n",
       "  403,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  51,\n",
       "  215,\n",
       "  65],\n",
       " [1,\n",
       "  592,\n",
       "  328,\n",
       "  67,\n",
       "  3018,\n",
       "  13,\n",
       "  72,\n",
       "  9,\n",
       "  2005,\n",
       "  1,\n",
       "  1249,\n",
       "  1287,\n",
       "  138,\n",
       "  26,\n",
       "  409,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  8,\n",
       "  48,\n",
       "  263,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  2037,\n",
       "  22,\n",
       "  26,\n",
       "  409,\n",
       "  2,\n",
       "  3,\n",
       "  87],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  375,\n",
       "  55,\n",
       "  52,\n",
       "  8,\n",
       "  1,\n",
       "  39,\n",
       "  70,\n",
       "  372,\n",
       "  41,\n",
       "  52,\n",
       "  744,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  276,\n",
       "  11,\n",
       "  1,\n",
       "  28,\n",
       "  2034,\n",
       "  24,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  348,\n",
       "  55,\n",
       "  276],\n",
       " [6,\n",
       "  155,\n",
       "  1,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  3019,\n",
       "  3020,\n",
       "  152,\n",
       "  253,\n",
       "  5,\n",
       "  788,\n",
       "  2,\n",
       "  3,\n",
       "  201,\n",
       "  6,\n",
       "  76,\n",
       "  225,\n",
       "  24,\n",
       "  30,\n",
       "  178,\n",
       "  14,\n",
       "  1,\n",
       "  2038,\n",
       "  30,\n",
       "  178,\n",
       "  5,\n",
       "  788,\n",
       "  2,\n",
       "  3,\n",
       "  201,\n",
       "  1018,\n",
       "  833,\n",
       "  1352,\n",
       "  2039,\n",
       "  1877],\n",
       " [1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  430,\n",
       "  93,\n",
       "  21,\n",
       "  1,\n",
       "  55,\n",
       "  160,\n",
       "  727,\n",
       "  15,\n",
       "  1,\n",
       "  375,\n",
       "  55,\n",
       "  52,\n",
       "  8,\n",
       "  1,\n",
       "  39,\n",
       "  70,\n",
       "  372,\n",
       "  41,\n",
       "  52,\n",
       "  744,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  276,\n",
       "  11,\n",
       "  1,\n",
       "  28,\n",
       "  456],\n",
       " [1,\n",
       "  118,\n",
       "  167,\n",
       "  23,\n",
       "  48,\n",
       "  996,\n",
       "  33,\n",
       "  233,\n",
       "  3021,\n",
       "  8,\n",
       "  1,\n",
       "  3022,\n",
       "  718,\n",
       "  387,\n",
       "  1,\n",
       "  735,\n",
       "  213,\n",
       "  33,\n",
       "  224,\n",
       "  374,\n",
       "  10,\n",
       "  7,\n",
       "  171,\n",
       "  1112,\n",
       "  23,\n",
       "  1,\n",
       "  44,\n",
       "  171,\n",
       "  138,\n",
       "  8,\n",
       "  718,\n",
       "  387],\n",
       " [1,\n",
       "  503,\n",
       "  403,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  262,\n",
       "  51,\n",
       "  5,\n",
       "  1,\n",
       "  1376,\n",
       "  476,\n",
       "  468,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  161,\n",
       "  13,\n",
       "  175,\n",
       "  14,\n",
       "  402,\n",
       "  712,\n",
       "  230,\n",
       "  548,\n",
       "  997,\n",
       "  10,\n",
       "  60,\n",
       "  476,\n",
       "  169,\n",
       "  8,\n",
       "  1,\n",
       "  503,\n",
       "  998,\n",
       "  468,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  24,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  637,\n",
       "  694,\n",
       "  3023,\n",
       "  21,\n",
       "  1,\n",
       "  1355,\n",
       "  22,\n",
       "  652,\n",
       "  3024,\n",
       "  11,\n",
       "  3025,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  3026,\n",
       "  2040,\n",
       "  85,\n",
       "  22,\n",
       "  652,\n",
       "  57,\n",
       "  42,\n",
       "  13,\n",
       "  2040,\n",
       "  3027,\n",
       "  3028,\n",
       "  3029,\n",
       "  108,\n",
       "  3030],\n",
       " [1,\n",
       "  3031,\n",
       "  1279,\n",
       "  13,\n",
       "  153,\n",
       "  26,\n",
       "  865,\n",
       "  1,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  206,\n",
       "  2041,\n",
       "  605,\n",
       "  14,\n",
       "  3032,\n",
       "  308,\n",
       "  3033,\n",
       "  101,\n",
       "  49,\n",
       "  32,\n",
       "  66,\n",
       "  14,\n",
       "  1,\n",
       "  177,\n",
       "  27,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  503,\n",
       "  403,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  262,\n",
       "  51,\n",
       "  5,\n",
       "  1,\n",
       "  1376,\n",
       "  476,\n",
       "  468,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  161,\n",
       "  13,\n",
       "  175,\n",
       "  14,\n",
       "  402,\n",
       "  712,\n",
       "  230,\n",
       "  548,\n",
       "  997,\n",
       "  10,\n",
       "  60,\n",
       "  476,\n",
       "  169,\n",
       "  8,\n",
       "  1,\n",
       "  503,\n",
       "  998,\n",
       "  468,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  860,\n",
       "  31,\n",
       "  7,\n",
       "  2031,\n",
       "  22,\n",
       "  54,\n",
       "  717,\n",
       "  12,\n",
       "  40,\n",
       "  14,\n",
       "  2032,\n",
       "  85,\n",
       "  2033,\n",
       "  232,\n",
       "  24,\n",
       "  58,\n",
       "  23,\n",
       "  54,\n",
       "  12,\n",
       "  40,\n",
       "  7,\n",
       "  307,\n",
       "  85,\n",
       "  22,\n",
       "  174,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  449],\n",
       " [6,\n",
       "  76,\n",
       "  225,\n",
       "  24,\n",
       "  30,\n",
       "  178,\n",
       "  14,\n",
       "  1,\n",
       "  2038,\n",
       "  30,\n",
       "  178,\n",
       "  5,\n",
       "  788,\n",
       "  2,\n",
       "  3,\n",
       "  201,\n",
       "  1018,\n",
       "  833,\n",
       "  1352,\n",
       "  2039,\n",
       "  3034,\n",
       "  2,\n",
       "  3,\n",
       "  201,\n",
       "  15,\n",
       "  3035,\n",
       "  9,\n",
       "  3036,\n",
       "  30,\n",
       "  178,\n",
       "  571,\n",
       "  1,\n",
       "  280,\n",
       "  608],\n",
       " [1,\n",
       "  1935,\n",
       "  211,\n",
       "  41,\n",
       "  4,\n",
       "  195,\n",
       "  13,\n",
       "  68,\n",
       "  3037,\n",
       "  1342,\n",
       "  5,\n",
       "  3038,\n",
       "  409,\n",
       "  4,\n",
       "  588,\n",
       "  90,\n",
       "  1,\n",
       "  556,\n",
       "  3039,\n",
       "  5,\n",
       "  3040,\n",
       "  879,\n",
       "  1377,\n",
       "  5,\n",
       "  42,\n",
       "  13,\n",
       "  3041,\n",
       "  129,\n",
       "  3042,\n",
       "  68,\n",
       "  2042,\n",
       "  8,\n",
       "  195,\n",
       "  115,\n",
       "  409,\n",
       "  4,\n",
       "  588,\n",
       "  90,\n",
       "  482,\n",
       "  117],\n",
       " [158,\n",
       "  168,\n",
       "  668,\n",
       "  99,\n",
       "  32,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  1266,\n",
       "  163,\n",
       "  61,\n",
       "  23,\n",
       "  532,\n",
       "  3043,\n",
       "  1137,\n",
       "  587,\n",
       "  26,\n",
       "  521,\n",
       "  521,\n",
       "  165,\n",
       "  172,\n",
       "  163,\n",
       "  32,\n",
       "  61,\n",
       "  23,\n",
       "  7,\n",
       "  2043,\n",
       "  19,\n",
       "  158,\n",
       "  168,\n",
       "  56,\n",
       "  99,\n",
       "  425,\n",
       "  521,\n",
       "  165],\n",
       " [8,\n",
       "  3044,\n",
       "  4,\n",
       "  3045,\n",
       "  53,\n",
       "  6,\n",
       "  794,\n",
       "  14,\n",
       "  1816,\n",
       "  3046,\n",
       "  10,\n",
       "  1,\n",
       "  907,\n",
       "  169,\n",
       "  38,\n",
       "  29,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [9,\n",
       "  607,\n",
       "  1,\n",
       "  106,\n",
       "  5,\n",
       "  1,\n",
       "  19,\n",
       "  6,\n",
       "  1201,\n",
       "  1,\n",
       "  746,\n",
       "  315,\n",
       "  31,\n",
       "  1,\n",
       "  46,\n",
       "  995,\n",
       "  10,\n",
       "  1,\n",
       "  243,\n",
       "  487,\n",
       "  103,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  53,\n",
       "  11,\n",
       "  487,\n",
       "  6,\n",
       "  984,\n",
       "  1,\n",
       "  243,\n",
       "  425,\n",
       "  3047,\n",
       "  103,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  53,\n",
       "  9,\n",
       "  285,\n",
       "  106],\n",
       " [11,\n",
       "  46,\n",
       "  1,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  11,\n",
       "  282,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  33,\n",
       "  15,\n",
       "  11,\n",
       "  30,\n",
       "  73,\n",
       "  4,\n",
       "  126,\n",
       "  35,\n",
       "  1749,\n",
       "  67,\n",
       "  407,\n",
       "  21,\n",
       "  789,\n",
       "  26,\n",
       "  1,\n",
       "  17,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  133,\n",
       "  33,\n",
       "  442,\n",
       "  10,\n",
       "  2035,\n",
       "  425,\n",
       "  9,\n",
       "  1351,\n",
       "  2036,\n",
       "  4,\n",
       "  192,\n",
       "  181,\n",
       "  14,\n",
       "  1,\n",
       "  39,\n",
       "  632,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  8,\n",
       "  335,\n",
       "  1,\n",
       "  28,\n",
       "  33,\n",
       "  181,\n",
       "  182,\n",
       "  4,\n",
       "  373,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [1,\n",
       "  62,\n",
       "  516,\n",
       "  23,\n",
       "  662,\n",
       "  13,\n",
       "  9,\n",
       "  723,\n",
       "  2044,\n",
       "  2045,\n",
       "  4,\n",
       "  1098,\n",
       "  87,\n",
       "  795,\n",
       "  617,\n",
       "  1,\n",
       "  93,\n",
       "  176,\n",
       "  11,\n",
       "  1,\n",
       "  3048,\n",
       "  3049,\n",
       "  62,\n",
       "  3050,\n",
       "  4,\n",
       "  840,\n",
       "  3051,\n",
       "  3052,\n",
       "  60,\n",
       "  11,\n",
       "  1903,\n",
       "  478,\n",
       "  164,\n",
       "  13,\n",
       "  176,\n",
       "  8,\n",
       "  1,\n",
       "  2044,\n",
       "  511,\n",
       "  2045,\n",
       "  4,\n",
       "  1098,\n",
       "  87],\n",
       " [11,\n",
       "  105,\n",
       "  49,\n",
       "  1,\n",
       "  15,\n",
       "  46,\n",
       "  28,\n",
       "  13,\n",
       "  21,\n",
       "  1,\n",
       "  3053,\n",
       "  213,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  4,\n",
       "  1,\n",
       "  872,\n",
       "  2011,\n",
       "  1514,\n",
       "  261,\n",
       "  5,\n",
       "  239,\n",
       "  28,\n",
       "  15,\n",
       "  8,\n",
       "  1,\n",
       "  58,\n",
       "  13,\n",
       "  1,\n",
       "  54,\n",
       "  310,\n",
       "  12,\n",
       "  40],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  9,\n",
       "  1294,\n",
       "  7,\n",
       "  2026,\n",
       "  11,\n",
       "  111,\n",
       "  136,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  375,\n",
       "  55,\n",
       "  52,\n",
       "  8,\n",
       "  1,\n",
       "  39,\n",
       "  70,\n",
       "  372,\n",
       "  41,\n",
       "  52,\n",
       "  744,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  276,\n",
       "  11,\n",
       "  1,\n",
       "  28,\n",
       "  456],\n",
       " [6,\n",
       "  15,\n",
       "  2028,\n",
       "  537,\n",
       "  39,\n",
       "  191,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  219,\n",
       "  868,\n",
       "  5,\n",
       "  1,\n",
       "  1331,\n",
       "  4,\n",
       "  15,\n",
       "  254,\n",
       "  7,\n",
       "  388,\n",
       "  5,\n",
       "  2029,\n",
       "  5,\n",
       "  7,\n",
       "  2030,\n",
       "  219,\n",
       "  389,\n",
       "  191,\n",
       "  10,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  4,\n",
       "  18,\n",
       "  124,\n",
       "  537,\n",
       "  55,\n",
       "  773],\n",
       " [6,\n",
       "  274,\n",
       "  44,\n",
       "  28,\n",
       "  4,\n",
       "  864,\n",
       "  298,\n",
       "  28,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  120,\n",
       "  20,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  94,\n",
       "  881,\n",
       "  6,\n",
       "  874,\n",
       "  181,\n",
       "  4,\n",
       "  182,\n",
       "  1,\n",
       "  133,\n",
       "  8,\n",
       "  512,\n",
       "  4,\n",
       "  353,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  20,\n",
       "  1370,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [1,\n",
       "  143,\n",
       "  3054,\n",
       "  8,\n",
       "  1253,\n",
       "  436,\n",
       "  129,\n",
       "  123,\n",
       "  23,\n",
       "  1997,\n",
       "  879,\n",
       "  318,\n",
       "  3055,\n",
       "  10,\n",
       "  3056,\n",
       "  31,\n",
       "  350,\n",
       "  31,\n",
       "  355,\n",
       "  134,\n",
       "  4,\n",
       "  510,\n",
       "  43,\n",
       "  3057,\n",
       "  134,\n",
       "  4,\n",
       "  510,\n",
       "  43,\n",
       "  129,\n",
       "  123,\n",
       "  458,\n",
       "  31,\n",
       "  7,\n",
       "  60,\n",
       "  143,\n",
       "  314,\n",
       "  8,\n",
       "  413,\n",
       "  436,\n",
       "  558],\n",
       " [6,\n",
       "  223,\n",
       "  101,\n",
       "  58,\n",
       "  11,\n",
       "  126,\n",
       "  61,\n",
       "  56,\n",
       "  35,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  51,\n",
       "  58,\n",
       "  67,\n",
       "  196,\n",
       "  179,\n",
       "  10,\n",
       "  1,\n",
       "  177,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  8,\n",
       "  124,\n",
       "  60,\n",
       "  1652,\n",
       "  580],\n",
       " [1,\n",
       "  142,\n",
       "  369,\n",
       "  13,\n",
       "  662,\n",
       "  1,\n",
       "  291,\n",
       "  767,\n",
       "  189,\n",
       "  162,\n",
       "  768,\n",
       "  97,\n",
       "  8,\n",
       "  2046,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  635,\n",
       "  396,\n",
       "  1,\n",
       "  291,\n",
       "  767,\n",
       "  3058,\n",
       "  768,\n",
       "  97,\n",
       "  8,\n",
       "  2046,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  9,\n",
       "  707,\n",
       "  1,\n",
       "  142,\n",
       "  771,\n",
       "  7,\n",
       "  1348,\n",
       "  189,\n",
       "  287],\n",
       " [6,\n",
       "  274,\n",
       "  44,\n",
       "  28,\n",
       "  4,\n",
       "  864,\n",
       "  298,\n",
       "  28,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  120,\n",
       "  20,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  94,\n",
       "  881,\n",
       "  6,\n",
       "  874,\n",
       "  181,\n",
       "  4,\n",
       "  182,\n",
       "  1,\n",
       "  133,\n",
       "  8,\n",
       "  512,\n",
       "  4,\n",
       "  353,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  20,\n",
       "  1370,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  430,\n",
       "  93,\n",
       "  21,\n",
       "  1,\n",
       "  55,\n",
       "  160,\n",
       "  727,\n",
       "  15,\n",
       "  1,\n",
       "  375,\n",
       "  55,\n",
       "  52,\n",
       "  8,\n",
       "  1,\n",
       "  39,\n",
       "  70,\n",
       "  372,\n",
       "  41,\n",
       "  52,\n",
       "  744,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  210,\n",
       "  276,\n",
       "  11,\n",
       "  1,\n",
       "  28,\n",
       "  456],\n",
       " [9,\n",
       "  3059,\n",
       "  1,\n",
       "  1287,\n",
       "  5,\n",
       "  1,\n",
       "  97,\n",
       "  82,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  258,\n",
       "  27,\n",
       "  29,\n",
       "  4,\n",
       "  685,\n",
       "  27,\n",
       "  29,\n",
       "  175,\n",
       "  8,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  101,\n",
       "  49,\n",
       "  32,\n",
       "  66,\n",
       "  14,\n",
       "  1,\n",
       "  177,\n",
       "  27,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  126,\n",
       "  61,\n",
       "  38,\n",
       "  29,\n",
       "  14,\n",
       "  1,\n",
       "  85,\n",
       "  28,\n",
       "  870,\n",
       "  587],\n",
       " [1367,\n",
       "  296,\n",
       "  29,\n",
       "  6,\n",
       "  225,\n",
       "  7,\n",
       "  212,\n",
       "  5,\n",
       "  1368,\n",
       "  1369,\n",
       "  9,\n",
       "  1,\n",
       "  258,\n",
       "  70,\n",
       "  35,\n",
       "  1001,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [599,\n",
       "  6,\n",
       "  181,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  51,\n",
       "  1373,\n",
       "  11,\n",
       "  41,\n",
       "  343,\n",
       "  10,\n",
       "  1,\n",
       "  60,\n",
       "  632,\n",
       "  5,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  274,\n",
       "  4,\n",
       "  737,\n",
       "  1,\n",
       "  28,\n",
       "  14,\n",
       "  1,\n",
       "  60,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [393,\n",
       "  98,\n",
       "  13,\n",
       "  76,\n",
       "  2047,\n",
       "  34,\n",
       "  1,\n",
       "  1378,\n",
       "  5,\n",
       "  663,\n",
       "  1379,\n",
       "  8,\n",
       "  1,\n",
       "  46,\n",
       "  75,\n",
       "  13,\n",
       "  72,\n",
       "  9,\n",
       "  1,\n",
       "  145,\n",
       "  516,\n",
       "  26,\n",
       "  463,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  4,\n",
       "  2048,\n",
       "  9,\n",
       "  8,\n",
       "  185,\n",
       "  2049,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  871,\n",
       "  34,\n",
       "  1338,\n",
       "  5,\n",
       "  1,\n",
       "  1029,\n",
       "  354,\n",
       "  8,\n",
       "  1,\n",
       "  1339,\n",
       "  22,\n",
       "  94,\n",
       "  67,\n",
       "  663],\n",
       " [31,\n",
       "  7,\n",
       "  78,\n",
       "  103,\n",
       "  6,\n",
       "  882,\n",
       "  7,\n",
       "  434,\n",
       "  99,\n",
       "  295,\n",
       "  87,\n",
       "  42,\n",
       "  13,\n",
       "  68,\n",
       "  1148,\n",
       "  5,\n",
       "  893,\n",
       "  3060,\n",
       "  69,\n",
       "  7,\n",
       "  3061,\n",
       "  184,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  99,\n",
       "  78,\n",
       "  103,\n",
       "  21,\n",
       "  1,\n",
       "  99,\n",
       "  1326,\n",
       "  152,\n",
       "  295,\n",
       "  87,\n",
       "  3062,\n",
       "  51,\n",
       "  3063,\n",
       "  93,\n",
       "  283,\n",
       "  68,\n",
       "  1374,\n",
       "  75,\n",
       "  5,\n",
       "  3064,\n",
       "  3065],\n",
       " [2050,\n",
       "  3066,\n",
       "  32,\n",
       "  1869,\n",
       "  3067,\n",
       "  3068,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  3069,\n",
       "  21,\n",
       "  1066,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  9,\n",
       "  860,\n",
       "  1,\n",
       "  255,\n",
       "  3070,\n",
       "  2050,\n",
       "  3071,\n",
       "  93,\n",
       "  11,\n",
       "  111,\n",
       "  5,\n",
       "  1,\n",
       "  1229,\n",
       "  3072,\n",
       "  1,\n",
       "  143,\n",
       "  3073,\n",
       "  5,\n",
       "  1066,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  226,\n",
       "  65,\n",
       "  50,\n",
       "  23,\n",
       "  625,\n",
       "  93,\n",
       "  32,\n",
       "  15,\n",
       "  9,\n",
       "  1301,\n",
       "  1,\n",
       "  3074,\n",
       "  5,\n",
       "  3075,\n",
       "  3076,\n",
       "  11,\n",
       "  1708,\n",
       "  1307],\n",
       " [1,\n",
       "  46,\n",
       "  75,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  69,\n",
       "  1,\n",
       "  27,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  41,\n",
       "  19,\n",
       "  11,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  48,\n",
       "  29,\n",
       "  13,\n",
       "  1,\n",
       "  17,\n",
       "  161,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  14,\n",
       "  7,\n",
       "  35,\n",
       "  4,\n",
       "  7,\n",
       "  41,\n",
       "  19,\n",
       "  50,\n",
       "  14,\n",
       "  559,\n",
       "  1022,\n",
       "  1337,\n",
       "  205,\n",
       "  602,\n",
       "  1,\n",
       "  2051,\n",
       "  28,\n",
       "  176,\n",
       "  26,\n",
       "  1,\n",
       "  159,\n",
       "  62,\n",
       "  942],\n",
       " [188,\n",
       "  23,\n",
       "  1305,\n",
       "  3077,\n",
       "  4,\n",
       "  2052,\n",
       "  44,\n",
       "  73,\n",
       "  508,\n",
       "  34,\n",
       "  24,\n",
       "  19,\n",
       "  3078,\n",
       "  1078,\n",
       "  2053,\n",
       "  341,\n",
       "  7,\n",
       "  432,\n",
       "  634,\n",
       "  3079,\n",
       "  5,\n",
       "  716,\n",
       "  19,\n",
       "  94,\n",
       "  901,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  24,\n",
       "  73,\n",
       "  19,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  7,\n",
       "  708,\n",
       "  647,\n",
       "  5,\n",
       "  716,\n",
       "  19,\n",
       "  94,\n",
       "  319,\n",
       "  1,\n",
       "  73,\n",
       "  377,\n",
       "  13,\n",
       "  254,\n",
       "  3080,\n",
       "  26,\n",
       "  112,\n",
       "  106,\n",
       "  4,\n",
       "  228,\n",
       "  1943,\n",
       "  901,\n",
       "  2,\n",
       "  3,\n",
       "  88],\n",
       " [1,\n",
       "  167,\n",
       "  509,\n",
       "  8,\n",
       "  1099,\n",
       "  2,\n",
       "  3,\n",
       "  197,\n",
       "  34,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  7,\n",
       "  324,\n",
       "  955,\n",
       "  3081,\n",
       "  30,\n",
       "  299,\n",
       "  3082,\n",
       "  8,\n",
       "  269,\n",
       "  9,\n",
       "  1836,\n",
       "  1,\n",
       "  1908,\n",
       "  495,\n",
       "  3083,\n",
       "  1332,\n",
       "  299,\n",
       "  515,\n",
       "  8,\n",
       "  1099,\n",
       "  2,\n",
       "  3,\n",
       "  197,\n",
       "  13,\n",
       "  268,\n",
       "  9,\n",
       "  104,\n",
       "  7,\n",
       "  2054,\n",
       "  957,\n",
       "  5,\n",
       "  7,\n",
       "  327,\n",
       "  710,\n",
       "  873,\n",
       "  1030,\n",
       "  8,\n",
       "  42,\n",
       "  1,\n",
       "  30,\n",
       "  299,\n",
       "  513,\n",
       "  32,\n",
       "  1380,\n",
       "  8,\n",
       "  363,\n",
       "  5,\n",
       "  708],\n",
       " [24,\n",
       "  391,\n",
       "  82,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  953,\n",
       "  238,\n",
       "  309,\n",
       "  41,\n",
       "  19,\n",
       "  1176,\n",
       "  128,\n",
       "  9,\n",
       "  78,\n",
       "  30,\n",
       "  178,\n",
       "  5,\n",
       "  457,\n",
       "  2,\n",
       "  3,\n",
       "  747,\n",
       "  4,\n",
       "  457,\n",
       "  2,\n",
       "  3,\n",
       "  1100,\n",
       "  10,\n",
       "  1,\n",
       "  883,\n",
       "  3084,\n",
       "  2,\n",
       "  3,\n",
       "  747,\n",
       "  97,\n",
       "  7,\n",
       "  2055,\n",
       "  1381,\n",
       "  19,\n",
       "  883,\n",
       "  170,\n",
       "  42,\n",
       "  2056,\n",
       "  9,\n",
       "  2057,\n",
       "  215,\n",
       "  5,\n",
       "  7,\n",
       "  30,\n",
       "  61,\n",
       "  23,\n",
       "  810,\n",
       "  30,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  136],\n",
       " [1,\n",
       "  2058,\n",
       "  22,\n",
       "  13,\n",
       "  7,\n",
       "  155,\n",
       "  4,\n",
       "  3085,\n",
       "  213,\n",
       "  5,\n",
       "  471,\n",
       "  22,\n",
       "  1296,\n",
       "  4,\n",
       "  2059,\n",
       "  2060,\n",
       "  8,\n",
       "  269,\n",
       "  9,\n",
       "  2061,\n",
       "  1,\n",
       "  1839,\n",
       "  587,\n",
       "  26,\n",
       "  171,\n",
       "  1,\n",
       "  2058,\n",
       "  22,\n",
       "  13,\n",
       "  15,\n",
       "  31,\n",
       "  1,\n",
       "  46,\n",
       "  4,\n",
       "  3086,\n",
       "  22,\n",
       "  351,\n",
       "  5,\n",
       "  7,\n",
       "  388,\n",
       "  5,\n",
       "  1,\n",
       "  471,\n",
       "  22,\n",
       "  3087,\n",
       "  63,\n",
       "  14,\n",
       "  327,\n",
       "  602,\n",
       "  3088,\n",
       "  1848,\n",
       "  1296,\n",
       "  4,\n",
       "  2059,\n",
       "  2060,\n",
       "  4,\n",
       "  98,\n",
       "  129,\n",
       "  123,\n",
       "  3089,\n",
       "  4,\n",
       "  1846],\n",
       " [1,\n",
       "  144,\n",
       "  32,\n",
       "  400,\n",
       "  4,\n",
       "  466,\n",
       "  10,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  51,\n",
       "  172,\n",
       "  93,\n",
       "  32,\n",
       "  1792,\n",
       "  21,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [44,\n",
       "  361,\n",
       "  67,\n",
       "  51,\n",
       "  660,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  720,\n",
       "  483,\n",
       "  20,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  6,\n",
       "  76,\n",
       "  182,\n",
       "  51,\n",
       "  63,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [1,\n",
       "  106,\n",
       "  32,\n",
       "  407,\n",
       "  26,\n",
       "  600,\n",
       "  378,\n",
       "  10,\n",
       "  1,\n",
       "  382,\n",
       "  81,\n",
       "  383,\n",
       "  87,\n",
       "  494,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  1910,\n",
       "  81,\n",
       "  8,\n",
       "  382,\n",
       "  383,\n",
       "  87],\n",
       " [1,\n",
       "  3090,\n",
       "  13,\n",
       "  7,\n",
       "  226,\n",
       "  99,\n",
       "  557,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  175,\n",
       "  14,\n",
       "  3091,\n",
       "  557,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  7,\n",
       "  403,\n",
       "  11,\n",
       "  390,\n",
       "  99,\n",
       "  226,\n",
       "  215,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  81],\n",
       " [6,\n",
       "  18,\n",
       "  518,\n",
       "  519,\n",
       "  64,\n",
       "  9,\n",
       "  285,\n",
       "  1,\n",
       "  106,\n",
       "  648,\n",
       "  2021,\n",
       "  18,\n",
       "  7,\n",
       "  520,\n",
       "  729,\n",
       "  437,\n",
       "  847,\n",
       "  1329,\n",
       "  14,\n",
       "  1,\n",
       "  518,\n",
       "  285,\n",
       "  469,\n",
       "  519,\n",
       "  64,\n",
       "  396,\n",
       "  256,\n",
       "  3092,\n",
       "  1316,\n",
       "  754,\n",
       "  754,\n",
       "  11,\n",
       "  713,\n",
       "  633,\n",
       "  1277,\n",
       "  4,\n",
       "  2009,\n",
       "  1,\n",
       "  1185,\n",
       "  435],\n",
       " [6,\n",
       "  69,\n",
       "  7,\n",
       "  158,\n",
       "  168,\n",
       "  56,\n",
       "  99,\n",
       "  884,\n",
       "  4,\n",
       "  521,\n",
       "  165,\n",
       "  23,\n",
       "  1,\n",
       "  616,\n",
       "  176,\n",
       "  11,\n",
       "  2062,\n",
       "  6,\n",
       "  18,\n",
       "  158,\n",
       "  168,\n",
       "  56,\n",
       "  99,\n",
       "  884,\n",
       "  4,\n",
       "  521,\n",
       "  165,\n",
       "  11,\n",
       "  48,\n",
       "  265],\n",
       " [435,\n",
       "  1343,\n",
       "  13,\n",
       "  196,\n",
       "  179,\n",
       "  10,\n",
       "  1092,\n",
       "  1382,\n",
       "  1079,\n",
       "  57,\n",
       "  250,\n",
       "  1136,\n",
       "  61,\n",
       "  23,\n",
       "  259,\n",
       "  32,\n",
       "  192,\n",
       "  3093,\n",
       "  10,\n",
       "  1092,\n",
       "  1382,\n",
       "  1079,\n",
       "  57],\n",
       " [1101,\n",
       "  3094,\n",
       "  1,\n",
       "  18,\n",
       "  5,\n",
       "  3095,\n",
       "  8,\n",
       "  1,\n",
       "  2063,\n",
       "  5,\n",
       "  242,\n",
       "  23,\n",
       "  1,\n",
       "  207,\n",
       "  1101,\n",
       "  626,\n",
       "  1101,\n",
       "  97,\n",
       "  1,\n",
       "  704,\n",
       "  82,\n",
       "  11,\n",
       "  293,\n",
       "  2063,\n",
       "  1101,\n",
       "  626],\n",
       " [6,\n",
       "  882,\n",
       "  1,\n",
       "  726,\n",
       "  5,\n",
       "  362,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  238,\n",
       "  875,\n",
       "  32,\n",
       "  118,\n",
       "  15,\n",
       "  8,\n",
       "  48,\n",
       "  62,\n",
       "  8,\n",
       "  362,\n",
       "  2,\n",
       "  3,\n",
       "  64],\n",
       " [1,\n",
       "  1219,\n",
       "  169,\n",
       "  122,\n",
       "  59,\n",
       "  33,\n",
       "  15,\n",
       "  9,\n",
       "  341,\n",
       "  1,\n",
       "  3096,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  59,\n",
       "  108,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  657,\n",
       "  1,\n",
       "  30,\n",
       "  132],\n",
       " [748, 81, 749, 16, 6, 50, 7, 531, 171, 10, 748, 83, 749, 16],\n",
       " [1,\n",
       "  376,\n",
       "  184,\n",
       "  15,\n",
       "  8,\n",
       "  24,\n",
       "  58,\n",
       "  461,\n",
       "  1,\n",
       "  103,\n",
       "  138,\n",
       "  8,\n",
       "  2064,\n",
       "  2,\n",
       "  3,\n",
       "  288,\n",
       "  1,\n",
       "  376,\n",
       "  184,\n",
       "  15,\n",
       "  8,\n",
       "  1,\n",
       "  58,\n",
       "  8,\n",
       "  185,\n",
       "  253,\n",
       "  461,\n",
       "  1,\n",
       "  29,\n",
       "  497,\n",
       "  8,\n",
       "  2064,\n",
       "  2,\n",
       "  3,\n",
       "  288,\n",
       "  4,\n",
       "  98,\n",
       "  1822,\n",
       "  790,\n",
       "  215,\n",
       "  26,\n",
       "  3097,\n",
       "  111,\n",
       "  30,\n",
       "  1401,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  736],\n",
       " [6,\n",
       "  260,\n",
       "  1050,\n",
       "  1372,\n",
       "  14,\n",
       "  155,\n",
       "  116,\n",
       "  173,\n",
       "  630,\n",
       "  4,\n",
       "  45,\n",
       "  165,\n",
       "  1,\n",
       "  41,\n",
       "  19,\n",
       "  13,\n",
       "  7,\n",
       "  100,\n",
       "  14,\n",
       "  819,\n",
       "  4,\n",
       "  116,\n",
       "  173,\n",
       "  630,\n",
       "  4,\n",
       "  45,\n",
       "  165],\n",
       " [362,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  1009,\n",
       "  861,\n",
       "  9,\n",
       "  24,\n",
       "  209,\n",
       "  3098,\n",
       "  3099,\n",
       "  9,\n",
       "  3100,\n",
       "  3101,\n",
       "  21,\n",
       "  362,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  11,\n",
       "  48,\n",
       "  62],\n",
       " [6,\n",
       "  18,\n",
       "  1076,\n",
       "  199,\n",
       "  1914,\n",
       "  14,\n",
       "  1915,\n",
       "  349,\n",
       "  4,\n",
       "  158,\n",
       "  168,\n",
       "  199,\n",
       "  1115,\n",
       "  14,\n",
       "  68,\n",
       "  753,\n",
       "  162,\n",
       "  21,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  158,\n",
       "  168,\n",
       "  668,\n",
       "  81,\n",
       "  8,\n",
       "  1,\n",
       "  92,\n",
       "  20,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  9,\n",
       "  262,\n",
       "  199,\n",
       "  1115,\n",
       "  23,\n",
       "  111,\n",
       "  250,\n",
       "  75,\n",
       "  14,\n",
       "  1401,\n",
       "  753,\n",
       "  669,\n",
       "  4,\n",
       "  106,\n",
       "  2137],\n",
       " [31,\n",
       "  11,\n",
       "  1,\n",
       "  1002,\n",
       "  3102,\n",
       "  98,\n",
       "  13,\n",
       "  2065,\n",
       "  9,\n",
       "  3103,\n",
       "  6,\n",
       "  3104,\n",
       "  9,\n",
       "  18,\n",
       "  7,\n",
       "  55,\n",
       "  213,\n",
       "  5,\n",
       "  1,\n",
       "  567,\n",
       "  103,\n",
       "  5,\n",
       "  742,\n",
       "  4,\n",
       "  1098,\n",
       "  36,\n",
       "  11,\n",
       "  478,\n",
       "  7,\n",
       "  55,\n",
       "  213,\n",
       "  5,\n",
       "  1,\n",
       "  567,\n",
       "  103,\n",
       "  8,\n",
       "  742,\n",
       "  4,\n",
       "  1098,\n",
       "  36,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  681,\n",
       "  8,\n",
       "  68,\n",
       "  3105,\n",
       "  592,\n",
       "  1,\n",
       "  846,\n",
       "  389,\n",
       "  1204,\n",
       "  4,\n",
       "  124,\n",
       "  3106,\n",
       "  8,\n",
       "  7,\n",
       "  160,\n",
       "  189,\n",
       "  8,\n",
       "  7,\n",
       "  3107],\n",
       " [1023,\n",
       "  1,\n",
       "  89,\n",
       "  395,\n",
       "  67,\n",
       "  109,\n",
       "  10,\n",
       "  1,\n",
       "  77,\n",
       "  80,\n",
       "  82,\n",
       "  12,\n",
       "  36,\n",
       "  6,\n",
       "  15,\n",
       "  77,\n",
       "  80,\n",
       "  11,\n",
       "  281,\n",
       "  70,\n",
       "  89,\n",
       "  12,\n",
       "  36],\n",
       " [8,\n",
       "  48,\n",
       "  167,\n",
       "  6,\n",
       "  939,\n",
       "  23,\n",
       "  78,\n",
       "  14,\n",
       "  158,\n",
       "  168,\n",
       "  668,\n",
       "  489,\n",
       "  521,\n",
       "  165,\n",
       "  791,\n",
       "  6,\n",
       "  18,\n",
       "  158,\n",
       "  168,\n",
       "  56,\n",
       "  99,\n",
       "  884,\n",
       "  4,\n",
       "  521,\n",
       "  165,\n",
       "  11,\n",
       "  48,\n",
       "  265],\n",
       " [6,\n",
       "  15,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  9,\n",
       "  210,\n",
       "  7,\n",
       "  3108,\n",
       "  654,\n",
       "  11,\n",
       "  111,\n",
       "  3109,\n",
       "  3110,\n",
       "  15,\n",
       "  1,\n",
       "  1205,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  9,\n",
       "  986,\n",
       "  522,\n",
       "  1611],\n",
       " [9,\n",
       "  1340,\n",
       "  11,\n",
       "  48,\n",
       "  515,\n",
       "  1742,\n",
       "  333,\n",
       "  21,\n",
       "  441,\n",
       "  130,\n",
       "  115,\n",
       "  2066,\n",
       "  2,\n",
       "  3,\n",
       "  339,\n",
       "  3111,\n",
       "  65,\n",
       "  414,\n",
       "  18,\n",
       "  5,\n",
       "  441,\n",
       "  130,\n",
       "  115,\n",
       "  3112,\n",
       "  2066,\n",
       "  2,\n",
       "  3,\n",
       "  339],\n",
       " [1,\n",
       "  29,\n",
       "  33,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  44,\n",
       "  4,\n",
       "  867,\n",
       "  236,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  213,\n",
       "  131,\n",
       "  12,\n",
       "  40,\n",
       "  8,\n",
       "  1747,\n",
       "  163,\n",
       "  50,\n",
       "  23,\n",
       "  485,\n",
       "  28,\n",
       "  154,\n",
       "  104,\n",
       "  214,\n",
       "  635,\n",
       "  9,\n",
       "  1042,\n",
       "  724,\n",
       "  206,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  15,\n",
       "  11,\n",
       "  46,\n",
       "  4,\n",
       "  281,\n",
       "  38],\n",
       " [6,\n",
       "  18,\n",
       "  226,\n",
       "  489,\n",
       "  21,\n",
       "  490,\n",
       "  4,\n",
       "  489,\n",
       "  14,\n",
       "  753,\n",
       "  162,\n",
       "  21,\n",
       "  404,\n",
       "  316,\n",
       "  4,\n",
       "  134,\n",
       "  25,\n",
       "  1,\n",
       "  1091,\n",
       "  763,\n",
       "  162,\n",
       "  33,\n",
       "  50,\n",
       "  14,\n",
       "  404,\n",
       "  316,\n",
       "  4,\n",
       "  134,\n",
       "  25],\n",
       " [8,\n",
       "  846,\n",
       "  27,\n",
       "  70,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  290,\n",
       "  31,\n",
       "  17,\n",
       "  83,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  35,\n",
       "  19,\n",
       "  13,\n",
       "  524,\n",
       "  8,\n",
       "  363,\n",
       "  5,\n",
       "  126,\n",
       "  218,\n",
       "  3113,\n",
       "  174,\n",
       "  21,\n",
       "  7,\n",
       "  3114,\n",
       "  13,\n",
       "  7,\n",
       "  60,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  69,\n",
       "  7,\n",
       "  158,\n",
       "  168,\n",
       "  56,\n",
       "  99,\n",
       "  884,\n",
       "  4,\n",
       "  521,\n",
       "  165,\n",
       "  23,\n",
       "  1,\n",
       "  616,\n",
       "  176,\n",
       "  11,\n",
       "  2062,\n",
       "  6,\n",
       "  18,\n",
       "  158,\n",
       "  168,\n",
       "  56,\n",
       "  99,\n",
       "  884,\n",
       "  4,\n",
       "  521,\n",
       "  165,\n",
       "  11,\n",
       "  48,\n",
       "  265],\n",
       " [1,\n",
       "  30,\n",
       "  73,\n",
       "  33,\n",
       "  153,\n",
       "  26,\n",
       "  464,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  1,\n",
       "  30,\n",
       "  73,\n",
       "  13,\n",
       "  417,\n",
       "  26,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  1,\n",
       "  1097,\n",
       "  538,\n",
       "  13,\n",
       "  15],\n",
       " [11,\n",
       "  46,\n",
       "  99,\n",
       "  163,\n",
       "  6,\n",
       "  15,\n",
       "  404,\n",
       "  152,\n",
       "  316,\n",
       "  4,\n",
       "  134,\n",
       "  156,\n",
       "  1,\n",
       "  99,\n",
       "  81,\n",
       "  15,\n",
       "  33,\n",
       "  404,\n",
       "  316,\n",
       "  4,\n",
       "  134,\n",
       "  156,\n",
       "  4,\n",
       "  107,\n",
       "  106,\n",
       "  67,\n",
       "  15,\n",
       "  2067,\n",
       "  1383,\n",
       "  369,\n",
       "  162,\n",
       "  3115,\n",
       "  435,\n",
       "  5,\n",
       "  83,\n",
       "  4,\n",
       "  7,\n",
       "  3116,\n",
       "  793,\n",
       "  5,\n",
       "  1,\n",
       "  1210,\n",
       "  5,\n",
       "  1,\n",
       "  212,\n",
       "  5,\n",
       "  924],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  60,\n",
       "  73,\n",
       "  122,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  30,\n",
       "  341,\n",
       "  1,\n",
       "  85,\n",
       "  500,\n",
       "  15,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  341,\n",
       "  1,\n",
       "  63,\n",
       "  8,\n",
       "  1,\n",
       "  22],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  60,\n",
       "  73,\n",
       "  122,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  30,\n",
       "  341,\n",
       "  1,\n",
       "  85,\n",
       "  500,\n",
       "  15,\n",
       "  1,\n",
       "  59,\n",
       "  108,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  657,\n",
       "  1,\n",
       "  30,\n",
       "  132],\n",
       " [8,\n",
       "  1,\n",
       "  217,\n",
       "  137,\n",
       "  187,\n",
       "  772,\n",
       "  336,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  1,\n",
       "  812,\n",
       "  13,\n",
       "  618,\n",
       "  3117,\n",
       "  3118,\n",
       "  1319,\n",
       "  1,\n",
       "  75,\n",
       "  5,\n",
       "  137,\n",
       "  1038,\n",
       "  8,\n",
       "  24,\n",
       "  167,\n",
       "  13,\n",
       "  725,\n",
       "  11,\n",
       "  44,\n",
       "  6,\n",
       "  18,\n",
       "  3119,\n",
       "  1038,\n",
       "  247,\n",
       "  8,\n",
       "  1,\n",
       "  217,\n",
       "  137,\n",
       "  187,\n",
       "  772,\n",
       "  336,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  11,\n",
       "  305,\n",
       "  6,\n",
       "  18,\n",
       "  3120,\n",
       "  3121,\n",
       "  3122],\n",
       " [202,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  33,\n",
       "  15,\n",
       "  9,\n",
       "  396,\n",
       "  78,\n",
       "  381,\n",
       "  9,\n",
       "  174,\n",
       "  3123,\n",
       "  99,\n",
       "  81,\n",
       "  5,\n",
       "  202,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  33,\n",
       "  15,\n",
       "  9,\n",
       "  121,\n",
       "  1,\n",
       "  3124,\n",
       "  19],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  14,\n",
       "  124,\n",
       "  107,\n",
       "  3125,\n",
       "  33,\n",
       "  233,\n",
       "  14,\n",
       "  7,\n",
       "  1348,\n",
       "  122,\n",
       "  176,\n",
       "  14,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  60,\n",
       "  73,\n",
       "  122,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  30,\n",
       "  341,\n",
       "  1,\n",
       "  85,\n",
       "  500,\n",
       "  15,\n",
       "  1,\n",
       "  59,\n",
       "  108,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  657,\n",
       "  1,\n",
       "  30,\n",
       "  132],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  59,\n",
       "  108,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  657,\n",
       "  1,\n",
       "  30,\n",
       "  1993,\n",
       "  109,\n",
       "  30,\n",
       "  73,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  14,\n",
       "  1,\n",
       "  107,\n",
       "  356,\n",
       "  73,\n",
       "  538,\n",
       "  82],\n",
       " [1,\n",
       "  41,\n",
       "  19,\n",
       "  15,\n",
       "  13,\n",
       "  1,\n",
       "  100,\n",
       "  22,\n",
       "  21,\n",
       "  367,\n",
       "  1090,\n",
       "  2068,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  15,\n",
       "  367,\n",
       "  1090,\n",
       "  698,\n",
       "  2068,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  31,\n",
       "  1,\n",
       "  710,\n",
       "  22],\n",
       " [44,\n",
       "  361,\n",
       "  67,\n",
       "  51,\n",
       "  660,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  720,\n",
       "  483,\n",
       "  20,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  6,\n",
       "  76,\n",
       "  182,\n",
       "  51,\n",
       "  63,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [11,\n",
       "  231,\n",
       "  475,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  555,\n",
       "  471,\n",
       "  658,\n",
       "  1346,\n",
       "  178,\n",
       "  4,\n",
       "  1347,\n",
       "  178,\n",
       "  8,\n",
       "  595,\n",
       "  4,\n",
       "  738,\n",
       "  2069,\n",
       "  337,\n",
       "  1346,\n",
       "  30,\n",
       "  178,\n",
       "  14,\n",
       "  3126,\n",
       "  326,\n",
       "  4,\n",
       "  869,\n",
       "  2070,\n",
       "  31,\n",
       "  350,\n",
       "  31,\n",
       "  1347,\n",
       "  30,\n",
       "  178,\n",
       "  14,\n",
       "  326,\n",
       "  4,\n",
       "  869,\n",
       "  2070,\n",
       "  34,\n",
       "  32,\n",
       "  587,\n",
       "  8,\n",
       "  475,\n",
       "  2,\n",
       "  3,\n",
       "  117,\n",
       "  4,\n",
       "  154,\n",
       "  104,\n",
       "  1928,\n",
       "  662,\n",
       "  253],\n",
       " [91,\n",
       "  418,\n",
       "  1902,\n",
       "  68,\n",
       "  1048,\n",
       "  26,\n",
       "  124,\n",
       "  1384,\n",
       "  3127,\n",
       "  1,\n",
       "  308,\n",
       "  1666,\n",
       "  3128,\n",
       "  5,\n",
       "  157,\n",
       "  670,\n",
       "  9,\n",
       "  42,\n",
       "  145,\n",
       "  1397,\n",
       "  1398,\n",
       "  7,\n",
       "  30,\n",
       "  26,\n",
       "  1,\n",
       "  1110,\n",
       "  98,\n",
       "  1111,\n",
       "  813,\n",
       "  814,\n",
       "  48,\n",
       "  13,\n",
       "  1823,\n",
       "  618,\n",
       "  91,\n",
       "  379,\n",
       "  319,\n",
       "  7,\n",
       "  30,\n",
       "  294,\n",
       "  7,\n",
       "  3129,\n",
       "  8,\n",
       "  24,\n",
       "  957,\n",
       "  13,\n",
       "  609,\n",
       "  26,\n",
       "  1,\n",
       "  1110,\n",
       "  98,\n",
       "  1111,\n",
       "  813,\n",
       "  814],\n",
       " [448,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  18,\n",
       "  3130,\n",
       "  9,\n",
       "  121,\n",
       "  3131,\n",
       "  3132,\n",
       "  238,\n",
       "  309,\n",
       "  13,\n",
       "  72,\n",
       "  9,\n",
       "  34,\n",
       "  5,\n",
       "  448,\n",
       "  2,\n",
       "  3,\n",
       "  88],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  60,\n",
       "  73,\n",
       "  122,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  30,\n",
       "  341,\n",
       "  1,\n",
       "  85,\n",
       "  500,\n",
       "  15,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  9,\n",
       "  341,\n",
       "  1,\n",
       "  63,\n",
       "  8,\n",
       "  1,\n",
       "  22],\n",
       " [1,\n",
       "  671,\n",
       "  34,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  3133,\n",
       "  1,\n",
       "  379,\n",
       "  8,\n",
       "  1,\n",
       "  1814,\n",
       "  902,\n",
       "  13,\n",
       "  406,\n",
       "  594,\n",
       "  379,\n",
       "  866,\n",
       "  277,\n",
       "  2,\n",
       "  3,\n",
       "  40,\n",
       "  157,\n",
       "  287,\n",
       "  4,\n",
       "  1375,\n",
       "  8,\n",
       "  1,\n",
       "  380,\n",
       "  1085,\n",
       "  23,\n",
       "  1,\n",
       "  1173,\n",
       "  330,\n",
       "  5,\n",
       "  406,\n",
       "  594,\n",
       "  379,\n",
       "  866,\n",
       "  277,\n",
       "  2,\n",
       "  3,\n",
       "  40],\n",
       " [11,\n",
       "  275,\n",
       "  1525,\n",
       "  831,\n",
       "  4,\n",
       "  954,\n",
       "  6,\n",
       "  15,\n",
       "  54,\n",
       "  22,\n",
       "  83,\n",
       "  12,\n",
       "  40,\n",
       "  11,\n",
       "  48,\n",
       "  265,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [1,\n",
       "  44,\n",
       "  133,\n",
       "  33,\n",
       "  181,\n",
       "  10,\n",
       "  1,\n",
       "  30,\n",
       "  274,\n",
       "  1612,\n",
       "  21,\n",
       "  272,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  1,\n",
       "  1940,\n",
       "  32,\n",
       "  419,\n",
       "  21,\n",
       "  1,\n",
       "  3134,\n",
       "  501,\n",
       "  176,\n",
       "  26,\n",
       "  1,\n",
       "  272,\n",
       "  20,\n",
       "  273,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  161,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  31,\n",
       "  7,\n",
       "  2071,\n",
       "  38,\n",
       "  161,\n",
       "  3135,\n",
       "  1,\n",
       "  29,\n",
       "  138,\n",
       "  3136,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [791,\n",
       "  1,\n",
       "  136,\n",
       "  2072,\n",
       "  183,\n",
       "  2073,\n",
       "  4,\n",
       "  1402,\n",
       "  90,\n",
       "  2065,\n",
       "  31,\n",
       "  3137,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  3138,\n",
       "  3139,\n",
       "  19,\n",
       "  46,\n",
       "  3140,\n",
       "  145,\n",
       "  666,\n",
       "  13,\n",
       "  9,\n",
       "  396,\n",
       "  98,\n",
       "  9,\n",
       "  1,\n",
       "  41,\n",
       "  19,\n",
       "  61,\n",
       "  2072,\n",
       "  82,\n",
       "  2073,\n",
       "  4,\n",
       "  1402,\n",
       "  90],\n",
       " [18,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  219,\n",
       "  7,\n",
       "  75,\n",
       "  5,\n",
       "  191,\n",
       "  21,\n",
       "  111,\n",
       "  3141,\n",
       "  219,\n",
       "  389,\n",
       "  191,\n",
       "  10,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  4,\n",
       "  18,\n",
       "  124,\n",
       "  537,\n",
       "  55,\n",
       "  773],\n",
       " [6,\n",
       "  223,\n",
       "  101,\n",
       "  58,\n",
       "  11,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  144,\n",
       "  32,\n",
       "  400,\n",
       "  4,\n",
       "  466,\n",
       "  10,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [8,\n",
       "  3142,\n",
       "  1,\n",
       "  161,\n",
       "  129,\n",
       "  9,\n",
       "  1224,\n",
       "  696,\n",
       "  495,\n",
       "  9,\n",
       "  853,\n",
       "  98,\n",
       "  3143,\n",
       "  12,\n",
       "  36,\n",
       "  9,\n",
       "  853,\n",
       "  1,\n",
       "  3144,\n",
       "  103,\n",
       "  1963,\n",
       "  696,\n",
       "  495,\n",
       "  13,\n",
       "  1,\n",
       "  60,\n",
       "  821,\n",
       "  495,\n",
       "  82,\n",
       "  12,\n",
       "  36],\n",
       " [1,\n",
       "  261,\n",
       "  5,\n",
       "  239,\n",
       "  28,\n",
       "  15,\n",
       "  8,\n",
       "  1,\n",
       "  58,\n",
       "  13,\n",
       "  1,\n",
       "  54,\n",
       "  310,\n",
       "  12,\n",
       "  40,\n",
       "  1,\n",
       "  54,\n",
       "  28,\n",
       "  75,\n",
       "  351,\n",
       "  5,\n",
       "  1921,\n",
       "  125,\n",
       "  5,\n",
       "  1,\n",
       "  305,\n",
       "  236,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [1,\n",
       "  3145,\n",
       "  164,\n",
       "  677,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  3146,\n",
       "  806,\n",
       "  497,\n",
       "  234,\n",
       "  2074,\n",
       "  2,\n",
       "  3,\n",
       "  740,\n",
       "  24,\n",
       "  128,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  2052,\n",
       "  603,\n",
       "  3147,\n",
       "  919,\n",
       "  1189,\n",
       "  8,\n",
       "  2074,\n",
       "  2,\n",
       "  3,\n",
       "  740],\n",
       " [8,\n",
       "  48,\n",
       "  263,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  2037,\n",
       "  22,\n",
       "  26,\n",
       "  409,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  172,\n",
       "  163,\n",
       "  95,\n",
       "  123,\n",
       "  15,\n",
       "  8,\n",
       "  601,\n",
       "  167,\n",
       "  26,\n",
       "  409,\n",
       "  2,\n",
       "  3,\n",
       "  87],\n",
       " [6,\n",
       "  223,\n",
       "  101,\n",
       "  58,\n",
       "  11,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  7,\n",
       "  101,\n",
       "  534,\n",
       "  3148,\n",
       "  29,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  38,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [51,\n",
       "  24,\n",
       "  49,\n",
       "  32,\n",
       "  664,\n",
       "  14,\n",
       "  7,\n",
       "  60,\n",
       "  27,\n",
       "  29,\n",
       "  66,\n",
       "  14,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  69,\n",
       "  60,\n",
       "  27,\n",
       "  49,\n",
       "  14,\n",
       "  107,\n",
       "  739],\n",
       " [7,\n",
       "  649,\n",
       "  128,\n",
       "  9,\n",
       "  1093,\n",
       "  142,\n",
       "  13,\n",
       "  9,\n",
       "  993,\n",
       "  1,\n",
       "  212,\n",
       "  5,\n",
       "  649,\n",
       "  63,\n",
       "  665,\n",
       "  740,\n",
       "  48,\n",
       "  812,\n",
       "  142,\n",
       "  216,\n",
       "  13,\n",
       "  1112,\n",
       "  26,\n",
       "  1,\n",
       "  784,\n",
       "  5,\n",
       "  1,\n",
       "  665,\n",
       "  103,\n",
       "  665,\n",
       "  740],\n",
       " [6,\n",
       "  223,\n",
       "  101,\n",
       "  58,\n",
       "  11,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  101,\n",
       "  49,\n",
       "  32,\n",
       "  66,\n",
       "  14,\n",
       "  1,\n",
       "  177,\n",
       "  27,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  430,\n",
       "  93,\n",
       "  21,\n",
       "  1,\n",
       "  55,\n",
       "  160,\n",
       "  2075,\n",
       "  24,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  348,\n",
       "  55,\n",
       "  276],\n",
       " [6,\n",
       "  274,\n",
       "  4,\n",
       "  737,\n",
       "  1,\n",
       "  28,\n",
       "  14,\n",
       "  1,\n",
       "  60,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  424,\n",
       "  1382,\n",
       "  4,\n",
       "  1102,\n",
       "  83,\n",
       "  4,\n",
       "  635,\n",
       "  18,\n",
       "  1102,\n",
       "  94,\n",
       "  21,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  223,\n",
       "  101,\n",
       "  58,\n",
       "  11,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  274,\n",
       "  4,\n",
       "  737,\n",
       "  1,\n",
       "  28,\n",
       "  14,\n",
       "  1,\n",
       "  60,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  223,\n",
       "  101,\n",
       "  58,\n",
       "  11,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  24,\n",
       "  101,\n",
       "  13,\n",
       "  7,\n",
       "  27,\n",
       "  235,\n",
       "  29,\n",
       "  50,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  46,\n",
       "  1,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  11,\n",
       "  282,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  144,\n",
       "  32,\n",
       "  400,\n",
       "  4,\n",
       "  466,\n",
       "  10,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  274,\n",
       "  4,\n",
       "  737,\n",
       "  1,\n",
       "  28,\n",
       "  14,\n",
       "  1,\n",
       "  60,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  223,\n",
       "  101,\n",
       "  58,\n",
       "  11,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  108,\n",
       "  152,\n",
       "  170,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  408,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  144,\n",
       "  32,\n",
       "  400,\n",
       "  4,\n",
       "  466,\n",
       "  10,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [8,\n",
       "  1319,\n",
       "  3149,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  939,\n",
       "  23,\n",
       "  390,\n",
       "  1187,\n",
       "  4,\n",
       "  976,\n",
       "  7,\n",
       "  3150,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  18,\n",
       "  3151,\n",
       "  8,\n",
       "  966,\n",
       "  14,\n",
       "  7,\n",
       "  3152,\n",
       "  52,\n",
       "  4,\n",
       "  1341],\n",
       " [6,\n",
       "  18,\n",
       "  271,\n",
       "  199,\n",
       "  14,\n",
       "  517,\n",
       "  349,\n",
       "  175,\n",
       "  10,\n",
       "  1,\n",
       "  92,\n",
       "  20,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  423,\n",
       "  271,\n",
       "  1314,\n",
       "  6,\n",
       "  18,\n",
       "  271,\n",
       "  199,\n",
       "  14,\n",
       "  517,\n",
       "  315,\n",
       "  798,\n",
       "  4,\n",
       "  1315,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [786,\n",
       "  1,\n",
       "  126,\n",
       "  193,\n",
       "  240,\n",
       "  1916,\n",
       "  1,\n",
       "  512,\n",
       "  22,\n",
       "  33,\n",
       "  473,\n",
       "  10,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  131,\n",
       "  51,\n",
       "  44,\n",
       "  28,\n",
       "  32,\n",
       "  119,\n",
       "  224,\n",
       "  4,\n",
       "  473,\n",
       "  10,\n",
       "  1,\n",
       "  148,\n",
       "  135,\n",
       "  110],\n",
       " [6,\n",
       "  18,\n",
       "  271,\n",
       "  199,\n",
       "  14,\n",
       "  517,\n",
       "  349,\n",
       "  175,\n",
       "  10,\n",
       "  1,\n",
       "  92,\n",
       "  20,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  1844,\n",
       "  366,\n",
       "  312,\n",
       "  184,\n",
       "  8,\n",
       "  445,\n",
       "  14,\n",
       "  1,\n",
       "  107,\n",
       "  302,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [1,\n",
       "  144,\n",
       "  32,\n",
       "  400,\n",
       "  4,\n",
       "  466,\n",
       "  10,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  105,\n",
       "  49,\n",
       "  32,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  17,\n",
       "  38,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  4,\n",
       "  823,\n",
       "  31,\n",
       "  461],\n",
       " [11,\n",
       "  46,\n",
       "  1,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  11,\n",
       "  282,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  274,\n",
       "  4,\n",
       "  737,\n",
       "  1,\n",
       "  28,\n",
       "  14,\n",
       "  1,\n",
       "  60,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  108,\n",
       "  152,\n",
       "  170,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  408,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  69,\n",
       "  60,\n",
       "  27,\n",
       "  49,\n",
       "  14,\n",
       "  107,\n",
       "  739],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  108,\n",
       "  152,\n",
       "  170,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  408,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [54,\n",
       "  12,\n",
       "  40,\n",
       "  13,\n",
       "  7,\n",
       "  307,\n",
       "  85,\n",
       "  22,\n",
       "  174,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  541,\n",
       "  219,\n",
       "  24,\n",
       "  1344,\n",
       "  146,\n",
       "  21,\n",
       "  1,\n",
       "  1913,\n",
       "  540,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  213,\n",
       "  170,\n",
       "  12,\n",
       "  40],\n",
       " [11,\n",
       "  51,\n",
       "  389,\n",
       "  1067,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  628,\n",
       "  39,\n",
       "  55,\n",
       "  287,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  8,\n",
       "  24,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  348,\n",
       "  55,\n",
       "  276],\n",
       " [1,\n",
       "  39,\n",
       "  55,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  430,\n",
       "  93,\n",
       "  21,\n",
       "  1,\n",
       "  55,\n",
       "  160,\n",
       "  2075,\n",
       "  24,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  348,\n",
       "  55,\n",
       "  276],\n",
       " [6,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  51,\n",
       "  24,\n",
       "  49,\n",
       "  32,\n",
       "  664,\n",
       "  14,\n",
       "  7,\n",
       "  60,\n",
       "  27,\n",
       "  29,\n",
       "  66,\n",
       "  14,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  746,\n",
       "  315,\n",
       "  369,\n",
       "  4,\n",
       "  520,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  9,\n",
       "  651,\n",
       "  106,\n",
       "  6,\n",
       "  18,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  14,\n",
       "  1,\n",
       "  331,\n",
       "  78,\n",
       "  166,\n",
       "  75,\n",
       "  9,\n",
       "  778],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  746,\n",
       "  315,\n",
       "  369,\n",
       "  4,\n",
       "  520,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  9,\n",
       "  651,\n",
       "  106,\n",
       "  6,\n",
       "  69,\n",
       "  1,\n",
       "  324,\n",
       "  1058,\n",
       "  811,\n",
       "  10,\n",
       "  1059,\n",
       "  1060,\n",
       "  315,\n",
       "  83,\n",
       "  14,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  150,\n",
       "  82,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  9,\n",
       "  374,\n",
       "  285,\n",
       "  1,\n",
       "  78,\n",
       "  166,\n",
       "  11,\n",
       "  111,\n",
       "  1845,\n",
       "  18,\n",
       "  1,\n",
       "  746,\n",
       "  315,\n",
       "  369,\n",
       "  4,\n",
       "  520,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  9,\n",
       "  651,\n",
       "  106],\n",
       " [362,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  3153,\n",
       "  10,\n",
       "  880,\n",
       "  1103,\n",
       "  3154,\n",
       "  4,\n",
       "  964,\n",
       "  3155,\n",
       "  11,\n",
       "  1344,\n",
       "  3156,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  18,\n",
       "  880,\n",
       "  1103,\n",
       "  11,\n",
       "  195,\n",
       "  115,\n",
       "  23,\n",
       "  1,\n",
       "  136,\n",
       "  472],\n",
       " [94,\n",
       "  35,\n",
       "  19,\n",
       "  17,\n",
       "  13,\n",
       "  7,\n",
       "  258,\n",
       "  20,\n",
       "  11,\n",
       "  27,\n",
       "  38,\n",
       "  49,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [94,\n",
       "  35,\n",
       "  19,\n",
       "  17,\n",
       "  13,\n",
       "  7,\n",
       "  258,\n",
       "  20,\n",
       "  11,\n",
       "  27,\n",
       "  38,\n",
       "  49,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [94,\n",
       "  35,\n",
       "  19,\n",
       "  17,\n",
       "  13,\n",
       "  7,\n",
       "  258,\n",
       "  20,\n",
       "  11,\n",
       "  27,\n",
       "  38,\n",
       "  49,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  24,\n",
       "  101,\n",
       "  13,\n",
       "  7,\n",
       "  27,\n",
       "  235,\n",
       "  29,\n",
       "  50,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  46,\n",
       "  1,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  11,\n",
       "  282,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [51,\n",
       "  24,\n",
       "  49,\n",
       "  32,\n",
       "  664,\n",
       "  14,\n",
       "  7,\n",
       "  60,\n",
       "  27,\n",
       "  29,\n",
       "  66,\n",
       "  14,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  51,\n",
       "  389,\n",
       "  1067,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  628,\n",
       "  39,\n",
       "  55,\n",
       "  287,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  8,\n",
       "  24,\n",
       "  58,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  9,\n",
       "  348,\n",
       "  55,\n",
       "  276],\n",
       " [154,\n",
       "  104,\n",
       "  397,\n",
       "  26,\n",
       "  3157,\n",
       "  1,\n",
       "  3158,\n",
       "  23,\n",
       "  7,\n",
       "  46,\n",
       "  22,\n",
       "  2076,\n",
       "  87,\n",
       "  1,\n",
       "  106,\n",
       "  154,\n",
       "  104,\n",
       "  1782,\n",
       "  407,\n",
       "  21,\n",
       "  7,\n",
       "  187,\n",
       "  31,\n",
       "  268,\n",
       "  26,\n",
       "  2076,\n",
       "  87],\n",
       " [1,\n",
       "  101,\n",
       "  49,\n",
       "  32,\n",
       "  66,\n",
       "  14,\n",
       "  1,\n",
       "  177,\n",
       "  27,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [94,\n",
       "  35,\n",
       "  19,\n",
       "  17,\n",
       "  13,\n",
       "  7,\n",
       "  258,\n",
       "  20,\n",
       "  11,\n",
       "  27,\n",
       "  38,\n",
       "  49,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  51,\n",
       "  24,\n",
       "  49,\n",
       "  32,\n",
       "  664,\n",
       "  14,\n",
       "  7,\n",
       "  60,\n",
       "  27,\n",
       "  29,\n",
       "  66,\n",
       "  14,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [8,\n",
       "  48,\n",
       "  128,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  2077,\n",
       "  108,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  748,\n",
       "  81,\n",
       "  749,\n",
       "  16,\n",
       "  2077,\n",
       "  13,\n",
       "  7,\n",
       "  595,\n",
       "  29,\n",
       "  34,\n",
       "  13,\n",
       "  66,\n",
       "  23,\n",
       "  1089,\n",
       "  5,\n",
       "  1,\n",
       "  748,\n",
       "  749,\n",
       "  16],\n",
       " [94,\n",
       "  35,\n",
       "  19,\n",
       "  17,\n",
       "  13,\n",
       "  7,\n",
       "  258,\n",
       "  20,\n",
       "  11,\n",
       "  27,\n",
       "  38,\n",
       "  49,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [172,\n",
       "  3159,\n",
       "  1263,\n",
       "  3160,\n",
       "  23,\n",
       "  1,\n",
       "  3161,\n",
       "  293,\n",
       "  31,\n",
       "  15,\n",
       "  8,\n",
       "  249,\n",
       "  338,\n",
       "  2,\n",
       "  3,\n",
       "  339,\n",
       "  2078,\n",
       "  154,\n",
       "  447,\n",
       "  547,\n",
       "  1,\n",
       "  2079,\n",
       "  5,\n",
       "  731,\n",
       "  31,\n",
       "  8,\n",
       "  1,\n",
       "  249,\n",
       "  511,\n",
       "  338,\n",
       "  2,\n",
       "  3,\n",
       "  339],\n",
       " [2054,\n",
       "  3162,\n",
       "  5,\n",
       "  3163,\n",
       "  32,\n",
       "  919,\n",
       "  8,\n",
       "  1,\n",
       "  257,\n",
       "  3164,\n",
       "  5,\n",
       "  1,\n",
       "  249,\n",
       "  257,\n",
       "  399,\n",
       "  338,\n",
       "  2,\n",
       "  3,\n",
       "  339,\n",
       "  2078,\n",
       "  154,\n",
       "  447,\n",
       "  547,\n",
       "  1,\n",
       "  2079,\n",
       "  5,\n",
       "  731,\n",
       "  31,\n",
       "  8,\n",
       "  1,\n",
       "  249,\n",
       "  511,\n",
       "  338,\n",
       "  2,\n",
       "  3,\n",
       "  339],\n",
       " [6,\n",
       "  69,\n",
       "  24,\n",
       "  19,\n",
       "  23,\n",
       "  7,\n",
       "  388,\n",
       "  5,\n",
       "  1,\n",
       "  1971,\n",
       "  1972,\n",
       "  425,\n",
       "  22,\n",
       "  463,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  1219,\n",
       "  169,\n",
       "  3165,\n",
       "  22,\n",
       "  463,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [786,\n",
       "  1,\n",
       "  118,\n",
       "  159,\n",
       "  62,\n",
       "  23,\n",
       "  2080,\n",
       "  1350,\n",
       "  1385,\n",
       "  1046,\n",
       "  2081,\n",
       "  2,\n",
       "  3,\n",
       "  40,\n",
       "  33,\n",
       "  3166,\n",
       "  8,\n",
       "  40,\n",
       "  1377,\n",
       "  655,\n",
       "  129,\n",
       "  123,\n",
       "  233,\n",
       "  23,\n",
       "  1797,\n",
       "  145,\n",
       "  154,\n",
       "  1000,\n",
       "  1385,\n",
       "  211,\n",
       "  372,\n",
       "  41,\n",
       "  3167,\n",
       "  523,\n",
       "  944,\n",
       "  123,\n",
       "  655,\n",
       "  23,\n",
       "  3168,\n",
       "  1380,\n",
       "  8,\n",
       "  372,\n",
       "  41,\n",
       "  1,\n",
       "  3169,\n",
       "  2080,\n",
       "  1350,\n",
       "  1385,\n",
       "  1046,\n",
       "  3170,\n",
       "  2081,\n",
       "  2,\n",
       "  3,\n",
       "  40,\n",
       "  3171,\n",
       "  3172,\n",
       "  2042,\n",
       "  8,\n",
       "  1,\n",
       "  304],\n",
       " [6,\n",
       "  15,\n",
       "  2082,\n",
       "  676,\n",
       "  1102,\n",
       "  2083,\n",
       "  4,\n",
       "  932,\n",
       "  64,\n",
       "  11,\n",
       "  3173,\n",
       "  3174,\n",
       "  1,\n",
       "  49,\n",
       "  10,\n",
       "  2082,\n",
       "  676,\n",
       "  1102,\n",
       "  2083,\n",
       "  4,\n",
       "  932,\n",
       "  64],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  382,\n",
       "  20,\n",
       "  383,\n",
       "  87,\n",
       "  11,\n",
       "  78,\n",
       "  532,\n",
       "  1328,\n",
       "  65,\n",
       "  14,\n",
       "  1198,\n",
       "  3175,\n",
       "  11,\n",
       "  51,\n",
       "  24,\n",
       "  610,\n",
       "  18,\n",
       "  1,\n",
       "  382,\n",
       "  152,\n",
       "  383,\n",
       "  87,\n",
       "  11,\n",
       "  58],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  81,\n",
       "  176,\n",
       "  26,\n",
       "  748,\n",
       "  325,\n",
       "  749,\n",
       "  16,\n",
       "  11,\n",
       "  105,\n",
       "  46,\n",
       "  4,\n",
       "  215,\n",
       "  2069,\n",
       "  18,\n",
       "  748,\n",
       "  749,\n",
       "  16,\n",
       "  31,\n",
       "  68,\n",
       "  81,\n",
       "  5,\n",
       "  531],\n",
       " [1386,\n",
       "  261,\n",
       "  14,\n",
       "  209,\n",
       "  275,\n",
       "  26,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  30,\n",
       "  132,\n",
       "  67,\n",
       "  417,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [6,\n",
       "  15,\n",
       "  3176,\n",
       "  134,\n",
       "  36,\n",
       "  11,\n",
       "  143,\n",
       "  5,\n",
       "  1998,\n",
       "  1075,\n",
       "  48,\n",
       "  873,\n",
       "  13,\n",
       "  327,\n",
       "  641,\n",
       "  14,\n",
       "  1,\n",
       "  317,\n",
       "  143,\n",
       "  314,\n",
       "  15,\n",
       "  11,\n",
       "  436,\n",
       "  134,\n",
       "  36],\n",
       " [83,\n",
       "  14,\n",
       "  94,\n",
       "  349,\n",
       "  10,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  882,\n",
       "  428,\n",
       "  78,\n",
       "  591,\n",
       "  106,\n",
       "  10,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [6,\n",
       "  15,\n",
       "  202,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  11,\n",
       "  51,\n",
       "  24,\n",
       "  215,\n",
       "  3177,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  42,\n",
       "  240,\n",
       "  1,\n",
       "  81,\n",
       "  5,\n",
       "  51,\n",
       "  493,\n",
       "  530,\n",
       "  33,\n",
       "  15,\n",
       "  8,\n",
       "  24,\n",
       "  678],\n",
       " [119,\n",
       "  198,\n",
       "  33,\n",
       "  109,\n",
       "  14,\n",
       "  148,\n",
       "  135,\n",
       "  110,\n",
       "  148,\n",
       "  13,\n",
       "  7,\n",
       "  70,\n",
       "  405,\n",
       "  3178,\n",
       "  119,\n",
       "  171,\n",
       "  135,\n",
       "  110],\n",
       " [30,\n",
       "  132,\n",
       "  23,\n",
       "  1,\n",
       "  85,\n",
       "  22,\n",
       "  32,\n",
       "  109,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  14,\n",
       "  1,\n",
       "  3179,\n",
       "  3180,\n",
       "  85,\n",
       "  22,\n",
       "  13,\n",
       "  593,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  92,\n",
       "  94,\n",
       "  452,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  215,\n",
       "  481,\n",
       "  1,\n",
       "  92,\n",
       "  553,\n",
       "  152,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [1,\n",
       "  257,\n",
       "  1371,\n",
       "  28,\n",
       "  33,\n",
       "  373,\n",
       "  10,\n",
       "  1,\n",
       "  498,\n",
       "  4,\n",
       "  885,\n",
       "  584,\n",
       "  52,\n",
       "  498,\n",
       "  4,\n",
       "  885,\n",
       "  36,\n",
       "  98,\n",
       "  1085,\n",
       "  23,\n",
       "  1,\n",
       "  1387,\n",
       "  584,\n",
       "  52,\n",
       "  498,\n",
       "  4,\n",
       "  885,\n",
       "  36],\n",
       " [11,\n",
       "  3181,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  39,\n",
       "  3182,\n",
       "  2084,\n",
       "  4,\n",
       "  86,\n",
       "  84,\n",
       "  982,\n",
       "  6,\n",
       "  3183,\n",
       "  51,\n",
       "  611,\n",
       "  14,\n",
       "  248,\n",
       "  119,\n",
       "  680,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  119,\n",
       "  171,\n",
       "  2084,\n",
       "  4,\n",
       "  86,\n",
       "  84],\n",
       " [6, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40, 51, 1, 28, 1009, 21, 54, 12, 40],\n",
       " [48,\n",
       "  28,\n",
       "  13,\n",
       "  236,\n",
       "  5,\n",
       "  1,\n",
       "  552,\n",
       "  22,\n",
       "  392,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  1,\n",
       "  46,\n",
       "  28,\n",
       "  787,\n",
       "  26,\n",
       "  1,\n",
       "  62,\n",
       "  942,\n",
       "  622,\n",
       "  21,\n",
       "  1,\n",
       "  552,\n",
       "  22,\n",
       "  392,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  42,\n",
       "  240,\n",
       "  621,\n",
       "  453,\n",
       "  26,\n",
       "  692,\n",
       "  5,\n",
       "  44,\n",
       "  31,\n",
       "  7,\n",
       "  1515,\n",
       "  41,\n",
       "  4,\n",
       "  13,\n",
       "  3184,\n",
       "  26],\n",
       " [1,\n",
       "  46,\n",
       "  75,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  69,\n",
       "  1,\n",
       "  27,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  41,\n",
       "  19,\n",
       "  11,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  223,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  161,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  30,\n",
       "  73,\n",
       "  660,\n",
       "  26,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  84,\n",
       "  179,\n",
       "  5,\n",
       "  68,\n",
       "  716,\n",
       "  19,\n",
       "  3185,\n",
       "  13,\n",
       "  1,\n",
       "  376,\n",
       "  73,\n",
       "  19,\n",
       "  5,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  84],\n",
       " [11, 48, 265, 6, 18, 1, 54, 22, 12, 40, 51, 1, 28, 1009, 21, 54, 12, 40],\n",
       " [30,\n",
       "  73,\n",
       "  13,\n",
       "  109,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  1386,\n",
       "  261,\n",
       "  14,\n",
       "  209,\n",
       "  275,\n",
       "  26,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [30,\n",
       "  73,\n",
       "  33,\n",
       "  233,\n",
       "  14,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  11,\n",
       "  105,\n",
       "  2085,\n",
       "  73,\n",
       "  13,\n",
       "  109,\n",
       "  14,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [1,\n",
       "  143,\n",
       "  188,\n",
       "  67,\n",
       "  176,\n",
       "  26,\n",
       "  1,\n",
       "  942,\n",
       "  10,\n",
       "  248,\n",
       "  143,\n",
       "  999,\n",
       "  8,\n",
       "  553,\n",
       "  392,\n",
       "  4,\n",
       "  1104,\n",
       "  64,\n",
       "  1,\n",
       "  65,\n",
       "  67,\n",
       "  397,\n",
       "  10,\n",
       "  3186,\n",
       "  1388,\n",
       "  392,\n",
       "  4,\n",
       "  1104,\n",
       "  64],\n",
       " [8,\n",
       "  1336,\n",
       "  167,\n",
       "  6,\n",
       "  154,\n",
       "  492,\n",
       "  3187,\n",
       "  345,\n",
       "  381,\n",
       "  5,\n",
       "  3188,\n",
       "  215,\n",
       "  4,\n",
       "  1977,\n",
       "  1161,\n",
       "  1,\n",
       "  3189,\n",
       "  5,\n",
       "  1,\n",
       "  355,\n",
       "  857,\n",
       "  842,\n",
       "  82,\n",
       "  509,\n",
       "  26,\n",
       "  2086,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  2086,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  587,\n",
       "  7,\n",
       "  857,\n",
       "  61,\n",
       "  82,\n",
       "  4,\n",
       "  68,\n",
       "  355,\n",
       "  3190,\n",
       "  842,\n",
       "  82],\n",
       " [1386,\n",
       "  261,\n",
       "  14,\n",
       "  209,\n",
       "  275,\n",
       "  26,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  30,\n",
       "  73,\n",
       "  13,\n",
       "  233,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [6,\n",
       "  223,\n",
       "  70,\n",
       "  89,\n",
       "  395,\n",
       "  11,\n",
       "  259,\n",
       "  211,\n",
       "  24,\n",
       "  606,\n",
       "  1664,\n",
       "  29,\n",
       "  1,\n",
       "  101,\n",
       "  4,\n",
       "  1,\n",
       "  493,\n",
       "  1665,\n",
       "  49,\n",
       "  10,\n",
       "  141,\n",
       "  77,\n",
       "  80,\n",
       "  12,\n",
       "  36,\n",
       "  14,\n",
       "  507,\n",
       "  6,\n",
       "  15,\n",
       "  77,\n",
       "  80,\n",
       "  11,\n",
       "  281,\n",
       "  70,\n",
       "  89,\n",
       "  12,\n",
       "  36],\n",
       " [1,\n",
       "  30,\n",
       "  73,\n",
       "  33,\n",
       "  153,\n",
       "  26,\n",
       "  464,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  30,\n",
       "  73,\n",
       "  13,\n",
       "  109,\n",
       "  14,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [30,\n",
       "  73,\n",
       "  33,\n",
       "  233,\n",
       "  14,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  11,\n",
       "  105,\n",
       "  2085,\n",
       "  73,\n",
       "  13,\n",
       "  109,\n",
       "  10,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [11,\n",
       "  1,\n",
       "  234,\n",
       "  5,\n",
       "  346,\n",
       "  347,\n",
       "  84,\n",
       "  9,\n",
       "  104,\n",
       "  514,\n",
       "  6,\n",
       "  95,\n",
       "  97,\n",
       "  7,\n",
       "  300,\n",
       "  384,\n",
       "  5,\n",
       "  734,\n",
       "  4,\n",
       "  1,\n",
       "  653,\n",
       "  301,\n",
       "  515,\n",
       "  8,\n",
       "  1047,\n",
       "  14,\n",
       "  248,\n",
       "  478,\n",
       "  301,\n",
       "  3191,\n",
       "  185,\n",
       "  131,\n",
       "  6,\n",
       "  714,\n",
       "  7,\n",
       "  1459,\n",
       "  9,\n",
       "  1,\n",
       "  1275,\n",
       "  3192,\n",
       "  5,\n",
       "  346,\n",
       "  347,\n",
       "  84,\n",
       "  4,\n",
       "  3193,\n",
       "  3194,\n",
       "  2071,\n",
       "  730,\n",
       "  125,\n",
       "  8,\n",
       "  3195,\n",
       "  643,\n",
       "  24,\n",
       "  115],\n",
       " [8,\n",
       "  269,\n",
       "  9,\n",
       "  225,\n",
       "  24,\n",
       "  82,\n",
       "  9,\n",
       "  7,\n",
       "  350,\n",
       "  3196,\n",
       "  126,\n",
       "  101,\n",
       "  6,\n",
       "  714,\n",
       "  7,\n",
       "  82,\n",
       "  34,\n",
       "  3197,\n",
       "  1272,\n",
       "  26,\n",
       "  3198,\n",
       "  1,\n",
       "  1105,\n",
       "  431,\n",
       "  21,\n",
       "  68,\n",
       "  376,\n",
       "  73,\n",
       "  19,\n",
       "  499,\n",
       "  2,\n",
       "  3,\n",
       "  197,\n",
       "  8,\n",
       "  24,\n",
       "  58,\n",
       "  6,\n",
       "  3199,\n",
       "  105,\n",
       "  1811,\n",
       "  4,\n",
       "  7,\n",
       "  3200,\n",
       "  3201,\n",
       "  1,\n",
       "  1002,\n",
       "  61,\n",
       "  23,\n",
       "  30,\n",
       "  132,\n",
       "  21,\n",
       "  716,\n",
       "  19,\n",
       "  83,\n",
       "  4,\n",
       "  1,\n",
       "  3202,\n",
       "  23,\n",
       "  132,\n",
       "  21,\n",
       "  68,\n",
       "  376,\n",
       "  19,\n",
       "  499,\n",
       "  2,\n",
       "  3,\n",
       "  197],\n",
       " [6,\n",
       "  1765,\n",
       "  1,\n",
       "  693,\n",
       "  5,\n",
       "  491,\n",
       "  9,\n",
       "  44,\n",
       "  21,\n",
       "  298,\n",
       "  4,\n",
       "  292,\n",
       "  10,\n",
       "  3203,\n",
       "  35,\n",
       "  1091,\n",
       "  2010,\n",
       "  3204,\n",
       "  2087,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  42,\n",
       "  1199,\n",
       "  1933,\n",
       "  1,\n",
       "  406,\n",
       "  212,\n",
       "  5,\n",
       "  3205,\n",
       "  688,\n",
       "  1,\n",
       "  212,\n",
       "  5,\n",
       "  3206,\n",
       "  6,\n",
       "  15,\n",
       "  7,\n",
       "  155,\n",
       "  35,\n",
       "  1091,\n",
       "  166,\n",
       "  3207,\n",
       "  42,\n",
       "  562,\n",
       "  1,\n",
       "  212,\n",
       "  5,\n",
       "  3208,\n",
       "  1349,\n",
       "  9,\n",
       "  3209,\n",
       "  145,\n",
       "  136,\n",
       "  283,\n",
       "  810,\n",
       "  2087,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [2088,\n",
       "  226,\n",
       "  1223,\n",
       "  2089,\n",
       "  129,\n",
       "  439,\n",
       "  123,\n",
       "  214,\n",
       "  9,\n",
       "  2090,\n",
       "  8,\n",
       "  1039,\n",
       "  805,\n",
       "  256,\n",
       "  1880,\n",
       "  781,\n",
       "  4,\n",
       "  3210,\n",
       "  36,\n",
       "  48,\n",
       "  129,\n",
       "  3211,\n",
       "  1,\n",
       "  18,\n",
       "  5,\n",
       "  3212,\n",
       "  1194,\n",
       "  513,\n",
       "  648,\n",
       "  3213,\n",
       "  9,\n",
       "  2061,\n",
       "  1,\n",
       "  534,\n",
       "  3214,\n",
       "  304,\n",
       "  3215,\n",
       "  8,\n",
       "  1,\n",
       "  1252,\n",
       "  128,\n",
       "  6,\n",
       "  262,\n",
       "  412,\n",
       "  2090,\n",
       "  206,\n",
       "  1,\n",
       "  1124,\n",
       "  5,\n",
       "  1,\n",
       "  3216,\n",
       "  4,\n",
       "  3217,\n",
       "  163,\n",
       "  8,\n",
       "  68,\n",
       "  2088,\n",
       "  226,\n",
       "  1223,\n",
       "  2089,\n",
       "  3218],\n",
       " [11,\n",
       "  34,\n",
       "  265,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  30,\n",
       "  3219,\n",
       "  62,\n",
       "  97,\n",
       "  26,\n",
       "  457,\n",
       "  2,\n",
       "  3,\n",
       "  747,\n",
       "  42,\n",
       "  562,\n",
       "  1,\n",
       "  623,\n",
       "  23,\n",
       "  3220,\n",
       "  1015,\n",
       "  252,\n",
       "  3221,\n",
       "  13,\n",
       "  1,\n",
       "  30,\n",
       "  34,\n",
       "  13,\n",
       "  72,\n",
       "  9,\n",
       "  941,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  3222,\n",
       "  2,\n",
       "  3,\n",
       "  747,\n",
       "  97,\n",
       "  7,\n",
       "  2055,\n",
       "  1381,\n",
       "  19,\n",
       "  883,\n",
       "  170,\n",
       "  42,\n",
       "  2056,\n",
       "  9,\n",
       "  2057,\n",
       "  215,\n",
       "  5,\n",
       "  7,\n",
       "  30,\n",
       "  61,\n",
       "  23,\n",
       "  810,\n",
       "  30,\n",
       "  8,\n",
       "  1,\n",
       "  139,\n",
       "  136],\n",
       " [1,\n",
       "  22,\n",
       "  13,\n",
       "  118,\n",
       "  593,\n",
       "  10,\n",
       "  7,\n",
       "  30,\n",
       "  73,\n",
       "  370,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  7,\n",
       "  720,\n",
       "  837,\n",
       "  5,\n",
       "  613,\n",
       "  433,\n",
       "  29,\n",
       "  13,\n",
       "  1,\n",
       "  126,\n",
       "  193,\n",
       "  42,\n",
       "  240,\n",
       "  239,\n",
       "  126,\n",
       "  218,\n",
       "  174,\n",
       "  21,\n",
       "  7,\n",
       "  239,\n",
       "  22,\n",
       "  1040,\n",
       "  30,\n",
       "  73,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [11,\n",
       "  434,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  99,\n",
       "  496,\n",
       "  2091,\n",
       "  295,\n",
       "  37,\n",
       "  42,\n",
       "  3223,\n",
       "  7,\n",
       "  1357,\n",
       "  3224,\n",
       "  168,\n",
       "  34,\n",
       "  3225,\n",
       "  1,\n",
       "  212,\n",
       "  5,\n",
       "  3226,\n",
       "  218,\n",
       "  8,\n",
       "  1,\n",
       "  46,\n",
       "  1617,\n",
       "  18,\n",
       "  1,\n",
       "  99,\n",
       "  496,\n",
       "  81,\n",
       "  295,\n",
       "  37,\n",
       "  5,\n",
       "  434,\n",
       "  99,\n",
       "  8,\n",
       "  48,\n",
       "  263],\n",
       " [106,\n",
       "  32,\n",
       "  3227,\n",
       "  10,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  14,\n",
       "  7,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  3228,\n",
       "  65,\n",
       "  67,\n",
       "  50,\n",
       "  10,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  14,\n",
       "  68,\n",
       "  331,\n",
       "  863,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  3229,\n",
       "  42,\n",
       "  6,\n",
       "  3230,\n",
       "  3231,\n",
       "  14,\n",
       "  7,\n",
       "  3232,\n",
       "  5,\n",
       "  1023,\n",
       "  480,\n",
       "  3233],\n",
       " [1,\n",
       "  171,\n",
       "  6,\n",
       "  18,\n",
       "  13,\n",
       "  429,\n",
       "  180,\n",
       "  84,\n",
       "  7,\n",
       "  529,\n",
       "  526,\n",
       "  629,\n",
       "  171,\n",
       "  42,\n",
       "  33,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  960,\n",
       "  735,\n",
       "  22,\n",
       "  1805,\n",
       "  1806,\n",
       "  1807,\n",
       "  3234,\n",
       "  13,\n",
       "  224,\n",
       "  14,\n",
       "  429,\n",
       "  180,\n",
       "  84,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  583,\n",
       "  1235],\n",
       " [195,\n",
       "  222,\n",
       "  5,\n",
       "  1,\n",
       "  3235,\n",
       "  3236,\n",
       "  5,\n",
       "  1,\n",
       "  1642,\n",
       "  3237,\n",
       "  6,\n",
       "  50,\n",
       "  68,\n",
       "  517,\n",
       "  3238,\n",
       "  271,\n",
       "  199,\n",
       "  21,\n",
       "  490,\n",
       "  557,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  10,\n",
       "  1,\n",
       "  28,\n",
       "  803,\n",
       "  21,\n",
       "  1,\n",
       "  3239,\n",
       "  3240,\n",
       "  50,\n",
       "  1,\n",
       "  163,\n",
       "  10,\n",
       "  1,\n",
       "  490,\n",
       "  81,\n",
       "  557,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  5,\n",
       "  271,\n",
       "  3241,\n",
       "  3242],\n",
       " [11,\n",
       "  305,\n",
       "  44,\n",
       "  6,\n",
       "  76,\n",
       "  95,\n",
       "  7,\n",
       "  29,\n",
       "  61,\n",
       "  23,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  11,\n",
       "  296,\n",
       "  6,\n",
       "  76,\n",
       "  508,\n",
       "  1,\n",
       "  235,\n",
       "  286,\n",
       "  5,\n",
       "  1,\n",
       "  126,\n",
       "  61,\n",
       "  1177,\n",
       "  4,\n",
       "  3243,\n",
       "  49,\n",
       "  42,\n",
       "  32,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  177,\n",
       "  3244,\n",
       "  1252,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  577,\n",
       "  119,\n",
       "  578,\n",
       "  1217,\n",
       "  5,\n",
       "  579,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  11,\n",
       "  231,\n",
       "  579,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  121,\n",
       "  576,\n",
       "  119,\n",
       "  3245,\n",
       "  11,\n",
       "  3246,\n",
       "  394,\n",
       "  10,\n",
       "  1,\n",
       "  429,\n",
       "  171,\n",
       "  180,\n",
       "  84,\n",
       "  14,\n",
       "  68,\n",
       "  2092,\n",
       "  623,\n",
       "  5,\n",
       "  3247],\n",
       " [6,\n",
       "  76,\n",
       "  728,\n",
       "  68,\n",
       "  1812,\n",
       "  5,\n",
       "  24,\n",
       "  128,\n",
       "  4,\n",
       "  34,\n",
       "  5,\n",
       "  2093,\n",
       "  4,\n",
       "  1302,\n",
       "  53,\n",
       "  6,\n",
       "  851,\n",
       "  7,\n",
       "  1860,\n",
       "  290,\n",
       "  34,\n",
       "  24,\n",
       "  128,\n",
       "  3248,\n",
       "  326,\n",
       "  5,\n",
       "  1,\n",
       "  479,\n",
       "  545,\n",
       "  3249,\n",
       "  7,\n",
       "  13,\n",
       "  642,\n",
       "  124,\n",
       "  793,\n",
       "  4,\n",
       "  1,\n",
       "  205,\n",
       "  326,\n",
       "  5,\n",
       "  1,\n",
       "  479,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  792,\n",
       "  5,\n",
       "  2093,\n",
       "  4,\n",
       "  1302,\n",
       "  53,\n",
       "  3250],\n",
       " [145,\n",
       "  592,\n",
       "  9,\n",
       "  972,\n",
       "  48,\n",
       "  304,\n",
       "  13,\n",
       "  9,\n",
       "  18,\n",
       "  7,\n",
       "  162,\n",
       "  369,\n",
       "  34,\n",
       "  13,\n",
       "  3251,\n",
       "  11,\n",
       "  709,\n",
       "  483,\n",
       "  764,\n",
       "  290,\n",
       "  31,\n",
       "  1,\n",
       "  189,\n",
       "  162,\n",
       "  451,\n",
       "  4,\n",
       "  690,\n",
       "  156,\n",
       "  11,\n",
       "  70,\n",
       "  3252,\n",
       "  1083,\n",
       "  13,\n",
       "  72,\n",
       "  9,\n",
       "  1,\n",
       "  1083,\n",
       "  11,\n",
       "  1,\n",
       "  189,\n",
       "  162,\n",
       "  8,\n",
       "  451,\n",
       "  4,\n",
       "  690,\n",
       "  156],\n",
       " [6,\n",
       "  18,\n",
       "  243,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  201,\n",
       "  11,\n",
       "  1718,\n",
       "  14,\n",
       "  331,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  1519,\n",
       "  69,\n",
       "  14,\n",
       "  1,\n",
       "  243,\n",
       "  527,\n",
       "  244,\n",
       "  4,\n",
       "  245,\n",
       "  201,\n",
       "  7,\n",
       "  78,\n",
       "  166,\n",
       "  5,\n",
       "  1122,\n",
       "  676,\n",
       "  415,\n",
       "  5,\n",
       "  326,\n",
       "  4,\n",
       "  444,\n",
       "  14,\n",
       "  528,\n",
       "  769,\n",
       "  214,\n",
       "  9,\n",
       "  1,\n",
       "  529,\n",
       "  677],\n",
       " [6,\n",
       "  121,\n",
       "  24,\n",
       "  433,\n",
       "  49,\n",
       "  8,\n",
       "  7,\n",
       "  60,\n",
       "  592,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  29,\n",
       "  102,\n",
       "  11,\n",
       "  41,\n",
       "  1813,\n",
       "  96,\n",
       "  25,\n",
       "  4,\n",
       "  60,\n",
       "  257,\n",
       "  299,\n",
       "  19,\n",
       "  6,\n",
       "  18,\n",
       "  102,\n",
       "  131,\n",
       "  96,\n",
       "  25,\n",
       "  11,\n",
       "  1093,\n",
       "  1,\n",
       "  209,\n",
       "  41,\n",
       "  19,\n",
       "  222],\n",
       " [1,\n",
       "  1751,\n",
       "  3253,\n",
       "  11,\n",
       "  24,\n",
       "  19,\n",
       "  13,\n",
       "  1,\n",
       "  1381,\n",
       "  14,\n",
       "  1985,\n",
       "  378,\n",
       "  3254,\n",
       "  995,\n",
       "  5,\n",
       "  457,\n",
       "  2,\n",
       "  3,\n",
       "  1100,\n",
       "  24,\n",
       "  19,\n",
       "  13,\n",
       "  68,\n",
       "  3255,\n",
       "  5,\n",
       "  1,\n",
       "  683,\n",
       "  1559,\n",
       "  5,\n",
       "  63,\n",
       "  3256,\n",
       "  19,\n",
       "  5,\n",
       "  457,\n",
       "  2,\n",
       "  3,\n",
       "  1100,\n",
       "  7,\n",
       "  82,\n",
       "  11,\n",
       "  78,\n",
       "  168,\n",
       "  418,\n",
       "  5,\n",
       "  63,\n",
       "  61,\n",
       "  23,\n",
       "  248,\n",
       "  91,\n",
       "  149],\n",
       " [6,\n",
       "  952,\n",
       "  7,\n",
       "  300,\n",
       "  2094,\n",
       "  1096,\n",
       "  19,\n",
       "  31,\n",
       "  1,\n",
       "  391,\n",
       "  238,\n",
       "  19,\n",
       "  10,\n",
       "  7,\n",
       "  684,\n",
       "  799,\n",
       "  446,\n",
       "  687,\n",
       "  1106,\n",
       "  309,\n",
       "  1107,\n",
       "  4,\n",
       "  1108,\n",
       "  329,\n",
       "  6,\n",
       "  421,\n",
       "  178,\n",
       "  26,\n",
       "  46,\n",
       "  7,\n",
       "  2095,\n",
       "  2096,\n",
       "  446,\n",
       "  687,\n",
       "  1106,\n",
       "  309,\n",
       "  1107,\n",
       "  4,\n",
       "  1108,\n",
       "  329,\n",
       "  10,\n",
       "  98,\n",
       "  31,\n",
       "  68,\n",
       "  2097,\n",
       "  23,\n",
       "  1,\n",
       "  2098,\n",
       "  190,\n",
       "  2099,\n",
       "  21,\n",
       "  24,\n",
       "  22],\n",
       " [1,\n",
       "  1678,\n",
       "  434,\n",
       "  837,\n",
       "  13,\n",
       "  68,\n",
       "  99,\n",
       "  434,\n",
       "  19,\n",
       "  175,\n",
       "  14,\n",
       "  1243,\n",
       "  7,\n",
       "  158,\n",
       "  168,\n",
       "  1679,\n",
       "  78,\n",
       "  122,\n",
       "  295,\n",
       "  37,\n",
       "  1,\n",
       "  3257,\n",
       "  5,\n",
       "  68,\n",
       "  99,\n",
       "  496,\n",
       "  19,\n",
       "  13,\n",
       "  34,\n",
       "  98,\n",
       "  13,\n",
       "  650,\n",
       "  2053,\n",
       "  4,\n",
       "  98,\n",
       "  129,\n",
       "  123,\n",
       "  268,\n",
       "  9,\n",
       "  104,\n",
       "  1132,\n",
       "  11,\n",
       "  7,\n",
       "  1180,\n",
       "  5,\n",
       "  434,\n",
       "  783,\n",
       "  295,\n",
       "  37],\n",
       " [1986,\n",
       "  13,\n",
       "  1987,\n",
       "  728,\n",
       "  31,\n",
       "  68,\n",
       "  345,\n",
       "  322,\n",
       "  62,\n",
       "  643,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  114,\n",
       "  127,\n",
       "  34,\n",
       "  1,\n",
       "  30,\n",
       "  157,\n",
       "  13,\n",
       "  1988,\n",
       "  26,\n",
       "  1,\n",
       "  75,\n",
       "  5,\n",
       "  149,\n",
       "  8,\n",
       "  42,\n",
       "  98,\n",
       "  3258,\n",
       "  9,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  5,\n",
       "  157,\n",
       "  114,\n",
       "  127,\n",
       "  63,\n",
       "  34,\n",
       "  194,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  104,\n",
       "  766,\n",
       "  72],\n",
       " [6,\n",
       "  232,\n",
       "  24,\n",
       "  58,\n",
       "  23,\n",
       "  54,\n",
       "  12,\n",
       "  40,\n",
       "  7,\n",
       "  307,\n",
       "  85,\n",
       "  22,\n",
       "  42,\n",
       "  13,\n",
       "  138,\n",
       "  8,\n",
       "  741,\n",
       "  8,\n",
       "  185,\n",
       "  3259,\n",
       "  58,\n",
       "  67,\n",
       "  196,\n",
       "  179,\n",
       "  23,\n",
       "  1,\n",
       "  54,\n",
       "  12,\n",
       "  40,\n",
       "  22,\n",
       "  42,\n",
       "  13,\n",
       "  7,\n",
       "  22,\n",
       "  733,\n",
       "  15,\n",
       "  8,\n",
       "  38,\n",
       "  4,\n",
       "  34,\n",
       "  129,\n",
       "  123,\n",
       "  15,\n",
       "  8,\n",
       "  697,\n",
       "  235,\n",
       "  143,\n",
       "  2100],\n",
       " [11,\n",
       "  1,\n",
       "  41,\n",
       "  19,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  25,\n",
       "  9,\n",
       "  348,\n",
       "  7,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  23,\n",
       "  1,\n",
       "  209,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  717,\n",
       "  14,\n",
       "  844,\n",
       "  1989,\n",
       "  1246,\n",
       "  14,\n",
       "  116,\n",
       "  3260,\n",
       "  41,\n",
       "  19,\n",
       "  13,\n",
       "  7,\n",
       "  100,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  19,\n",
       "  50,\n",
       "  10,\n",
       "  822,\n",
       "  14,\n",
       "  155,\n",
       "  116,\n",
       "  173,\n",
       "  4,\n",
       "  559,\n",
       "  567],\n",
       " [1,\n",
       "  391,\n",
       "  310,\n",
       "  13,\n",
       "  3261,\n",
       "  26,\n",
       "  1,\n",
       "  1054,\n",
       "  22,\n",
       "  278,\n",
       "  2,\n",
       "  3,\n",
       "  43,\n",
       "  131,\n",
       "  42,\n",
       "  240,\n",
       "  84,\n",
       "  1308,\n",
       "  21,\n",
       "  2101,\n",
       "  7,\n",
       "  1197,\n",
       "  5,\n",
       "  3262,\n",
       "  125,\n",
       "  1054,\n",
       "  278,\n",
       "  2,\n",
       "  3,\n",
       "  43,\n",
       "  13,\n",
       "  7,\n",
       "  310,\n",
       "  5,\n",
       "  84,\n",
       "  655,\n",
       "  1308,\n",
       "  459,\n",
       "  21,\n",
       "  1,\n",
       "  495,\n",
       "  188,\n",
       "  5,\n",
       "  2101,\n",
       "  399,\n",
       "  10,\n",
       "  1689,\n",
       "  3263,\n",
       "  363,\n",
       "  352,\n",
       "  3264,\n",
       "  3265,\n",
       "  4,\n",
       "  3266,\n",
       "  3267],\n",
       " [6,\n",
       "  952,\n",
       "  7,\n",
       "  300,\n",
       "  2094,\n",
       "  1096,\n",
       "  19,\n",
       "  31,\n",
       "  1,\n",
       "  391,\n",
       "  238,\n",
       "  19,\n",
       "  10,\n",
       "  7,\n",
       "  684,\n",
       "  799,\n",
       "  446,\n",
       "  687,\n",
       "  1106,\n",
       "  309,\n",
       "  1107,\n",
       "  4,\n",
       "  1108,\n",
       "  329,\n",
       "  6,\n",
       "  421,\n",
       "  178,\n",
       "  26,\n",
       "  46,\n",
       "  7,\n",
       "  2095,\n",
       "  2096,\n",
       "  446,\n",
       "  687,\n",
       "  1106,\n",
       "  309,\n",
       "  1107,\n",
       "  4,\n",
       "  1108,\n",
       "  329,\n",
       "  10,\n",
       "  98,\n",
       "  31,\n",
       "  68,\n",
       "  2097,\n",
       "  23,\n",
       "  1,\n",
       "  2098,\n",
       "  190,\n",
       "  2099,\n",
       "  21,\n",
       "  24,\n",
       "  22],\n",
       " [1,\n",
       "  136,\n",
       "  585,\n",
       "  85,\n",
       "  28,\n",
       "  13,\n",
       "  118,\n",
       "  593,\n",
       "  10,\n",
       "  59,\n",
       "  8,\n",
       "  105,\n",
       "  3268,\n",
       "  4,\n",
       "  3269,\n",
       "  398,\n",
       "  1631,\n",
       "  26,\n",
       "  1,\n",
       "  3270,\n",
       "  5,\n",
       "  1286,\n",
       "  3271,\n",
       "  705,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  101,\n",
       "  30,\n",
       "  132,\n",
       "  67,\n",
       "  153,\n",
       "  26,\n",
       "  464,\n",
       "  59,\n",
       "  8,\n",
       "  105,\n",
       "  398,\n",
       "  4,\n",
       "  817,\n",
       "  10,\n",
       "  1,\n",
       "  356,\n",
       "  370,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [51,\n",
       "  1,\n",
       "  41,\n",
       "  65,\n",
       "  371,\n",
       "  15,\n",
       "  8,\n",
       "  24,\n",
       "  58,\n",
       "  32,\n",
       "  2025,\n",
       "  155,\n",
       "  116,\n",
       "  291,\n",
       "  1372,\n",
       "  50,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  11,\n",
       "  1,\n",
       "  41,\n",
       "  19,\n",
       "  6,\n",
       "  15,\n",
       "  51,\n",
       "  427,\n",
       "  691,\n",
       "  4,\n",
       "  1,\n",
       "  275,\n",
       "  1065,\n",
       "  5,\n",
       "  1,\n",
       "  85,\n",
       "  691,\n",
       "  4,\n",
       "  50,\n",
       "  7,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  14,\n",
       "  155,\n",
       "  116,\n",
       "  173,\n",
       "  10,\n",
       "  102,\n",
       "  96,\n",
       "  2,\n",
       "  3,\n",
       "  88],\n",
       " [328,\n",
       "  32,\n",
       "  61,\n",
       "  23,\n",
       "  91,\n",
       "  113,\n",
       "  42,\n",
       "  1257,\n",
       "  643,\n",
       "  1,\n",
       "  1258,\n",
       "  34,\n",
       "  72,\n",
       "  63,\n",
       "  194,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  114,\n",
       "  127,\n",
       "  675,\n",
       "  1783,\n",
       "  95,\n",
       "  728,\n",
       "  1293,\n",
       "  1784,\n",
       "  26,\n",
       "  1785,\n",
       "  1,\n",
       "  207,\n",
       "  1786,\n",
       "  26,\n",
       "  1,\n",
       "  91,\n",
       "  113,\n",
       "  42,\n",
       "  270,\n",
       "  34,\n",
       "  63,\n",
       "  1017,\n",
       "  8,\n",
       "  72,\n",
       "  149,\n",
       "  237,\n",
       "  9,\n",
       "  95,\n",
       "  72,\n",
       "  365,\n",
       "  114,\n",
       "  127],\n",
       " [393,\n",
       "  6,\n",
       "  849,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  717,\n",
       "  12,\n",
       "  40,\n",
       "  617,\n",
       "  98,\n",
       "  13,\n",
       "  733,\n",
       "  15,\n",
       "  8,\n",
       "  1,\n",
       "  235,\n",
       "  3272,\n",
       "  11,\n",
       "  572,\n",
       "  3273,\n",
       "  58,\n",
       "  67,\n",
       "  196,\n",
       "  179,\n",
       "  23,\n",
       "  1,\n",
       "  54,\n",
       "  12,\n",
       "  40,\n",
       "  22,\n",
       "  42,\n",
       "  13,\n",
       "  7,\n",
       "  22,\n",
       "  733,\n",
       "  15,\n",
       "  8,\n",
       "  38,\n",
       "  4,\n",
       "  34,\n",
       "  129,\n",
       "  123,\n",
       "  15,\n",
       "  8,\n",
       "  697,\n",
       "  235,\n",
       "  143,\n",
       "  2100],\n",
       " [11,\n",
       "  1,\n",
       "  683,\n",
       "  1312,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  367,\n",
       "  207,\n",
       "  313,\n",
       "  100,\n",
       "  22,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  42,\n",
       "  240,\n",
       "  450,\n",
       "  11,\n",
       "  698,\n",
       "  21,\n",
       "  800,\n",
       "  571,\n",
       "  9,\n",
       "  1313,\n",
       "  153,\n",
       "  21,\n",
       "  206,\n",
       "  83,\n",
       "  1061,\n",
       "  30,\n",
       "  3274,\n",
       "  344,\n",
       "  11,\n",
       "  30,\n",
       "  467,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  800,\n",
       "  28,\n",
       "  21,\n",
       "  1,\n",
       "  367,\n",
       "  1062,\n",
       "  310,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37],\n",
       " [26,\n",
       "  3275,\n",
       "  513,\n",
       "  23,\n",
       "  1,\n",
       "  2102,\n",
       "  30,\n",
       "  3276,\n",
       "  72,\n",
       "  9,\n",
       "  34,\n",
       "  138,\n",
       "  8,\n",
       "  1099,\n",
       "  2,\n",
       "  3,\n",
       "  197,\n",
       "  1,\n",
       "  3277,\n",
       "  128,\n",
       "  3278,\n",
       "  327,\n",
       "  852,\n",
       "  545,\n",
       "  1,\n",
       "  513,\n",
       "  32,\n",
       "  214,\n",
       "  1,\n",
       "  212,\n",
       "  1,\n",
       "  118,\n",
       "  13,\n",
       "  7,\n",
       "  1356,\n",
       "  5,\n",
       "  1,\n",
       "  3279,\n",
       "  161,\n",
       "  138,\n",
       "  8,\n",
       "  1099,\n",
       "  2,\n",
       "  3,\n",
       "  197],\n",
       " [3280,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  750,\n",
       "  7,\n",
       "  3281,\n",
       "  2103,\n",
       "  2104,\n",
       "  3282,\n",
       "  330,\n",
       "  9,\n",
       "  1872,\n",
       "  1,\n",
       "  191,\n",
       "  771,\n",
       "  3283,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  97,\n",
       "  7,\n",
       "  700,\n",
       "  128,\n",
       "  9,\n",
       "  2103,\n",
       "  2104,\n",
       "  78,\n",
       "  11,\n",
       "  293,\n",
       "  303,\n",
       "  42,\n",
       "  3284,\n",
       "  3285,\n",
       "  51,\n",
       "  1,\n",
       "  125,\n",
       "  8,\n",
       "  306,\n",
       "  4,\n",
       "  51,\n",
       "  795,\n",
       "  8,\n",
       "  804,\n",
       "  1888,\n",
       "  11],\n",
       " [6,\n",
       "  95,\n",
       "  1820,\n",
       "  2105,\n",
       "  34,\n",
       "  61,\n",
       "  23,\n",
       "  346,\n",
       "  347,\n",
       "  84,\n",
       "  24,\n",
       "  234,\n",
       "  154,\n",
       "  104,\n",
       "  7,\n",
       "  514,\n",
       "  234,\n",
       "  5,\n",
       "  1883,\n",
       "  3286,\n",
       "  26,\n",
       "  3287,\n",
       "  1,\n",
       "  300,\n",
       "  384,\n",
       "  734,\n",
       "  4,\n",
       "  1,\n",
       "  653,\n",
       "  301,\n",
       "  1775,\n",
       "  6,\n",
       "  1776,\n",
       "  1,\n",
       "  112,\n",
       "  301,\n",
       "  513,\n",
       "  5,\n",
       "  346,\n",
       "  347,\n",
       "  84,\n",
       "  4,\n",
       "  1777,\n",
       "  1,\n",
       "  653,\n",
       "  301,\n",
       "  515],\n",
       " [11,\n",
       "  1,\n",
       "  683,\n",
       "  1312,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  367,\n",
       "  207,\n",
       "  313,\n",
       "  100,\n",
       "  22,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  42,\n",
       "  240,\n",
       "  450,\n",
       "  11,\n",
       "  698,\n",
       "  21,\n",
       "  800,\n",
       "  571,\n",
       "  9,\n",
       "  1313,\n",
       "  153,\n",
       "  21,\n",
       "  206,\n",
       "  83,\n",
       "  1061,\n",
       "  30,\n",
       "  3288,\n",
       "  15,\n",
       "  1,\n",
       "  836,\n",
       "  450,\n",
       "  21,\n",
       "  1,\n",
       "  207,\n",
       "  313,\n",
       "  100,\n",
       "  22,\n",
       "  180,\n",
       "  4,\n",
       "  246,\n",
       "  37,\n",
       "  9,\n",
       "  535,\n",
       "  1,\n",
       "  467,\n",
       "  5,\n",
       "  18,\n",
       "  5,\n",
       "  111,\n",
       "  624,\n",
       "  843],\n",
       " [317,\n",
       "  134,\n",
       "  36,\n",
       "  13,\n",
       "  7,\n",
       "  75,\n",
       "  5,\n",
       "  143,\n",
       "  975,\n",
       "  15,\n",
       "  11,\n",
       "  355,\n",
       "  3289,\n",
       "  1075,\n",
       "  48,\n",
       "  873,\n",
       "  13,\n",
       "  327,\n",
       "  641,\n",
       "  14,\n",
       "  1,\n",
       "  317,\n",
       "  143,\n",
       "  314,\n",
       "  15,\n",
       "  11,\n",
       "  436,\n",
       "  134,\n",
       "  36],\n",
       " [662,\n",
       "  6,\n",
       "  732,\n",
       "  1,\n",
       "  106,\n",
       "  5,\n",
       "  1,\n",
       "  60,\n",
       "  27,\n",
       "  35,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  46,\n",
       "  75,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  69,\n",
       "  1,\n",
       "  27,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  41,\n",
       "  19,\n",
       "  11,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  582,\n",
       "  832,\n",
       "  46,\n",
       "  22,\n",
       "  33,\n",
       "  564,\n",
       "  21,\n",
       "  1,\n",
       "  54,\n",
       "  310,\n",
       "  12,\n",
       "  40,\n",
       "  1,\n",
       "  49,\n",
       "  11,\n",
       "  1,\n",
       "  44,\n",
       "  572,\n",
       "  35,\n",
       "  558,\n",
       "  67,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  1334,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [6,\n",
       "  1389,\n",
       "  1,\n",
       "  200,\n",
       "  8,\n",
       "  105,\n",
       "  1390,\n",
       "  21,\n",
       "  1391,\n",
       "  9,\n",
       "  420,\n",
       "  191,\n",
       "  462,\n",
       "  426,\n",
       "  83,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  1392,\n",
       "  42,\n",
       "  1074,\n",
       "  1393,\n",
       "  354,\n",
       "  5,\n",
       "  420,\n",
       "  191,\n",
       "  83,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  14,\n",
       "  39,\n",
       "  191,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [6,\n",
       "  1389,\n",
       "  1,\n",
       "  200,\n",
       "  8,\n",
       "  105,\n",
       "  1390,\n",
       "  21,\n",
       "  1391,\n",
       "  9,\n",
       "  420,\n",
       "  191,\n",
       "  462,\n",
       "  426,\n",
       "  83,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  1392,\n",
       "  42,\n",
       "  1074,\n",
       "  1393,\n",
       "  354,\n",
       "  5,\n",
       "  420,\n",
       "  191,\n",
       "  83,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  14,\n",
       "  39,\n",
       "  191,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [6,\n",
       "  18,\n",
       "  102,\n",
       "  131,\n",
       "  96,\n",
       "  25,\n",
       "  11,\n",
       "  1093,\n",
       "  1,\n",
       "  209,\n",
       "  41,\n",
       "  19,\n",
       "  3290,\n",
       "  18,\n",
       "  7,\n",
       "  100,\n",
       "  371,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  353,\n",
       "  22,\n",
       "  4,\n",
       "  18,\n",
       "  102,\n",
       "  96,\n",
       "  25,\n",
       "  11,\n",
       "  371,\n",
       "  3291,\n",
       "  648,\n",
       "  282],\n",
       " [1,\n",
       "  255,\n",
       "  1362,\n",
       "  231,\n",
       "  1363,\n",
       "  1364,\n",
       "  104,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  1,\n",
       "  29,\n",
       "  33,\n",
       "  50,\n",
       "  23,\n",
       "  1,\n",
       "  44,\n",
       "  4,\n",
       "  867,\n",
       "  236,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  213,\n",
       "  131,\n",
       "  12,\n",
       "  40],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  39,\n",
       "  52,\n",
       "  14,\n",
       "  39,\n",
       "  191,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37,\n",
       "  6,\n",
       "  1389,\n",
       "  1,\n",
       "  200,\n",
       "  8,\n",
       "  105,\n",
       "  1390,\n",
       "  21,\n",
       "  1391,\n",
       "  9,\n",
       "  420,\n",
       "  191,\n",
       "  462,\n",
       "  426,\n",
       "  83,\n",
       "  10,\n",
       "  1,\n",
       "  39,\n",
       "  1392,\n",
       "  42,\n",
       "  1074,\n",
       "  1393,\n",
       "  354,\n",
       "  5,\n",
       "  420,\n",
       "  191,\n",
       "  83,\n",
       "  74,\n",
       "  71,\n",
       "  2,\n",
       "  3,\n",
       "  37],\n",
       " [589,\n",
       "  6,\n",
       "  69,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  65,\n",
       "  14,\n",
       "  24,\n",
       "  132,\n",
       "  10,\n",
       "  1,\n",
       "  432,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  662,\n",
       "  6,\n",
       "  732,\n",
       "  1,\n",
       "  106,\n",
       "  5,\n",
       "  1,\n",
       "  60,\n",
       "  27,\n",
       "  35,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  46,\n",
       "  75,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  69,\n",
       "  1,\n",
       "  27,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  41,\n",
       "  19,\n",
       "  11,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  17,\n",
       "  161,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  33,\n",
       "  15,\n",
       "  11,\n",
       "  1293,\n",
       "  3292,\n",
       "  4,\n",
       "  3293,\n",
       "  1190],\n",
       " [1,\n",
       "  3294,\n",
       "  3295,\n",
       "  247,\n",
       "  22,\n",
       "  6,\n",
       "  15,\n",
       "  13,\n",
       "  7,\n",
       "  647,\n",
       "  5,\n",
       "  1,\n",
       "  275,\n",
       "  187,\n",
       "  294,\n",
       "  3296,\n",
       "  2106,\n",
       "  2,\n",
       "  3,\n",
       "  43,\n",
       "  48,\n",
       "  3297,\n",
       "  9,\n",
       "  7,\n",
       "  300,\n",
       "  213,\n",
       "  5,\n",
       "  1,\n",
       "  275,\n",
       "  187,\n",
       "  2106,\n",
       "  2,\n",
       "  3,\n",
       "  43],\n",
       " [523,\n",
       "  129,\n",
       "  123,\n",
       "  7,\n",
       "  390,\n",
       "  1306,\n",
       "  5,\n",
       "  167,\n",
       "  23,\n",
       "  195,\n",
       "  115,\n",
       "  190,\n",
       "  413,\n",
       "  950,\n",
       "  5,\n",
       "  1170,\n",
       "  409,\n",
       "  4,\n",
       "  588,\n",
       "  90,\n",
       "  11,\n",
       "  7,\n",
       "  612,\n",
       "  3298,\n",
       "  5,\n",
       "  1,\n",
       "  1183,\n",
       "  5,\n",
       "  195,\n",
       "  115,\n",
       "  462,\n",
       "  409,\n",
       "  4,\n",
       "  588,\n",
       "  90],\n",
       " [48,\n",
       "  62,\n",
       "  930,\n",
       "  13,\n",
       "  881,\n",
       "  138,\n",
       "  8,\n",
       "  1,\n",
       "  62,\n",
       "  497,\n",
       "  263,\n",
       "  639,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  7,\n",
       "  514,\n",
       "  497,\n",
       "  5,\n",
       "  1,\n",
       "  46,\n",
       "  4,\n",
       "  147,\n",
       "  691,\n",
       "  154,\n",
       "  104,\n",
       "  289,\n",
       "  190,\n",
       "  1,\n",
       "  62,\n",
       "  497,\n",
       "  263,\n",
       "  639,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [1,\n",
       "  101,\n",
       "  385,\n",
       "  104,\n",
       "  417,\n",
       "  26,\n",
       "  1,\n",
       "  17,\n",
       "  38,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1229,\n",
       "  24,\n",
       "  101,\n",
       "  13,\n",
       "  1,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  232,\n",
       "  206,\n",
       "  2041,\n",
       "  605,\n",
       "  3299,\n",
       "  602,\n",
       "  30,\n",
       "  605],\n",
       " [137,\n",
       "  334,\n",
       "  8,\n",
       "  436,\n",
       "  542,\n",
       "  334,\n",
       "  234,\n",
       "  931,\n",
       "  543,\n",
       "  4,\n",
       "  544,\n",
       "  387,\n",
       "  887,\n",
       "  1,\n",
       "  137,\n",
       "  8,\n",
       "  7,\n",
       "  566,\n",
       "  8,\n",
       "  1,\n",
       "  827,\n",
       "  5,\n",
       "  7,\n",
       "  189,\n",
       "  426,\n",
       "  83,\n",
       "  1863,\n",
       "  242,\n",
       "  32,\n",
       "  771,\n",
       "  137,\n",
       "  242,\n",
       "  524,\n",
       "  26,\n",
       "  542,\n",
       "  334,\n",
       "  234,\n",
       "  543,\n",
       "  4,\n",
       "  544,\n",
       "  387],\n",
       " [98,\n",
       "  1085,\n",
       "  23,\n",
       "  1,\n",
       "  1387,\n",
       "  584,\n",
       "  52,\n",
       "  498,\n",
       "  4,\n",
       "  885,\n",
       "  36,\n",
       "  98,\n",
       "  3300,\n",
       "  584,\n",
       "  1259,\n",
       "  21,\n",
       "  1,\n",
       "  1387,\n",
       "  52,\n",
       "  498,\n",
       "  4,\n",
       "  885,\n",
       "  36,\n",
       "  4,\n",
       "  1707,\n",
       "  257,\n",
       "  1135,\n",
       "  4,\n",
       "  1240,\n",
       "  940,\n",
       "  283,\n",
       "  130,\n",
       "  3301,\n",
       "  1380,\n",
       "  26,\n",
       "  3302,\n",
       "  3303],\n",
       " [6,\n",
       "  18,\n",
       "  1335,\n",
       "  2107,\n",
       "  2,\n",
       "  3,\n",
       "  36,\n",
       "  9,\n",
       "  160,\n",
       "  372,\n",
       "  41,\n",
       "  283,\n",
       "  7,\n",
       "  2108,\n",
       "  3304,\n",
       "  3305,\n",
       "  372,\n",
       "  41,\n",
       "  133,\n",
       "  9,\n",
       "  2108,\n",
       "  827,\n",
       "  6,\n",
       "  121,\n",
       "  23,\n",
       "  1,\n",
       "  108,\n",
       "  152,\n",
       "  1335,\n",
       "  2107,\n",
       "  2,\n",
       "  3,\n",
       "  36],\n",
       " [1,\n",
       "  46,\n",
       "  75,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  69,\n",
       "  1,\n",
       "  27,\n",
       "  35,\n",
       "  19,\n",
       "  4,\n",
       "  41,\n",
       "  19,\n",
       "  11,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  35,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  424,\n",
       "  24,\n",
       "  65],\n",
       " [6,\n",
       "  262,\n",
       "  77,\n",
       "  80,\n",
       "  14,\n",
       "  604,\n",
       "  411,\n",
       "  31,\n",
       "  138,\n",
       "  8,\n",
       "  12,\n",
       "  36,\n",
       "  6,\n",
       "  76,\n",
       "  871,\n",
       "  364,\n",
       "  311,\n",
       "  297,\n",
       "  3306,\n",
       "  460,\n",
       "  23,\n",
       "  507,\n",
       "  1330,\n",
       "  5,\n",
       "  77,\n",
       "  80,\n",
       "  14,\n",
       "  3307,\n",
       "  12,\n",
       "  36],\n",
       " [1,\n",
       "  1268,\n",
       "  236,\n",
       "  5,\n",
       "  28,\n",
       "  622,\n",
       "  21,\n",
       "  846,\n",
       "  4,\n",
       "  1269,\n",
       "  583,\n",
       "  1270,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  28,\n",
       "  75,\n",
       "  12,\n",
       "  40,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  13,\n",
       "  66,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  449],\n",
       " [11,\n",
       "  1394,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  883,\n",
       "  9,\n",
       "  69,\n",
       "  30,\n",
       "  178,\n",
       "  23,\n",
       "  1,\n",
       "  54,\n",
       "  1394,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  94,\n",
       "  325,\n",
       "  3308,\n",
       "  3309,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  31,\n",
       "  281,\n",
       "  28],\n",
       " [1,\n",
       "  2109,\n",
       "  3310,\n",
       "  401,\n",
       "  2110,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  13,\n",
       "  1,\n",
       "  1383,\n",
       "  11,\n",
       "  1,\n",
       "  3311,\n",
       "  1384,\n",
       "  2109,\n",
       "  2110,\n",
       "  2,\n",
       "  3,\n",
       "  3312,\n",
       "  13,\n",
       "  68,\n",
       "  401,\n",
       "  7,\n",
       "  330,\n",
       "  4,\n",
       "  7,\n",
       "  422,\n",
       "  1384,\n",
       "  11,\n",
       "  352,\n",
       "  41,\n",
       "  3313,\n",
       "  1478,\n",
       "  4,\n",
       "  764],\n",
       " [6,\n",
       "  50,\n",
       "  7,\n",
       "  100,\n",
       "  41,\n",
       "  19,\n",
       "  23,\n",
       "  1,\n",
       "  917,\n",
       "  185,\n",
       "  5,\n",
       "  1,\n",
       "  44,\n",
       "  353,\n",
       "  22,\n",
       "  1133,\n",
       "  480,\n",
       "  63,\n",
       "  10,\n",
       "  1,\n",
       "  614,\n",
       "  20,\n",
       "  615,\n",
       "  87,\n",
       "  14,\n",
       "  1,\n",
       "  155,\n",
       "  116,\n",
       "  173,\n",
       "  342,\n",
       "  4,\n",
       "  474,\n",
       "  197,\n",
       "  105,\n",
       "  41,\n",
       "  65,\n",
       "  18,\n",
       "  155,\n",
       "  116,\n",
       "  173,\n",
       "  342,\n",
       "  4,\n",
       "  474,\n",
       "  197],\n",
       " [6,\n",
       "  15,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43,\n",
       "  1161,\n",
       "  14,\n",
       "  1,\n",
       "  3314,\n",
       "  705,\n",
       "  9,\n",
       "  3315,\n",
       "  1,\n",
       "  46,\n",
       "  1890,\n",
       "  3316,\n",
       "  374,\n",
       "  593,\n",
       "  1,\n",
       "  305,\n",
       "  236,\n",
       "  9,\n",
       "  111,\n",
       "  5,\n",
       "  1,\n",
       "  3317,\n",
       "  14,\n",
       "  1,\n",
       "  59,\n",
       "  122,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  43],\n",
       " [24,\n",
       "  323,\n",
       "  1208,\n",
       "  128,\n",
       "  11,\n",
       "  412,\n",
       "  221,\n",
       "  4,\n",
       "  322,\n",
       "  5,\n",
       "  1307,\n",
       "  443,\n",
       "  4,\n",
       "  438,\n",
       "  64,\n",
       "  412,\n",
       "  221,\n",
       "  4,\n",
       "  322,\n",
       "  5,\n",
       "  689,\n",
       "  3318,\n",
       "  1,\n",
       "  221,\n",
       "  188,\n",
       "  11,\n",
       "  105,\n",
       "  1,\n",
       "  1207,\n",
       "  443,\n",
       "  4,\n",
       "  438,\n",
       "  64,\n",
       "  4,\n",
       "  1,\n",
       "  3319,\n",
       "  128],\n",
       " [409,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  225,\n",
       "  1,\n",
       "  286,\n",
       "  5,\n",
       "  493,\n",
       "  1153,\n",
       "  15,\n",
       "  56,\n",
       "  78,\n",
       "  65,\n",
       "  366,\n",
       "  312,\n",
       "  532,\n",
       "  1328,\n",
       "  4,\n",
       "  3320,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  18,\n",
       "  56,\n",
       "  78,\n",
       "  381,\n",
       "  1186,\n",
       "  99,\n",
       "  4,\n",
       "  913,\n",
       "  9,\n",
       "  1000,\n",
       "  1311,\n",
       "  23,\n",
       "  694,\n",
       "  695],\n",
       " [2111,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  1263,\n",
       "  3321,\n",
       "  23,\n",
       "  1,\n",
       "  3322,\n",
       "  3323,\n",
       "  147,\n",
       "  31,\n",
       "  68,\n",
       "  1968,\n",
       "  3324,\n",
       "  751,\n",
       "  9,\n",
       "  1,\n",
       "  29,\n",
       "  5,\n",
       "  2111,\n",
       "  2,\n",
       "  3,\n",
       "  64,\n",
       "  31,\n",
       "  1,\n",
       "  1254,\n",
       "  3325,\n",
       "  29],\n",
       " [6,\n",
       "  214,\n",
       "  1,\n",
       "  366,\n",
       "  312,\n",
       "  894,\n",
       "  576,\n",
       "  78,\n",
       "  103,\n",
       "  21,\n",
       "  1,\n",
       "  202,\n",
       "  56,\n",
       "  78,\n",
       "  403,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  366,\n",
       "  312,\n",
       "  4,\n",
       "  405,\n",
       "  189,\n",
       "  65,\n",
       "  67,\n",
       "  66,\n",
       "  14,\n",
       "  1,\n",
       "  202,\n",
       "  204,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  152,\n",
       "  11,\n",
       "  405,\n",
       "  200,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  3326,\n",
       "  81],\n",
       " [6,\n",
       "  492,\n",
       "  3327,\n",
       "  9,\n",
       "  1151,\n",
       "  3328,\n",
       "  3329,\n",
       "  3330,\n",
       "  5,\n",
       "  1,\n",
       "  3331,\n",
       "  1316,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  121,\n",
       "  7,\n",
       "  230,\n",
       "  5,\n",
       "  1,\n",
       "  284,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  108,\n",
       "  152,\n",
       "  170,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  408,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  225,\n",
       "  1,\n",
       "  19,\n",
       "  659,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  35,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  214,\n",
       "  9,\n",
       "  1063,\n",
       "  645],\n",
       " [98,\n",
       "  129,\n",
       "  7,\n",
       "  1377,\n",
       "  1128,\n",
       "  2092,\n",
       "  136,\n",
       "  1298,\n",
       "  602,\n",
       "  217,\n",
       "  298,\n",
       "  187,\n",
       "  3332,\n",
       "  3333,\n",
       "  225,\n",
       "  9,\n",
       "  1907,\n",
       "  742,\n",
       "  2,\n",
       "  3,\n",
       "  40,\n",
       "  6,\n",
       "  192,\n",
       "  719,\n",
       "  8,\n",
       "  327,\n",
       "  741,\n",
       "  7,\n",
       "  1960,\n",
       "  298,\n",
       "  22,\n",
       "  1,\n",
       "  217,\n",
       "  298,\n",
       "  187,\n",
       "  742,\n",
       "  2,\n",
       "  3,\n",
       "  40],\n",
       " [1,\n",
       "  101,\n",
       "  49,\n",
       "  32,\n",
       "  66,\n",
       "  14,\n",
       "  1,\n",
       "  177,\n",
       "  27,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  118,\n",
       "  112,\n",
       "  1088,\n",
       "  32,\n",
       "  60,\n",
       "  49,\n",
       "  10,\n",
       "  408,\n",
       "  294,\n",
       "  1360,\n",
       "  50,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  144,\n",
       "  32,\n",
       "  400,\n",
       "  4,\n",
       "  466,\n",
       "  10,\n",
       "  229,\n",
       "  21,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  118,\n",
       "  112,\n",
       "  1088,\n",
       "  32,\n",
       "  60,\n",
       "  49,\n",
       "  10,\n",
       "  408,\n",
       "  294,\n",
       "  1360,\n",
       "  50,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  444,\n",
       "  166,\n",
       "  33,\n",
       "  75,\n",
       "  9,\n",
       "  778,\n",
       "  4,\n",
       "  1,\n",
       "  19,\n",
       "  33,\n",
       "  50,\n",
       "  574,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  746,\n",
       "  315,\n",
       "  369,\n",
       "  4,\n",
       "  520,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  9,\n",
       "  651,\n",
       "  106],\n",
       " [1,\n",
       "  444,\n",
       "  166,\n",
       "  33,\n",
       "  75,\n",
       "  9,\n",
       "  778,\n",
       "  4,\n",
       "  1,\n",
       "  19,\n",
       "  33,\n",
       "  50,\n",
       "  574,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  18,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  14,\n",
       "  1,\n",
       "  331,\n",
       "  78,\n",
       "  166,\n",
       "  75,\n",
       "  9,\n",
       "  778],\n",
       " [6,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  225,\n",
       "  1,\n",
       "  19,\n",
       "  659,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  35,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  214,\n",
       "  9,\n",
       "  1063,\n",
       "  645],\n",
       " [328,\n",
       "  944,\n",
       "  268,\n",
       "  34,\n",
       "  1,\n",
       "  217,\n",
       "  137,\n",
       "  187,\n",
       "  485,\n",
       "  2112,\n",
       "  336,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  137,\n",
       "  242,\n",
       "  32,\n",
       "  3334,\n",
       "  217,\n",
       "  137,\n",
       "  187,\n",
       "  485,\n",
       "  336,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  3335,\n",
       "  697,\n",
       "  293,\n",
       "  354,\n",
       "  34,\n",
       "  32,\n",
       "  1691,\n",
       "  9,\n",
       "  3336,\n",
       "  1700,\n",
       "  3337,\n",
       "  4,\n",
       "  1623],\n",
       " [328,\n",
       "  944,\n",
       "  268,\n",
       "  34,\n",
       "  1,\n",
       "  217,\n",
       "  137,\n",
       "  187,\n",
       "  485,\n",
       "  2112,\n",
       "  336,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  137,\n",
       "  242,\n",
       "  32,\n",
       "  3338,\n",
       "  4,\n",
       "  1865,\n",
       "  90,\n",
       "  15,\n",
       "  137,\n",
       "  242,\n",
       "  5,\n",
       "  1,\n",
       "  217,\n",
       "  137,\n",
       "  187,\n",
       "  336,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  31,\n",
       "  7,\n",
       "  250],\n",
       " [661,\n",
       "  2,\n",
       "  3,\n",
       "  40,\n",
       "  714,\n",
       "  7,\n",
       "  1754,\n",
       "  11,\n",
       "  46,\n",
       "  2043,\n",
       "  65,\n",
       "  11,\n",
       "  55,\n",
       "  227,\n",
       "  1,\n",
       "  673,\n",
       "  5,\n",
       "  227,\n",
       "  19,\n",
       "  67,\n",
       "  509,\n",
       "  8,\n",
       "  661,\n",
       "  2,\n",
       "  3,\n",
       "  40,\n",
       "  4,\n",
       "  661,\n",
       "  4,\n",
       "  3339,\n",
       "  37],\n",
       " [6,\n",
       "  1354,\n",
       "  1,\n",
       "  160,\n",
       "  200,\n",
       "  8,\n",
       "  357,\n",
       "  283,\n",
       "  389,\n",
       "  191,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  8,\n",
       "  335,\n",
       "  1,\n",
       "  28,\n",
       "  33,\n",
       "  181,\n",
       "  182,\n",
       "  4,\n",
       "  373,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [94,\n",
       "  35,\n",
       "  19,\n",
       "  17,\n",
       "  13,\n",
       "  7,\n",
       "  258,\n",
       "  20,\n",
       "  11,\n",
       "  27,\n",
       "  38,\n",
       "  49,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  161,\n",
       "  13,\n",
       "  66,\n",
       "  23,\n",
       "  1089,\n",
       "  5,\n",
       "  68,\n",
       "  177,\n",
       "  27,\n",
       "  38,\n",
       "  161,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [11,\n",
       "  1,\n",
       "  357,\n",
       "  28,\n",
       "  456,\n",
       "  139,\n",
       "  3340,\n",
       "  588,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  13,\n",
       "  1,\n",
       "  254,\n",
       "  250,\n",
       "  11,\n",
       "  3341,\n",
       "  3342,\n",
       "  2113,\n",
       "  1096,\n",
       "  5,\n",
       "  1,\n",
       "  39,\n",
       "  826,\n",
       "  29,\n",
       "  588,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  13,\n",
       "  15,\n",
       "  11,\n",
       "  1,\n",
       "  357,\n",
       "  28,\n",
       "  456],\n",
       " [98,\n",
       "  33,\n",
       "  145,\n",
       "  5,\n",
       "  1,\n",
       "  606,\n",
       "  1067,\n",
       "  8,\n",
       "  1,\n",
       "  502,\n",
       "  159,\n",
       "  62,\n",
       "  57,\n",
       "  2114,\n",
       "  2,\n",
       "  3,\n",
       "  57,\n",
       "  3343,\n",
       "  13,\n",
       "  15,\n",
       "  31,\n",
       "  1,\n",
       "  298,\n",
       "  28,\n",
       "  75,\n",
       "  8,\n",
       "  1,\n",
       "  502,\n",
       "  57,\n",
       "  159,\n",
       "  62,\n",
       "  2114,\n",
       "  2,\n",
       "  3,\n",
       "  57],\n",
       " [6,\n",
       "  1354,\n",
       "  1,\n",
       "  160,\n",
       "  200,\n",
       "  8,\n",
       "  357,\n",
       "  283,\n",
       "  389,\n",
       "  191,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53,\n",
       "  8,\n",
       "  335,\n",
       "  1,\n",
       "  28,\n",
       "  33,\n",
       "  181,\n",
       "  182,\n",
       "  4,\n",
       "  373,\n",
       "  10,\n",
       "  39,\n",
       "  120,\n",
       "  86,\n",
       "  2,\n",
       "  3,\n",
       "  53],\n",
       " [188,\n",
       "  32,\n",
       "  516,\n",
       "  23,\n",
       "  1,\n",
       "  147,\n",
       "  28,\n",
       "  10,\n",
       "  1358,\n",
       "  260,\n",
       "  14,\n",
       "  1,\n",
       "  502,\n",
       "  1388,\n",
       "  392,\n",
       "  4,\n",
       "  1104,\n",
       "  64,\n",
       "  6,\n",
       "  871,\n",
       "  1358,\n",
       "  286,\n",
       "  3344,\n",
       "  10,\n",
       "  1,\n",
       "  2051,\n",
       "  1388,\n",
       "  21,\n",
       "  1,\n",
       "  159,\n",
       "  62,\n",
       "  392,\n",
       "  4,\n",
       "  1104,\n",
       "  64],\n",
       " [1,\n",
       "  818,\n",
       "  807,\n",
       "  26,\n",
       "  1,\n",
       "  985,\n",
       "  1609,\n",
       "  371,\n",
       "  8,\n",
       "  1,\n",
       "  572,\n",
       "  3345,\n",
       "  75,\n",
       "  33,\n",
       "  3346,\n",
       "  1610,\n",
       "  3347,\n",
       "  153,\n",
       "  14,\n",
       "  1,\n",
       "  60,\n",
       "  985,\n",
       "  41,\n",
       "  19,\n",
       "  14,\n",
       "  819,\n",
       "  4,\n",
       "  116,\n",
       "  173,\n",
       "  630,\n",
       "  4,\n",
       "  3348,\n",
       "  188,\n",
       "  5,\n",
       "  48,\n",
       "  340,\n",
       "  1010,\n",
       "  8,\n",
       "  193,\n",
       "  94,\n",
       "  42,\n",
       "  881,\n",
       "  1021,\n",
       "  172,\n",
       "  65,\n",
       "  14,\n",
       "  1,\n",
       "  280,\n",
       "  1088,\n",
       "  83,\n",
       "  3349,\n",
       "  3350,\n",
       "  4,\n",
       "  94,\n",
       "  7,\n",
       "  985,\n",
       "  371,\n",
       "  14,\n",
       "  116,\n",
       "  173,\n",
       "  131,\n",
       "  630,\n",
       "  4,\n",
       "  45],\n",
       " [6,\n",
       "  3351,\n",
       "  1,\n",
       "  3352,\n",
       "  304,\n",
       "  31,\n",
       "  7,\n",
       "  199,\n",
       "  62,\n",
       "  26,\n",
       "  359,\n",
       "  99,\n",
       "  65,\n",
       "  14,\n",
       "  68,\n",
       "  3353,\n",
       "  1053,\n",
       "  4,\n",
       "  7,\n",
       "  2067,\n",
       "  1383,\n",
       "  369,\n",
       "  162,\n",
       "  10,\n",
       "  1,\n",
       "  404,\n",
       "  20,\n",
       "  316,\n",
       "  4,\n",
       "  134,\n",
       "  25,\n",
       "  6,\n",
       "  18,\n",
       "  404,\n",
       "  316,\n",
       "  4,\n",
       "  134,\n",
       "  25,\n",
       "  31,\n",
       "  1,\n",
       "  99,\n",
       "  122],\n",
       " [393,\n",
       "  98,\n",
       "  13,\n",
       "  76,\n",
       "  2047,\n",
       "  34,\n",
       "  1,\n",
       "  1378,\n",
       "  5,\n",
       "  663,\n",
       "  1379,\n",
       "  8,\n",
       "  1,\n",
       "  46,\n",
       "  75,\n",
       "  13,\n",
       "  72,\n",
       "  9,\n",
       "  1,\n",
       "  145,\n",
       "  516,\n",
       "  26,\n",
       "  463,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  4,\n",
       "  2048,\n",
       "  9,\n",
       "  8,\n",
       "  185,\n",
       "  2049,\n",
       "  2,\n",
       "  3,\n",
       "  87,\n",
       "  76,\n",
       "  3354,\n",
       "  179,\n",
       "  34,\n",
       "  1,\n",
       "  941,\n",
       "  1378,\n",
       "  5,\n",
       "  663,\n",
       "  1379,\n",
       "  190,\n",
       "  838,\n",
       "  472,\n",
       "  325,\n",
       "  2105,\n",
       "  34,\n",
       "  675,\n",
       "  5,\n",
       "  789,\n",
       "  32,\n",
       "  3355,\n",
       "  3356,\n",
       "  3357,\n",
       "  3358,\n",
       "  294,\n",
       "  650,\n",
       "  3359,\n",
       "  63],\n",
       " [1290,\n",
       "  290,\n",
       "  31,\n",
       "  172,\n",
       "  32,\n",
       "  3360,\n",
       "  9,\n",
       "  1340,\n",
       "  11,\n",
       "  8,\n",
       "  7,\n",
       "  3361,\n",
       "  130,\n",
       "  234,\n",
       "  5,\n",
       "  3362,\n",
       "  1251,\n",
       "  290,\n",
       "  31,\n",
       "  1,\n",
       "  145,\n",
       "  97,\n",
       "  8,\n",
       "  2115,\n",
       "  2,\n",
       "  3,\n",
       "  1084,\n",
       "  1,\n",
       "  255,\n",
       "  1145,\n",
       "  5,\n",
       "  1,\n",
       "  130,\n",
       "  994,\n",
       "  9,\n",
       "  3363,\n",
       "  129,\n",
       "  123,\n",
       "  1,\n",
       "  3364,\n",
       "  3365,\n",
       "  128,\n",
       "  97,\n",
       "  8,\n",
       "  2115,\n",
       "  2,\n",
       "  3,\n",
       "  1084],\n",
       " [51,\n",
       "  172,\n",
       "  299,\n",
       "  65,\n",
       "  32,\n",
       "  484,\n",
       "  10,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  590,\n",
       "  34,\n",
       "  1,\n",
       "  238,\n",
       "  19,\n",
       "  3366,\n",
       "  68,\n",
       "  1022,\n",
       "  3367,\n",
       "  1341,\n",
       "  608,\n",
       "  185,\n",
       "  3368,\n",
       "  751,\n",
       "  9,\n",
       "  34,\n",
       "  19,\n",
       "  31,\n",
       "  17,\n",
       "  2116,\n",
       "  1281,\n",
       "  98,\n",
       "  33,\n",
       "  50,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [423,\n",
       "  3369,\n",
       "  362,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  97,\n",
       "  9,\n",
       "  18,\n",
       "  880,\n",
       "  1103,\n",
       "  11,\n",
       "  3370,\n",
       "  3371,\n",
       "  5,\n",
       "  195,\n",
       "  1042,\n",
       "  3372,\n",
       "  360,\n",
       "  1,\n",
       "  3373,\n",
       "  5,\n",
       "  168,\n",
       "  1375,\n",
       "  97,\n",
       "  26,\n",
       "  362,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  590,\n",
       "  34,\n",
       "  6,\n",
       "  657,\n",
       "  318,\n",
       "  1333,\n",
       "  1103,\n",
       "  11,\n",
       "  594],\n",
       " [6,\n",
       "  69,\n",
       "  7,\n",
       "  1076,\n",
       "  199,\n",
       "  19,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  11,\n",
       "  111,\n",
       "  1134,\n",
       "  4,\n",
       "  147,\n",
       "  10,\n",
       "  361,\n",
       "  21,\n",
       "  1,\n",
       "  1746,\n",
       "  31,\n",
       "  46,\n",
       "  934,\n",
       "  24,\n",
       "  29,\n",
       "  13,\n",
       "  7,\n",
       "  226,\n",
       "  19,\n",
       "  407,\n",
       "  10,\n",
       "  1076,\n",
       "  199,\n",
       "  31,\n",
       "  175,\n",
       "  8,\n",
       "  1,\n",
       "  92,\n",
       "  20,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [6,\n",
       "  360,\n",
       "  1,\n",
       "  1671,\n",
       "  8,\n",
       "  646,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  10,\n",
       "  1,\n",
       "  1649,\n",
       "  5,\n",
       "  3374,\n",
       "  178,\n",
       "  31,\n",
       "  679,\n",
       "  9,\n",
       "  7,\n",
       "  1922,\n",
       "  238,\n",
       "  1706,\n",
       "  18,\n",
       "  3375,\n",
       "  646,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  9,\n",
       "  535,\n",
       "  1,\n",
       "  3376,\n",
       "  4,\n",
       "  3377,\n",
       "  868,\n",
       "  5,\n",
       "  68,\n",
       "  477,\n",
       "  2113],\n",
       " [48,\n",
       "  13,\n",
       "  1374,\n",
       "  9,\n",
       "  68,\n",
       "  99,\n",
       "  14,\n",
       "  1,\n",
       "  3378,\n",
       "  1169,\n",
       "  93,\n",
       "  31,\n",
       "  8,\n",
       "  561,\n",
       "  2,\n",
       "  3,\n",
       "  90,\n",
       "  1,\n",
       "  3379,\n",
       "  5,\n",
       "  257,\n",
       "  93,\n",
       "  154,\n",
       "  76,\n",
       "  104,\n",
       "  3380,\n",
       "  26,\n",
       "  1,\n",
       "  18,\n",
       "  5,\n",
       "  91,\n",
       "  30,\n",
       "  658,\n",
       "  31,\n",
       "  3381,\n",
       "  26,\n",
       "  561,\n",
       "  2,\n",
       "  3,\n",
       "  90],\n",
       " [51,\n",
       "  58,\n",
       "  32,\n",
       "  223,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  27,\n",
       "  38,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  14,\n",
       "  7,\n",
       "  532,\n",
       "  126,\n",
       "  1298,\n",
       "  5,\n",
       "  3382,\n",
       "  1,\n",
       "  3383,\n",
       "  5,\n",
       "  35,\n",
       "  693,\n",
       "  1,\n",
       "  65,\n",
       "  32,\n",
       "  50,\n",
       "  848,\n",
       "  10,\n",
       "  7,\n",
       "  27,\n",
       "  35,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  34,\n",
       "  15,\n",
       "  1,\n",
       "  642,\n",
       "  3384,\n",
       "  65,\n",
       "  9,\n",
       "  341,\n",
       "  1,\n",
       "  28],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  348,\n",
       "  7,\n",
       "  70,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  19,\n",
       "  66,\n",
       "  23,\n",
       "  1,\n",
       "  606,\n",
       "  442,\n",
       "  28,\n",
       "  31,\n",
       "  138,\n",
       "  1415,\n",
       "  751,\n",
       "  9,\n",
       "  34,\n",
       "  19,\n",
       "  31,\n",
       "  17,\n",
       "  2116,\n",
       "  1281,\n",
       "  98,\n",
       "  33,\n",
       "  50,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [51,\n",
       "  58,\n",
       "  67,\n",
       "  23,\n",
       "  44,\n",
       "  236,\n",
       "  5,\n",
       "  876,\n",
       "  119,\n",
       "  198,\n",
       "  23,\n",
       "  1,\n",
       "  828,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  133,\n",
       "  8,\n",
       "  1,\n",
       "  217,\n",
       "  187,\n",
       "  1235,\n",
       "  213,\n",
       "  131,\n",
       "  638,\n",
       "  2,\n",
       "  3,\n",
       "  110,\n",
       "  172,\n",
       "  1057,\n",
       "  67,\n",
       "  397,\n",
       "  8,\n",
       "  58,\n",
       "  23,\n",
       "  1,\n",
       "  217,\n",
       "  187,\n",
       "  638,\n",
       "  2,\n",
       "  3,\n",
       "  110,\n",
       "  14,\n",
       "  1,\n",
       "  3385,\n",
       "  264,\n",
       "  52,\n",
       "  1166,\n",
       "  26],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  30,\n",
       "  132,\n",
       "  9,\n",
       "  644,\n",
       "  7,\n",
       "  126,\n",
       "  193,\n",
       "  26,\n",
       "  865,\n",
       "  1,\n",
       "  641,\n",
       "  126,\n",
       "  654,\n",
       "  370,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  36,\n",
       "  9,\n",
       "  51,\n",
       "  2117,\n",
       "  18,\n",
       "  1,\n",
       "  1097,\n",
       "  5,\n",
       "  903,\n",
       "  4,\n",
       "  2118,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  36,\n",
       "  132,\n",
       "  31,\n",
       "  7,\n",
       "  370,\n",
       "  469,\n",
       "  9,\n",
       "  681,\n",
       "  63,\n",
       "  2119,\n",
       "  585,\n",
       "  9,\n",
       "  111,\n",
       "  205],\n",
       " [6,\n",
       "  407,\n",
       "  7,\n",
       "  685,\n",
       "  235,\n",
       "  19,\n",
       "  11,\n",
       "  1,\n",
       "  69,\n",
       "  3386,\n",
       "  14,\n",
       "  1,\n",
       "  60,\n",
       "  580,\n",
       "  5,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  76,\n",
       "  225,\n",
       "  14,\n",
       "  1,\n",
       "  60,\n",
       "  27,\n",
       "  29,\n",
       "  5,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  14,\n",
       "  60,\n",
       "  302,\n",
       "  590,\n",
       "  11,\n",
       "  1,\n",
       "  3387,\n",
       "  2120,\n",
       "  42,\n",
       "  6,\n",
       "  75,\n",
       "  9,\n",
       "  869],\n",
       " [7,\n",
       "  941,\n",
       "  1306,\n",
       "  5,\n",
       "  420,\n",
       "  28,\n",
       "  13,\n",
       "  15,\n",
       "  9,\n",
       "  858,\n",
       "  1,\n",
       "  3388,\n",
       "  775,\n",
       "  9,\n",
       "  3389,\n",
       "  1073,\n",
       "  11,\n",
       "  7,\n",
       "  497,\n",
       "  5,\n",
       "  1,\n",
       "  82,\n",
       "  462,\n",
       "  569,\n",
       "  4,\n",
       "  2121,\n",
       "  16,\n",
       "  1175,\n",
       "  1264,\n",
       "  7,\n",
       "  212,\n",
       "  5,\n",
       "  658,\n",
       "  72,\n",
       "  9,\n",
       "  1,\n",
       "  212,\n",
       "  5,\n",
       "  1073,\n",
       "  13,\n",
       "  318,\n",
       "  7,\n",
       "  3390,\n",
       "  11,\n",
       "  914,\n",
       "  188,\n",
       "  569,\n",
       "  4,\n",
       "  2121,\n",
       "  3391],\n",
       " [6,\n",
       "  18,\n",
       "  1,\n",
       "  30,\n",
       "  132,\n",
       "  9,\n",
       "  644,\n",
       "  7,\n",
       "  126,\n",
       "  193,\n",
       "  26,\n",
       "  865,\n",
       "  1,\n",
       "  641,\n",
       "  126,\n",
       "  654,\n",
       "  370,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  36,\n",
       "  9,\n",
       "  51,\n",
       "  2117,\n",
       "  18,\n",
       "  1,\n",
       "  1097,\n",
       "  5,\n",
       "  903,\n",
       "  4,\n",
       "  2118,\n",
       "  59,\n",
       "  47,\n",
       "  4,\n",
       "  45,\n",
       "  36,\n",
       "  132,\n",
       "  31,\n",
       "  7,\n",
       "  370,\n",
       "  469,\n",
       "  9,\n",
       "  681,\n",
       "  63,\n",
       "  2119,\n",
       "  585,\n",
       "  9,\n",
       "  111,\n",
       "  205],\n",
       " [83,\n",
       "  11,\n",
       "  7,\n",
       "  296,\n",
       "  60,\n",
       "  133,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  453,\n",
       "  540,\n",
       "  5,\n",
       "  1,\n",
       "  1395,\n",
       "  801,\n",
       "  22,\n",
       "  2122,\n",
       "  2123,\n",
       "  84,\n",
       "  1,\n",
       "  143,\n",
       "  22,\n",
       "  13,\n",
       "  7,\n",
       "  388,\n",
       "  5,\n",
       "  68,\n",
       "  1818,\n",
       "  213,\n",
       "  5,\n",
       "  1,\n",
       "  1395,\n",
       "  801,\n",
       "  22,\n",
       "  2122,\n",
       "  7,\n",
       "  869,\n",
       "  480,\n",
       "  30,\n",
       "  3392,\n",
       "  22,\n",
       "  5,\n",
       "  1395,\n",
       "  44,\n",
       "  2123,\n",
       "  84],\n",
       " [6,\n",
       "  3393,\n",
       "  14,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40,\n",
       "  8,\n",
       "  269,\n",
       "  9,\n",
       "  95,\n",
       "  7,\n",
       "  85,\n",
       "  3394,\n",
       "  22,\n",
       "  11,\n",
       "  1394,\n",
       "  4,\n",
       "  3395,\n",
       "  11,\n",
       "  44,\n",
       "  63,\n",
       "  8,\n",
       "  1,\n",
       "  257,\n",
       "  1371,\n",
       "  32,\n",
       "  174,\n",
       "  21,\n",
       "  7,\n",
       "  1261,\n",
       "  30,\n",
       "  73,\n",
       "  5,\n",
       "  125,\n",
       "  21,\n",
       "  1,\n",
       "  54,\n",
       "  85,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [1,\n",
       "  3396,\n",
       "  21,\n",
       "  987,\n",
       "  62,\n",
       "  94,\n",
       "  195,\n",
       "  115,\n",
       "  8,\n",
       "  465,\n",
       "  176,\n",
       "  493,\n",
       "  691,\n",
       "  11,\n",
       "  1,\n",
       "  62,\n",
       "  988,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  6,\n",
       "  631,\n",
       "  8,\n",
       "  105,\n",
       "  573,\n",
       "  7,\n",
       "  4,\n",
       "  989,\n",
       "  5,\n",
       "  987,\n",
       "  62,\n",
       "  94,\n",
       "  195,\n",
       "  115,\n",
       "  8,\n",
       "  465,\n",
       "  988,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  14,\n",
       "  68,\n",
       "  990,\n",
       "  5,\n",
       "  24,\n",
       "  991,\n",
       "  29],\n",
       " [3397,\n",
       "  7,\n",
       "  3398,\n",
       "  583,\n",
       "  143,\n",
       "  10,\n",
       "  1,\n",
       "  17,\n",
       "  29,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  31,\n",
       "  7,\n",
       "  1871,\n",
       "  6,\n",
       "  484,\n",
       "  1,\n",
       "  235,\n",
       "  82,\n",
       "  1878,\n",
       "  642,\n",
       "  23,\n",
       "  112,\n",
       "  708,\n",
       "  3399,\n",
       "  147,\n",
       "  24,\n",
       "  82,\n",
       "  6,\n",
       "  223,\n",
       "  112,\n",
       "  2014,\n",
       "  35,\n",
       "  58,\n",
       "  10,\n",
       "  1,\n",
       "  27,\n",
       "  235,\n",
       "  29,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [1,\n",
       "  656,\n",
       "  1748,\n",
       "  93,\n",
       "  32,\n",
       "  192,\n",
       "  15,\n",
       "  9,\n",
       "  69,\n",
       "  7,\n",
       "  158,\n",
       "  168,\n",
       "  56,\n",
       "  2091,\n",
       "  295,\n",
       "  87,\n",
       "  9,\n",
       "  496,\n",
       "  1,\n",
       "  261,\n",
       "  358,\n",
       "  1400,\n",
       "  602,\n",
       "  1,\n",
       "  3400,\n",
       "  18,\n",
       "  1,\n",
       "  158,\n",
       "  168,\n",
       "  56,\n",
       "  99,\n",
       "  496,\n",
       "  103,\n",
       "  295,\n",
       "  87,\n",
       "  9,\n",
       "  1301,\n",
       "  7,\n",
       "  496,\n",
       "  269,\n",
       "  11,\n",
       "  111,\n",
       "  501,\n",
       "  5,\n",
       "  3401],\n",
       " [24,\n",
       "  391,\n",
       "  82,\n",
       "  13,\n",
       "  61,\n",
       "  23,\n",
       "  1,\n",
       "  953,\n",
       "  238,\n",
       "  309,\n",
       "  41,\n",
       "  19,\n",
       "  1176,\n",
       "  128,\n",
       "  9,\n",
       "  78,\n",
       "  30,\n",
       "  178,\n",
       "  5,\n",
       "  457,\n",
       "  2,\n",
       "  3,\n",
       "  747,\n",
       "  4,\n",
       "  457,\n",
       "  2,\n",
       "  3,\n",
       "  1100,\n",
       "  10,\n",
       "  1,\n",
       "  883,\n",
       "  3402,\n",
       "  51,\n",
       "  5,\n",
       "  1,\n",
       "  642,\n",
       "  558,\n",
       "  6,\n",
       "  225,\n",
       "  1,\n",
       "  238,\n",
       "  30,\n",
       "  178,\n",
       "  5,\n",
       "  457,\n",
       "  2,\n",
       "  3,\n",
       "  747,\n",
       "  14,\n",
       "  112,\n",
       "  168,\n",
       "  3403,\n",
       "  105,\n",
       "  61,\n",
       "  23,\n",
       "  815,\n",
       "  450,\n",
       "  4,\n",
       "  660,\n",
       "  26,\n",
       "  60,\n",
       "  91,\n",
       "  1057],\n",
       " [6,\n",
       "  396,\n",
       "  1,\n",
       "  529,\n",
       "  526,\n",
       "  19,\n",
       "  376,\n",
       "  1105,\n",
       "  1297,\n",
       "  4,\n",
       "  1,\n",
       "  3404,\n",
       "  103,\n",
       "  3405,\n",
       "  165,\n",
       "  9,\n",
       "  210,\n",
       "  1,\n",
       "  3406,\n",
       "  3407,\n",
       "  18,\n",
       "  1105,\n",
       "  103,\n",
       "  1105,\n",
       "  1297,\n",
       "  9,\n",
       "  3408,\n",
       "  1,\n",
       "  3409,\n",
       "  136],\n",
       " [6,\n",
       "  871,\n",
       "  259,\n",
       "  563,\n",
       "  2,\n",
       "  3,\n",
       "  156,\n",
       "  5,\n",
       "  35,\n",
       "  29,\n",
       "  619,\n",
       "  460,\n",
       "  659,\n",
       "  1,\n",
       "  1332,\n",
       "  44,\n",
       "  3410,\n",
       "  216,\n",
       "  35,\n",
       "  693,\n",
       "  574,\n",
       "  1,\n",
       "  259,\n",
       "  222,\n",
       "  563,\n",
       "  2,\n",
       "  3,\n",
       "  156],\n",
       " [98,\n",
       "  13,\n",
       "  7,\n",
       "  60,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  51,\n",
       "  24,\n",
       "  49,\n",
       "  32,\n",
       "  664,\n",
       "  14,\n",
       "  7,\n",
       "  60,\n",
       "  27,\n",
       "  29,\n",
       "  66,\n",
       "  14,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [393,\n",
       "  6,\n",
       "  15,\n",
       "  17,\n",
       "  20,\n",
       "  31,\n",
       "  27,\n",
       "  296,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  1,\n",
       "  101,\n",
       "  49,\n",
       "  32,\n",
       "  66,\n",
       "  14,\n",
       "  1,\n",
       "  177,\n",
       "  27,\n",
       "  38,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [324,\n",
       "  142,\n",
       "  13,\n",
       "  260,\n",
       "  10,\n",
       "  1,\n",
       "  3411,\n",
       "  431,\n",
       "  142,\n",
       "  745,\n",
       "  2,\n",
       "  3,\n",
       "  36,\n",
       "  1,\n",
       "  2124,\n",
       "  5,\n",
       "  142,\n",
       "  211,\n",
       "  112,\n",
       "  72,\n",
       "  63,\n",
       "  13,\n",
       "  1158,\n",
       "  10,\n",
       "  249,\n",
       "  745,\n",
       "  2,\n",
       "  3,\n",
       "  36],\n",
       " [393,\n",
       "  6,\n",
       "  15,\n",
       "  17,\n",
       "  20,\n",
       "  31,\n",
       "  27,\n",
       "  296,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [98,\n",
       "  13,\n",
       "  7,\n",
       "  60,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  19,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  66,\n",
       "  27,\n",
       "  56,\n",
       "  35,\n",
       "  49,\n",
       "  10,\n",
       "  1,\n",
       "  220,\n",
       "  108,\n",
       "  20,\n",
       "  17,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [94,\n",
       "  35,\n",
       "  19,\n",
       "  17,\n",
       "  13,\n",
       "  7,\n",
       "  258,\n",
       "  20,\n",
       "  11,\n",
       "  27,\n",
       "  38,\n",
       "  49,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  393,\n",
       "  6,\n",
       "  15,\n",
       "  17,\n",
       "  20,\n",
       "  31,\n",
       "  27,\n",
       "  296,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16],\n",
       " [3412,\n",
       "  98,\n",
       "  13,\n",
       "  460,\n",
       "  26,\n",
       "  249,\n",
       "  142,\n",
       "  152,\n",
       "  745,\n",
       "  2,\n",
       "  3,\n",
       "  36,\n",
       "  1,\n",
       "  2124,\n",
       "  5,\n",
       "  142,\n",
       "  211,\n",
       "  112,\n",
       "  72,\n",
       "  63,\n",
       "  13,\n",
       "  1158,\n",
       "  10,\n",
       "  249,\n",
       "  745,\n",
       "  2,\n",
       "  3,\n",
       "  36],\n",
       " [6,\n",
       "  18,\n",
       "  100,\n",
       "  41,\n",
       "  65,\n",
       "  14,\n",
       "  116,\n",
       "  839,\n",
       "  96,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  1,\n",
       "  41,\n",
       "  65,\n",
       "  32,\n",
       "  407,\n",
       "  10,\n",
       "  1,\n",
       "  102,\n",
       "  20,\n",
       "  96,\n",
       "  2,\n",
       "  3,\n",
       "  88,\n",
       "  14,\n",
       "  155,\n",
       "  116,\n",
       "  173],\n",
       " [6,\n",
       "  882,\n",
       "  428,\n",
       "  78,\n",
       "  591,\n",
       "  106,\n",
       "  10,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  18,\n",
       "  1,\n",
       "  746,\n",
       "  315,\n",
       "  369,\n",
       "  4,\n",
       "  520,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  9,\n",
       "  651,\n",
       "  106],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  92,\n",
       "  94,\n",
       "  452,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  15,\n",
       "  99,\n",
       "  908,\n",
       "  21,\n",
       "  92,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  4,\n",
       "  794,\n",
       "  14,\n",
       "  7,\n",
       "  212,\n",
       "  5,\n",
       "  163],\n",
       " [6,\n",
       "  882,\n",
       "  428,\n",
       "  78,\n",
       "  591,\n",
       "  106,\n",
       "  10,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  6,\n",
       "  69,\n",
       "  1,\n",
       "  324,\n",
       "  1058,\n",
       "  811,\n",
       "  10,\n",
       "  1059,\n",
       "  1060,\n",
       "  315,\n",
       "  83,\n",
       "  14,\n",
       "  150,\n",
       "  151,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " [393,\n",
       "  6,\n",
       "  15,\n",
       "  17,\n",
       "  20,\n",
       "  31,\n",
       "  27,\n",
       "  296,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  17,\n",
       "  20,\n",
       "  12,\n",
       "  2,\n",
       "  3,\n",
       "  16,\n",
       "  9,\n",
       "  69,\n",
       "  60,\n",
       "  27,\n",
       "  49,\n",
       "  14,\n",
       "  107,\n",
       "  739],\n",
       " [54,\n",
       "  12,\n",
       "  40,\n",
       "  13,\n",
       "  7,\n",
       "  307,\n",
       "  85,\n",
       "  22,\n",
       "  174,\n",
       "  21,\n",
       "  1,\n",
       "  208,\n",
       "  5,\n",
       "  1,\n",
       "  203,\n",
       "  541,\n",
       "  15,\n",
       "  1,\n",
       "  44,\n",
       "  140,\n",
       "  5,\n",
       "  1,\n",
       "  54,\n",
       "  22,\n",
       "  12,\n",
       "  40],\n",
       " [6,\n",
       "  3413,\n",
       "  1,\n",
       "  2125,\n",
       "  65,\n",
       "  561,\n",
       "  4,\n",
       "  451,\n",
       "  117,\n",
       "  11,\n",
       "  7,\n",
       "  1336,\n",
       "  678,\n",
       "  3414,\n",
       "  3415,\n",
       "  11,\n",
       "  1,\n",
       "  19,\n",
       "  83,\n",
       "  8,\n",
       "  561,\n",
       "  4,\n",
       "  451,\n",
       "  117,\n",
       "  42,\n",
       "  13,\n",
       "  7,\n",
       "  2125,\n",
       "  19],\n",
       " [6,\n",
       "  15,\n",
       "  1,\n",
       "  81,\n",
       "  5,\n",
       "  1,\n",
       "  92,\n",
       "  94,\n",
       "  452,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25,\n",
       "  791,\n",
       "  6,\n",
       "  15,\n",
       "  1,\n",
       "  60,\n",
       "  437,\n",
       "  1930,\n",
       "  1053,\n",
       "  8,\n",
       "  1,\n",
       "  92,\n",
       "  20,\n",
       "  253,\n",
       "  79,\n",
       "  2,\n",
       "  3,\n",
       "  25],\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merubah kata menjadi token\n",
    "token = Tokenizer(num_words=top_words)\n",
    "token.fit_on_texts(words)\n",
    "sequences = token.texts_to_sequences(words)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'the',\n",
       " 2: 'et',\n",
       " 3: 'al',\n",
       " 4: 'and',\n",
       " 5: 'of',\n",
       " 6: 'we',\n",
       " 7: 'a',\n",
       " 8: 'in',\n",
       " 9: 'to',\n",
       " 10: 'using',\n",
       " 11: 'for',\n",
       " 12: 'koehn',\n",
       " 13: 'is',\n",
       " 14: 'with',\n",
       " 15: 'used',\n",
       " 16: '2007',\n",
       " 17: 'moses',\n",
       " 18: 'use',\n",
       " 19: 'model',\n",
       " 20: 'toolkit',\n",
       " 21: 'from',\n",
       " 22: 'corpus',\n",
       " 23: 'on',\n",
       " 24: 'our',\n",
       " 25: '2011',\n",
       " 26: 'by',\n",
       " 27: 'phrasebased',\n",
       " 28: 'data',\n",
       " 29: 'system',\n",
       " 30: 'word',\n",
       " 31: 'as',\n",
       " 32: 'are',\n",
       " 33: 'was',\n",
       " 34: 'that',\n",
       " 35: 'translation',\n",
       " 36: '2004',\n",
       " 37: '2006',\n",
       " 38: 'smt',\n",
       " 39: 'stanford',\n",
       " 40: '2005',\n",
       " 41: 'language',\n",
       " 42: 'which',\n",
       " 43: '2003',\n",
       " 44: 'english',\n",
       " 45: 'ney',\n",
       " 46: 'training',\n",
       " 47: 'och',\n",
       " 48: 'this',\n",
       " 49: 'systems',\n",
       " 50: 'trained',\n",
       " 51: 'all',\n",
       " 52: 'parser',\n",
       " 53: '2014',\n",
       " 54: 'europarl',\n",
       " 55: 'dependency',\n",
       " 56: 'machine',\n",
       " 57: '2009',\n",
       " 58: 'experiments',\n",
       " 59: 'giza',\n",
       " 60: 'standard',\n",
       " 61: 'based',\n",
       " 62: 'task',\n",
       " 63: 'words',\n",
       " 64: '2012',\n",
       " 65: 'models',\n",
       " 66: 'built',\n",
       " 67: 'were',\n",
       " 68: 'an',\n",
       " 69: 'train',\n",
       " 70: 'statistical',\n",
       " 71: 'marneffe',\n",
       " 72: 'similar',\n",
       " 73: 'alignment',\n",
       " 74: 'de',\n",
       " 75: 'set',\n",
       " 76: 'also',\n",
       " 77: 'bootstrap',\n",
       " 78: 'learning',\n",
       " 79: 'pedregosa',\n",
       " 80: 'resampling',\n",
       " 81: 'implementation',\n",
       " 82: 'method',\n",
       " 83: '1',\n",
       " 84: '2000',\n",
       " 85: 'parallel',\n",
       " 86: 'manning',\n",
       " 87: '2002',\n",
       " 88: '2013',\n",
       " 89: 'significance',\n",
       " 90: '2008',\n",
       " 91: 'distributional',\n",
       " 92: 'scikitlearn',\n",
       " 93: 'features',\n",
       " 94: '2',\n",
       " 95: 'have',\n",
       " 96: 'heafield',\n",
       " 97: 'proposed',\n",
       " 98: 'it',\n",
       " 99: 'svm',\n",
       " 100: '5gram',\n",
       " 101: 'baseline',\n",
       " 102: 'kenlm',\n",
       " 103: 'algorithm',\n",
       " 104: 'be',\n",
       " 105: 'both',\n",
       " 106: 'parameters',\n",
       " 107: 'default',\n",
       " 108: 'software',\n",
       " 109: 'performed',\n",
       " 110: '1994',\n",
       " 111: 'each',\n",
       " 112: 'two',\n",
       " 113: 'hypothesis',\n",
       " 114: 'harris',\n",
       " 115: 'analysis',\n",
       " 116: 'kneserney',\n",
       " 117: '2010',\n",
       " 118: 'first',\n",
       " 119: 'pos',\n",
       " 120: 'corenlp',\n",
       " 121: 'build',\n",
       " 122: 'tool',\n",
       " 123: 'been',\n",
       " 124: 'its',\n",
       " 125: 'sentences',\n",
       " 126: 'phrase',\n",
       " 127: '1954',\n",
       " 128: 'approach',\n",
       " 129: 'has',\n",
       " 130: 'semantic',\n",
       " 131: '3',\n",
       " 132: 'alignments',\n",
       " 133: 'text',\n",
       " 134: 'lin',\n",
       " 135: 'schmid',\n",
       " 136: 'sentence',\n",
       " 137: 'discourse',\n",
       " 138: 'described',\n",
       " 139: 'same',\n",
       " 140: 'side',\n",
       " 141: 'paired',\n",
       " 142: 'similarity',\n",
       " 143: 'evaluation',\n",
       " 144: 'corpora',\n",
       " 145: 'one',\n",
       " 146: 'grammar',\n",
       " 147: 'test',\n",
       " 148: 'treetagger',\n",
       " 149: 'contexts',\n",
       " 150: 'adagrad',\n",
       " 151: 'duchi',\n",
       " 152: 'package',\n",
       " 153: 'obtained',\n",
       " 154: 'can',\n",
       " 155: 'modified',\n",
       " 156: '2001',\n",
       " 157: 'meaning',\n",
       " 158: 'support',\n",
       " 159: 'shared',\n",
       " 160: 'parse',\n",
       " 161: 'decoder',\n",
       " 162: 'kernel',\n",
       " 163: 'classifiers',\n",
       " 164: 'annotation',\n",
       " 165: '1995',\n",
       " 166: 'rate',\n",
       " 167: 'work',\n",
       " 168: 'vector',\n",
       " 169: 'available',\n",
       " 170: '5',\n",
       " 171: 'tagger',\n",
       " 172: 'these',\n",
       " 173: 'smoothing',\n",
       " 174: 'extracted',\n",
       " 175: 'implemented',\n",
       " 176: 'provided',\n",
       " 177: 'opensource',\n",
       " 178: 'embeddings',\n",
       " 179: 'out',\n",
       " 180: 'brants',\n",
       " 181: 'tokenized',\n",
       " 182: 'lemmatized',\n",
       " 183: 'dataset',\n",
       " 184: 'classifier',\n",
       " 185: 'section',\n",
       " 186: 'partofspeech',\n",
       " 187: 'treebank',\n",
       " 188: 'results',\n",
       " 189: 'tree',\n",
       " 190: 'at',\n",
       " 191: 'dependencies',\n",
       " 192: 'then',\n",
       " 193: 'table',\n",
       " 194: 'occur',\n",
       " 195: 'sentiment',\n",
       " 196: 'carried',\n",
       " 197: '1996',\n",
       " 198: 'tagging',\n",
       " 199: 'regression',\n",
       " 200: 'trees',\n",
       " 201: '2015',\n",
       " 202: 'weka',\n",
       " 203: 'european',\n",
       " 204: 'hall',\n",
       " 205: 'other',\n",
       " 206: 'over',\n",
       " 207: 'web',\n",
       " 208: 'proceedings',\n",
       " 209: 'target',\n",
       " 210: 'obtain',\n",
       " 211: 'between',\n",
       " 212: 'number',\n",
       " 213: 'version',\n",
       " 214: 'applied',\n",
       " 215: 'classification',\n",
       " 216: 'measure',\n",
       " 217: 'penn',\n",
       " 218: 'pairs',\n",
       " 219: 'extract',\n",
       " 220: 'open',\n",
       " 221: 'disambiguation',\n",
       " 222: 'score',\n",
       " 223: 'conducted',\n",
       " 224: 'tagged',\n",
       " 225: 'compare',\n",
       " 226: 'linear',\n",
       " 227: 'parsing',\n",
       " 228: 'p',\n",
       " 229: 'scripts',\n",
       " 230: 'state',\n",
       " 231: 'example',\n",
       " 232: 'run',\n",
       " 233: 'done',\n",
       " 234: 'theory',\n",
       " 235: 'mt',\n",
       " 236: 'part',\n",
       " 237: 'tend',\n",
       " 238: 'neural',\n",
       " 239: 'bilingual',\n",
       " 240: 'contains',\n",
       " 241: 'but',\n",
       " 242: 'relations',\n",
       " 243: 'adam',\n",
       " 244: 'kingma',\n",
       " 245: 'ba',\n",
       " 246: 'franz',\n",
       " 247: 'annotated',\n",
       " 248: 'their',\n",
       " 249: 'wordnet',\n",
       " 250: 'feature',\n",
       " 251: 'resource',\n",
       " 252: 'like',\n",
       " 253: '4',\n",
       " 254: 'only',\n",
       " 255: 'most',\n",
       " 256: 'random',\n",
       " 257: 'lexical',\n",
       " 258: 'stateoftheart',\n",
       " 259: 'bleu',\n",
       " 260: 'computed',\n",
       " 261: 'source',\n",
       " 262: 'perform',\n",
       " 263: 'paper',\n",
       " 264: 'hpsg',\n",
       " 265: 'purpose',\n",
       " 266: 'brat',\n",
       " 267: 'stenetorp',\n",
       " 268: 'shown',\n",
       " 269: 'order',\n",
       " 270: 'states',\n",
       " 271: 'logistic',\n",
       " 272: 'nltk',\n",
       " 273: 'bird',\n",
       " 274: 'tokenize',\n",
       " 275: 'french',\n",
       " 276: 'parses',\n",
       " 277: 'copestake',\n",
       " 278: 'kim',\n",
       " 279: 'morphological',\n",
       " 280: 'following',\n",
       " 281: 'testing',\n",
       " 282: 'decoding',\n",
       " 283: 'into',\n",
       " 284: 'art',\n",
       " 285: 'update',\n",
       " 286: 'performance',\n",
       " 287: 'representation',\n",
       " 288: '1999',\n",
       " 289: 'found',\n",
       " 290: 'such',\n",
       " 291: 'smoothed',\n",
       " 292: 'arabic',\n",
       " 293: 'relation',\n",
       " 294: 'or',\n",
       " 295: 'joachims',\n",
       " 296: 'reference',\n",
       " 297: 'intervals',\n",
       " 298: 'chinese',\n",
       " 299: 'reordering',\n",
       " 300: 'new',\n",
       " 301: 'lp',\n",
       " 302: 'settings',\n",
       " 303: 'extraction',\n",
       " 304: 'problem',\n",
       " 305: 'german',\n",
       " 306: 'texts',\n",
       " 307: 'multilingual',\n",
       " 308: 'context',\n",
       " 309: 'network',\n",
       " 310: 'collection',\n",
       " 311: 'confidence',\n",
       " 312: 'bayes',\n",
       " 313: '1t',\n",
       " 314: 'metric',\n",
       " 315: 'loss',\n",
       " 316: 'chang',\n",
       " 317: 'rouge',\n",
       " 318: 'not',\n",
       " 319: 'where',\n",
       " 320: 'analyses',\n",
       " 321: 'flickinger',\n",
       " 322: 'clustering',\n",
       " 323: 'previous',\n",
       " 324: 'concept',\n",
       " 325: '7',\n",
       " 326: '50',\n",
       " 327: 'more',\n",
       " 328: 'they',\n",
       " 329: '1997',\n",
       " 330: 'framework',\n",
       " 331: 'initial',\n",
       " 332: 'different',\n",
       " 333: 'information',\n",
       " 334: 'structure',\n",
       " 335: 'addition',\n",
       " 336: 'prasad',\n",
       " 337: 'evaluate',\n",
       " 338: 'miller',\n",
       " 339: '1990',\n",
       " 340: 'experiment',\n",
       " 341: 'align',\n",
       " 342: 'chen',\n",
       " 343: 'modeling',\n",
       " 344: '8',\n",
       " 345: 'unsupervised',\n",
       " 346: 'cho',\n",
       " 347: 'chai',\n",
       " 348: 'create',\n",
       " 349: 'regularization',\n",
       " 350: 'well',\n",
       " 351: 'consists',\n",
       " 352: 'human',\n",
       " 353: 'gigaword',\n",
       " 354: 'types',\n",
       " 355: 'automatic',\n",
       " 356: 'growdiagfinaland',\n",
       " 357: 'ontonotes',\n",
       " 358: 'documents',\n",
       " 359: 'building',\n",
       " 360: 'follow',\n",
       " 361: 'annotations',\n",
       " 362: 'socher',\n",
       " 363: 'terms',\n",
       " 364: '95',\n",
       " 365: 'meanings',\n",
       " 366: 'naive',\n",
       " 367: 'google',\n",
       " 368: 'indomain',\n",
       " 369: 'function',\n",
       " 370: 'heuristic',\n",
       " 371: 'lm',\n",
       " 372: 'natural',\n",
       " 373: 'parsed',\n",
       " 374: 'automatically',\n",
       " 375: 'lexicalized',\n",
       " 376: 'hmm',\n",
       " 377: 'distribution',\n",
       " 378: 'sampling',\n",
       " 379: 'semantics',\n",
       " 380: 'erg',\n",
       " 381: 'methods',\n",
       " 382: 'mallet',\n",
       " 383: 'mccallum',\n",
       " 384: 'type',\n",
       " 385: 'will',\n",
       " 386: 'ie',\n",
       " 387: '1988',\n",
       " 388: 'subset',\n",
       " 389: 'syntactic',\n",
       " 390: 'large',\n",
       " 391: 'second',\n",
       " 392: 'dahlmeier',\n",
       " 393: 'finally',\n",
       " 394: 'languages',\n",
       " 395: 'tests',\n",
       " 396: 'apply',\n",
       " 397: 'evaluated',\n",
       " 398: 'directions',\n",
       " 399: 'database',\n",
       " 400: 'tokenised',\n",
       " 401: 'architecture',\n",
       " 402: 'weighted',\n",
       " 403: 'library',\n",
       " 404: 'libsvm',\n",
       " 405: 'decision',\n",
       " 406: 'minimal',\n",
       " 407: 'estimated',\n",
       " 408: 'pbmt',\n",
       " 409: 'pang',\n",
       " 410: '005',\n",
       " 411: 'estimation',\n",
       " 412: 'joint',\n",
       " 413: 'various',\n",
       " 414: 'made',\n",
       " 415: 'size',\n",
       " 416: 'forest',\n",
       " 417: 'created',\n",
       " 418: 'representations',\n",
       " 419: 'taken',\n",
       " 420: 'labeled',\n",
       " 421: 'generated',\n",
       " 422: 'development',\n",
       " 423: 'bullet',\n",
       " 424: 'implement',\n",
       " 425: '6',\n",
       " 426: 'figure',\n",
       " 427: 'monolingual',\n",
       " 428: 'online',\n",
       " 429: 'tnt',\n",
       " 430: 'extracting',\n",
       " 431: 'path',\n",
       " 432: 'popular',\n",
       " 433: 'pbsmt',\n",
       " 434: 'ranking',\n",
       " 435: 'parameter',\n",
       " 436: 'summarization',\n",
       " 437: 'gradient',\n",
       " 438: 'strube',\n",
       " 439: 'recently',\n",
       " 440: 'formalisms',\n",
       " 441: 'latent',\n",
       " 442: 'preprocessed',\n",
       " 443: 'fahrni',\n",
       " 444: 'dropout',\n",
       " 445: 'scikit',\n",
       " 446: 'term',\n",
       " 447: 'manually',\n",
       " 448: 'li',\n",
       " 449: 'parliament',\n",
       " 450: 'counts',\n",
       " 451: 'collins',\n",
       " 452: 'module',\n",
       " 453: 'written',\n",
       " 454: 'significant',\n",
       " 455: 'sennrich',\n",
       " 456: 'sets',\n",
       " 457: 'mikolov',\n",
       " 458: 'adopted',\n",
       " 459: 'selected',\n",
       " 460: 'measured',\n",
       " 461: 'follows',\n",
       " 462: 'see',\n",
       " 463: 'baroni',\n",
       " 464: 'running',\n",
       " 465: 'twitter',\n",
       " 466: 'truecased',\n",
       " 467: 'frequency',\n",
       " 468: 'allauzen',\n",
       " 469: 'rule',\n",
       " 470: 'domain',\n",
       " 471: 'brown',\n",
       " 472: 'level',\n",
       " 473: 'lemmatised',\n",
       " 474: 'goodman',\n",
       " 475: 'turian',\n",
       " 476: 'operations',\n",
       " 477: 'event',\n",
       " 478: 'argument',\n",
       " 479: 'time',\n",
       " 480: 'million',\n",
       " 481: 'uses',\n",
       " 482: 'liu',\n",
       " 483: 'nlp',\n",
       " 484: 'tested',\n",
       " 485: 'pdtb',\n",
       " 486: 'lexicon',\n",
       " 487: 'optimization',\n",
       " 488: '0001',\n",
       " 489: 'svms',\n",
       " 490: 'liblinear',\n",
       " 491: 'translations',\n",
       " 492: 'therefore',\n",
       " 493: 'three',\n",
       " 494: '10',\n",
       " 495: 'search',\n",
       " 496: 'rank',\n",
       " 497: 'description',\n",
       " 498: 'clark',\n",
       " 499: 'vogel',\n",
       " 500: 'datawe',\n",
       " 501: 'list',\n",
       " 502: 'conll',\n",
       " 503: 'openfst',\n",
       " 504: 'cohen',\n",
       " 505: 'e',\n",
       " 506: 'enpt',\n",
       " 507: '1000',\n",
       " 508: 'show',\n",
       " 509: 'presented',\n",
       " 510: 'hovy',\n",
       " 511: 'project',\n",
       " 512: 'wikipedia',\n",
       " 513: 'constraints',\n",
       " 514: 'complete',\n",
       " 515: 'constraint',\n",
       " 516: 'reported',\n",
       " 517: 'l2',\n",
       " 518: 'adadelta',\n",
       " 519: 'zeiler',\n",
       " 520: 'minibatch',\n",
       " 521: 'vapnik',\n",
       " 522: 'constituent',\n",
       " 523: 'there',\n",
       " 524: 'defined',\n",
       " 525: 'minimum',\n",
       " 526: 'markov',\n",
       " 527: 'optimizer',\n",
       " 528: 'probability',\n",
       " 529: 'hidden',\n",
       " 530: 'algorithms',\n",
       " 531: 'crf',\n",
       " 532: 'maximum',\n",
       " 533: '12',\n",
       " 534: 'error',\n",
       " 535: 'determine',\n",
       " 536: 'buckwalter',\n",
       " 537: 'collapsed',\n",
       " 538: 'symmetrization',\n",
       " 539: 'including',\n",
       " 540: 'portion',\n",
       " 541: 'parliamentwe',\n",
       " 542: 'rhetorical',\n",
       " 543: 'mann',\n",
       " 544: 'thompson',\n",
       " 545: 'when',\n",
       " 546: 'relies',\n",
       " 547: 'specify',\n",
       " 548: 'transducers',\n",
       " 549: 'wall',\n",
       " 550: 'street',\n",
       " 551: 'journal',\n",
       " 552: 'nucle',\n",
       " 553: 'python',\n",
       " 554: 'improve',\n",
       " 555: 'compared',\n",
       " 556: 'rapid',\n",
       " 557: 'fan',\n",
       " 558: 'tasks',\n",
       " 559: 'no',\n",
       " 560: '2016a',\n",
       " 561: 'koo',\n",
       " 562: 'measures',\n",
       " 563: 'papineni',\n",
       " 564: 'drawn',\n",
       " 565: 'semeval',\n",
       " 566: 'document',\n",
       " 567: 'pruning',\n",
       " 568: 'tags',\n",
       " 569: 'agirre',\n",
       " 570: 'pasha',\n",
       " 571: 'through',\n",
       " 572: 'spanish',\n",
       " 573: 'subtask',\n",
       " 574: 'via',\n",
       " 575: 'segmentation',\n",
       " 576: 'supervised',\n",
       " 577: 'universal',\n",
       " 578: 'tagset',\n",
       " 579: 'petrov',\n",
       " 580: 'configuration',\n",
       " 581: '1993',\n",
       " 582: 'spanishenglish',\n",
       " 583: 'full',\n",
       " 584: 'ccg',\n",
       " 585: 'aligned',\n",
       " 586: 'provides',\n",
       " 587: 'introduced',\n",
       " 588: 'lee',\n",
       " 589: 'additionally',\n",
       " 590: 'except',\n",
       " 591: 'updating',\n",
       " 592: 'way',\n",
       " 593: 'wordaligned',\n",
       " 594: 'recursion',\n",
       " 595: 'ner',\n",
       " 596: 'tjong',\n",
       " 597: 'sang',\n",
       " 598: 'meulder',\n",
       " 599: 'preprocessing',\n",
       " 600: 'gibbs',\n",
       " 601: 'related',\n",
       " 602: 'than',\n",
       " 603: 'linguistic',\n",
       " 604: 'bounds',\n",
       " 605: 'strings',\n",
       " 606: 'best',\n",
       " 607: 'learn',\n",
       " 608: 'procedure',\n",
       " 609: 'known',\n",
       " 610: 'experimentswe',\n",
       " 611: 'nouns',\n",
       " 612: 'detailed',\n",
       " 613: 'every',\n",
       " 614: 'srilm',\n",
       " 615: 'stolcke',\n",
       " 616: 'tweets',\n",
       " 617: 'given',\n",
       " 618: 'called',\n",
       " 619: 'output',\n",
       " 620: 'times',\n",
       " 621: 'essays',\n",
       " 622: 'comes',\n",
       " 623: 'accuracy',\n",
       " 624: 'candidate',\n",
       " 625: 'corpusbased',\n",
       " 626: '1998',\n",
       " 627: 'zhang',\n",
       " 628: 'basic',\n",
       " 629: 'trigram',\n",
       " 630: 'kneser',\n",
       " 631: 'participated',\n",
       " 632: 'tokenizer',\n",
       " 633: 'f',\n",
       " 634: 'loglinear',\n",
       " 635: 'directly',\n",
       " 636: 'matrix',\n",
       " 637: 'translated',\n",
       " 638: 'marcus',\n",
       " 639: 'rosenthal',\n",
       " 640: 'statistically',\n",
       " 641: 'consistent',\n",
       " 642: 'above',\n",
       " 643: 'under',\n",
       " 644: 'construct',\n",
       " 645: 'sequences',\n",
       " 646: 'collobert',\n",
       " 647: 'variant',\n",
       " 648: 'during',\n",
       " 649: 'common',\n",
       " 650: 'very',\n",
       " 651: 'optimize',\n",
       " 652: 'tiedemann',\n",
       " 653: 'adjunct',\n",
       " 654: 'pair',\n",
       " 655: 'research',\n",
       " 656: 'entity',\n",
       " 657: 'do',\n",
       " 658: 'clusters',\n",
       " 659: 'against',\n",
       " 660: 'produced',\n",
       " 661: 'mcdonald',\n",
       " 662: 'here',\n",
       " 663: 'compounds',\n",
       " 664: 'contrasted',\n",
       " 665: 'lesk',\n",
       " 666: 'idea',\n",
       " 667: 'differences',\n",
       " 668: 'machines',\n",
       " 669: 'kernels',\n",
       " 670: 'according',\n",
       " 671: 'formalism',\n",
       " 672: 'upon',\n",
       " 673: 'details',\n",
       " 674: 'gao',\n",
       " 675: 'many',\n",
       " 676: 'batch',\n",
       " 677: 'layer',\n",
       " 678: 'study',\n",
       " 679: 'input',\n",
       " 680: 'tag',\n",
       " 681: 'find',\n",
       " 682: 'components',\n",
       " 683: 'contextual',\n",
       " 684: 'long',\n",
       " 685: 'hierarchical',\n",
       " 686: 'processing',\n",
       " 687: 'memory',\n",
       " 688: 'calculate',\n",
       " 689: 'mentions',\n",
       " 690: 'duffy',\n",
       " 691: 'datasets',\n",
       " 692: 'learners',\n",
       " 693: 'quality',\n",
       " 694: 'movie',\n",
       " 695: 'reviews',\n",
       " 696: 'beam',\n",
       " 697: 'several',\n",
       " 698: 'ngrams',\n",
       " 699: 'n',\n",
       " 700: 'novel',\n",
       " 701: 'lda',\n",
       " 702: 'salton',\n",
       " 703: 'qcatrain',\n",
       " 704: 'bootstrapping',\n",
       " 705: 'heuristics',\n",
       " 706: 'madamira',\n",
       " 707: 'estimate',\n",
       " 708: 'simple',\n",
       " 709: 'particular',\n",
       " 710: 'general',\n",
       " 711: 'difference',\n",
       " 712: 'finite',\n",
       " 713: 'w',\n",
       " 714: 'present',\n",
       " 715: 'nivre',\n",
       " 716: 'ibm',\n",
       " 717: 'v7',\n",
       " 718: 'church',\n",
       " 719: 'describe',\n",
       " 720: 'core',\n",
       " 721: 'lowercased',\n",
       " 722: 'improvement',\n",
       " 723: 'produce',\n",
       " 724: 'connectives',\n",
       " 725: 'fixed',\n",
       " 726: 'setting',\n",
       " 727: 'treeswe',\n",
       " 728: 'considered',\n",
       " 729: 'stochastic',\n",
       " 730: 'scrambled',\n",
       " 731: 'entities',\n",
       " 732: 'review',\n",
       " 733: 'widely',\n",
       " 734: 'marker',\n",
       " 735: 'dutch',\n",
       " 736: 'named',\n",
       " 737: 'frequentcase',\n",
       " 738: 'chunking',\n",
       " 739: 'configurations',\n",
       " 740: '1986',\n",
       " 741: 'detail',\n",
       " 742: 'xue',\n",
       " 743: 'conll2003',\n",
       " 744: 'ver203',\n",
       " 745: 'pedersen',\n",
       " 746: 'crossentropy',\n",
       " 747: '2013a',\n",
       " 748: 'crfsuite',\n",
       " 749: 'okazaki',\n",
       " 750: 'propose',\n",
       " 751: 'refer',\n",
       " 752: 'calculated',\n",
       " 753: 'rbf',\n",
       " 754: '001',\n",
       " 755: 'coding',\n",
       " 756: '353',\n",
       " 757: 'assigned',\n",
       " 758: 'graph',\n",
       " 759: 'lfgit',\n",
       " 760: 'lfg',\n",
       " 761: 'relationswe',\n",
       " 762: 'associated',\n",
       " 763: 'distance',\n",
       " 764: 'applications',\n",
       " 765: 'computational',\n",
       " 766: 'semantically',\n",
       " 767: 'partial',\n",
       " 768: 'sptk',\n",
       " 769: '02',\n",
       " 770: 'showed',\n",
       " 771: 'among',\n",
       " 772: '20',\n",
       " 773: 'format',\n",
       " 774: 'transliteration',\n",
       " 775: 'topics',\n",
       " 776: 'filter',\n",
       " 777: 'hand',\n",
       " 778: '05',\n",
       " 779: 'aims',\n",
       " 780: 'wsj',\n",
       " 781: 'roth',\n",
       " 782: 'structures',\n",
       " 783: 'problems',\n",
       " 784: 'definition',\n",
       " 785: 'gb',\n",
       " 786: 'since',\n",
       " 787: 'released',\n",
       " 788: 'dhillon',\n",
       " 789: 'them',\n",
       " 790: 'sequence',\n",
       " 791: 'specifically',\n",
       " 792: 'prediction',\n",
       " 793: 'value',\n",
       " 794: 'experimented',\n",
       " 795: 'labels',\n",
       " 796: 'querying',\n",
       " 797: 'those',\n",
       " 798: 'penalty',\n",
       " 799: 'short',\n",
       " 800: 'unigrams',\n",
       " 801: 'national',\n",
       " 802: 'interval',\n",
       " 803: 'collected',\n",
       " 804: 'knowledge',\n",
       " 805: 'conditional',\n",
       " 806: 'generative',\n",
       " 807: 'achieved',\n",
       " 808: 'provide',\n",
       " 809: 'typically',\n",
       " 810: 'another',\n",
       " 811: 'stage',\n",
       " 812: 'sense',\n",
       " 813: 'firth',\n",
       " 814: '1957',\n",
       " 815: 'cooccurrence',\n",
       " 816: 'space',\n",
       " 817: 'symmetrizing',\n",
       " 818: 'perplexity',\n",
       " 819: 'interpolation',\n",
       " 820: 'develop',\n",
       " 821: 'approximate',\n",
       " 822: 'lmplz',\n",
       " 823: 'constructed',\n",
       " 824: 'selection',\n",
       " 825: 'bitext',\n",
       " 826: 'coreference',\n",
       " 827: 'form',\n",
       " 828: 'postagged',\n",
       " 829: 'holmqvist',\n",
       " 830: 'reordered',\n",
       " 831: 'polish',\n",
       " 832: 's2e',\n",
       " 833: 'any',\n",
       " 834: 'semeval2014',\n",
       " 835: '9',\n",
       " 836: 'unigram',\n",
       " 837: 'component',\n",
       " 838: 'token',\n",
       " 839: 'discounting',\n",
       " 840: 'm',\n",
       " 841: 'frenchenglish',\n",
       " 842: 'construction',\n",
       " 843: 'synonym',\n",
       " 844: 'approximately',\n",
       " 845: 'parliamentary',\n",
       " 846: 'current',\n",
       " 847: 'descent',\n",
       " 848: 'up',\n",
       " 849: 'consider',\n",
       " 850: 'explanation',\n",
       " 851: 'chose',\n",
       " 852: 'effective',\n",
       " 853: 'make',\n",
       " 854: 'date',\n",
       " 855: 'derived',\n",
       " 856: 'eg',\n",
       " 857: 'dictionary',\n",
       " 858: 'map',\n",
       " 859: 'york',\n",
       " 860: 'select',\n",
       " 861: 'closest',\n",
       " 862: 'sourcetotarget',\n",
       " 863: 'base',\n",
       " 864: 'segment',\n",
       " 865: 'applying',\n",
       " 866: 'mrs',\n",
       " 867: 'danish',\n",
       " 868: 'arguments',\n",
       " 869: '100',\n",
       " 870: 'previously',\n",
       " 871: 'report',\n",
       " 872: 'news',\n",
       " 873: 'restriction',\n",
       " 874: 'sentencesplit',\n",
       " 875: 'networks',\n",
       " 876: 'speech',\n",
       " 877: 'workshop',\n",
       " 878: 'caseless',\n",
       " 879: 'content',\n",
       " 880: 'recursive',\n",
       " 881: 'further',\n",
       " 882: 'adopt',\n",
       " 883: 'word2vec',\n",
       " 884: 'cortes',\n",
       " 885: 'curran',\n",
       " 886: 'gold',\n",
       " 887: 'represents',\n",
       " 888: 'msd',\n",
       " 889: 'finkelstein',\n",
       " 890: 'containing',\n",
       " 891: 'ratings',\n",
       " 892: 'abstract',\n",
       " 893: 'preference',\n",
       " 894: 'probabilistic',\n",
       " 895: 'levenshtein',\n",
       " 896: '1966',\n",
       " 897: 'logic',\n",
       " 898: 'taylor',\n",
       " 899: 'cristianini',\n",
       " 900: 'l',\n",
       " 901: 'dyer',\n",
       " 902: 'grammars',\n",
       " 903: 'direct',\n",
       " 904: '1968',\n",
       " 905: 'linguistics',\n",
       " 906: 'henderson',\n",
       " 907: 'freely',\n",
       " 908: 'implementations',\n",
       " 909: 'recognize',\n",
       " 910: 'rely',\n",
       " 911: 'depth',\n",
       " 912: 'vilar',\n",
       " 913: 'maxent',\n",
       " 914: 'good',\n",
       " 915: 'mani',\n",
       " 916: 'habash',\n",
       " 917: 'xinhua',\n",
       " 918: 'multiclass',\n",
       " 919: 'represented',\n",
       " 920: 'stem',\n",
       " 921: 'polynomial',\n",
       " 922: 'griffiths',\n",
       " 923: 'steyvers',\n",
       " 924: 'd',\n",
       " 925: 'around',\n",
       " 926: 'nps',\n",
       " 927: 'weischedel',\n",
       " 928: 'shallow',\n",
       " 929: 'setupfor',\n",
       " 930: 'setup',\n",
       " 931: 'rst',\n",
       " 932: 'foster',\n",
       " 933: 'solved',\n",
       " 934: 'examples',\n",
       " 935: 'yeh',\n",
       " 936: 'benefit',\n",
       " 937: 'does',\n",
       " 938: 'mapping',\n",
       " 939: 'focus',\n",
       " 940: 'rules',\n",
       " 941: 'small',\n",
       " 942: 'organizers',\n",
       " 943: 'brill',\n",
       " 944: 'had',\n",
       " 945: 'appearing',\n",
       " 946: 'similarities',\n",
       " 947: 'nonensembled',\n",
       " 948: 'lefttoright',\n",
       " 949: 'means',\n",
       " 950: 'levels',\n",
       " 951: 'gram',\n",
       " 952: 'introduce',\n",
       " 953: 'recurrent',\n",
       " 954: 'swedish',\n",
       " 955: 'however',\n",
       " 956: 'huang',\n",
       " 957: 'case',\n",
       " 958: 'whole',\n",
       " 959: 'across',\n",
       " 960: 'spoken',\n",
       " 961: 'walker',\n",
       " 962: 'marelli',\n",
       " 963: '500',\n",
       " 964: 'dynamic',\n",
       " 965: '1989',\n",
       " 966: 'combination',\n",
       " 967: 'indicates',\n",
       " 968: 'wang',\n",
       " 969: 'henceforth',\n",
       " 970: 'selecting',\n",
       " 971: 'ltag',\n",
       " 972: 'solve',\n",
       " 973: 'complexity',\n",
       " 974: '2017',\n",
       " 975: 'metrics',\n",
       " 976: 'exploit',\n",
       " 977: 'mgiza',\n",
       " 978: 'statistic',\n",
       " 979: 'meaningsdistributional',\n",
       " 980: 'process',\n",
       " 981: 'compute',\n",
       " 982: 'next',\n",
       " 983: 'fed',\n",
       " 984: 'employed',\n",
       " 985: '6gram',\n",
       " 986: 'lemmatise',\n",
       " 987: 'semeval2013',\n",
       " 988: 'wilson',\n",
       " 989: 'b',\n",
       " 990: 'adaptation',\n",
       " 991: 'existing',\n",
       " 992: 'negra',\n",
       " 993: 'count',\n",
       " 994: 'approaches',\n",
       " 995: 'objective',\n",
       " 996: 'topic',\n",
       " 997: 'wfsts',\n",
       " 998: 'libraries',\n",
       " 999: 'script',\n",
       " 1000: 'detect',\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([439, 523, 129, 123, 7, 1403, 1116, 9, 1404, 1, 603, 2140, 2141, 1, 755, 49, 888, 4, 1405, 1406, 2, 3, 117, 439, 523, 129, 123, 7, 1403, 1116, 9, 1404, 1, 755, 49, 888, 4, 1405, 1406, 2, 3, 117]),\n",
       "       list([1, 118, 145, 13, 1, 1117, 183, 889, 2, 3, 156, 890, 756, 218, 5, 44, 63, 34, 95, 123, 757, 142, 891, 26, 1407, 118, 145, 13, 1, 1117, 183, 889, 2, 3, 156, 42, 240, 756, 218, 5, 44, 63, 34, 95, 123, 757, 142, 891, 26, 1408]),\n",
       "       list([892, 157, 287, 1409, 1410, 2, 3, 88, 13, 7, 130, 671, 319, 1, 157, 5, 7, 136, 13, 2142, 31, 7, 1411, 1412, 2143, 157, 287, 1409, 1410, 2, 3, 88, 13, 7, 130, 671, 2144, 1, 157, 5, 7, 136, 31, 7, 1411, 1412, 758]),\n",
       "       ...,\n",
       "       list([48, 384, 5, 1027, 250, 33, 318, 15, 26, 1028, 2, 3, 40, 294, 2139, 4, 86, 3444, 231, 1028, 2, 3, 40, 2133, 1, 18, 5, 1027, 93, 26, 10, 600, 378]),\n",
       "       list([48, 384, 5, 1027, 250, 33, 318, 15, 26, 1028, 2, 3, 40, 294, 2139, 4, 86, 3445, 1399, 286, 5, 1, 97, 103, 14, 1027, 93, 13, 1400, 602, 34, 5, 1028, 2, 3, 40, 4, 1109, 14, 34, 5, 2139, 4, 86, 37]),\n",
       "       list([91, 379, 13, 61, 23, 1, 666, 34, 813, 814, 8, 205, 63, 1, 157, 5, 7, 30, 13, 601, 9, 1, 149, 98, 2128, 2129, 2130, 1396, 2131, 1397, 1398, 7, 30, 26, 1, 1110, 98, 1111, 813, 814])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) \n",
    "sequences = np.array(sequences) #dikelompokkan per satu array\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[439, 523, 129, 123, 7, 1403, 1116, 9, 1404, 1, 603, 2140, 2141, 1, 755, 49, 888, 4, 1405, 1406, 2, 3, 117, 439, 523, 129, 123, 7, 1403, 1116, 9, 1404, 1, 755, 49, 888, 4, 1405, 1406, 2, 3, 117], [1, 118, 145, 13, 1, 1117, 183, 889, 2, 3, 156, 890, 756, 218, 5, 44, 63, 34, 95, 123, 757, 142, 891, 26, 1407, 118, 145, 13, 1, 1117, 183, 889, 2, 3, 156, 42, 240, 756, 218, 5, 44, 63, 34, 95, 123, 757, 142, 891, 26, 1408], [892, 157, 287, 1409, 1410, 2, 3, 88, 13, 7, 130, 671, 319, 1, 157, 5, 7, 136, 13, 2142, 31, 7, 1411, 1412, 2143, 157, 287, 1409, 1410, 2, 3, 88, 13, 7, 130, 671, 2144, 1, 157, 5, 7, 136, 31, 7, 1411, 1412, 758], [6, 262, 77, 80, 14, 604, 411, 31, 138, 8, 12, 36, 6, 262, 77, 80, 14, 604, 411, 31, 138, 26, 12, 36], [98, 13, 15, 9, 158, 130, 320, 8, 264, 44, 251, 146, 380, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 759, 13, 15, 9, 158, 130, 320, 8, 1, 264, 44, 251, 146, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 760], [6, 18, 39, 52, 74, 71, 2, 3, 37, 9, 210, 160, 200, 4, 55, 761, 118, 18, 7, 55, 52, 74, 71, 2, 3, 37, 9, 160, 111, 136, 4, 219, 1, 75, 5, 55, 242, 762, 14, 1, 136], [1413, 2, 3, 288, 750, 68, 2145, 2146, 322, 103, 11, 1118, 893, 1414, 61, 23, 7, 894, 441, 2147, 2148, 1413, 2, 3, 288, 750, 68, 2149, 322, 2150, 11, 2151], [1, 895, 763, 895, 896, 211, 112, 605, 13, 524, 31, 1, 525, 212, 5, 2152, 476, 2153, 2154, 4, 2155, 895, 763, 2156, 68, 2157, 5, 1, 142, 211, 112, 605, 895, 896], [6, 18, 1, 17, 20, 12, 2, 3, 16, 9, 348, 7, 70, 27, 56, 35, 19, 66, 23, 1, 606, 442, 28, 31, 138, 1415, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [6, 262, 77, 80, 14, 604, 411, 31, 138, 8, 12, 36, 6, 262, 77, 80, 14, 604, 411, 31, 138, 26, 12, 36], [98, 13, 15, 9, 158, 130, 320, 8, 264, 44, 251, 146, 380, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 759, 13, 15, 9, 158, 130, 320, 8, 1, 264, 44, 251, 146, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 760], [98, 13, 15, 9, 158, 130, 320, 8, 1, 44, 264, 146, 380, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 759, 13, 15, 9, 158, 130, 320, 8, 1, 264, 44, 251, 146, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 760], [98, 13, 15, 9, 158, 130, 320, 8, 264, 44, 146, 380, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 759, 13, 15, 9, 158, 130, 320, 8, 1, 264, 44, 251, 146, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 760], [98, 13, 15, 9, 158, 130, 320, 8, 1, 44, 264, 146, 380, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 759, 13, 15, 9, 158, 130, 320, 8, 1, 264, 44, 251, 146, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 760], [6, 121, 672, 24, 323, 526, 897, 61, 128, 11, 412, 324, 221, 4, 322, 443, 4, 438, 64, 6, 121, 672, 24, 323, 128, 11, 412, 324, 221, 4, 322, 443, 4, 438, 64], [673, 1119, 99, 4, 2158, 154, 104, 289, 8, 898, 4, 899, 36, 673, 1119, 99, 4, 2159, 154, 104, 289, 8, 898, 4, 899, 36], [6, 607, 1, 106, 10, 7, 1416, 608, 14, 900, 83, 1417, 349, 1418, 4, 674, 16, 6, 607, 1, 106, 10, 7, 1416, 2160, 608, 14, 900, 83, 1417, 349, 1418, 4, 674, 16], [6, 18, 1, 1419, 161, 1420, 901, 2, 3, 117, 253, 4, 121, 902, 10, 124, 81, 5, 1, 1421, 1422, 303, 82, 138, 8, 1423, 2161, 903, 35, 6, 18, 1, 1419, 161, 1420, 901, 2, 3, 117, 253, 4, 121, 902, 10, 124, 81, 5, 1, 1421, 1422, 303, 82, 138, 8, 1423, 16], [48, 13, 609, 31, 1, 91, 113, 114, 904, 48, 13, 609, 31, 1, 91, 113, 8, 905, 114, 904], [51, 24, 65, 31, 350, 31, 1, 52, 138, 8, 906, 43, 32, 232, 254, 2162, 65, 31, 350, 31, 1, 52, 138, 8, 906, 43, 32, 232, 254, 2163], [11, 605, 675, 290, 162, 1120, 1424, 14, 413, 764, 8, 765, 1425, 4, 765, 905, 898, 4, 899, 36, 11, 605, 7, 2164, 5, 290, 162, 1120, 1424, 14, 675, 764, 8, 765, 1425, 4, 765, 905, 898, 4, 899, 36], [1, 411, 5, 1, 766, 291, 767, 189, 162, 768, 13, 414, 169, 26, 68, 1121, 213, 5, 1426, 108, 170, 1427, 37, 1, 411, 5, 1, 766, 291, 767, 189, 162, 768, 13, 414, 169, 26, 68, 1121, 213, 5, 1426, 108, 325, 1427, 37], [6, 69, 14, 1, 243, 527, 244, 4, 245, 201, 7, 78, 166, 5, 1122, 676, 415, 5, 326, 4, 444, 14, 528, 769, 214, 9, 1, 529, 2165, 69, 14, 1, 243, 527, 244, 4, 245, 201, 7, 78, 166, 5, 1122, 676, 415, 5, 326, 4, 444, 14, 528, 769, 214, 9, 1, 529, 677], [8, 24, 1123, 678, 6, 18, 1, 907, 169, 908, 8, 202, 1428, 4, 1429, 40, 8, 24, 1123, 678, 6, 18, 1, 907, 169, 81, 5, 99, 8, 202, 1428, 4, 1429, 40], [327, 439, 1430, 4, 1431, 117, 770, 34, 1, 1432, 5, 1, 1124, 1433, 26, 2166, 530, 1125, 1, 1434, 5, 7, 1435, 322, 2167, 1430, 4, 1431, 117, 770, 34, 1, 1432, 5, 1, 1124, 1433, 26, 2168, 530, 1125, 1, 1434, 5, 7, 1435, 322, 128], [328, 32, 61, 23, 1, 91, 113, 114, 904, 4, 26, 1436, 190, 7, 75, 5, 477, 1437, 1438, 478, 1439, 95, 7, 72, 377, 328, 1440, 9, 909, 1441, 2169, 381, 910, 23, 1, 91, 113, 114, 904, 4, 26, 1436, 190, 7, 75, 5, 477, 1437, 1438, 478, 1439, 95, 7, 72, 377, 1440, 9, 909, 1441, 477], [30, 73, 13, 109, 10, 59, 47, 4, 45, 43, 30, 73, 13, 233, 10, 59, 47, 4, 45, 43], [30, 73, 13, 109, 10, 59, 47, 4, 45, 43, 30, 73, 13, 233, 10, 59, 47, 4, 45, 43], [6, 15, 382, 108, 383, 87, 11, 531, 610, 15, 382, 108, 383, 87, 11, 2170, 58], [1, 2171, 351, 5, 7, 1126, 5, 1442, 130, 1127, 524, 11, 206, 1443, 611, 14, 7, 532, 911, 5, 1444, 1445, 2, 3, 329, 1, 2172, 2173, 351, 5, 7, 1126, 5, 1442, 130, 1127, 524, 11, 206, 1443, 611, 14, 7, 532, 911, 5, 533, 1445, 2, 3, 329], [1, 612, 1446, 13, 176, 8, 1, 1128, 213, 5, 1, 263, 278, 2, 3, 88, 7, 612, 1446, 23, 1, 188, 13, 176, 8, 1, 1128, 213, 5, 1, 263, 278, 2, 3, 88], [1, 118, 145, 13, 1, 2174, 756, 131, 183, 889, 2, 3, 156, 890, 756, 218, 5, 44, 63, 34, 95, 123, 757, 142, 891, 26, 1407, 118, 145, 13, 1, 1117, 183, 889, 2, 3, 156, 42, 240, 756, 218, 5, 44, 63, 34, 95, 123, 757, 142, 891, 26, 1408], [7, 330, 11, 352, 534, 115, 4, 534, 215, 129, 123, 97, 8, 912, 2, 3, 37, 241, 252, 352, 143, 48, 13, 76, 7, 479, 1129, 2175, 330, 11, 352, 534, 115, 129, 123, 97, 8, 912, 2, 3, 37, 241, 31, 613, 352, 143, 48, 13, 76, 7, 479, 1129, 62], [913, 184, 13, 7, 914, 231, 5, 48, 1130, 915, 2, 3, 37, 913, 184, 13, 68, 231, 5, 48, 1130, 915, 2, 3, 37], [771, 172, 2176, 1447, 13, 145, 5, 1, 1448, 4, 1449, 1450, 5, 133, 61, 1451, 1452, 8, 1, 207, 772, 134, 2, 3, 16, 1447, 13, 145, 5, 1, 2177, 1448, 4, 1449, 1450, 5, 133, 61, 1451, 1452, 8, 1, 207, 772, 134, 2, 3, 16], [11, 1, 886, 599, 4, 51, 3433, 302, 6, 751, 1, 3434, 9, 1, 159, 62, 2127, 263, 3435, 2, 3, 88], [11, 599, 6, 15, 1131, 279, 115, 4, 221, 11, 292, 916, 2, 3, 57, 42, 13, 145, 5, 1, 255, 1132, 292, 599, 2178, 48, 265, 6, 18, 1131, 279, 115, 4, 221, 11, 292, 916, 2, 3, 57, 42, 13, 145, 5, 1, 255, 1132, 292, 599, 2179], [6, 50, 7, 100, 41, 19, 23, 1, 917, 185, 5, 1, 44, 353, 22, 1133, 480, 63, 10, 1, 614, 20, 615, 87, 14, 1, 155, 116, 2180, 100, 41, 19, 33, 50, 23, 1, 917, 185, 5, 1, 44, 353, 22, 1133, 480, 63, 10, 1, 614, 20, 615, 87, 14, 155, 116, 173], [9, 535, 130, 384, 4, 2181, 6, 69, 112, 99, 918, 163, 10, 99, 918, 1453, 2, 3, 36, 9, 535, 130, 354, 4, 2182, 6, 69, 112, 99, 918, 163, 10, 99, 918, 1453, 2, 3, 36], [6, 18, 1, 445, 81, 5, 256, 416, 79, 2, 3, 25, 6, 18, 1, 445, 81, 5, 99, 79, 2, 3, 25], [111, 446, 8, 1, 679, 133, 385, 104, 919, 26, 124, 920, 4, 119, 680, 8, 1, 280, 773, 2183, 10, 536, 774, 536, 87, 111, 446, 8, 1, 679, 133, 13, 919, 26, 124, 920, 4, 119, 680, 10, 536, 774, 536, 87], [68, 103, 1, 1454, 82, 1455, 1456, 154, 681, 1457, 9, 1, 1458, 1134, 304, 8, 921, 2184, 103, 1, 1454, 82, 1455, 1456, 129, 123, 97, 34, 154, 681, 7, 1459, 9, 1, 1458, 1134, 304, 8, 921, 479], [6, 18, 537, 600, 378, 922, 4, 923, 36, 9, 1460, 1, 106, 5, 1, 19, 4, 1, 441, 2185, 1135, 4, 775, 2186, 11, 616, 617, 1461, 28, 924, 2187, 18, 537, 600, 378, 922, 4, 923, 36, 9, 1460, 1, 106, 5, 1, 19, 617, 1461, 28, 924, 600, 378, 13, 7, 526, 2188, 2189, 2190, 82, 42, 1462, 2191, 2192], [6, 18, 1, 39, 55, 52, 71, 2, 3, 37, 6, 18, 1, 39, 52, 14, 39, 191, 74, 71, 2, 3, 37], [776, 1136, 32, 1463, 10, 1464, 1137, 1465, 4, 1466, 117, 1136, 32, 1463, 10, 1464, 1137, 1465, 4, 1466, 117], [355, 136, 73, 5, 1, 46, 28, 33, 176, 26, 1467, 305, 4, 1, 777, 132, 5, 1, 63, 8, 1, 147, 28, 67, 417, 26, 246, 47, 4, 1468, 45, 47, 4, 45, 43, 355, 136, 73, 5, 1, 46, 28, 33, 176, 26, 1467, 305, 4, 1, 777, 132, 5, 1, 63, 8, 1, 2193, 4, 147, 28, 67, 417, 26, 246, 47, 4, 1468, 45, 47, 4, 45, 43], [6, 18, 1, 150, 527, 151, 2, 3, 25, 14, 331, 78, 166, 75, 9, 1469, 18, 150, 151, 2, 3, 25, 14, 1, 331, 78, 166, 75, 9, 778], [192, 6, 1138, 30, 73, 10, 59, 47, 4, 45, 43, 14, 1, 107, 356, 73, 538, 1139, 109, 30, 73, 10, 59, 47, 4, 45, 43, 14, 1, 107, 356, 73, 538, 82], [11, 231, 1470, 134, 4, 1471, 156, 779, 9, 1472, 332, 418, 5, 1, 139, 130, 293, 386, 72, 55, 2194, 231, 1470, 134, 4, 1471, 156, 779, 9, 1472, 332, 418, 5, 1, 139, 130, 293, 10, 91, 142, 5, 55, 2195], [1, 164, 33, 109, 447, 10, 1, 266, 164, 122, 267, 2, 3, 64, 1, 164, 33, 109, 10, 1, 266, 94, 122, 267, 2, 3, 64], [29, 97, 26, 448, 2, 3, 37, 481, 7, 1473, 128, 9, 216, 136, 2196, 72, 130, 142, 216, 97, 26, 448, 2, 3, 37, 481, 7, 1473, 128, 9, 216, 136, 142], [48, 22, 240, 925, 1474, 926, 247, 11, 333, 1475, 539, 1476, 1140, 926, 4, 248, 2197, 8, 326, 306, 419, 21, 1, 780, 540, 5, 1, 357, 22, 927, 2, 3, 2198, 351, 5, 326, 306, 419, 21, 1, 780, 540, 5, 1, 357, 22, 927, 2, 3, 25, 14, 1141, 1474, 926, 247, 11, 333, 1475, 539, 1476, 1140, 926, 4, 248, 1477], [51, 1478, 1142, 31, 679, 1, 22, 358, 442, 14, 7, 186, 171, 253, 4, 928, 52, 170, 1479, 4, 781, 156, 51, 682, 1142, 31, 679, 1, 22, 358, 442, 14, 7, 186, 171, 94, 4, 928, 52, 131, 1479, 4, 781, 156], [21, 1, 1480, 167, 5, 1481, 165, 683, 142, 129, 123, 15, 11, 1143, 11, 7, 684, 2199, 1, 1480, 167, 5, 1481, 165, 1143, 21, 1109, 144, 129, 123, 1482, 11, 7, 684, 479], [54, 12, 40, 13, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 541, 232, 24, 58, 23, 54, 12, 40, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 449], [1144, 35, 143, 5, 125, 14, 1483, 115, 33, 118, 97, 8, 482, 2, 3, 117, 1484, 33, 618, 1144, 8, 482, 2, 3, 117], [11, 359, 1, 101, 38, 29, 6, 15, 1, 177, 38, 20, 17, 12, 2, 3, 16, 8, 124, 60, 929, 359, 24, 38, 49, 1, 177, 38, 20, 17, 12, 2, 3, 16, 33, 15, 8, 124, 60, 930], [542, 334, 234, 931, 543, 4, 544, 387, 145, 5, 1, 255, 1145, 1485, 5, 137, 887, 306, 26, 420, 685, 782, 618, 137, 200, 2200, 334, 234, 931, 543, 4, 544, 387, 145, 5, 1, 255, 1145, 1485, 5, 137, 2201, 7, 189, 287, 5, 7, 137, 609, 31, 7, 137, 189, 1486], [8, 335, 1, 1487, 82, 8, 932, 2, 3, 37, 11, 126, 193, 173, 13, 76, 2202, 335, 1, 1487, 82, 932, 2, 3, 37, 11, 126, 193, 173, 33, 76, 15], [2203, 41, 686, 1488, 4, 1489, 1490, 1491, 40, 13, 61, 23, 1, 666, 34, 483, 783, 154, 104, 933, 26, 1492, 5, 933, 934, 5, 1, 304, 1493, 8, 2204, 41, 686, 1488, 4, 1489, 1490, 1491, 40, 13, 61, 23, 1, 666, 34, 483, 783, 154, 104, 933, 26, 1492, 5, 933, 934, 5, 1, 304, 8, 687], [6, 688, 70, 89, 5, 286, 667, 10, 1146, 1147, 935, 84, 6, 484, 1, 89, 5, 667, 10, 1146, 1147, 935, 84], [11, 1148, 56, 35, 235, 49, 154, 936, 21, 46, 23, 125, 174, 21, 85, 294, 1109, 358, 1494, 21, 1, 207, 1495, 4, 1496, 40, 8, 335, 56, 35, 235, 49, 154, 104, 1149, 26, 46, 23, 125, 174, 21, 85, 294, 1109, 358, 1497, 21, 1, 207, 1495, 4, 1496, 40], [83, 14, 94, 349, 10, 150, 151, 2, 3, 25, 83, 349, 10, 150, 151, 2, 3, 25], [2205, 937, 1, 2206, 19, 2207, 2208, 1150, 1, 1498, 1499, 2, 3, 165, 222, 13, 1, 525, 212, 5, 1500, 211, 689, 9, 104, 1501, 294, 1502, 545, 938, 1, 619, 9, 7, 886, 2209, 1498, 222, 1499, 2, 3, 165, 450, 1, 525, 212, 5, 1500, 211, 689, 9, 104, 1501, 294, 1502, 545, 938, 7, 29, 2210, 9, 7, 886, 60, 2211, 75], [8, 1, 308, 5, 48, 263, 6, 385, 104, 2212, 23, 1, 388, 189, 2213, 162, 138, 8, 451, 4, 690, 87, 42, 546, 23, 7, 1503, 784, 34, 937, 318, 1151, 9, 1504, 2214, 385, 939, 23, 1, 389, 189, 162, 138, 8, 451, 4, 690, 87, 42, 546, 23, 7, 1503, 784, 34, 937, 318, 1151, 9, 1504, 2215, 940], [54, 12, 40, 13, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 2216, 54, 22, 12, 40, 13, 66, 21, 1, 208, 5, 1, 203, 449], [6, 95, 15, 1505, 7, 1152, 108, 122, 9, 547, 1506, 1507, 4, 548, 1508, 57, 1, 452, 33, 175, 10, 1505, 7, 1152, 108, 122, 9, 547, 1506, 1507, 4, 548, 1508, 57], [6, 15, 1, 139, 147, 75, 15, 8, 448, 2, 3, 36, 11, 24, 281, 170, 6, 15, 1, 139, 147, 28, 31, 8, 448, 2, 3, 36], [1, 217, 137, 187, 485, 336, 2, 3, 90, 13, 7, 390, 22, 247, 14, 137, 242, 1509, 1, 549, 550, 551, 236, 5, 1, 217, 2217, 137, 187, 1, 217, 137, 187, 485, 13, 7, 22, 5, 549, 550, 551, 2218, 247, 14, 137, 242, 336, 2, 3, 90], [6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25, 6, 15, 7, 785, 81, 5, 1, 92, 152, 79, 2, 3, 25], [786, 1, 1153, 15, 30, 142, 691, 1154, 7, 941, 212, 5, 30, 218, 6, 76, 18, 1, 1510, 183, 1511, 2, 3, 64, 5, 1512, 30, 218, 1155, 21, 63, 34, 194, 190, 1156, 1513, 620, 8, 7, 390, 207, 1514, 391, 13, 1, 1510, 183, 1511, 2, 3, 64, 5, 1512, 63, 218, 1155, 21, 63, 34, 194, 190, 1156, 1513, 620, 8, 7, 390, 207, 22], [1, 46, 28, 5, 1, 159, 62, 13, 1, 552, 22, 392, 2, 3, 88, 42, 240, 621, 453, 26, 692, 5, 44, 1, 46, 28, 787, 26, 1, 62, 942, 622, 21, 1, 552, 22, 392, 2, 3, 88, 42, 240, 621, 453, 26, 692, 5, 44, 31, 7, 1515, 41], [51, 29, 81, 33, 233, 10, 553, 4, 1, 177, 56, 78, 20, 92, 79, 2, 3, 25, 51, 5, 1, 56, 78, 33, 233, 10, 92, 79, 2, 3, 25], [98, 129, 123, 268, 34, 7, 1516, 75, 5, 2219, 154, 104, 15, 9, 1157, 554, 161, 623, 11, 413, 783, 8, 483, 906, 4, 943, 288, 98, 129, 123, 684, 1158, 8, 483, 34, 7, 1516, 75, 5, 1457, 21, 7, 161, 154, 104, 2220, 294, 2221, 8, 269, 9, 554, 1, 623, 8, 413, 783, 906, 4, 943, 288], [8, 1, 88, 29, 6, 944, 15, 1159, 486, 1517, 2, 3, 117, 8, 24, 29, 6, 15, 1, 195, 486, 176, 26, 1159, 1517, 2, 3, 117], [393, 6, 76, 225, 1, 693, 5, 1, 624, 126, 178, 14, 30, 178, 788, 2, 3, 25, 26, 1160, 789, 31, 93, 8, 7, 531, 61, 790, 2222, 76, 555, 1, 693, 5, 1, 624, 126, 178, 14, 1, 2223, 178, 26, 1160, 789, 31, 93, 788, 2, 3, 25, 1161, 14, 1, 101, 93, 8, 1, 531, 171], [172, 381, 32, 61, 23, 1, 91, 113, 42, 270, 34, 63, 945, 8, 1, 139, 149, 237, 9, 95, 72, 157, 114, 127, 625, 1518, 360, 1, 60, 91, 113, 42, 270, 34, 63, 945, 8, 1, 139, 149, 237, 9, 95, 72, 157, 114, 127], [51, 361, 67, 233, 10, 1, 266, 556, 164, 122, 267, 2, 3, 64, 1, 361, 67, 414, 10, 1, 266, 556, 164, 122, 267, 2, 3, 64], [555, 9, 249, 1162, 626, 523, 32, 946, 241, 76, 454, 2224, 9, 249, 1162, 626, 523, 32, 946, 31, 350, 31, 2225, 667], [11, 46, 6, 18, 243, 244, 4, 245, 201, 11, 487, 14, 68, 331, 78, 166, 5, 1519, 18, 243, 244, 4, 245, 201, 11, 487, 14, 331, 78, 166, 5, 488], [11, 24, 184, 6, 18, 489, 791, 1, 490, 99, 108, 152, 557, 2, 3, 90, 42, 13, 1163, 9, 133, 215, 558, 14, 390, 1164, 5, 93, 4, 390, 2226, 6, 18, 1, 490, 99, 152, 557, 2, 3, 90, 31, 98, 13, 1163, 9, 133, 215, 558, 14, 390, 1164, 5, 93, 4, 306], [1, 118, 112, 58, 2227, 1, 792, 5, 1, 195, 5, 694, 695, 8, 1, 39, 195, 187, 362, 2, 3, 2228, 118, 112, 58, 2229, 2230, 1, 195, 5, 694, 695, 362, 2, 3, 88], [6, 15, 254, 1, 947, 948, 232, 386, 559, 1520, 1521, 31, 233, 26, 455, 2, 3, 560, 14, 696, 415, 5, 170, 170, 1165, 1522, 1, 1523, 1524, 15, 254, 1, 947, 948, 232, 386, 559, 1520, 1521, 31, 233, 26, 455, 2, 3, 560, 14, 696, 415, 5, 533, 107, 793], [6, 15, 254, 1, 947, 948, 232, 455, 2, 3, 560, 14, 696, 415, 5, 170, 170, 1165, 1522, 1, 1523, 1524, 15, 254, 1, 947, 948, 232, 455, 2, 3, 560, 14, 696, 415, 5, 533, 107, 793], [1, 888, 279, 755, 29, 33, 1166, 11, 7, 2231, 5, 394, 539, 1525, 1526, 36, 1, 888, 279, 755, 29, 13, 7, 2232, 755, 29, 1166, 11, 697, 394, 1526, 36], [1, 126, 1167, 67, 421, 26, 949, 5, 1168, 30, 132, 153, 14, 59, 47, 4, 45, 43, 1, 126, 193, 33, 421, 2233, 1168, 30, 132, 153, 14, 59, 47, 4, 45, 43], [30, 73, 13, 109, 10, 59, 47, 4, 45, 43, 30, 73, 13, 109, 14, 59, 47, 4, 45, 43], [6, 794, 14, 697, 950, 5, 1169, 1170, 10, 422, 28, 4, 280, 561, 2, 3, 2234, 561, 2, 3, 90, 6, 76, 794, 14, 10, 112, 456, 5, 1169, 795, 14, 332, 950, 5, 1170], [1171, 2, 3, 16, 216, 1, 936, 21, 250, 1527, 31, 1, 1172, 8, 1, 78, 1528, 14, 250, 2235, 2, 3, 16, 337, 936, 21, 250, 1527, 8, 363, 5, 1, 1172, 8, 78, 1528], [6, 66, 7, 155, 116, 291, 100, 41, 19, 10, 1, 44, 140, 5, 1, 46, 28, 4, 109, 796, 14, 102, 96, 25, 46, 4, 796, 5, 7, 155, 116, 291, 170, 951, 41, 19, 32, 233, 23, 1, 44, 140, 5, 1, 46, 28, 10, 102, 96, 25], [7, 1173, 2236, 115, 154, 104, 289, 8, 1529, 4, 627, 36, 1, 1173, 1174, 154, 104, 289, 8, 1529, 4, 627, 36], [1, 118, 19, 6, 952, 13, 61, 23, 1, 953, 238, 309, 41, 19, 5, 457, 2, 3, 117, 1, 19, 31, 138, 1175, 2237, 13, 1530, 9, 1, 953, 238, 309, 41, 19, 1176, 5, 457, 2, 3, 117], [1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16, 1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [1, 184, 58, 67, 196, 179, 10, 1, 1531, 108, 295, 288, 169, 190, 1532, 14, 7, 921, 162, 94, 2238, 184, 2239, 67, 196, 179, 10, 1, 1531, 108, 295, 288, 169, 190, 1532, 14, 1, 107, 226, 162, 11, 1, 60, 250, 143], [1, 46, 28, 5, 1, 159, 62, 13, 1, 552, 22, 392, 2, 3, 88, 42, 240, 621, 453, 26, 692, 5, 1533, 46, 28, 11, 1, 62, 13, 21, 1, 552, 22, 392, 2, 3, 88, 68, 2240, 310, 5, 621, 453, 26, 2241, 692, 5, 44], [2242, 1534, 2, 3, 88, 13, 1, 1535, 907, 169, 257, 251, 11, 2243, 1534, 2, 3, 88, 13, 1, 255, 2244, 220, 257, 251, 11, 954], [11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16, 11, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [955, 797, 1177, 49, 232, 1536, 8, 1537, 479, 956, 2, 3, 37, 955, 290, 1177, 49, 232, 1536, 8, 1537, 479, 956, 2, 3, 37], [1, 100, 209, 41, 19, 33, 50, 10, 102, 96, 25, 6, 50, 68, 44, 100, 41, 19, 10, 102, 96, 25], [11, 51, 58, 6, 15, 1, 17, 38, 29, 12, 2, 3, 16, 11, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [6, 337, 24, 82, 23, 1, 280, 28, 456, 423, 2245, 422, 75, 5, 1, 357, 28, 176, 26, 1, 1538, 159, 62, 1539, 2, 3, 64, 423, 2246, 147, 75, 5, 1, 357, 28, 176, 26, 1, 1538, 159, 62, 1539, 2, 3, 64], [6, 18, 1, 17, 27, 35, 29, 12, 2, 3, 16, 9, 424, 24, 2247, 18, 1, 258, 27, 56, 35, 29, 17, 12, 2, 3, 16, 9, 262, 24, 56, 35, 58], [241, 6, 1178, 459, 1540, 5, 1, 46, 28, 15, 8, 448, 2, 3, 36, 31, 24, 46, 28, 4, 1, 1541, 31, 1, 422, 28, 31, 268, 8, 193, 170, 786, 1, 46, 28, 15, 8, 448, 2, 3, 36, 13, 1530, 31, 1, 2248, 5, 24, 46, 4, 422, 28, 6, 1542, 98, 31, 2249, 8, 193, 425], [1, 259, 222, 562, 1, 1543, 5, 698, 206, 51, 699, 9, 253, 8, 24, 957, 14, 1179, 9, 7, 296, 35, 14, 7, 798, 11, 799, 491, 563, 2, 3, 156, 259, 222, 48, 222, 562, 1, 1543, 5, 800, 2250, 2251, 4, 2252, 14, 1179, 9, 7, 958, 75, 5, 296, 491, 14, 7, 798, 11, 2253, 799, 125, 563, 2, 3, 156], [1, 46, 28, 5, 1, 159, 62, 13, 1, 552, 22, 392, 2, 3, 88, 42, 240, 621, 453, 26, 692, 5, 1533, 46, 28, 176, 11, 1, 62, 13, 7, 388, 5, 1, 552, 2254, 22, 392, 2, 3, 88, 42, 2255, 621, 453, 8, 44, 26, 2256, 190, 1, 801, 2257, 5, 2258], [147, 28, 33, 564, 21, 1, 220, 1544, 801, 22, 1545, 4, 1546, 36, 1547, 959, 7, 1180, 5, 1548, 4, 21, 105, 1, 960, 4, 453, 1549, 5, 1, 1181, 459, 1, 183, 5, 2259, 4, 2260, 88, 42, 33, 564, 21, 1, 220, 1544, 801, 22, 1547, 1545, 4, 1546, 36, 959, 7, 1180, 5, 1548, 4, 21, 105, 1, 960, 4, 453, 1549, 5, 1, 22], [6, 216, 70, 89, 10, 364, 311, 297, 260, 14, 141, 77, 80, 12, 36, 1, 70, 89, 395, 10, 364, 311, 802, 32, 460, 14, 141, 77, 80, 12, 36], [6, 18, 1, 44, 540, 5, 1, 1550, 40, 293, 303, 183, 961, 2, 3, 37, 6, 337, 24, 293, 303, 29, 23, 1, 44, 540, 5, 1, 1550, 40, 22, 961, 2, 3, 37], [48, 28, 33, 803, 11, 1, 53, 565, 1551, 962, 2, 3, 53, 4, 351, 5, 1552, 136, 218, 14, 1553, 11, 46, 963, 31, 7, 422, 2261, 2262, 2263, 804, 1182, 13, 21, 62, 83, 5, 1, 53, 565, 1551, 962, 2, 3, 53, 4, 351, 5, 1552, 247, 136, 218, 14, 1553, 11, 46, 963, 31, 7, 422, 75], [1, 227, 19, 15, 11, 2264, 227, 13, 7, 964, 805, 256, 1183, 1554, 1555, 2, 3, 16, 268, 8, 426, 325, 24, 700, 227, 19, 13, 1, 964, 805, 256, 1183, 1554, 1555, 2, 3, 16, 268, 8, 426, 94], [441, 1556, 1557, 701, 13, 7, 806, 19, 42, 1558, 7, 566, 19, 702, 965, 31, 7, 1184, 528, 5, 441, 775, 966, 5, 441, 775, 2265, 1556, 1557, 701, 13, 7, 806, 19, 42, 1558, 7, 566, 19, 2266, 31, 7, 1559, 5, 63, 702, 965, 31, 7, 1184, 528, 5, 441, 775], [91, 113, 234, 114, 127, 967, 34, 63, 34, 194, 8, 1, 139, 308, 237, 9, 95, 72, 1560, 492, 461, 1, 91, 113, 114, 127, 42, 270, 34, 63, 34, 194, 8, 1, 139, 149, 237, 9, 95, 72, 365], [91, 113, 234, 114, 127, 967, 34, 63, 34, 194, 8, 1, 139, 308, 237, 9, 95, 72, 1560, 492, 461, 1, 91, 113, 114, 127, 42, 270, 34, 63, 34, 194, 8, 1, 139, 149, 237, 9, 95, 72, 365], [8, 57, 1561, 968, 968, 2, 3, 57, 15, 1562, 163, 23, 447, 247, 28, 42, 2267, 2268, 5, 2269, 57, 1561, 968, 968, 2, 3, 57, 15, 1562, 163, 23, 447, 247, 28, 4, 807, 925, 2270, 623], [6, 66, 7, 100, 41, 19, 23, 1, 44, 140, 5, 703, 10, 102, 96, 25, 6, 66, 7, 155, 116, 291, 100, 41, 19, 10, 1, 44, 140, 5, 1, 46, 28, 4, 109, 796, 14, 102, 96, 25], [1, 1185, 493, 65, 32, 51, 366, 312, 163, 50, 23, 1, 367, 207, 313, 100, 22, 969, 367, 22, 180, 4, 246, 37, 1, 205, 65, 32, 50, 23, 1563, 44, 28, 1, 367, 207, 313, 100, 22, 969, 367, 180, 4, 246, 37, 14, 1, 366, 312, 1186, 103], [6, 396, 704, 1564, 2, 3, 90, 23, 1, 30, 1565, 26, 447, 970, 494, 1187, 11, 2271, 4, 892, 63, 462, 193, 494, 6, 192, 396, 704, 1564, 2, 3, 90, 23, 1, 2272, 4, 2273, 1565, 26, 970, 494, 1187, 11, 2274, 4, 2275, 611, 4, 2276, 462, 193, 83], [172, 381, 32, 61, 23, 1, 91, 113, 42, 270, 34, 63, 945, 8, 1, 139, 149, 237, 9, 95, 72, 157, 114, 127, 98, 492, 461, 1, 91, 113, 114, 127, 42, 270, 34, 63, 34, 194, 8, 1, 139, 149, 237, 9, 95, 72, 365], [172, 188, 2277, 1, 936, 5, 10, 971, 61, 93, 4, 2278, 1, 113, 34, 971, 61, 93, 808, 7, 700, 75, 5, 892, 93, 34, 1566, 1, 777, 459, 93, 451, 84, 24, 113, 13, 34, 1, 971, 61, 93, 808, 7, 700, 75, 5, 892, 93, 34, 1566, 1, 777, 459, 93, 21, 451, 84, 4, 1, 971, 61, 93, 385, 1157, 554], [1567, 1568, 4, 1569, 1570, 129, 1571, 30, 2279, 1568, 4, 1569, 1570, 13, 75, 5, 1571, 30, 218], [1, 89, 395, 67, 109, 10, 1, 77, 80, 82, 12, 36, 70, 89, 395, 32, 109, 10, 77, 80, 12, 36], [1, 107, 1572, 495, 103, 13, 1188, 567, 956, 4, 1573, 16, 1, 495, 13, 809, 196, 179, 10, 1, 1188, 567, 103, 956, 4, 1573, 16], [368, 28, 13, 1189, 15, 9, 972, 1, 304, 5, 28, 1574, 1575, 4, 1576, 25, 368, 28, 254, 2280, 1, 304, 5, 28, 1574, 1575, 4, 1576, 25], [51, 58, 67, 196, 179, 10, 1, 177, 38, 20, 17, 12, 2, 3, 16, 51, 1, 58, 32, 196, 179, 8, 17, 20, 12, 2, 3, 16], [118, 6, 396, 705, 9, 535, 212, 4, 1577, 61, 23, 30, 1190, 249, 338, 339, 4, 186, 2281, 192, 396, 705, 9, 535, 212, 4, 1577, 11, 1, 1191, 61, 23, 30, 1190, 30, 2282, 338, 339, 4, 119, 568], [496, 99, 295, 87, 13, 7, 82, 61, 23, 158, 168, 668, 489, 11, 42, 6, 18, 254, 226, 669, 9, 2283, 973, 2284, 48, 62, 6, 18, 2285, 295, 87, 42, 13, 7, 82, 61, 23, 158, 168, 668, 489], [7, 327, 612, 497, 5, 1, 62, 154, 104, 289, 8, 1192, 2, 3, 974, 7, 2286, 497, 5, 1, 22, 4, 975, 154, 104, 289, 8, 2287, 497, 263, 1192, 2, 3, 974], [7, 189, 162, 369, 13, 7, 1578, 162, 1579, 288, 524, 206, 218, 5, 2288, 162, 2289, 1120, 32, 1578, 669, 1579, 288, 524, 206, 218, 5, 200], [6, 121, 672, 24, 323, 128, 11, 412, 324, 221, 4, 322, 443, 4, 438, 64, 11, 221, 4, 322, 6, 121, 672, 24, 323, 167, 443, 4, 438, 64], [1580, 314, 134, 36, 13, 15, 11, 1, 143, 6, 15, 1, 2290, 143, 314, 134, 36], [6, 15, 382, 108, 383, 87, 11, 531, 610, 15, 382, 20, 383, 87, 11, 531, 81], [6, 976, 7, 1193, 330, 14, 1194, 78, 4, 1581, 282, 9, 424, 1, 412, 19, 627, 4, 498, 25, 24, 412, 227, 19, 2291, 7, 1193, 330, 14, 1194, 78, 4, 1581, 282, 627, 4, 498, 25, 1121, 21, 7, 1195, 1193, 227, 19], [1, 164, 33, 109, 10, 1, 266, 94, 122, 267, 2, 3, 64, 1, 361, 67, 414, 10, 1, 266, 556, 164, 122, 267, 2, 3, 64], [11, 359, 1, 30, 73, 65, 6, 18, 977, 674, 4, 499, 90, 9, 121, 1, 30, 73, 65, 6, 15, 1, 977, 152, 674, 4, 499, 90], [1, 1196, 5, 1, 164, 33, 397, 10, 1, 1582, 978, 1583, 197, 6, 397, 164, 1196, 26, 10, 1, 1582, 978, 1583, 197], [98, 492, 461, 1, 91, 113, 114, 127, 42, 270, 34, 63, 34, 194, 8, 1, 139, 149, 237, 9, 95, 72, 979, 65, 5, 157, 360, 1, 91, 113, 114, 127, 42, 270, 34, 112, 63, 34, 194, 8, 72, 149, 95, 72, 365], [48, 183, 13, 1584, 5, 2292, 2293, 5, 125, 21, 1, 1585, 340, 340, 83, 8, 1586, 2, 3, 87, 11, 7, 1197, 5, 2294, 2295, 183, 13, 1584, 5, 1587, 136, 2296, 21, 58, 94, 1585, 4, 131, 2297, 2298, 8, 1586, 2, 3, 87, 11, 7, 1197, 5, 2299, 125], [1, 89, 395, 67, 109, 10, 1, 77, 80, 82, 12, 36, 7, 70, 89, 147, 33, 109, 10, 1, 77, 80, 82, 12, 36], [6, 18, 1, 60, 73, 122, 59, 47, 4, 45, 43, 9, 30, 341, 1, 85, 500, 18, 1, 59, 122, 47, 4, 45, 43, 9, 341, 63, 8, 24, 85, 144], [6, 18, 1, 60, 73, 122, 59, 47, 4, 45, 43, 9, 30, 341, 1, 85, 500, 18, 1, 59, 122, 47, 4, 45, 43, 9, 341, 63, 8, 24, 85, 144], [1, 669, 32, 1588, 10, 1198, 980, 199, 1589, 1590, 4, 1591, 37, 1198, 980, 199, 1589, 1590, 4, 1591, 37], [9, 2300, 48, 569, 2, 3, 57, 15, 1592, 1593, 14, 84, 1594, 9, 981, 1595, 946, 5, 63, 23, 7, 22, 5, 1199, 1596, 2301, 810, 167, 7, 22, 5, 1199, 1596, 2302, 33, 15, 26, 569, 2, 3, 57, 9, 981, 1595, 946, 5, 1, 63, 8, 1, 147, 456, 10, 1, 1592, 1593, 23, 84, 1594], [6, 216, 70, 89, 10, 364, 311, 297, 260, 14, 141, 77, 80, 12, 36, 6, 752, 89, 10, 141, 77, 80, 12, 36], [6, 18, 1, 39, 55, 52, 342, 4, 86, 53, 190, 48, 811, 4, 95, 318, 794, 14, 2303, 8, 48, 167, 6, 18, 1, 39, 238, 55, 52, 342, 4, 86, 53], [982, 7, 1597, 33, 181, 4, 983, 283, 706, 570, 2, 3, 53, 7, 279, 115, 122, 11, 292, 2304, 570, 2, 3, 53, 13, 7, 279, 115, 4, 221, 122, 5, 292], [1598, 165, 129, 97, 7, 704, 82, 11, 30, 812, 221, 1598, 165, 97, 290, 7, 82, 11, 30, 812, 221, 42, 6, 751, 9, 31, 427, 704], [6, 76, 501, 1, 323, 258, 286, 21, 7, 1599, 38, 29, 1200, 2, 3, 53, 14, 1, 259, 5, 2305, 76, 501, 1, 188, 21, 38, 19, 1200, 2, 3, 53, 31, 7, 2306], [6, 15, 243, 244, 4, 245, 53, 14, 7, 78, 166, 5, 1600, 18, 1, 243, 244, 4, 245, 53, 103, 9, 1201, 1, 1601, 5, 1, 315, 11, 228, 2307, 4, 228, 2308, 14, 7, 78, 166, 5, 494], [91, 379, 13, 61, 23, 1, 666, 34, 813, 814, 8, 205, 63, 1, 157, 5, 7, 30, 13, 601, 9, 1, 149, 98, 2128, 2129, 2130, 1396, 2131, 1397, 1398, 7, 30, 26, 1, 1110, 98, 1111, 813, 814], [8, 269, 9, 707, 1, 628, 257, 142, 369, 984, 8, 1, 1601, 2309, 4, 768, 1602, 7, 815, 30, 816, 13, 1603, 571, 1, 91, 115, 5, 1, 1202, 22, 463, 2, 3, 57, 1, 815, 30, 816, 13, 1603, 571, 1, 91, 115, 5, 1, 1202, 22, 463, 2, 3, 57], [6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25, 6, 15, 1, 92, 20, 9, 69, 24, 163, 79, 2, 3, 25], [1, 35, 19, 33, 50, 26, 59, 47, 4, 45, 43, 4, 1, 629, 33, 50, 26, 1, 2310, 70, 41, 343, 20, 2311, 1604, 4, 1605, 329, 1, 528, 2312, 13, 260, 10, 7, 708, 629, 41, 19, 34, 33, 50, 10, 1, 2313, 41, 343, 20, 1604, 4, 1605, 329], [48, 13, 7, 2314, 5, 1, 1606, 2315, 8, 1607, 4, 1608, 110, 48, 13, 72, 9, 1, 1606, 2316, 8, 1607, 4, 1608, 110], [6, 15, 60, 163, 169, 8, 92, 152, 79, 2, 3, 25, 6, 15, 7, 785, 81, 5, 1, 92, 152, 79, 2, 3, 25], [6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25, 11, 1, 226, 271, 199, 81, 6, 15, 92, 79, 2, 3, 25], [11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16, 393, 6, 15, 17, 20, 31, 27, 296, 12, 2, 3, 16], [253, 30, 132, 32, 417, 26, 1203, 1, 28, 8, 105, 398, 14, 59, 170, 4, 817, 1, 112, 50, 132, 47, 4, 45, 43, 101, 30, 132, 67, 153, 26, 464, 59, 8, 105, 398, 4, 817, 10, 1, 356, 370, 47, 4, 45, 43], [1, 818, 807, 26, 1, 425, 951, 1609, 371, 8, 1, 572, 2317, 422, 75, 33, 2318, 1610, 2319, 153, 14, 7, 60, 985, 41, 19, 14, 819, 4, 116, 173, 630, 4, 45, 165, 1, 41, 19, 13, 7, 100, 14, 819, 4, 116, 173, 630, 4, 45, 165], [6, 15, 60, 163, 169, 8, 92, 152, 79, 2, 3, 25, 6, 15, 1, 92, 20, 9, 69, 24, 163, 79, 2, 3, 25], [70, 56, 35, 13, 809, 109, 10, 27, 49, 12, 2, 3, 16, 98, 13, 7, 60, 27, 56, 35, 19, 12, 2, 3, 16], [249, 338, 2, 3, 339, 13, 68, 428, 685, 257, 399, 42, 240, 130, 333, 1119, 44, 2320, 249, 428, 257, 399, 338, 2, 3, 339], [6, 18, 1, 445, 81, 5, 256, 416, 79, 2, 3, 25, 6, 15, 7, 785, 81, 5, 1, 92, 152, 79, 2, 3, 25], [6, 986, 1, 1204, 5, 111, 522, 14, 148, 135, 110, 6, 15, 1, 1205, 148, 135, 110, 9, 986, 522, 1611], [24, 133, 686, 481, 1, 372, 41, 20, 272, 273, 2, 3, 57, 186, 198, 33, 2321, 10, 1, 372, 41, 20, 272, 273, 2, 3, 57], [328, 15, 1, 1206, 164, 122, 266, 267, 2, 3, 64, 11, 1, 164, 1, 164, 33, 109, 447, 10, 1, 266, 164, 122, 267, 2, 3, 64], [24, 29, 631, 8, 987, 62, 94, 195, 115, 8, 465, 988, 2, 3, 88, 6, 631, 8, 105, 573, 7, 4, 989, 5, 987, 62, 94, 195, 115, 8, 465, 988, 2, 3, 88, 14, 68, 990, 5, 24, 991, 29], [1, 44, 133, 33, 181, 10, 1, 30, 274, 1612, 21, 272, 273, 2, 3, 57, 6, 1613, 1, 133, 10, 1, 107, 1614, 21, 272, 273, 2, 3, 57], [1, 1615, 67, 373, 10, 1, 39, 120, 108, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [70, 56, 35, 13, 809, 109, 10, 27, 49, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [1, 44, 140, 33, 181, 10, 1, 17, 20, 12, 2, 3, 16, 1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16], [6, 50, 68, 44, 100, 41, 19, 10, 102, 96, 25, 6, 66, 7, 629, 41, 19, 14, 116, 173, 10, 102, 20, 96, 25], [51, 5, 1, 133, 28, 21, 2322, 33, 181, 10, 1, 272, 632, 273, 2, 3, 57, 6, 1613, 1, 133, 10, 1, 107, 1614, 21, 272, 273, 2, 3, 57], [24, 56, 35, 49, 32, 50, 10, 17, 131, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16, 11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16], [6, 121, 672, 24, 323, 128, 11, 412, 324, 221, 4, 322, 443, 4, 438, 64, 1207, 1616, 24, 323, 1208, 128, 11, 324, 221, 443, 4, 438, 64], [6, 15, 243, 244, 4, 245, 53, 14, 7, 78, 166, 5, 2323, 6, 18, 243, 244, 4, 245, 53, 14, 7, 78, 166, 5, 2324, 4, 7, 676, 415, 5, 1587], [11, 51, 58, 6, 15, 1, 17, 38, 29, 12, 2, 3, 16, 11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16], [1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16, 1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16], [11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16, 11, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16, 118, 6, 15, 1, 17, 20, 12, 2, 3, 16, 11, 70, 56, 35], [11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16, 1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [6, 820, 35, 65, 10, 1, 27, 17, 12, 2, 3, 16, 38, 1209, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [6, 15, 429, 180, 84, 50, 23, 1, 992, 46, 1617, 984, 1, 429, 171, 180, 84, 42, 33, 50, 23, 1, 2325, 502, 46, 28], [1, 1615, 67, 373, 10, 1, 39, 120, 108, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [6, 820, 35, 65, 10, 1, 27, 17, 12, 2, 3, 16, 38, 1209, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [1, 446, 467, 993, 13, 2326, 14, 1, 1210, 566, 467, 8, 1, 147, 310, 702, 4, 1618, 387, 1619, 13, 7, 60, 70, 82, 34, 2327, 1, 467, 5, 7, 446, 8, 7, 709, 566, 14, 124, 1210, 566, 467, 8, 710, 18, 702, 4, 1618, 387], [6, 2328, 1, 70, 89, 5, 667, 8, 222, 14, 68, 821, 1211, 147, 344, 1620, 965, 2329, 7, 454, 2330, 8, 2331, 2332, 1621, 70, 89, 5, 1, 711, 8, 633, 83, 222, 11, 112, 994, 574, 68, 821, 1211, 147, 1620, 965], [7, 330, 11, 352, 534, 115, 4, 534, 215, 129, 123, 97, 8, 912, 2, 3, 37, 4, 7, 612, 115, 5, 1, 153, 188, 129, 123, 196, 2333, 330, 11, 352, 534, 115, 129, 123, 97, 8, 912, 2, 3, 37, 241, 31, 613, 352, 143, 48, 13, 76, 7, 479, 1129, 62], [11, 231, 316, 2, 3, 57, 289, 34, 1, 528, 5, 2334, 358, 13, 318, 1622, 7, 914, 2335, 5, 352, 2336, 2, 3, 57, 2337, 34, 145, 1623, 13, 34, 1, 995, 369, 5, 996, 65, 937, 318, 1622, 1624, 350, 14, 352, 1212], [23, 1, 298, 140, 6, 15, 1, 279, 2338, 138, 8, 1625, 2, 3, 57, 50, 23, 1, 46, 28, 5, 1626, 2339, 9, 262, 30, 575, 4, 119, 198, 4, 15, 1, 2340, 2341, 29, 1625, 2, 3, 57, 50, 23, 1, 46, 28, 33, 15, 9, 262, 30, 575, 4, 198, 4, 1, 101, 52, 33, 15, 9, 160, 1, 125, 8, 1, 353, 22], [223, 10, 1, 17, 27, 161, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [223, 10, 1, 17, 27, 161, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 223, 10, 1, 17, 27, 161, 12, 2, 3, 16], [192, 1, 1213, 28, 33, 109, 11, 1214, 119, 198, 227, 1215, 4, 1627, 10, 39, 120, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [6, 214, 77, 80, 12, 36, 9, 216, 70, 89, 228, 410, 5, 24, 65, 555, 9, 7, 1628, 216, 89, 5, 188, 10, 77, 80, 190, 228, 410, 12, 36], [48, 29, 481, 1, 2342, 1629, 401, 138, 26, 1630, 2, 3, 201, 359, 23, 167, 26, 2343, 2, 3, 53, 6, 1631, 1, 1629, 401, 14, 1632, 97, 26, 1630, 2, 3, 201], [1, 41, 19, 13, 7, 100, 102, 96, 25, 19, 50, 10, 822, 14, 155, 116, 173, 4, 559, 1633, 66, 7, 629, 41, 19, 14, 116, 173, 10, 102, 20, 96, 25], [402, 712, 230, 548, 1634, 15, 8, 24, 19, 32, 823, 14, 503, 468, 2, 3, 16, 1, 161, 13, 175, 14, 402, 712, 230, 548, 997, 10, 60, 476, 169, 8, 1, 503, 998, 468, 2, 3, 16], [402, 712, 230, 548, 1634, 15, 8, 24, 19, 32, 823, 14, 503, 468, 2, 3, 16, 1, 161, 13, 175, 14, 402, 712, 230, 548, 997, 10, 60, 476, 169, 8, 1, 503, 998, 468, 2, 3, 16], [192, 1, 1213, 28, 33, 109, 11, 1214, 119, 198, 227, 1215, 4, 1627, 10, 39, 120, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [98, 13, 7, 2344, 5, 1, 19, 97, 26, 1635, 2, 3, 57, 24, 118, 101, 13, 2345, 7, 2346, 576, 184, 61, 23, 1, 167, 5, 1635, 2, 3, 57], [6, 18, 92, 79, 2, 3, 25, 1, 56, 78, 403, 11, 553, 11, 1216, 1, 332, 2347, 15, 1, 92, 56, 78, 403, 79, 2, 3, 25, 11, 105, 1216, 24, 215, 65, 4, 1636, 70, 250, 824], [6, 18, 1, 577, 119, 578, 1217, 5, 579, 2, 3, 64, 48, 8, 1637, 546, 23, 1, 139, 1218, 250, 19, 809, 1638, 21, 7, 159, 186, 119, 287, 290, 31, 1, 577, 119, 578, 5, 579, 2, 3, 64], [6, 18, 1, 577, 119, 578, 1217, 5, 579, 2, 3, 64, 48, 8, 1637, 546, 23, 1, 139, 1218, 250, 19, 809, 1638, 21, 7, 159, 186, 119, 287, 290, 31, 1, 577, 119, 578, 5, 579, 2, 3, 64], [1, 531, 13, 50, 10, 2348, 21, 1, 280, 1218, 682, 423, 706, 13, 7, 1219, 169, 122, 11, 279, 115, 4, 221, 5, 2349, 4, 2350, 133, 570, 2, 3, 2351, 570, 2, 3, 53, 13, 7, 279, 115, 4, 221, 122, 5, 292], [11, 41, 343, 6, 50, 7, 1220, 100, 116, 291, 371, 19, 23, 1, 209, 386, 44, 140, 5, 1, 46, 825, 10, 102, 96, 25, 6, 66, 7, 155, 630, 45, 291, 100, 41, 19, 10, 1, 44, 140, 5, 1, 46, 28, 4, 109, 796, 14, 102, 96, 25, 325], [14, 1, 46, 999, 5, 1, 17, 20, 12, 2, 3, 16, 6, 442, 1, 46, 144, 14, 229, 1221, 8, 1, 17, 20, 12, 2, 3, 16], [1, 30, 73, 33, 50, 10, 59, 47, 4, 45, 43, 14, 1, 580, 356, 73, 538, 1139, 109, 30, 73, 10, 59, 47, 4, 45, 43, 14, 1, 107, 356, 73, 538, 82], [6, 18, 226, 489, 21, 490, 4, 489, 14, 753, 162, 21, 404, 316, 4, 134, 25, 31, 24, 1222, 6, 18, 404, 14, 7, 226, 162, 316, 4, 134, 25], [6, 547, 1, 685, 2352, 8, 363, 5, 7, 1639, 29, 1640, 2, 3, 165, 6, 547, 24, 964, 1223, 103, 31, 7, 1639, 29, 1640, 2, 3, 165], [6, 18, 39, 52, 74, 71, 2, 3, 37, 9, 210, 160, 200, 4, 55, 761, 18, 1, 39, 55, 52, 74, 71, 2, 3, 37, 11, 430, 55, 431, 4, 186, 93], [8, 48, 167, 6, 18, 1, 39, 238, 55, 52, 342, 4, 86, 53, 8, 269, 9, 1000, 1, 2353, 2354, 6, 1224, 39, 52, 342, 4, 86, 53], [6, 18, 39, 52, 74, 71, 2, 3, 37, 9, 210, 160, 200, 4, 55, 761, 76, 210, 1, 55, 160, 5, 1, 125, 10, 1, 39, 52, 74, 71, 2, 3, 37], [1, 78, 103, 15, 8, 24, 826, 1001, 13, 1225, 1641, 581, 1, 1226, 184, 15, 8, 1, 58, 13, 1, 1225, 405, 189, 184, 1641, 581], [6, 18, 39, 52, 74, 71, 2, 3, 37, 9, 210, 160, 200, 4, 55, 761, 18, 1, 39, 55, 52, 74, 71, 2, 3, 37, 11, 430, 55, 431, 4, 186, 93], [91, 379, 462, 504, 4, 2355, 57, 11, 68, 2127, 13, 61, 23, 1, 1642, 34, 63, 34, 194, 8, 72, 149, 237, 9, 104, 766, 601, 114, 127, 1, 1002, 34, 13, 1, 255, 432, 546, 23, 1, 91, 113, 34, 2356, 1227, 1, 666, 34, 63, 14, 72, 157, 237, 9, 194, 8, 72, 149, 114, 127], [248, 167, 13, 236, 5, 1, 258, 292, 279, 171, 706, 570, 2, 3, 53, 706, 570, 2, 3, 53, 13, 7, 279, 115, 4, 221, 122, 5, 292], [6, 18, 39, 52, 74, 71, 2, 3, 37, 9, 210, 160, 200, 4, 55, 761, 76, 210, 1, 55, 160, 5, 1, 125, 10, 1, 39, 52, 74, 71, 2, 3, 37], [1572, 461, 1, 634, 128, 9, 27, 35, 47, 4, 45, 36, 8, 42, 1, 405, 469, 129, 1, 2357, 226, 827, 1643, 1644, 505, 713, 505, 633, 2358, 634, 128, 9, 27, 35, 47, 4, 45, 36, 635, 65, 1, 1645, 35, 377, 1646, 713, 83, 1647, 1648, 713, 505, 633, 83, 319, 505, 13, 1, 209, 1228], [46, 28, 32, 61, 23, 7, 1649, 5, 1229, 828, 44, 144, 94, 21, 1, 1003, 399, 1004, 84, 105, 2359, 144, 32, 169, 21, 1, 1003, 399, 1004, 84], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16], [100, 41, 65, 5, 1650, 4, 44, 67, 50, 10, 102, 96, 25, 6, 66, 7, 100, 41, 19, 23, 1, 44, 140, 5, 703, 10, 102, 96, 25], [6, 15, 1, 293, 215, 183, 5, 1, 565, 117, 62, 344, 1005, 2, 3, 117, 6, 397, 24, 19, 23, 7, 130, 293, 215, 62, 565, 117, 62, 344, 1005, 2, 3, 117], [6, 192, 232, 30, 73, 14, 59, 47, 4, 45, 43, 8, 105, 398, 14, 1, 107, 106, 15, 8, 1651, 109, 30, 73, 10, 59, 47, 4, 45, 43, 14, 1, 107, 356, 73, 538, 82], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 11, 51, 58, 6, 15, 1, 17, 38, 29, 12, 2, 3, 16], [1, 89, 395, 67, 109, 10, 1, 77, 80, 82, 12, 36, 1, 70, 89, 395, 10, 364, 311, 802, 32, 460, 14, 141, 77, 80, 12, 36], [51, 58, 67, 196, 179, 10, 1, 177, 38, 20, 17, 12, 2, 3, 16, 8, 124, 60, 1652, 580, 1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16], [172, 125, 95, 192, 104, 983, 283, 68, 2360, 264, 52, 1230, 1653, 84, 14, 380, 2361, 125, 32, 983, 283, 1, 1230, 264, 52, 1653, 84, 14, 1, 2362, 2363], [1, 41, 19, 13, 7, 100, 102, 96, 25, 19, 50, 10, 822, 14, 155, 116, 173, 4, 559, 2364, 100, 209, 41, 19, 33, 50, 10, 102, 96, 25], [504, 2, 3, 64, 714, 7, 1231, 103, 11, 1654, 411, 241, 1, 2365, 5, 1, 1654, 19, 4, 124, 1231, 103, 9, 2366, 13, 2367, 4, 2368, 6, 714, 68, 103, 11, 1655, 2369, 2370, 9, 1, 1231, 103, 5, 504, 2, 3, 64, 241, 1232, 9, 2371, 4, 424], [91, 113, 234, 114, 127, 967, 34, 63, 34, 194, 8, 1, 139, 308, 237, 9, 95, 72, 1656, 1657, 1658, 8, 1006, 1659, 72, 308, 5, 112, 363, 8, 1, 470, 4, 363, 34, 194, 8, 1, 139, 149, 237, 9, 95, 72, 365, 114, 127], [91, 113, 234, 114, 127, 967, 34, 63, 34, 194, 8, 1, 139, 308, 237, 9, 95, 72, 1656, 1657, 1658, 8, 1006, 1659, 72, 308, 5, 112, 363, 8, 1, 470, 4, 363, 34, 194, 8, 1, 139, 149, 237, 9, 95, 72, 365, 114, 127], [6, 18, 1, 39, 55, 52, 71, 2, 3, 37, 6, 118, 18, 7, 55, 52, 74, 71, 2, 3, 37, 9, 160, 111, 136, 4, 219, 1, 75, 5, 55, 242, 762, 14, 1, 136], [6, 76, 2372, 1, 340, 5, 829, 2, 3, 64, 23, 48, 2373, 76, 196, 179, 7, 1660, 433, 340, 319, 1, 1007, 32, 830, 61, 23, 1, 1233, 132, 153, 26, 1661, 340, 5, 829, 2, 3, 64], [6, 820, 35, 65, 10, 1, 27, 17, 12, 2, 3, 16, 38, 2374, 10, 1, 17, 27, 161, 12, 2, 3, 16], [119, 198, 33, 109, 14, 148, 135, 110, 1, 147, 75, 33, 224, 14, 1, 275, 148, 135, 110], [6, 15, 1, 1662, 715, 2, 3, 16, 11, 227, 2375, 1, 227, 2376, 1008, 15, 1662, 715, 2, 3, 16, 213, 2377], [215, 481, 1, 92, 553, 152, 79, 2, 3, 25, 6, 15, 7, 785, 81, 5, 1, 92, 152, 79, 2, 3, 25], [6, 18, 1, 39, 55, 52, 71, 2, 3, 37, 172, 93, 67, 153, 10, 1, 39, 52, 94, 71, 2, 3, 37], [6, 15, 243, 244, 4, 245, 53, 14, 7, 78, 166, 5, 1600, 15, 243, 31, 1, 527, 244, 4, 245, 53], [30, 73, 13, 109, 10, 59, 47, 4, 45, 43, 48, 13, 233, 10, 716, 19, 83, 471, 2, 3, 581, 4, 59, 47, 4, 45, 43], [1, 147, 75, 33, 224, 14, 1, 275, 148, 135, 110, 11, 1, 275, 140, 1, 148, 135, 110, 33, 15], [11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16, 223, 10, 1, 17, 27, 161, 12, 2, 3, 16], [119, 198, 33, 109, 14, 148, 135, 110, 119, 198, 13, 109, 10, 1, 1663, 189, 171, 135, 110], [11, 359, 1, 30, 73, 65, 6, 18, 977, 674, 4, 499, 90, 11, 30, 132, 6, 15, 977, 674, 4, 499, 90], [1, 831, 28, 13, 419, 21, 1, 54, 22, 12, 40, 51, 1, 28, 1009, 21, 54, 12, 40], [1, 2378, 22, 13, 54, 717, 12, 40, 1, 831, 28, 13, 419, 21, 1, 54, 22, 12, 40], [91, 65, 5, 157, 360, 1, 91, 113, 114, 127, 42, 270, 34, 112, 63, 34, 194, 8, 72, 149, 95, 72, 979, 142, 546, 23, 1, 91, 113, 34, 72, 363, 1010, 8, 72, 149, 114, 127], [223, 10, 1, 17, 27, 161, 12, 2, 3, 16, 1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16], [54, 94, 12, 40, 98, 13, 7, 22, 5, 85, 306, 8, 1011, 394, 21, 1, 208, 5, 1, 203, 449, 8, 24, 235, 58, 6, 2379, 275, 283, 572, 4, 18, 1, 280, 144, 9, 607, 24, 35, 49, 423, 54, 22, 12, 40, 1, 54, 85, 22, 13, 219], [6, 223, 70, 89, 395, 11, 259, 211, 24, 606, 1664, 29, 1, 101, 4, 1, 493, 1665, 49, 10, 141, 77, 80, 12, 36, 14, 507, 70, 89, 13, 484, 23, 1, 259, 314, 10, 141, 77, 80, 12, 36, 14, 699, 507, 4, 228, 410], [1, 44, 140, 33, 181, 10, 1, 17, 20, 12, 2, 3, 16, 1, 101, 385, 104, 417, 26, 1, 17, 38, 20, 12, 2, 3, 16], [6, 820, 35, 65, 10, 1, 27, 17, 12, 2, 3, 16, 38, 1209, 18, 1, 17, 27, 35, 29, 12, 2, 3, 16, 9, 424, 24, 65], [1, 101, 385, 104, 417, 26, 1, 17, 38, 20, 12, 2, 3, 16, 11, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [1, 1399, 636, 13, 402, 10, 1012, 1013, 333, 718, 4, 1014, 339, 7, 432, 216, 5, 48, 2380, 13, 1012, 1013, 333, 2381, 718, 4, 1014, 339], [1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16, 1, 1015, 32, 637, 10, 7, 27, 29, 66, 10, 17, 12, 2, 3, 16, 1, 1234, 75], [1, 2382, 198, 129, 123, 414, 14, 1, 189, 171, 135, 110, 119, 198, 13, 109, 10, 1, 1663, 189, 171, 135, 110], [11, 1, 2383, 5, 119, 568, 6, 18, 1, 1205, 148, 135, 110, 11, 105, 6, 18, 148, 135, 110, 14, 41, 1666, 65, 386, 275, 19, 11, 275, 306, 44, 11, 44, 306], [1, 831, 28, 13, 419, 21, 1, 54, 22, 12, 40, 1, 54, 22, 12, 40, 13, 66, 21, 1, 208, 5, 1, 203, 449], [51, 361, 67, 233, 10, 1, 266, 556, 164, 122, 267, 2, 3, 64, 1, 164, 33, 109, 10, 1, 266, 94, 122, 267, 2, 3, 64], [1, 582, 832, 46, 22, 33, 564, 21, 1, 54, 310, 12, 40, 1, 831, 28, 13, 419, 21, 1, 54, 22, 12, 40], [6, 15, 429, 180, 84, 50, 23, 1, 992, 46, 2384, 13, 224, 14, 429, 180, 84, 50, 23, 1, 583, 1235], [11, 51, 58, 6, 15, 1, 17, 38, 29, 12, 2, 3, 16, 11, 2385, 6, 76, 95, 7, 29, 61, 23, 17, 12, 2, 3, 16], [112, 1236, 28, 456, 95, 123, 1667, 1227, 26, 1016, 4, 638, 165, 68, 1237, 287, 11, 2386, 129, 123, 1667, 1227, 26, 1016, 4, 638, 165], [105, 5, 24, 49, 67, 61, 23, 1, 17, 161, 12, 2, 3, 16, 1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16], [11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16, 1, 101, 385, 104, 417, 26, 1, 17, 38, 20, 12, 2, 3, 16], [625, 157, 418, 910, 23, 1, 91, 113, 42, 1668, 34, 63, 1017, 8, 7, 72, 75, 5, 149, 32, 76, 72, 8, 157, 114, 127, 91, 65, 5, 157, 360, 1, 91, 113, 114, 127, 42, 270, 34, 112, 63, 34, 194, 8, 72, 149, 95, 72, 365], [439, 1669, 2, 3, 53, 97, 68, 345, 78, 103, 11, 374, 1203, 125, 8, 7, 566, 14, 2387, 2388, 2389, 1669, 2, 3, 53, 97, 7, 1670, 345, 128, 11, 1203, 2390, 340, 2391, 14, 762, 133, 1671, 1018, 833, 903, 2392], [7, 91, 142, 19, 13, 823, 61, 23, 1, 91, 113, 114, 127, 63, 34, 194, 8, 1, 139, 149, 237, 9, 2393, 72, 979, 65, 5, 157, 360, 1, 91, 113, 114, 127, 42, 270, 34, 112, 63, 34, 194, 8, 72, 149, 95, 72, 365], [48, 263, 1238, 1, 673, 5, 24, 29, 34, 631, 8, 1, 573, 7, 5, 834, 62, 835, 195, 115, 8, 465, 639, 2, 3, 53, 8, 1, 280, 6, 385, 719, 1, 29, 14, 42, 6, 631, 8, 1, 1239, 2394, 215, 573, 5, 195, 115, 8, 465, 62, 835, 5, 565, 53, 639, 2, 3, 53], [6, 15, 1, 27, 19, 17, 12, 2, 3, 16, 11, 1, 58, 14, 51, 1, 60, 302, 539, 7, 375, 299, 19, 4, 7, 100, 41, 2395, 33, 66, 14, 1, 17, 20, 12, 2, 3, 16, 10, 1, 2396, 60, 720, 93, 539, 7, 100, 41, 19], [1240, 1241, 146, 584, 13, 7, 603, 671, 34, 887, 105, 1, 1019, 4, 379, 5, 41, 1020, 197, 1240, 1241, 146, 584, 1020, 197, 13, 7, 375, 146, 671, 34, 129, 123, 15, 11, 105, 2397, 2398, 389, 227, 4, 130, 227], [6, 15, 1, 256, 1672, 81, 5, 92, 20, 79, 2, 3, 25, 14, 326, 2399, 15, 1, 226, 99, 81, 14, 107, 106, 4, 256, 416, 14, 326, 200, 105, 169, 190, 92, 79, 2, 3, 25], [1, 97, 19, 1673, 1, 701, 330, 5, 1674, 2, 3, 43, 1, 1675, 5, 1674, 2, 3, 43, 1021, 701, 14, 2400, 4, 1184, 836, 65, 10, 1, 818, 5, 1, 19], [6, 15, 1, 46, 185, 5, 1, 183, 21, 1676, 2, 3, 25, 1676, 2, 3, 25, 176, 7, 183, 5, 828, 616, 1242, 1141, 2401, 5, 616, 1155, 21, 145, 709, 2402, 2403, 2404, 117], [100, 41, 65, 5, 1650, 4, 44, 67, 50, 10, 102, 96, 25, 8, 269, 9, 337, 1, 1677, 5, 111, 29, 6, 69, 100, 41, 65, 11, 111, 41, 10, 102, 96, 25], [1, 144, 32, 118, 181, 4, 721, 10, 1, 17, 229, 192, 182, 4, 224, 26, 186, 119, 10, 1, 148, 135, 110, 1, 22, 33, 192, 374, 224, 14, 186, 333, 10, 148, 135, 110], [1, 70, 89, 147, 13, 76, 196, 179, 14, 141, 77, 80, 82, 14, 228, 488, 297, 12, 36, 1, 722, 13, 640, 454, 670, 9, 141, 77, 80, 147, 12, 36], [1, 1678, 434, 837, 13, 68, 99, 434, 19, 175, 14, 1243, 7, 158, 168, 1679, 78, 122, 295, 37, 1, 1222, 13, 175, 31, 7, 434, 837, 50, 14, 1243, 295, 37], [1, 70, 89, 147, 13, 76, 196, 179, 14, 141, 77, 80, 82, 14, 228, 488, 297, 12, 36, 7, 70, 89, 147, 33, 109, 10, 1, 77, 80, 82, 12, 36], [6, 18, 1, 272, 20, 1680, 4, 273, 87, 9, 219, 1, 1244, 2405, 21, 111, 1681, 18, 1, 2406, 136, 2407, 21, 272, 1680, 4, 273, 87, 9, 262, 105, 136, 4, 30, 575, 23, 111, 133, 1245], [6, 2408, 1, 261, 1246, 9, 42, 111, 209, 2409, 2410, 13, 255, 585, 14, 1, 82, 5, 1682, 2, 3, 201, 6, 681, 48, 82, 586, 68, 1022, 1023, 259, 2132, 42, 13, 641, 14, 1, 2411, 8, 1682, 2, 3, 201], [6, 910, 23, 1, 1247, 585, 257, 130, 251, 97, 26, 1683, 2, 3, 1684, 9, 262, 2412, 709, 1, 2413, 5, 48, 263, 13, 7, 300, 345, 2414, 128, 9, 1685, 61, 23, 1, 1247, 585, 251, 2415, 587, 26, 1683, 2, 3, 1684], [9, 1157, 554, 1, 333, 303, 1248, 7, 22, 618, 1686, 129, 123, 247, 11, 1024, 1249, 4, 124, 603, 1687, 8, 1250, 306, 1688, 2, 3, 90, 1, 1686, 22, 13, 7, 447, 247, 22, 11, 1024, 4, 1249, 1689, 838, 472, 4, 248, 603, 1687, 136, 472, 1688, 2, 3, 90], [1, 452, 5, 826, 1251, 1221, 8, 1, 2416, 1252, 13, 2417, 61, 23, 1, 39, 2418, 2419, 2420, 29, 588, 2, 3, 88, 6, 396, 1, 39, 826, 1251, 29, 588, 2, 3, 88], [6, 232, 24, 58, 23, 54, 12, 40, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 2421, 94, 12, 40, 98, 13, 7, 22, 5, 85, 306, 8, 1011, 394, 21, 1, 208, 5, 1, 203, 449], [6, 15, 1, 92, 79, 2, 3, 25, 81, 5, 172, 163, 14, 248, 107, 435, 302, 11, 24, 610, 15, 7, 785, 81, 5, 1, 92, 152, 79, 2, 3, 25], [6, 15, 1, 92, 79, 2, 3, 25, 81, 5, 172, 163, 14, 248, 107, 435, 302, 11, 24, 610, 15, 1, 92, 20, 9, 69, 24, 163, 79, 2, 3, 25], [355, 1253, 436, 1690, 779, 190, 970, 1, 1691, 333, 21, 1025, 358, 23, 1, 139, 996, 9, 723, 7, 1692, 915, 156, 355, 133, 436, 779, 9, 374, 723, 7, 799, 4, 2422, 1692, 5, 1254, 294, 1025, 358, 915, 156], [589, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16, 11, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [6, 15, 1, 92, 79, 2, 3, 25, 81, 5, 172, 163, 14, 248, 107, 435, 302, 11, 24, 610, 15, 1, 92, 79, 2, 3, 25, 81, 5, 1693, 4, 1, 1694, 20], [589, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16, 11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16], [6, 69, 163, 11, 111, 5, 1, 642, 250, 354, 4, 11, 1, 583, 250, 75, 23, 1, 46, 75, 5, 111, 22, 10, 1, 107, 580, 5, 1, 366, 312, 81, 5, 202, 204, 2, 3, 57, 6, 69, 256, 416, 163, 1255, 156, 10, 202, 204, 2, 3, 57, 11, 111, 1695, 4, 76, 11, 1, 412, 19], [8, 335, 1, 22, 33, 473, 10, 1, 148, 1256, 135, 110, 83, 1, 22, 13, 473, 4, 224, 26, 186, 23, 105, 1026, 10, 1, 148, 135, 110], [328, 32, 61, 23, 91, 113, 42, 1257, 643, 1, 1258, 34, 72, 63, 194, 8, 72, 149, 114, 127, 91, 65, 5, 157, 360, 1, 91, 113, 114, 127, 42, 270, 34, 112, 63, 34, 194, 8, 72, 149, 95, 72, 365], [6, 15, 1, 139, 147, 75, 15, 8, 448, 2, 3, 36, 11, 24, 281, 170, 241, 6, 1178, 459, 1540, 5, 1, 46, 28, 15, 8, 448, 2, 3, 36, 31, 24, 46, 28, 4, 1, 1541, 31, 1, 422, 28, 31, 268, 8, 193, 170], [6, 820, 35, 65, 10, 1, 27, 17, 12, 2, 3, 16, 38, 2423, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16], [6, 15, 1027, 93, 61, 23, 1028, 2, 3, 40, 11, 231, 1028, 2, 3, 40, 2133, 1, 18, 5, 1027, 93, 26, 10, 600, 378], [11, 359, 1, 101, 38, 29, 6, 15, 1, 177, 38, 20, 17, 12, 2, 3, 16, 8, 124, 60, 929, 51, 58, 6, 15, 1, 17, 38, 29, 12, 2, 3, 16], [11, 35, 6, 18, 17, 12, 2, 3, 16, 14, 375, 299, 1695, 4, 1, 97, 19, 14, 441, 1259, 2424, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [9, 909, 2425, 724, 6, 644, 7, 501, 5, 991, 724, 420, 8, 1, 217, 137, 187, 336, 2, 3, 2426, 8, 335, 6, 508, 34, 1, 441, 287, 2427, 350, 14, 1, 2428, 5, 137, 724, 8, 1, 217, 137, 187, 336, 2, 3, 90], [9, 644, 41, 65, 4, 216, 818, 6, 18, 614, 615, 87, 14, 1696, 155, 116, 839, 342, 4, 474, 197, 4, 14, 7, 725, 1029, 105, 41, 65, 18, 155, 116, 173, 342, 4, 474, 197], [6, 18, 600, 378, 9, 707, 1, 1697, 5, 699, 4, 840, 2429, 179, 1, 1260, 106, 922, 4, 923, 36, 6, 18, 1, 600, 378, 61, 701, 922, 4, 923, 36], [2430, 13, 318, 7, 2431, 774, 21, 1, 292, 999, 31, 13, 11, 231, 1, 536, 774, 536, 36, 83, 292, 774, 13, 509, 8, 1, 536, 1030, 536, 36], [9, 644, 41, 65, 4, 216, 818, 6, 18, 614, 615, 87, 14, 1696, 155, 116, 839, 342, 4, 474, 197, 4, 14, 7, 725, 1029, 105, 41, 65, 18, 155, 116, 173, 342, 4, 474, 197], [6, 121, 672, 24, 323, 526, 897, 61, 128, 11, 412, 324, 221, 4, 322, 443, 4, 438, 64, 1207, 1616, 24, 323, 1208, 128, 11, 324, 221, 443, 4, 438, 64], [6, 18, 1, 2432, 238, 894, 41, 19, 401, 5, 1698, 2, 3, 88, 31, 268, 8, 426, 253, 6, 360, 1, 238, 309, 401, 5, 1698, 2, 3, 88, 10, 112, 529, 2433, 5, 2434, 226, 2435], [1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16, 6, 50, 7, 212, 5, 841, 38, 49, 10, 1, 17, 20, 12, 2, 3, 16, 8, 124, 107, 726], [11, 359, 1, 101, 38, 29, 6, 15, 1, 177, 38, 20, 17, 12, 2, 3, 16, 8, 124, 60, 929, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 38, 491, 7, 27, 38, 29, 66, 10, 1, 17, 20, 12, 2, 3, 16, 4, 1, 958, 582, 183, 590, 1, 125, 8, 1, 147, 75, 33, 15, 9, 637, 1], [280, 251, 310, 4, 842, 7, 38, 19, 11, 2436, 33, 50, 10, 1, 17, 20, 12, 2, 3, 16, 10, 124, 101, 1699, 50, 7, 19, 10, 17, 20, 12, 2, 3, 16, 23, 1, 46, 28, 31, 24, 101, 29], [145, 1261, 128, 9, 143, 13, 317, 134, 4, 510, 43, 42, 13, 1700, 61, 23, 1031, 815, 211, 355, 4, 352, 2437, 82, 317, 134, 4, 510, 43, 13, 61, 23, 1031, 2438, 211, 1, 2439, 4, 2440, 1032], [6, 18, 1, 207, 313, 100, 22, 180, 4, 246, 37, 9, 981, 1, 41, 19, 222, 11, 7, 1681, 15, 1, 836, 450, 21, 1, 207, 313, 100, 22, 180, 4, 246, 37, 9, 535, 1, 467, 5, 18, 5, 111, 624, 843], [1, 2441, 1701, 61, 195, 115, 62, 13, 7, 2442, 5, 834, 62, 253, 1702, 2, 3, 53, 48, 263, 1238, 1, 128, 5, 1, 130, 2443, 511, 2444, 9, 62, 253, 5, 565, 53, 1701, 61, 195, 115, 1702, 2, 3, 53], [1, 57, 2445, 483, 159, 62, 278, 2, 3, 57, 2446, 190, 430, 2447, 319, 145, 5, 1, 477, 354, 33, 2448, 2449, 1262, 2450, 57, 159, 62, 2451, 23, 477, 303, 278, 2, 3, 57, 1263, 23, 477, 354, 5, 2452, 973], [1, 522, 308, 19, 1033, 11, 1264, 1265, 276, 1034, 4, 86, 87, 33, 1, 118, 345, 128, 9, 2453, 7, 2454, 101, 1, 1033, 13, 7, 806, 19, 11, 1, 345, 1703, 5, 1266, 1265, 276, 206, 645, 5, 186, 119, 568, 1034, 4, 86, 87], [1, 1033, 13, 7, 806, 19, 11, 1, 345, 1703, 5, 1266, 1265, 276, 206, 645, 5, 186, 119, 568, 1034, 4, 86, 87, 1, 666, 5, 1704, 7, 522, 26, 124, 2455, 4, 7, 332, 784, 5, 308, 13, 15, 26, 1, 1033, 345, 227, 19, 1034, 4, 86, 87], [6, 18, 1, 81, 176, 26, 1705, 2, 3, 201, 2456, 254, 1, 55, 276, 34, 32, 983, 9, 248, 1706, 18, 1, 55, 189, 684, 2457, 687, 309, 2458, 97, 26, 1705, 2, 3, 201, 2459, 2460, 248, 107, 55, 52, 14, 24, 213, 34, 1707, 1708, 63], [54, 12, 40, 13, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 541, 18, 1, 275, 44, 85, 22, 844, 533, 480, 125, 21, 1, 22, 5, 203, 845, 208, 12, 40, 31, 1, 28, 23, 42, 1267, 13, 109, 9, 133], [1, 39, 55, 52, 74, 71, 2, 3, 37, 13, 15, 11, 430, 93, 21, 1, 55, 160, 727, 15, 1, 39, 55, 52, 74, 71, 2, 3, 37, 9, 160, 1, 125, 34, 1154, 7, 624, 1024, 1709, 4, 174, 1, 280, 93, 21, 1, 55, 160, 200], [1, 39, 55, 52, 74, 71, 2, 3, 37, 13, 15, 11, 430, 93, 21, 1, 55, 160, 727, 15, 1, 39, 55, 52, 74, 71, 2, 3, 37, 9, 160, 1, 125, 34, 1154, 7, 624, 1024, 1709, 4, 174, 1, 280, 93, 21, 1, 55, 160, 200], [1, 1268, 236, 5, 28, 622, 21, 846, 4, 1269, 583, 1270, 5, 1, 54, 28, 75, 12, 40, 1, 28, 15, 11, 1, 58, 138, 8, 48, 263, 622, 1710, 21, 1711, 491, 512, 4, 1, 54, 22, 5, 203, 845, 208, 12, 40], [6, 225, 1, 97, 19, 9, 24, 81, 5, 1, 1712, 19, 138, 8, 646, 2, 3, 25, 214, 9, 1713, 198, 1, 19, 401, 268, 8, 426, 83, 13, 7, 2461, 647, 5, 1, 1271, 401, 5, 646, 2, 3, 25], [98, 13, 7, 27, 29, 66, 10, 1, 17, 20, 12, 2, 3, 16, 4, 2462, 10, 254, 1, 442, 400, 721, 85, 28, 176, 11, 1, 159, 2463, 491, 7, 27, 38, 29, 66, 10, 1, 17, 20, 12, 2, 3, 16, 4, 1, 958, 582, 183, 590, 1, 125, 8, 1, 147, 75, 33, 15, 9, 637, 1, 159, 62], [8, 48, 185, 6, 118, 1714, 1, 1247, 189, 19, 5, 1715, 2, 3, 90, 4, 952, 7, 700, 2464, 1715, 2, 3, 90, 1, 2465, 19, 68, 819, 211, 1, 836, 19, 4, 1, 1716, 19, 33, 76, 728, 545, 227, 700, 125, 42, 2466, 7, 1717, 286], [1, 54, 22, 12, 40, 13, 66, 21, 1, 208, 5, 1, 203, 541, 18, 1, 275, 44, 85, 22, 844, 533, 480, 125, 21, 1, 22, 5, 203, 845, 208, 12, 40, 31, 1, 28, 23, 42, 1267, 13, 109, 9, 133], [1, 54, 22, 12, 40, 13, 66, 21, 1, 208, 5, 1, 203, 541, 18, 1, 275, 44, 85, 22, 844, 533, 480, 125, 21, 1, 22, 5, 203, 845, 208, 12, 40, 31, 1, 28, 23, 42, 1267, 13, 109, 9, 133], [6, 18, 243, 244, 4, 245, 201, 11, 1718, 14, 331, 78, 166, 5, 2467, 19, 33, 50, 648, 326, 1719, 10, 1, 2468, 647, 5, 729, 437, 847, 243, 244, 4, 245, 201, 14, 68, 331, 78, 166, 5, 488], [6, 18, 1, 177, 17, 20, 12, 2, 3, 16, 9, 121, 7, 60, 27, 38, 29, 42, 1720, 848, 9, 344, 63, 1272, 8, 1, 17, 126, 2469, 18, 1, 17, 27, 35, 29, 12, 2, 3, 16, 9, 424, 24, 65], [6, 15, 1, 2470, 122, 8, 59, 47, 4, 45, 43, 9, 607, 1, 30, 2471, 76, 15, 59, 30, 73, 122, 47, 4, 45, 43, 23, 1, 139, 1721, 4, 803, 2472, 2473, 9, 1, 73, 5, 2474, 2475, 8, 2476, 4, 44], [6, 192, 18, 1, 126, 303, 1722, 8, 1, 17, 70, 56, 35, 29, 12, 2, 3, 16, 9, 219, 7, 126, 193, 42, 1723, 206, 1191, 6, 18, 1, 17, 27, 35, 29, 12, 2, 3, 16, 9, 424, 24, 65], [126, 218, 32, 174, 21, 2477, 132, 153, 14, 59, 47, 4, 45, 43, 126, 218, 67, 174, 21, 1724, 30, 132, 4, 1725, 421, 26, 59, 47, 4, 45, 43, 10, 1, 966, 5, 705, 356, 4, 2478], [791, 6, 121, 2479, 1, 1726, 1727, 2480, 15, 26, 1728, 2, 3, 117, 11, 343, 465, 2481, 42, 385, 104, 24, 1035, 2482, 167, 13, 1273, 26, 1, 1726, 376, 128, 5, 1728, 2, 3, 117, 1274, 1, 19, 6, 751, 9, 31, 1, 1727, 376, 2483, 1274, 4, 6, 849, 48, 24, 1035, 101], [252, 346, 347, 84, 24, 115, 76, 586, 1, 139, 850, 11, 413, 730, 125, 290, 31, 1, 1729, 1730, 842, 1731, 8, 2484, 1, 301, 513, 8, 253, 325, 4, 344, 6, 154, 808, 7, 1232, 850, 9, 413, 730, 125, 539, 1, 1275, 1732, 9, 1, 115, 5, 346, 347, 84], [6, 750, 7, 2485, 708, 4, 2486, 345, 19, 1112, 26, 1, 427, 402, 636, 1733, 1734, 19, 97, 8, 1735, 4, 1736, 64, 42, 6, 1113, 9, 1, 2487, 1276, 642, 6, 1113, 1, 1734, 19, 97, 8, 1735, 4, 1736, 64, 9, 239, 4, 307, 302, 26, 2488, 1, 112, 427, 682, 9, 18, 7, 159, 1737], [24, 167, 13, 76, 601, 9, 1036, 4, 1037, 40, 319, 1, 142, 211, 1, 63, 23, 1, 431, 1738, 112, 731, 8, 1, 55, 758, 13, 15, 9, 1739, 7, 162, 2489, 55, 431, 13, 1, 1740, 431, 211, 1, 112, 731, 8, 7, 55, 160, 758, 4, 129, 123, 268, 9, 104, 1741, 11, 293, 303, 1036, 4, 1037, 40], [1273, 26, 323, 167, 6, 1742, 7, 467, 993, 5, 2490, 137, 1038, 42, 67, 289, 9, 104, 1, 255, 649, 959, 1, 2491, 22, 1743, 2, 3, 25, 2492, 5, 1, 542, 93, 1, 137, 1038, 42, 32, 289, 9, 104, 1, 255, 2493, 8, 24, 58, 2494, 14, 797, 289, 8, 1, 2495, 2496, 22, 1743, 2, 3, 25], [1, 2497, 286, 950, 67, 807, 10, 7, 1039, 406, 487, 103, 11, 46, 7, 158, 168, 184, 10, 921, 669, 1744, 626, 193, 344, 9, 193, 533, 508, 1, 2498, 633, 2499, 1745, 153, 1040, 494, 1041, 10, 1039, 406, 487, 103, 1277, 1744, 626, 11, 46, 7, 158, 168, 2500], [8, 48, 1746, 5, 48, 263, 6, 1714, 601, 167, 1, 381, 11, 111, 29, 4, 58, 4, 188, 11, 111, 573, 10, 1, 28, 176, 26, 834, 62, 835, 195, 115, 8, 2501, 263, 1238, 1, 673, 5, 24, 29, 34, 631, 8, 1, 573, 7, 5, 834, 62, 835, 195, 115, 8, 465, 639, 2, 3, 53], [8, 1747, 163, 50, 23, 485, 28, 154, 104, 214, 635, 9, 1042, 724, 206, 1, 44, 140, 5, 1, 54, 22, 12, 40, 15, 11, 46, 4, 281, 2502, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40], [8, 1, 982, 185, 6, 1278, 732, 343, 5, 1748, 1749, 8, 7, 1599, 376, 73, 19, 499, 2, 3, 197, 47, 4, 45, 1750, 6, 1278, 732, 1, 376, 61, 30, 73, 65, 499, 197, 47, 4, 45, 1750], [48, 1279, 13, 640, 454, 190, 228, 410, 670, 9, 77, 80, 147, 12, 36, 1, 722, 13, 640, 454, 670, 9, 141, 77, 80, 147, 12, 36], [1751, 14, 2503, 1752, 4, 1753, 36, 1280, 434, 381, 32, 2504, 1, 255, 733, 15, 345, 128, 11, 2505, 2506, 1, 345, 128, 1280, 434, 381, 32, 258, 1752, 4, 1753, 36], [1281, 5, 24, 2507, 14, 1, 202, 152, 204, 2, 3, 57, 6, 851, 48, 122, 11, 81, 2508, 6, 18, 1, 202, 204, 2, 3, 57, 81, 5, 1, 708, 2509, 11, 24, 58], [444, 1043, 2, 3, 53, 13, 175, 14, 7, 444, 166, 5, 769, 9, 1044, 1, 19, 21, 2510, 1043, 2, 3, 53, 13, 7, 650, 852, 349, 1754, 9, 1044, 2511, 5, 7, 309], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 11, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [31, 1276, 8, 185, 131, 6, 153, 191, 21, 1, 619, 5, 1, 39, 52, 74, 71, 4, 86, 90, 6, 853, 18, 5, 55, 160, 333, 21, 1, 39, 55, 52, 74, 71, 4, 86, 90], [6, 274, 4, 1282, 51, 5, 1, 144, 10, 1283, 787, 14, 17, 12, 2, 3, 16, 6, 442, 1, 46, 144, 14, 229, 1221, 8, 1, 17, 20, 12, 2, 3, 16], [9, 210, 172, 6, 18, 1, 39, 55, 52, 74, 71, 2, 3, 37, 4, 1, 1755, 73, 21, 185, 1756, 18, 1, 39, 55, 52, 74, 71, 2, 3, 37, 11, 430, 55, 431, 4, 186, 93], [1, 2512, 65, 32, 50, 10, 243, 527, 244, 4, 245, 53, 14, 68, 331, 78, 166, 5, 2513, 106, 32, 2514, 26, 243, 527, 244, 4, 245, 53, 14, 1, 78, 166, 488], [1, 1149, 132, 2515, 7, 1172, 5, 193, 344, 685, 375, 299, 19, 1757, 4, 86, 90, 145, 5, 1, 27, 49, 2516, 2517, 7, 375, 299, 19, 1757, 4, 86, 90], [1, 70, 89, 395, 10, 364, 311, 802, 32, 460, 14, 141, 77, 80, 12, 36, 7, 70, 89, 147, 33, 109, 10, 1, 77, 80, 82, 12, 36], [9, 210, 172, 6, 18, 1, 39, 55, 52, 74, 71, 2, 3, 37, 4, 1, 1755, 73, 21, 185, 1756, 18, 1, 39, 55, 52, 74, 71, 2, 3, 37, 11, 430, 55, 431, 4, 186, 93], [6, 18, 1, 150, 103, 151, 2, 3, 25, 9, 651, 1, 805, 2518, 2519, 5, 1, 500, 18, 428, 78, 9, 69, 19, 106, 591, 1, 106, 10, 1, 150, 103, 151, 2, 3, 25], [1, 205, 13, 21, 1758, 1759, 2, 3, 53, 1, 1284, 5, 30, 1045, 13, 2520, 18, 1, 2521, 2522, 30, 178, 21, 1758, 1759, 2, 3, 53], [6, 18, 1, 150, 527, 151, 2, 3, 25, 14, 331, 78, 166, 75, 9, 1469, 18, 428, 78, 9, 69, 19, 106, 591, 1, 106, 10, 1, 150, 103, 151, 2, 3, 25], [6, 340, 14, 1, 27, 70, 56, 35, 20, 17, 12, 2, 3, 16, 8, 269, 9, 69, 7, 1285, 44, 29, 4, 9, 508, 1, 1760, 5, 1, 1761, 1762, 15, 1, 17, 20, 12, 2, 3, 16, 9, 121, 7, 126, 61, 56, 35, 29, 14, 7, 1286, 100, 371, 50, 23, 1, 209, 140, 5, 24, 825], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16], [6, 360, 1, 784, 8, 504, 2, 3, 64, 5, 2523, 76, 18, 172, 2524, 2525, 9, 348, 1025, 65, 11, 1, 103, 5, 504, 2, 3, 64], [1, 41, 19, 13, 7, 100, 102, 96, 25, 19, 50, 10, 822, 14, 155, 116, 173, 4, 559, 1633, 50, 68, 44, 100, 41, 19, 10, 102, 96, 25], [31, 1276, 8, 185, 131, 6, 153, 191, 21, 1, 619, 5, 1, 39, 52, 74, 71, 4, 86, 90, 6, 853, 18, 5, 55, 160, 333, 21, 1, 39, 55, 52, 74, 71, 4, 86, 90], [76, 6, 337, 23, 1, 1046, 236, 5, 1, 1182, 183, 962, 2, 3, 53, 4, 508, 34, 24, 128, 2526, 9, 2527, 391, 13, 1, 1046, 236, 5, 1, 1182, 183, 962, 2, 3, 53], [24, 19, 129, 7, 1763, 334, 1764, 2, 3, 581, 14, 112, 2528, 111, 686, 7, 136, 8, 2529, 323, 167, 18, 136, 343, 14, 7, 1763, 334, 1764, 2, 3, 581], [1, 2530, 2531, 2532, 5, 1, 22, 461, 1, 854, 198, 1030, 961, 2, 3, 156, 1, 1127, 15, 9, 69, 1, 854, 171, 32, 855, 635, 21, 1, 854, 198, 1030, 961, 2, 3, 2533], [6, 76, 210, 1, 55, 160, 5, 1, 125, 10, 1, 39, 52, 74, 71, 2, 3, 37, 9, 1765, 1, 1287, 5, 1288, 23, 55, 227, 6, 1224, 1, 39, 55, 52, 131, 71, 2, 3, 37], [1, 1399, 636, 13, 402, 10, 1012, 1013, 333, 718, 4, 1014, 339, 6, 644, 2534, 815, 636, 1766, 613, 2535, 8, 1, 636, 13, 1, 1012, 1013, 333, 211, 1, 112, 63, 718, 4, 1014, 339], [6, 219, 1289, 1290, 10, 112, 381, 1767, 1768, 1769, 4, 1770, 88, 4, 2536, 612, 1771, 8, 2537, 253, 76, 462, 2538, 2539, 219, 1290, 21, 2540, 10, 1767, 1768, 1769, 4, 1770, 88, 4, 24, 97, 2541, 29], [444, 1043, 2, 3, 53, 13, 175, 14, 7, 444, 166, 5, 769, 9, 1044, 1, 19, 21, 2542, 1, 619, 5, 1, 2543, 677, 13, 2544, 9, 7, 444, 677, 1043, 2, 3, 53], [1, 1772, 377, 33, 407, 26, 1, 77, 82, 504, 165, 1, 77, 378, 82, 586, 7, 592, 11, 1773, 1291, 7, 378, 377, 11, 7, 978, 545, 1, 377, 13, 318, 609, 504, 165], [1, 70, 89, 147, 13, 76, 196, 179, 14, 141, 77, 80, 82, 14, 228, 488, 297, 12, 36, 1, 89, 281, 13, 109, 26, 141, 77, 80, 12, 36], [1, 77, 378, 82, 586, 7, 592, 11, 1773, 1291, 7, 378, 377, 11, 7, 978, 545, 1, 377, 13, 318, 609, 504, 165, 1, 1772, 377, 33, 407, 26, 1, 77, 82, 504, 165], [11, 1292, 6, 18, 1, 1250, 28, 21, 1774, 652, 57, 11, 841, 58, 6, 15, 1, 1774, 85, 22, 652, 57, 42, 32, 1292, 358, 21, 1, 203, 2545, 2546], [1, 235, 58, 67, 196, 179, 10, 1, 60, 634, 27, 38, 20, 17, 12, 2, 3, 16, 1, 35, 29, 13, 50, 10, 1, 350, 609, 17, 20, 12, 2, 3, 16], [11, 1, 234, 5, 346, 347, 84, 9, 104, 514, 6, 95, 97, 7, 300, 384, 5, 734, 4, 1, 653, 301, 515, 8, 1047, 14, 248, 478, 301, 1775, 6, 1776, 1, 112, 301, 513, 5, 346, 347, 84, 4, 1777, 1, 653, 301, 515], [131, 14, 172, 200, 725, 1, 767, 1778, 14, 1179, 9, 106, 32, 260, 574, 1, 1779, 571, 782, 103, 1780, 4, 1781, 197, 1778, 32, 260, 1782, 574, 1779, 571, 334, 1780, 4, 1781, 197], [675, 1783, 95, 728, 1293, 1784, 26, 1785, 1, 207, 1786, 26, 1, 91, 113, 42, 270, 34, 63, 1017, 8, 72, 149, 237, 9, 95, 72, 365, 114, 127, 91, 65, 5, 157, 360, 1, 91, 113, 114, 127, 42, 270, 34, 112, 63, 34, 194, 8, 72, 149, 95, 72, 365], [6, 15, 202, 204, 2, 3, 57, 11, 51, 24, 215, 610, 18, 1, 202, 20, 204, 2, 3, 57, 11, 24, 576, 78, 58], [239, 144, 1, 22, 15, 8, 1, 280, 58, 13, 1, 628, 1787, 1048, 22, 1788, 2, 3, 87, 1, 58, 32, 196, 179, 23, 7, 388, 5, 1, 628, 1787, 1048, 22, 2547, 1788, 2, 3, 87, 31, 98, 13, 15, 11, 1, 2548, 28, 2549, 1789, 5, 1, 2550, 143, 2551], [118, 6, 15, 1, 17, 20, 12, 2, 3, 16, 11, 70, 56, 1790, 17, 20, 12, 2, 3, 16, 13, 15, 11, 126, 303], [810, 85, 22, 13, 1, 2552, 307, 85, 22, 1791, 2, 3, 37, 22, 1791, 2, 3, 37], [1, 85, 22, 13, 593, 10, 59, 47, 4, 45, 43, 48, 13, 233, 10, 716, 19, 83, 471, 2, 3, 581, 4, 59, 47, 4, 45, 43], [6, 1175, 2553, 2554, 31, 7, 130, 136, 215, 62, 8, 7, 1271, 401, 2555, 1, 2556, 1271, 19, 5, 278, 53, 7, 647, 5, 646, 2, 3, 25, 6, 225, 1, 97, 19, 9, 24, 81, 5, 1, 1712, 19, 138, 8, 646, 2, 3, 25, 214, 9, 1713, 198], [51, 172, 93, 32, 1792, 21, 17, 12, 2, 3, 16, 51, 1, 58, 32, 196, 179, 8, 17, 20, 12, 2, 3, 16], [11, 48, 265, 6, 18, 1, 54, 22, 12, 40, 11, 24, 58, 6, 18, 2557, 125, 21, 54, 12, 40, 11, 111, 41, 654, 280, 1, 628, 930, 5, 652, 53], [1, 306, 67, 118, 374, 1049, 4, 181, 494, 4, 192, 328, 67, 186, 224, 26, 429, 171, 180, 84, 42, 33, 50, 23, 1, 2558, 502, 46, 28, 1, 2559, 28, 33, 224, 10, 429, 180, 84, 10, 7, 19, 50, 23, 1, 549, 550, 551], [2560, 829, 2, 3, 64, 830, 261, 63, 61, 23, 30, 73, 1793, 6, 1125, 299, 261, 2561, 2, 3, 64, 509, 7, 82, 319, 261, 133, 13, 830, 9, 1794, 1, 209, 30, 269, 61, 23, 30, 73], [11, 41, 343, 6, 18, 1, 44, 353, 22, 14, 100, 371, 175, 14, 1, 102, 20, 96, 25, 41, 19, 11, 51, 1795, 49, 6, 18, 7, 1050, 116, 291, 41, 19, 50, 10, 1, 102, 20, 96, 25], [11, 24, 701, 908, 6, 18, 382, 383, 87, 11, 46, 239, 996, 65, 6, 18, 382, 20, 383, 87], [6, 18, 1, 60, 1796, 75, 5, 55, 795, 74, 71, 2, 3, 37, 6, 18, 1, 39, 52, 14, 39, 191, 74, 71, 2, 3, 37], [982, 6, 337, 1797, 350, 1, 973, 562, 97, 8, 1171, 2, 3, 16, 1624, 14, 722, 8, 286, 4, 722, 8, 78, 2562, 76, 225, 164, 2563, 8, 363, 5, 1, 78, 166, 72, 9, 1171, 2, 3, 16, 590, 34, 6, 707, 4, 225, 1, 532, 722, 8, 1, 78, 166], [1019, 8, 2564, 13, 247, 280, 1, 55, 61, 671, 15, 8, 1, 2565, 55, 187, 42, 33, 76, 15, 8, 1, 305, 992, 22, 1798, 2, 3, 329, 11, 1, 305, 58, 6, 15, 1, 992, 22, 1798, 2, 3, 329], [6, 15, 1, 92, 79, 2, 3, 25, 81, 5, 1693, 4, 1, 1694, 2566, 18, 1, 445, 81, 5, 99, 79, 2, 3, 25], [6, 15, 1, 92, 20, 9, 69, 24, 163, 79, 2, 3, 25, 11, 1799, 6, 15, 92, 79, 2, 3, 25, 9, 69, 163], [1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16, 6, 50, 7, 60, 17, 101, 12, 2, 3, 16, 23, 1, 139, 46, 28, 4, 15, 1, 139, 253, 951, 41, 19, 9, 1294, 2567], [172, 93, 67, 153, 10, 1, 39, 52, 94, 71, 2, 3, 37, 6, 18, 1, 39, 52, 14, 39, 191, 74, 71, 2, 3, 37], [6, 66, 7, 100, 41, 19, 23, 1, 44, 140, 5, 703, 10, 102, 96, 25, 145, 13, 7, 1051, 41, 19, 66, 10, 102, 96, 25, 4, 50, 206, 7, 155, 213, 5, 1, 247, 22, 8, 42, 613, 98, 13, 1295, 14, 124, 384, 856, 98, 477], [8, 1, 205, 140, 1, 275, 22, 13, 186, 119, 224, 26, 10, 148, 122, 135, 110, 11, 2568, 133, 14, 186, 4, 2569, 2570, 1, 22, 13, 473, 4, 224, 26, 186, 23, 105, 1026, 10, 1, 148, 135, 110], [1, 1800, 1019, 857, 1801, 2, 3, 110, 1, 1800, 1019, 857, 1801, 2, 3, 110, 471, 22, 2571, 4, 1296, 1297, 129, 123, 15, 31, 7, 296, 22, 8, 675, 765, 764], [6, 986, 1, 1204, 5, 111, 522, 14, 148, 135, 110, 119, 198, 33, 109, 14, 1, 148, 135, 110], [14, 1, 102, 20, 96, 25, 11, 41, 343, 6, 15, 1, 102, 20, 96, 25, 11, 60, 1031, 343, 14, 68, 1031, 1298, 5, 170], [6, 1278, 732, 1, 376, 61, 30, 73, 65, 499, 197, 47, 4, 45, 84, 47, 13, 1, 376, 73, 19, 5, 47, 4, 45, 84], [6, 18, 1, 39, 52, 14, 39, 191, 71, 2, 3, 37, 172, 93, 67, 153, 10, 1, 39, 52, 94, 71, 2, 3, 37], [6, 18, 1, 39, 52, 14, 39, 191, 74, 71, 2, 3, 37, 6, 18, 1, 60, 1796, 75, 5, 55, 795, 74, 71, 2, 3, 37], [51, 1, 1032, 32, 397, 10, 317, 134, 36, 11, 1, 436, 62, 6, 225, 188, 10, 317, 134, 36], [1, 516, 311, 297, 67, 407, 10, 77, 80, 12, 36, 6, 752, 89, 10, 141, 77, 80, 12, 36], [6, 175, 2572, 10, 1, 1802, 403, 1803, 2, 3, 117, 1804, 260, 10, 1, 355, 2573, 2574, 5, 1802, 1803, 2, 3, 117, 42, 1299, 7, 1300, 2575], [11, 1, 226, 271, 199, 81, 6, 15, 92, 79, 2, 3, 25, 11, 1799, 6, 15, 92, 79, 2, 3, 25, 9, 69, 163], [1, 171, 6, 18, 13, 429, 180, 84, 7, 529, 526, 629, 171, 42, 33, 50, 23, 1, 960, 735, 22, 1805, 1806, 1807, 2576, 28, 33, 224, 10, 429, 180, 84, 10, 7, 19, 50, 23, 1, 549, 550, 551], [6, 95, 15, 1, 81, 138, 8, 1808, 4, 1809, 288, 14, 405, 200, 5, 911, 725, 9, 1810, 2577, 1, 78, 103, 6, 15, 1300, 2578, 14, 2579, 1811, 163, 42, 2580, 68, 1812, 5, 405, 200, 5, 725, 911, 1808, 4, 1809, 288], [6, 66, 7, 100, 41, 19, 23, 1, 44, 140, 5, 703, 10, 102, 96, 25, 6, 121, 24, 433, 49, 8, 7, 60, 592, 10, 1, 17, 29, 102, 11, 41, 1813, 96, 25, 4, 60, 257, 299, 19], [1814, 406, 594, 379, 2581, 31, 236, 5, 1, 583, 264, 2582, 1, 380, 76, 1815, 169, 7, 2583, 287, 5, 2584, 379, 8, 1, 773, 5, 406, 594, 2585, 619, 1, 146, 2586, 612, 130, 418, 8, 1, 827, 5, 406, 594, 379, 277, 2, 3, 40], [252, 1, 2587, 159, 62, 1, 16, 159, 62, 2588, 23, 55, 227, 4, 779, 190, 2589, 258, 56, 78, 530, 214, 9, 48, 62, 715, 2, 3, 16, 145, 5, 1, 118, 2590, 190, 42, 470, 990, 33, 1816, 33, 1, 16, 502, 159, 62, 23, 55, 227, 715, 2, 3, 16], [252, 346, 347, 84, 24, 115, 76, 586, 1, 139, 850, 11, 413, 730, 125, 290, 31, 1, 1729, 1730, 842, 1731, 8, 2591, 252, 346, 347, 84, 24, 115, 154, 76, 1301, 34, 1, 730, 136, 2592, 13, 1817, 1793, 2593, 13, 1818], [1, 126, 193, 13, 174, 21, 7, 239, 133, 585, 23, 1, 30, 472, 10, 856, 59, 47, 4, 45, 43, 7, 720, 837, 5, 613, 433, 29, 13, 1, 126, 193, 42, 240, 239, 126, 218, 174, 21, 7, 239, 22, 1040, 30, 73, 47, 4, 45, 43], [367, 207, 313, 180, 4, 246, 37, 129, 123, 15, 9, 688, 446, 2594, 42, 13, 15, 31, 7, 216, 5, 1, 2595, 5, 1, 2596, 367, 207, 313, 28, 180, 4, 246, 37, 129, 123, 268, 9, 2597, 7, 1400, 222, 955, 48, 28, 33, 318, 169, 648, 1, 2598, 5, 48, 655], [6, 118, 18, 7, 55, 52, 74, 71, 2, 3, 37, 9, 160, 111, 136, 4, 219, 1, 75, 5, 55, 242, 762, 14, 1, 136, 7, 514, 75, 5, 52, 568, 4, 1, 82, 15, 9, 858, 21, 7, 522, 9, 7, 1819, 55, 146, 154, 104, 289, 8, 74, 71, 2, 3, 37], [6, 118, 18, 7, 55, 52, 74, 71, 2, 3, 37, 9, 160, 111, 136, 4, 219, 1, 75, 5, 55, 242, 762, 14, 1, 136, 7, 514, 75, 5, 52, 568, 4, 1, 82, 15, 9, 858, 21, 7, 522, 9, 7, 1819, 55, 146, 154, 104, 289, 8, 74, 71, 2, 3, 37], [172, 320, 808, 68, 1237, 241, 1820, 327, 2599, 850, 9, 1, 2600, 5, 1302, 2, 3, 37, 2601, 328, 2602, 1821, 886, 1259, 11, 1, 2603, 5, 2604, 2, 3, 37, 2605, 34, 60, 285, 1822, 2606, 602, 2607, 285, 42, 328, 2608, 9, 1, 2609, 34, 1, 1002, 1823, 285, 2610, 7, 886, 1174, 414, 848, 5, 1821, 1824], [6, 340, 14, 1, 27, 70, 56, 35, 20, 17, 12, 2, 3, 16, 8, 269, 9, 69, 7, 1285, 44, 29, 4, 9, 508, 1, 1760, 5, 1, 1761, 1762, 714, 7, 29, 34, 2611, 7, 710, 17, 70, 56, 35, 29, 12, 2, 3, 16, 4, 2612, 98, 9, 1, 1250, 470], [11, 24, 22, 6, 1178, 459, 358, 21, 1, 2613, 185, 5, 1, 300, 859, 620, 22, 1052, 90, 21, 1, 1303, 2614, 859, 620, 351, 5, 963, 256, 125, 21, 1, 300, 859, 620, 22, 1052, 90, 1303, 16, 6, 459, 254, 125, 34, 2615, 190, 1156, 145, 736, 656, 670, 9, 1, 2616], [328, 15, 1, 1206, 164, 122, 266, 267, 2, 3, 64, 11, 1, 164, 6, 849, 1, 280, 354, 5, 2617, 2134, 1, 266, 122, 267, 2, 3, 64, 33, 15, 11, 164], [51, 144, 67, 419, 21, 1, 1003, 399, 1004, 84, 1, 143, 75, 33, 2618, 5, 2619, 2620, 21, 1, 471, 2621, 28, 5, 1, 1003, 399, 1004, 84], [1, 29, 421, 616, 67, 397, 10, 317, 562, 134, 36, 1, 1032, 21, 1, 642, 103, 11, 1, 2622, 67, 397, 61, 23, 317, 975, 134, 36], [1, 1825, 843, 824, 62, 13, 9, 860, 1, 766, 861, 30, 9, 7, 209, 21, 7, 501, 5, 1304, 2623, 1826, 4, 1827, 329, 1, 1825, 843, 183, 1826, 4, 1827, 329, 351, 5, 1828, 1226, 63, 11, 111, 5, 42, 253, 1829, 63, 32, 617, 4, 1, 62, 13, 9, 860, 1, 1829, 30, 255, 72, 9, 1, 1226], [6, 688, 24, 93, 10, 1, 102, 20, 96, 25, 83, 6, 15, 1, 102, 20, 96, 25, 9, 121, 51, 41, 65, 15, 8, 48, 167, 386, 105, 11, 28, 824, 4, 11, 1, 235, 49, 15, 11, 1830, 143], [6, 18, 1, 445, 81, 5, 256, 416, 79, 2, 3, 25, 1, 1053, 15, 13, 7, 256, 416, 1053, 8, 1, 81, 176, 26, 92, 79, 2, 3, 25], [11, 48, 265, 6, 18, 1, 54, 22, 12, 40, 1, 70, 857, 11, 48, 62, 33, 174, 21, 1, 1305, 54, 325, 22, 12, 40], [1, 70, 89, 395, 10, 364, 311, 802, 32, 460, 14, 141, 77, 80, 12, 36, 6, 15, 77, 80, 11, 281, 70, 89, 12, 36], [6, 192, 232, 30, 73, 14, 59, 47, 4, 45, 43, 8, 105, 398, 14, 1, 107, 106, 15, 8, 1651, 15, 1, 59, 108, 47, 4, 45, 43, 9, 657, 1, 30, 132], [6, 66, 7, 100, 41, 19, 23, 1, 44, 140, 5, 703, 10, 102, 96, 25, 6, 688, 24, 93, 10, 1, 102, 20, 96, 25], [6, 15, 59, 47, 4, 45, 43, 9, 341, 1, 63, 8, 1, 1181, 192, 232, 30, 73, 14, 59, 47, 4, 45, 43, 8, 105, 398, 14, 1, 107, 106, 15, 8, 17], [1, 126, 1167, 67, 421, 26, 949, 5, 1168, 30, 132, 153, 14, 59, 47, 4, 45, 43, 1, 30, 73, 33, 153, 26, 464, 59, 47, 4, 45, 43], [1831, 2, 3, 88, 214, 1832, 776, 19, 9, 607, 4, 707, 2624, 2625, 8, 248, 1833, 1834, 30, 575, 2626, 1, 2627, 5, 1833, 1834, 298, 30, 575, 1, 1832, 776, 128, 97, 26, 1831, 2, 3, 88, 1, 2628, 793, 4, 1, 2629, 2630, 2631, 1, 1835, 5], [561, 2, 3, 90, 95, 97, 9, 18, 30, 658, 31, 93, 9, 554, 1280, 70, 55, 227, 11, 44, 4, 2632, 269, 9, 1836, 1, 1306, 5, 247, 28, 9, 69, 7, 55, 52, 561, 2, 3, 90, 15, 30, 658, 260, 21, 2633, 28, 31, 93, 11, 46, 7, 52], [327, 439, 570, 2, 3, 53, 417, 706, 7, 29, 11, 279, 115, 4, 221, 5, 292, 48, 29, 154, 104, 15, 9, 554, 1, 623, 5, 1837, 1838, 29, 2634, 1, 1721, 32, 1213, 14, 1, 279, 115, 4, 221, 29, 706, 570, 2, 3, 53, 34, 2635, 7, 649, 2636, 5, 1837, 1839], [6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 1840, 359, 24, 38, 49, 1, 177, 38, 20, 17, 12, 2, 3, 16, 33, 15, 8, 124, 60, 930], [423, 366, 1841, 6, 18, 1842, 647, 14, 1843, 173, 435, 83, 79, 2, 3, 25, 6, 18, 1, 1844, 366, 312, 184, 8, 445, 14, 1, 107, 302, 79, 2, 3, 25], [6, 18, 150, 151, 2, 3, 25, 14, 1, 331, 78, 166, 75, 9, 2637, 18, 428, 78, 9, 69, 19, 106, 591, 1, 106, 10, 1, 150, 103, 151, 2, 3, 25], [1, 235, 58, 67, 196, 179, 10, 1, 60, 634, 27, 38, 20, 17, 12, 2, 3, 16, 1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [6, 18, 1, 150, 82, 151, 2, 3, 25, 9, 374, 285, 1, 78, 166, 11, 111, 1845, 18, 428, 78, 9, 69, 19, 106, 591, 1, 106, 10, 1, 150, 103, 151, 2, 3, 25], [771, 1, 991, 1846, 144, 1, 1847, 22, 338, 2, 3, 110, 13, 145, 5, 1, 255, 733, 2638, 1847, 22, 338, 2, 3, 110, 13, 145, 5, 1, 2639, 2640, 169, 447, 1848, 144, 11, 1685], [6, 18, 1, 490, 158, 168, 56, 99, 316, 4, 134, 25, 184, 11, 46, 4, 232, 2641, 1041, 11, 2642, 2643, 158, 168, 56, 316, 4, 134, 25, 184, 13, 50, 23, 1, 169, 46, 28], [6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16, 6, 274, 4, 1282, 51, 5, 1, 144, 10, 1283, 787, 14, 17, 12, 2, 3, 16, 83], [6, 66, 7, 862, 433, 19, 21, 1, 239, 470, 22, 10, 1, 17, 20, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [1, 2644, 32, 374, 421, 574, 30, 73, 10, 59, 47, 4, 45, 84, 23, 85, 2645, 192, 414, 18, 5, 1, 59, 108, 47, 4, 45, 84, 9, 262, 30, 73, 23, 1, 85, 144], [523, 32, 112, 1849, 994, 9, 686, 2646, 28, 1288, 4, 470, 990, 1850, 88, 523, 32, 112, 994, 97, 8, 1, 2647, 9, 2648, 48, 304, 1850, 88], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 6, 50, 7, 19, 10, 17, 20, 12, 2, 3, 16, 23, 1, 46, 28, 31, 24, 101, 29], [1, 62, 5, 2649, 689, 9, 1292, 1307, 8, 1152, 133, 4, 938, 172, 689, 9, 7, 804, 863, 33, 439, 97, 8, 1851, 1852, 143, 1853, 88, 1854, 2, 3, 88, 6, 289, 1, 1535, 5, 290, 22, 9, 104, 1, 183, 21, 1, 62, 9, 909, 2650, 689, 8, 2651, 133, 2652, 2653, 26, 1851, 1852, 143, 1853, 2654, 8, 88, 1854, 2, 3, 88], [6, 66, 7, 862, 433, 19, 21, 1, 239, 470, 22, 10, 1, 17, 20, 12, 2, 3, 16, 6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16], [6, 76, 18, 2655, 916, 2, 3, 57, 9, 2656, 4, 274, 1, 292, 140, 5, 1, 1181, 18, 1, 1131, 152, 916, 2, 3, 57, 9, 2657, 1, 920, 4, 1, 279, 93, 5, 1, 113, 4, 296, 35], [105, 46, 4, 281, 28, 2658, 5, 2659, 1308, 174, 21, 1, 1054, 22, 278, 2, 3, 90, 1, 28, 176, 11, 1, 159, 62, 13, 1855, 21, 1, 1054, 22, 278, 2, 3, 90], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 6, 66, 7, 862, 433, 19, 21, 1, 239, 470, 22, 10, 1, 17, 20, 12, 2, 3, 16], [9, 219, 24, 186, 119, 93, 6, 118, 680, 1, 1856, 10, 1, 272, 119, 171, 273, 2, 3, 57, 6, 15, 1, 943, 171, 176, 26, 272, 11, 24, 119, 198, 273, 2, 3, 57], [1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16, 6, 274, 4, 1282, 51, 5, 1, 144, 10, 1283, 787, 14, 17, 12, 2, 3, 16, 83], [6, 50, 7, 19, 10, 17, 20, 12, 2, 3, 16, 23, 1, 46, 28, 31, 24, 101, 2660, 101, 13, 7, 27, 235, 29, 50, 10, 1, 17, 20, 12, 2, 3, 16], [11, 1309, 4, 147, 1055, 6, 15, 1857, 1858, 1055, 21, 1, 1310, 279, 399, 1056, 2, 3, 165, 11, 44, 6, 15, 1, 1310, 399, 1056, 2, 3, 165, 9, 864, 44, 63, 283, 2661], [192, 6, 1138, 30, 73, 10, 59, 47, 4, 45, 43, 14, 1, 107, 356, 73, 538, 1139, 192, 232, 30, 73, 14, 59, 47, 4, 45, 43, 8, 105, 398, 14, 1, 107, 106, 15, 8, 17], [409, 2, 3, 87, 95, 516, 1, 2662, 5, 865, 56, 78, 1057, 9, 1, 2663, 2664, 2, 3, 87, 18, 56, 78, 381, 1186, 99, 4, 913, 9, 1000, 1311, 23, 694, 695], [6, 69, 1, 324, 1058, 811, 10, 1059, 1060, 315, 83, 14, 150, 151, 2, 3, 25, 6, 18, 428, 78, 9, 69, 19, 106, 591, 1, 106, 10, 1, 150, 103, 151, 2, 3, 25], [11, 1, 683, 1312, 6, 18, 1, 367, 207, 313, 100, 22, 180, 4, 246, 37, 42, 240, 450, 11, 698, 21, 800, 571, 9, 1313, 153, 21, 206, 83, 1061, 30, 2665, 1062, 22, 180, 4, 246, 37, 13, 7, 183, 1242, 5, 1, 450, 11, 698, 153, 21, 83, 1061, 494, 533, 63, 5, 44, 207, 133, 1859, 9, 7, 525, 1835, 1860, 772], [6, 66, 7, 862, 433, 19, 21, 1, 239, 470, 22, 10, 1, 17, 20, 12, 2, 3, 16, 1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16], [423, 366, 1841, 6, 18, 1842, 647, 14, 1843, 173, 435, 83, 79, 2, 3, 25, 423, 271, 1314, 6, 18, 271, 199, 14, 517, 315, 798, 4, 1315, 79, 2, 3, 25], [6, 66, 7, 862, 433, 19, 21, 1, 239, 470, 22, 10, 1, 17, 20, 12, 2, 3, 16, 6, 274, 4, 737, 1, 28, 14, 1, 60, 229, 21, 1, 17, 20, 12, 2, 3, 16], [68, 2666, 155, 2667, 1050, 41, 19, 13, 407, 10, 1, 102, 20, 96, 2, 3, 88, 1, 41, 19, 13, 7, 60, 100, 19, 407, 21, 1, 427, 28, 10, 155, 116, 173, 1018, 567, 865, 102, 1248, 96, 2, 3, 88], [11, 1563, 28, 697, 2668, 853, 18, 5, 1, 207, 313, 100, 22, 969, 1062, 180, 4, 246, 37, 6, 15, 1, 836, 450, 21, 1, 207, 313, 100, 22, 180, 4, 246, 37, 9, 535, 1, 467, 5, 18, 5, 111, 624, 843], [6, 216, 70, 89, 10, 364, 311, 297, 260, 14, 141, 77, 80, 12, 36, 1, 1861, 2669, 1862, 364, 311, 297, 2670, 9, 1, 1035, 29, 10, 1, 141, 77, 80, 82, 12, 36], [83, 1, 65, 32, 823, 10, 1225, 405, 189, 163, 31, 175, 1316, 202, 204, 2, 3, 57, 14, 107, 435, 1317, 202, 1318, 81, 5, 99, 204, 2, 3, 57, 33, 15, 31, 184, 14, 107, 435, 302], [98, 492, 461, 1, 91, 113, 114, 127, 42, 270, 34, 63, 34, 194, 8, 1, 139, 149, 237, 9, 95, 72, 979, 142, 546, 23, 1, 91, 113, 34, 72, 363, 1010, 8, 72, 149, 114, 127], [849, 1, 112, 934, 2671, 564, 21, 1, 217, 137, 187, 485, 336, 2, 3, 90, 5, 7, 1863, 4, 7, 1319, 293, 1864, 2672, 4, 1865, 90, 15, 137, 242, 5, 1, 217, 137, 187, 336, 2, 3, 90, 31, 7, 250], [1143, 13, 61, 23, 1, 91, 113, 114, 127, 2673, 34, 63, 14, 72, 157, 95, 72, 1697, 959, 394, 91, 142, 546, 23, 1, 91, 113, 34, 72, 363, 1010, 8, 72, 149, 114, 127], [6, 18, 1, 259, 222, 31, 1035, 2674, 42, 13, 2675, 23, 1, 422, 75, 10, 1, 1866, 1867, 103, 1868, 2, 3, 87, 1, 487, 13, 233, 10, 1, 1866, 1867, 103, 21, 1, 1244, 2676, 2677, 1868, 2, 3, 87], [1, 293, 792, 62, 5, 2678, 386, 13, 2679, 4, 1869, 332, 21, 205, 130, 293, 792, 62, 252, 1870, 62, 344, 1005, 2, 3, 57, 1, 1870, 62, 344, 183, 13, 7, 733, 15, 1871, 11, 293, 215, 1005, 2, 3, 57], [56, 35, 29, 302, 6, 15, 7, 27, 70, 56, 35, 19, 31, 175, 8, 1, 17, 20, 12, 2, 3, 16, 11, 56, 1320, 225, 1, 19, 659, 1, 17, 27, 35, 29, 12, 2, 3, 16, 214, 9, 1063, 645], [6, 18, 1, 866, 320, 34, 32, 660, 26, 1, 264, 44, 251, 146, 380, 321, 84, 1, 58, 32, 196, 179, 23, 7, 2680, 2681, 264, 146, 11, 44, 1, 2682, 44, 251, 146, 380, 277, 4, 321, 84], [8, 111, 957, 1, 722, 5, 2683, 2684, 38, 206, 1, 101, 38, 13, 640, 454, 1196, 5, 2685, 10, 77, 80, 12, 36, 8, 51, 1064, 188, 32, 640, 454, 2686, 280, 1, 654, 77, 80, 12, 36], [9, 1872, 2687, 1321, 1140, 462, 231, 94, 6, 219, 7, 501, 5, 2688, 611, 42, 547, 359, 1065, 856, 2689, 294, 2690, 21, 1, 710, 1873, 486, 1874, 2, 3, 896, 6, 219, 7, 501, 890, 925, 2691, 1875, 611, 21, 249, 4, 7, 501, 890, 925, 963, 611, 34, 547, 2692, 1876, 21, 1, 710, 1873, 486, 1874, 2, 3, 896], [1, 2693, 287, 228, 33, 7, 60, 2694, 1877, 1733, 10, 1, 139, 93, 31, 661, 2, 3, 40, 1, 212, 5, 93, 174, 21, 1, 2695, 46, 75, 33, 1150, 2696, 2697, 10, 1, 250, 75, 1878, 26, 661, 2, 3, 40], [8, 24, 167, 252, 1879, 2, 3, 117, 6, 76, 849, 1, 137, 575, 62, 31, 7, 790, 1322, 2698, 9, 1, 167, 5, 1879, 2, 3, 117, 24, 863, 19, 481, 805, 256, 1880, 83, 9, 607, 7, 790, 1322, 19], [6, 18, 1, 28, 34, 67, 2699, 4, 442, 26, 1066, 2, 3, 90, 169, 11, 2700, 8, 248, 1881, 428, 2701, 583, 673, 5, 1, 1123, 2702, 28, 1414, 4, 599, 154, 104, 289, 8, 1066, 2, 3, 90, 4, 1, 1881, 2703], [6, 50, 1882, 55, 1067, 11, 425, 394, 10, 1, 1323, 159, 62, 691, 1068, 4, 1069, 37, 292, 867, 735, 1285, 1324, 4, 2704, 1882, 227, 58, 1304, 394, 21, 1, 502, 1766, 159, 62, 32, 15, 867, 735, 1324, 4, 954, 1068, 4, 1069, 37], [24, 301, 513, 61, 23, 1, 300, 384, 734, 32, 2705, 14, 797, 5, 346, 347, 84, 23, 1, 145, 777, 4, 32, 2706, 9, 2707, 14, 1, 1883, 211, 2708, 4, 868, 23, 1, 205, 2709, 1, 234, 5, 346, 347, 84, 9, 104, 514, 6, 95, 97, 7, 300, 384, 5, 734, 4, 1, 653, 301, 515, 8, 1047, 14, 248, 478, 301, 515], [1, 780, 146, 2710, 1, 1884, 549, 550, 551, 780, 187, 125, 638, 2, 3, 110, 1, 128, 509, 8, 48, 263, 13, 7, 118, 1116, 9, 1885, 848, 729, 227, 49, 61, 23, 2711, 2712, 2713, 902, 9, 1, 1884, 549, 550, 551, 969, 780], [640, 454, 188, 752, 14, 141, 77, 80, 12, 36, 11, 259, 4, 1114, 32, 2135, 14, 2136, 228, 754, 4, 228, 2714, 752, 89, 10, 141, 77, 80, 12, 36], [8, 281, 6, 15, 525, 312, 1070, 282, 1071, 4, 1072, 36, 1188, 567, 4, 1, 1886, 790, 19, 1200, 2, 3, 25, 1325, 1, 35, 14, 525, 312, 1070, 1071, 4, 1072, 36], [1325, 1, 35, 14, 525, 312, 1070, 1071, 4, 1072, 36, 589, 6, 385, 225, 112, 405, 940, 1, 649, 532, 2715, 858, 405, 469, 4, 1, 525, 312, 1070, 2716, 405, 469, 1071, 4, 1072, 36], [6, 18, 7, 2717, 1118, 893, 19, 1887, 16, 6, 121, 23, 7, 1262, 1118, 893, 19, 1887, 16, 34, 1888, 124, 2718, 23, 30, 142, 8, 7, 168, 816], [6, 15, 1889, 1041, 75, 1, 311, 802, 9, 364, 4, 15, 1, 2719, 608, 11, 2720, 143, 134, 36, 1580, 314, 134, 36, 13, 15, 11, 1, 143], [1, 471, 22, 224, 14, 249, 1073, 338, 2, 3, 581, 1, 1073, 8, 249, 32, 2721, 670, 9, 1, 467, 28, 8, 1, 447, 224, 251, 2722, 1890, 338, 2, 3, 581], [31, 68, 81, 6, 18, 99, 1326, 295, 288, 31, 7, 576, 184, 11, 2723, 4, 2724, 6, 851, 158, 168, 668, 10, 99, 1326, 295, 288], [11, 35, 1167, 1, 17, 29, 12, 2, 3, 16, 31, 350, 31, 1891, 2725, 19, 2726, 17, 1892, 1891, 1892, 2727, 190, 2728, 2729, 225, 1, 19, 659, 1, 17, 27, 35, 29, 12, 2, 3, 16, 214, 9, 1063, 645], [1, 1893, 776, 5, 1894, 2, 3, 57, 2730, 206, 2731, 2732, 21, 1, 1895, 26, 2733, 51, 1896, 2734, 4, 2735, 1893, 776, 1482, 1897, 26, 1894, 2, 3, 57, 1493, 1, 2736, 1895, 2737, 226, 2738, 973, 8, 1, 415, 5, 1, 2739], [6, 719, 68, 1327, 9, 1, 259, 222, 563, 2, 3, 156, 34, 385, 2740, 172, 2741, 6, 662, 719, 7, 226, 1327, 9, 1, 2742, 222, 563, 2, 3, 156, 42, 1462, 290, 7, 2743], [6, 214, 1, 366, 312, 894, 576, 78, 103, 21, 1, 202, 56, 78, 403, 204, 2, 3, 57, 6, 223, 58, 10, 1260, 366, 312, 184, 175, 8, 1, 202, 20, 204, 2, 3, 57], [6, 1201, 1, 2744, 1328, 315, 10, 2745, 487, 4, 1, 243, 285, 469, 244, 4, 245, 53, 1, 309, 13, 50, 10, 1329, 14, 2746, 1898, 10, 1, 243, 285, 469, 244, 4, 245, 53], [70, 89, 8, 259, 222, 711, 33, 460, 26, 10, 141, 77, 80, 12, 36, 70, 89, 5, 1, 711, 211, 49, 13, 260, 14, 141, 77, 80, 12, 36, 228, 410, 83, 2747, 1330], [6, 216, 70, 89, 10, 364, 311, 297, 260, 14, 141, 77, 80, 12, 36, 11, 51, 188, 6, 260, 248, 311, 297, 228, 410, 26, 949, 5, 77, 80, 12, 36], [131, 31, 1331, 6, 1142, 51, 568, 34, 858, 9, 1899, 8, 1, 577, 680, 2748, 21, 579, 2, 3, 64, 8, 1, 119, 680, 472, 6, 2749, 15, 1, 577, 578, 97, 26, 579, 2, 3, 64, 8, 938, 1332, 568, 283, 577, 2750], [6, 18, 1, 866, 320, 34, 32, 660, 26, 1, 264, 44, 251, 146, 380, 321, 84, 662, 6, 18, 83, 1, 2751, 146, 52, 344, 4, 94, 1, 264, 44, 251, 146, 277, 4, 321, 84, 4, 1230, 52], [6, 216, 70, 89, 10, 364, 311, 297, 260, 14, 141, 77, 80, 12, 36, 11, 70, 89, 281, 6, 18, 7, 141, 77, 80, 82, 97, 8, 12, 36], [23, 130, 2752, 1322, 2753, 4, 1900, 87, 118, 509, 7, 29, 61, 23, 7, 70, 184, 42, 13, 50, 23, 7, 2754, 144, 2755, 4, 1900, 87, 67, 1, 118, 9, 719, 7, 70, 29, 50, 23, 1, 28, 21, 1, 2756, 511, 9, 374, 1901, 130, 1876], [6, 396, 1, 729, 437, 847, 103, 14, 1898, 4, 1, 518, 285, 469, 519, 64, 6, 18, 7, 520, 415, 5, 869, 4, 18, 518, 519, 64, 31, 1, 437, 285, 82], [6, 15, 1, 226, 99, 81, 14, 107, 106, 4, 256, 416, 14, 326, 200, 105, 169, 190, 92, 79, 2, 3, 25, 6, 18, 1, 445, 81, 5, 99, 79, 2, 3, 25], [6, 147, 1, 70, 89, 5, 667, 211, 413, 235, 49, 10, 1, 77, 80, 82, 12, 36, 70, 89, 395, 32, 109, 10, 77, 80, 12, 36], [8, 709, 1, 52, 1299, 1, 1195, 227, 103, 715, 36, 83, 1, 52, 1299, 1, 1195, 103, 715, 36, 4, 98, 492, 1815, 18, 5, 7, 1333, 4, 7, 2757], [6, 18, 518, 519, 64, 9, 285, 1, 106, 648, 2758, 1, 46, 608, 6, 18, 518, 519, 64, 9, 285, 19, 106, 14, 7, 520, 415, 1828], [6, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40, 1, 49, 11, 1, 44, 572, 35, 558, 67, 50, 23, 1, 1334, 54, 22, 12, 40], [584, 13, 7, 375, 234, 5, 146, 1020, 156, 1, 679, 5, 1, 1335, 29, 13, 7, 389, 115, 8, 1, 827, 5, 7, 1174, 5, 2759, 1241, 146, 584, 1020, 156], [1, 188, 11, 2760, 4, 1484, 95, 870, 123, 516, 8, 482, 2, 3, 117, 131, 1144, 35, 143, 5, 125, 14, 1483, 115, 33, 118, 97, 8, 482, 2, 3, 117], [1, 130, 287, 13, 406, 594, 379, 277, 2, 3, 40, 1, 380, 1074, 406, 594, 379, 866, 277, 2, 3, 40, 320, 42, 32, 2761, 782, 34, 2762, 1902, 1903, 478, 242, 4, 205, 28], [1904, 2763, 13, 1, 305, 2764, 9, 2765, 1905, 4, 1906, 329, 1336, 167, 2766, 104, 9, 1113, 1, 305, 183, 26, 1160, 1022, 1337, 9, 1, 2767, 11, 1148, 1904, 1905, 4, 1906, 329], [463, 2, 3, 87, 871, 34, 1338, 5, 1, 1029, 354, 8, 1, 1339, 22, 94, 67, 663, 463, 2, 3, 87, 2768, 1, 1907, 480, 63, 305, 1339, 872, 22, 4, 2769, 34, 663, 1340, 11, 1338, 5, 1, 30, 354, 241, 254, 325, 5, 1, 1908, 838, 993, 14, 2770, 5, 663], [105, 41, 65, 18, 155, 116, 173, 342, 4, 474, 197, 100, 41, 65, 32, 50, 206, 1, 1909, 5, 1, 46, 28, 10, 614, 615, 87, 14, 155, 116, 839, 342, 4, 474, 197], [6, 15, 382, 383, 87, 11, 48, 340, 4, 1, 139, 2771, 2772, 15, 8, 1, 340, 516, 8, 193, 131, 494, 6, 15, 1, 1910, 81, 8, 382, 383, 87], [6, 752, 89, 10, 141, 77, 80, 12, 36, 1, 70, 89, 147, 13, 76, 196, 179, 14, 141, 77, 80, 82, 14, 228, 488, 297, 12, 36], [589, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16, 393, 6, 15, 17, 20, 31, 27, 296, 12, 2, 3, 16], [11, 359, 1, 101, 38, 29, 6, 15, 1, 177, 38, 20, 17, 12, 2, 3, 16, 8, 124, 60, 2773, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 302], [48, 188, 8, 1, 130, 2774, 2775, 182, 9, 2776, 10, 1, 39, 120, 1256, 86, 2, 3, 53, 6, 76, 182, 51, 63, 10, 39, 120, 86, 2, 3, 53], [6, 15, 60, 163, 169, 8, 92, 152, 79, 2, 3, 25, 131, 6, 15, 271, 199, 2777, 226, 99, 770, 1141, 1, 139, 188, 11, 1, 1233, 1341, 1, 81, 33, 419, 21, 92, 152, 79, 2, 3, 25], [542, 334, 234, 543, 4, 544, 387, 2778, 9, 1, 118, 2779, 861, 1342, 9, 24, 167, 351, 5, 1911, 5, 137, 242, 8, 1, 308, 5, 542, 334, 234, 543, 4, 544, 387], [105, 41, 65, 18, 155, 116, 173, 342, 4, 474, 197, 100, 41, 65, 32, 50, 206, 1, 1909, 5, 1, 46, 28, 10, 614, 615, 87, 14, 155, 116, 839, 342, 4, 474, 197], [6, 69, 1, 324, 1058, 811, 10, 1059, 1060, 315, 14, 150, 151, 2, 3, 25, 83, 10, 150, 151, 2, 3, 25], [11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16, 51, 24, 35, 49, 32, 61, 23, 17, 12, 2, 3, 16, 4, 60, 682, 11, 46, 4, 1343, 1, 65], [6, 18, 317, 222, 31, 24, 143, 314, 134, 36, 14, 60, 1912, 344, 6, 1075, 48, 873, 13, 327, 641, 14, 1, 317, 143, 314, 15, 11, 436, 134, 36], [11, 359, 1, 101, 38, 29, 6, 15, 1, 177, 38, 20, 17, 12, 2, 3, 16, 8, 124, 60, 929, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16], [6, 18, 1, 54, 85, 22, 12, 40, 31, 1, 825, 21, 42, 9, 219, 1, 2780, 2781, 239, 2782, 219, 24, 1344, 146, 21, 1, 1913, 540, 5, 1, 54, 22, 213, 170, 12, 40], [6, 18, 1076, 199, 1914, 14, 1915, 349, 4, 158, 168, 199, 1115, 14, 68, 753, 162, 21, 92, 79, 2, 3, 25, 423, 271, 1314, 6, 18, 271, 199, 14, 517, 315, 798, 4, 1315, 79, 2, 3, 25], [51, 28, 15, 8, 24, 58, 32, 874, 721, 4, 274, 10, 1, 120, 20, 86, 2, 3, 53, 6, 274, 44, 28, 4, 864, 298, 28, 10, 1, 39, 120, 20, 86, 2, 3, 53], [786, 1, 126, 193, 240, 1916, 1, 512, 22, 33, 473, 10, 1, 148, 135, 110, 83, 1, 22, 13, 473, 4, 224, 26, 186, 23, 105, 1026, 10, 1, 148, 135, 110], [11, 48, 265, 6, 15, 1, 271, 199, 184, 21, 92, 14, 517, 2783, 79, 2, 3, 25, 83, 6, 18, 271, 199, 14, 517, 349, 175, 10, 1, 92, 20, 79, 2, 3, 25], [51, 24, 49, 32, 664, 14, 7, 60, 27, 29, 66, 14, 17, 12, 2, 3, 16, 24, 56, 35, 49, 48, 1303, 32, 7, 2784, 21, 24, 323, 17, 12, 2, 3, 16, 61, 49, 21, 2785], [6, 147, 24, 975, 8, 1, 726, 5, 1, 1345, 57, 143, 62, 1917, 2, 3, 57, 1, 555, 49, 32, 397, 23, 1, 2786, 1150, 872, 35, 62, 5, 1345, 57, 1917, 2, 3, 57], [172, 163, 95, 123, 15, 8, 601, 167, 26, 409, 2, 3, 87, 6, 397, 24, 82, 14, 694, 732, 358, 34, 67, 15, 8, 409, 2, 3, 87], [145, 13, 21, 475, 2, 3, 117, 1, 1284, 5, 30, 1045, 13, 1918, 231, 475, 2, 3, 117, 770, 34, 1, 1919, 1920, 11, 30, 178, 13, 2787], [30, 73, 10, 59, 20, 47, 4, 45, 84, 1, 107, 580, 31, 169, 8, 46, 229, 11, 2788, 73, 13, 109, 26, 59, 47, 4, 45, 84, 8, 105, 398, 14, 1, 107, 726], [6, 289, 34, 10, 150, 151, 2, 3, 25, 9, 285, 1, 106, 13, 650, 852, 83, 10, 150, 151, 2, 3, 25], [1, 217, 137, 187, 485, 336, 2, 3, 90, 13, 7, 390, 22, 247, 14, 137, 242, 1509, 1, 549, 550, 551, 236, 5, 1, 217, 2789, 5, 1, 255, 1741, 1337, 11, 137, 724, 8, 44, 13, 1, 217, 137, 187, 336, 2, 3, 90], [1, 29, 33, 50, 23, 1, 44, 4, 867, 236, 5, 1, 54, 22, 213, 131, 12, 40, 1, 54, 28, 75, 351, 5, 1921, 125, 5, 1, 305, 236, 5, 1, 54, 22, 12, 40], [11, 231, 475, 2, 3, 117, 555, 471, 658, 1346, 178, 4, 1347, 178, 8, 595, 4, 738, 2790, 2, 3, 117, 214, 30, 178, 9, 738, 4, 736, 656, 1077, 595], [6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16, 8, 709, 6, 18, 17, 12, 2, 3, 16, 9, 69, 7, 427, 27, 235, 29, 23, 1, 2791, 22], [1896, 2792, 23, 48, 29, 95, 268, 34, 1, 256, 416, 1255, 156, 1078, 205, 199, 530, 252, 158, 168, 199, 294, 1922, 2793, 2794, 29, 11, 1, 391, 62, 13, 61, 23, 256, 416, 1255, 156], [6, 50, 7, 212, 5, 841, 38, 49, 10, 1, 17, 20, 12, 2, 3, 16, 8, 124, 107, 1923, 15, 1, 17, 20, 12, 2, 3, 16, 9, 69, 60, 27, 49, 14, 107, 739], [423, 2795, 627, 2, 3, 201, 97, 9, 18, 928, 1924, 238, 875, 9, 19, 112, 868, 2796, 2, 3, 201, 2797, 7, 928, 1924, 238, 309, 4, 2798, 1925, 286], [51, 28, 15, 8, 24, 58, 32, 874, 721, 4, 274, 10, 1, 120, 20, 86, 2, 3, 53, 6, 274, 44, 28, 4, 864, 298, 28, 10, 1, 39, 120, 20, 86, 2, 3, 53], [1, 1268, 236, 5, 28, 622, 21, 846, 4, 1269, 2799, 1270, 5, 1, 54, 183, 12, 40, 1, 261, 5, 239, 28, 15, 8, 1, 58, 13, 1, 54, 310, 12, 40], [6, 1075, 48, 873, 13, 327, 641, 14, 1, 317, 143, 314, 15, 11, 436, 134, 36, 317, 134, 36, 13, 1, 1670, 355, 314, 1153, 15, 9, 337, 1, 133, 436, 188], [589, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16, 6, 15, 1, 17, 20, 12, 2, 3, 16, 9, 69, 60, 27, 49, 14, 107, 739], [589, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16, 1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [1, 161, 2800, 11, 1, 606, 35, 617, 7, 75, 5, 65, 2801, 840, 505, 1008, 83, 2802, 2803, 83, 633, 1277, 83, 26, 2804, 1, 634, 250, 222, 47, 4, 45, 36, 505, 1008, 83, 1643, 1644, 386, 1008, 83, 840, 2805, 634, 128, 9, 27, 35, 47, 4, 45, 36, 635, 65, 1, 1645, 35, 377, 1646, 713, 83, 1647, 1648, 713, 505, 633, 83, 319, 505, 13, 1, 209, 1228], [1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16, 6, 15, 1, 17, 20, 12, 2, 3, 16, 9, 69, 60, 27, 49, 14, 107, 739], [1, 2806, 21, 1, 1202, 32, 400, 4, 473, 10, 148, 135, 110, 31, 176, 26, 1, 2807, 51, 44, 28, 32, 119, 224, 4, 473, 10, 1, 148, 135, 110], [9, 2808, 1, 2809, 5, 782, 6, 1321, 876, 171, 1, 44, 353, 22, 1926, 4, 1927, 43, 1, 629, 209, 41, 19, 13, 50, 21, 1, 917, 540, 5, 44, 353, 22, 1926, 4, 1927, 43], [101, 30, 132, 67, 153, 26, 464, 59, 8, 105, 398, 4, 817, 10, 1, 356, 370, 47, 4, 45, 43, 35, 65, 67, 50, 206, 1, 239, 28, 34, 33, 374, 593, 10, 59, 47, 4, 45, 43, 8, 105, 398, 4, 1, 2810, 370, 33, 15, 9, 2811], [1, 188, 2812, 8, 193, 131, 32, 153, 14, 1, 1318, 184, 50, 10, 1, 202, 403, 204, 2, 3, 57, 23, 24, 1928, 565, 88, 422, 4, 46, 144, 2813, 2814, 720, 19, 13, 7, 405, 189, 184, 50, 23, 1, 2815, 85, 46, 28, 10, 202, 204, 2, 3, 57], [6, 50, 7, 212, 5, 841, 38, 49, 10, 1, 17, 20, 12, 2, 3, 16, 8, 124, 107, 1923, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [1, 286, 5, 24, 103, 13, 555, 14, 1, 221, 623, 153, 14, 7, 1929, 5, 1, 665, 103, 131, 665, 740, 42, 1325, 1, 157, 5, 68, 2816, 30, 26, 2817, 34, 30, 812, 221, 13, 109, 10, 1, 665, 103, 665, 740], [6, 18, 1, 445, 81, 5, 99, 79, 2, 3, 25, 6, 18, 7, 2818, 92, 79, 2, 3, 25, 81, 5, 256, 1672, 4, 489, 8, 553, 31, 350, 31, 7, 650, 852, 437, 1930, 200, 81, 21, 1, 2819, 2820], [11, 231, 475, 2, 3, 117, 770, 34, 1, 1919, 1920, 11, 30, 178, 13, 62, 1348, 475, 2, 3, 117, 214, 30, 178, 9, 738, 4, 736, 656, 1077, 595], [2821, 481, 272, 273, 2, 3, 57, 11, 119, 198, 4, 1215, 7, 1931, 11, 1931, 1838, 4, 249, 1162, 626, 9, 2822, 1, 2823, 6, 15, 1, 943, 171, 176, 26, 272, 11, 24, 119, 198, 273, 2, 3, 57], [51, 24, 35, 49, 32, 61, 23, 17, 12, 2, 3, 16, 4, 60, 682, 11, 46, 4, 1343, 1, 2824, 24, 49, 32, 664, 14, 7, 60, 27, 29, 66, 14, 17, 12, 2, 3, 16], [1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16, 6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16], [589, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16, 11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16], [6, 2825, 2826, 2827, 129, 655, 1932, 11, 905, 11, 1636, 1, 2828, 23, 1, 1602, 5, 1, 362, 2, 3, 88, 2829, 984, 1, 258, 195, 2830, 26, 362, 2, 3, 88, 11, 48, 265], [98, 129, 123, 268, 8, 323, 167, 23, 293, 303, 34, 1, 1740, 55, 431, 211, 833, 112, 731, 1933, 1, 333, 1934, 9, 2831, 7, 1935, 211, 789, 1036, 4, 1037, 40, 24, 167, 13, 76, 601, 9, 1036, 4, 1037, 40, 319, 1, 142, 211, 1, 63, 23, 1, 431, 1738, 112, 731, 8, 1, 55, 758, 13, 15, 9, 1739, 7, 162, 369], [172, 530, 67, 15, 9, 2832, 8, 1, 1, 1048, 472, 62, 573, 7, 4, 1239, 472, 62, 573, 989, 5, 1, 834, 62, 835, 195, 115, 8, 465, 639, 2, 3, 1, 29, 34, 153, 1, 606, 286, 8, 1, 195, 115, 190, 1239, 472, 62, 5, 565, 88, 1192, 2, 3, 88, 4, 53, 639, 2, 3, 53, 1497, 465, 9, 121, 2833, 2834], [27, 38, 29, 7, 60, 2835, 2836, 27, 38, 29, 33, 66, 10, 1, 220, 261, 17, 20, 12, 2, 3, 16, 14, 106, 75, 72, 9, 797, 5, 2837, 2838, 491, 7, 27, 38, 29, 66, 10, 1, 17, 20, 12, 2, 3, 16, 4, 1, 958, 582, 183, 590, 1, 125, 8, 1, 147, 75, 33, 15, 9, 637, 1], [11, 1, 234, 5, 346, 347, 84, 9, 104, 514, 6, 95, 97, 7, 300, 384, 5, 734, 4, 1, 653, 301, 515, 8, 1047, 14, 248, 478, 301, 2839, 1, 301, 513, 8, 253, 325, 4, 344, 6, 154, 808, 7, 1232, 850, 9, 413, 730, 125, 539, 1, 1275, 1732, 9, 1, 115, 5, 346, 347, 84], [51, 603, 361, 1349, 11, 93, 119, 1007, 325, 276, 32, 21, 39, 120, 86, 2, 3, 53, 236, 5, 876, 198, 4, 736, 656, 1077, 11, 1936, 1937, 93, 32, 196, 179, 10, 39, 120, 86, 2, 3, 53], [103, 170, 1862, 7, 1938, 103, 11, 1, 1289, 619, 1939, 2, 3, 37, 9, 121, 7, 52, 6, 18, 7, 1289, 184, 9, 821, 1, 2840, 4, 396, 1, 1938, 2841, 103, 1939, 2, 3, 37, 11, 435, 411], [1, 62, 13, 236, 5, 1, 130, 143, 64, 877, 569, 2, 3, 64, 130, 1350, 142, 2842, 13, 1, 62, 5, 2843, 1, 142, 5, 7, 654, 5, 125, 23, 7, 1885, 21, 83, 9, 170, 569, 2, 3, 64], [6, 15, 1, 139, 164, 2844, 31, 1079, 2, 3, 16, 6, 76, 18, 2845, 9, 225, 659, 1, 2846, 99, 82, 4, 99, 101, 5, 1079, 2, 3, 16], [1, 39, 52, 83, 71, 2, 3, 37, 33, 15, 9, 723, 51, 55, 2847, 139, 2848, 5, 980, 33, 214, 9, 1, 217, 187, 10, 1, 39, 2849, 29, 9, 723, 55, 361, 74, 71, 2, 3, 37], [83, 1040, 1214, 6, 2850, 4, 920, 616, 4, 1351, 1940, 21, 111, 1597, 10, 1, 272, 20, 273, 2, 3, 57, 11, 1941, 6, 1049, 4, 181, 1, 28, 10, 272, 273, 2, 3, 57], [56, 35, 29, 302, 6, 15, 7, 27, 70, 56, 35, 19, 31, 175, 8, 1, 17, 20, 12, 2, 3, 16, 11, 56, 1320, 18, 1, 17, 27, 35, 29, 12, 2, 3, 16, 9, 424, 24, 65], [1, 126, 193, 13, 174, 21, 7, 239, 133, 585, 23, 1, 30, 472, 10, 856, 59, 47, 4, 45, 43, 1, 22, 13, 118, 593, 10, 7, 30, 73, 370, 47, 4, 45, 43], [48, 401, 13, 650, 72, 9, 1, 330, 5, 1080, 1081, 4, 1082, 36, 131, 2851, 13, 7, 310, 5, 108, 682, 11, 372, 41, 686, 61, 23, 1, 2852, 1080, 330, 1081, 4, 1082, 36], [2853, 10, 1159, 6, 18, 1, 486, 128, 97, 26, 1942, 4, 482, 36, 8, 269, 9, 644, 1, 257, 1352, 804, 636, 2854, 1943, 6, 18, 1, 195, 486, 421, 26, 1942, 4, 482, 36], [1944, 6, 1945, 18, 1, 39, 120, 20, 131, 86, 2, 3, 53, 9, 1946, 111, 838, 11, 1, 136, 218, 8, 1, 143, 500, 18, 1, 39, 120, 878, 171, 11, 186, 198, 86, 2, 3, 53], [26, 726, 699, 2855, 4, 2856, 11, 51, 2857, 1, 1300, 162, 154, 104, 1947, 9, 1, 162, 97, 8, 451, 4, 690, 156, 1, 1083, 13, 72, 9, 1, 1083, 11, 1, 189, 162, 8, 451, 4, 690, 156], [94, 6, 484, 1, 711, 8, 286, 11, 70, 89, 10, 68, 821, 1211, 608, 935, 84, 14, 2858, 1330, 6, 688, 70, 89, 5, 286, 667, 10, 1146, 1147, 935, 84], [1944, 6, 1945, 18, 1, 39, 120, 20, 131, 86, 2, 3, 53, 9, 1946, 111, 838, 11, 1, 136, 218, 8, 1, 143, 500, 18, 1, 39, 120, 878, 171, 11, 186, 198, 86, 2, 3, 53], [51, 603, 361, 1349, 11, 93, 119, 1007, 325, 276, 32, 21, 39, 120, 86, 2, 3, 53, 236, 5, 876, 198, 4, 736, 656, 1077, 11, 1936, 1937, 93, 32, 196, 179, 10, 39, 120, 86, 2, 3, 53], [8, 1948, 4, 1949, 36, 1, 324, 5, 758, 61, 2859, 33, 15, 9, 496, 7, 75, 5, 125, 8, 2860, 2861, 1253, 2862, 33, 76, 1, 19, 15, 9, 496, 125, 8, 1948, 4, 1949, 36], [89, 33, 484, 10, 7, 141, 77, 12, 36, 14, 507, 1353, 2863, 89, 13, 484, 23, 1, 259, 314, 10, 141, 77, 80, 12, 36, 14, 699, 507, 4, 228, 410], [11, 1309, 4, 147, 1055, 6, 15, 1857, 1858, 1055, 21, 1, 1310, 279, 399, 1056, 2, 3, 165, 9, 2864, 48, 216, 6, 260, 1, 2865, 142, 211, 63, 4, 248, 279, 2866, 21, 1, 2867, 399, 1056, 2, 3, 165], [6, 15, 102, 96, 25, 9, 348, 1051, 41, 65, 14, 116, 173, 23, 1, 209, 140, 5, 1, 239, 46, 2868, 6, 15, 1, 102, 20, 96, 25, 9, 121, 51, 41, 65, 15, 8, 48, 167, 386, 105, 11, 28, 824, 4, 11, 1, 235, 49, 15, 11, 1830, 143], [6, 66, 7, 100, 41, 19, 23, 1, 44, 140, 5, 703, 10, 102, 96, 25, 6, 15, 102, 96, 25, 9, 348, 1051, 41, 65, 14, 116, 173, 23, 1, 209, 140, 5, 1, 239, 46, 144], [68, 707, 5, 1, 2869, 5, 7, 2870, 1165, 7, 477, 1859, 33, 260, 206, 1, 247, 44, 353, 2871, 22, 1950, 2, 3, 64, 8, 1, 872, 1861, 6, 508, 1, 1396, 5, 7, 388, 5, 247, 44, 353, 1950, 2, 3, 64], [6, 18, 1, 207, 313, 100, 22, 180, 4, 246, 37, 9, 981, 1, 41, 19, 222, 11, 7, 2872, 1, 2873, 13, 318, 169, 833, 327, 6, 18, 1, 207, 313, 100, 22, 180, 4, 246, 37, 9, 219, 1, 367, 763, 250], [56, 35, 29, 302, 6, 15, 7, 27, 70, 56, 35, 19, 31, 175, 8, 1, 17, 20, 12, 2, 3, 16, 11, 56, 1790, 17, 38, 20, 12, 2, 3, 16, 586, 7, 514, 70, 35, 29, 2874, 643, 1, 2875, 2876], [56, 35, 29, 302, 6, 15, 7, 27, 70, 56, 35, 19, 31, 175, 8, 1, 17, 20, 12, 2, 3, 16, 11, 56, 1320, 50, 7, 19, 10, 17, 20, 12, 2, 3, 16, 23, 1, 46, 28, 31, 24, 101, 29], [7, 1245, 13, 7, 406, 2877, 334, 1242, 5, 2878, 1951, 5, 63, 1952, 2, 3, 37, 1, 1245, 1042, 578, 13, 7, 2879, 213, 5, 2880, 578, 1952, 2, 3, 37], [2881, 7, 388, 5, 1567, 183, 14, 1953, 30, 218, 338, 4, 1954, 1084, 6, 18, 7, 75, 5, 1953, 30, 218, 21, 7, 678, 196, 179, 26, 338, 4, 1954, 1084], [6, 153, 1955, 1212, 10, 1, 266, 164, 122, 267, 2, 3, 64, 21, 112, 1956, 131, 51, 361, 67, 233, 10, 1, 266, 556, 164, 122, 267, 2, 3, 64], [6, 1354, 1, 160, 200, 8, 357, 283, 389, 191, 10, 39, 120, 86, 2, 3, 53, 6, 18, 1, 39, 120, 878, 171, 11, 186, 198, 86, 2, 3, 53], [437, 1957, 370, 9, 1044, 1, 1958, 437, 304, 1959, 88, 11, 1, 1958, 437, 304, 1244, 2882, 154, 104, 807, 26, 1957, 1, 1804, 1959, 88], [6, 153, 1955, 1212, 10, 1, 266, 164, 122, 267, 2, 3, 64, 21, 112, 1956, 131, 1, 361, 67, 414, 10, 1, 266, 556, 164, 122, 267, 2, 3, 64], [6, 192, 719, 8, 327, 741, 7, 1960, 298, 22, 1, 217, 298, 187, 742, 2, 3, 40, 11, 298, 6, 18, 1, 217, 298, 187, 213, 2883, 1626, 742, 2, 3, 40], [8, 269, 9, 1621, 70, 89, 5, 1, 153, 188, 6, 18, 1, 141, 77, 80, 82, 12, 36, 42, 2884, 1, 528, 2885, 34, 7, 460, 711, 11, 70, 89, 281, 6, 18, 7, 141, 77, 80, 82, 97, 8, 12, 36], [70, 89, 8, 259, 222, 711, 33, 460, 26, 10, 141, 77, 80, 12, 36, 89, 33, 484, 10, 7, 141, 77, 12, 36, 14, 507, 1353, 1961], [1, 1675, 29, 1085, 23, 1, 2886, 5, 1, 29, 138, 8, 1962, 4, 781, 2887, 1, 2888, 184, 481, 7, 1588, 29, 359, 23, 167, 138, 8, 4, 1962, 4, 781, 2889], [11, 2890, 1, 1934, 2891, 6, 18, 1, 1062, 22, 425, 180, 4, 246, 37, 31, 7, 1963, 1327, 6, 18, 1716, 450, 21, 1, 207, 313, 22, 180, 4, 246, 37], [126, 218, 67, 174, 21, 1724, 30, 132, 4, 1725, 421, 26, 59, 47, 4, 45, 43, 10, 1, 966, 5, 705, 356, 4, 2892, 132, 67, 417, 10, 59, 47, 4, 45, 43], [6, 1964, 51, 5, 24, 58, 8, 202, 204, 2, 3, 57, 10, 271, 2893, 204, 2, 3, 57, 42, 240, 1, 81, 5, 51, 493, 530, 33, 15, 8, 24, 678], [105, 144, 67, 174, 21, 1, 220, 85, 22, 1355, 652, 64, 1, 85, 28, 67, 419, 21, 1355, 652, 64, 42, 586, 1334, 144, 14, 164], [6, 1964, 51, 5, 24, 58, 8, 202, 204, 2, 3, 57, 10, 271, 2894, 223, 58, 10, 1260, 366, 312, 184, 175, 8, 1, 202, 20, 204, 2, 3, 57], [1, 582, 832, 46, 22, 33, 564, 21, 1, 54, 310, 12, 40, 1, 58, 939, 23, 35, 21, 305, 9, 44, 10, 1, 54, 28, 12, 40], [384, 29, 1673, 1, 384, 29, 34, 13, 66, 283, 1, 1080, 425, 330, 1081, 4, 1082, 36, 48, 401, 13, 650, 72, 9, 1, 330, 5, 1080, 1081, 4, 1082, 36], [11, 205, 394, 6, 18, 1, 144, 414, 169, 11, 1, 1323, 159, 62, 1068, 4, 1069, 37, 6, 18, 1, 1323, 1068, 4, 1069, 37, 377, 28, 21, 2895, 332, 394, 292, 2896, 735, 2897, 1324, 572, 4, 954], [193, 170, 1021, 24, 299, 19, 14, 7, 1356, 5, 1, 299, 19, 97, 8, 1086, 4, 1087, 57, 6, 1965, 34, 24, 19, 1078, 1, 19, 97, 8, 1086, 4, 1087, 57, 8, 51, 1064], [193, 170, 1021, 24, 299, 19, 14, 7, 1356, 5, 1, 299, 19, 97, 8, 1086, 4, 1087, 57, 6, 1965, 34, 24, 19, 1078, 1, 19, 97, 8, 1086, 4, 1087, 57, 8, 51, 1064], [172, 188, 2898, 797, 617, 8, 2899, 2, 3, 43, 319, 1, 1357, 162, 2900, 2901, 1717, 1358, 286, 602, 1, 1966, 2902, 2, 3, 43, 95, 268, 1, 1966, 162, 9, 104, 2903, 8, 1967, 4, 1, 1357, 162, 8, 1967, 131, 319, 840, 4, 699, 32, 1, 212, 5, 2904, 8, 200, 1006, 83, 4, 1006, 94, 1864], [11, 231, 357, 510, 2, 3, 37, 7, 1359, 164, 511, 851, 48, 2905, 48, 1968, 7, 1262, 1359, 164, 2906, 618, 1, 357, 511, 510, 2, 3, 37, 33, 2907], [31, 11, 1969, 35, 6, 18, 1, 39, 52, 74, 71, 2, 3, 37, 9, 210, 44, 1970, 727, 18, 39, 52, 74, 71, 2, 3, 37, 9, 210, 160, 200, 4, 55, 242], [11, 105, 44, 4, 305, 6, 15, 1, 1321, 876, 171, 148, 135, 110, 9, 210, 2908, 11, 305, 28, 1138, 6, 15, 1, 148, 135, 110, 632], [6, 69, 24, 19, 23, 7, 388, 5, 1, 1971, 1972, 425, 22, 463, 2, 3, 57, 6, 66, 7, 804, 863, 1899, 94, 1824, 83, 10, 1, 2909, 22, 463, 2, 3, 57], [1, 1015, 32, 637, 10, 7, 27, 29, 66, 10, 17, 12, 2, 3, 16, 1, 1234, 1973, 118, 112, 1088, 32, 60, 49, 10, 408, 294, 1360, 50, 10, 17, 12, 2, 3, 16], [1, 1015, 32, 637, 10, 7, 27, 29, 66, 10, 17, 12, 2, 3, 16, 1, 1234, 1973, 161, 13, 66, 23, 1089, 5, 68, 177, 27, 38, 161, 17, 12, 2, 3, 16], [6, 216, 70, 89, 10, 364, 311, 297, 260, 14, 141, 77, 80, 12, 36, 89, 33, 484, 10, 7, 141, 77, 12, 36, 14, 507, 1353, 1961], [31, 11, 1969, 35, 6, 18, 1, 39, 52, 74, 71, 2, 3, 37, 9, 210, 44, 1970, 727, 18, 39, 52, 74, 71, 2, 3, 37, 9, 210, 160, 200, 4, 55, 242], [63, 67, 2910, 4, 182, 10, 1, 249, 1256, 8, 1, 272, 94, 20, 273, 2, 3, 57, 11, 1941, 6, 1049, 4, 181, 1, 28, 10, 272, 273, 2, 3, 57], [44, 125, 32, 373, 283, 55, 782, 26, 39, 52, 71, 2, 3, 37, 1, 1817, 242, 32, 51, 1, 537, 191, 660, 26, 1, 39, 55, 52, 71, 2, 3, 37], [105, 5, 24, 49, 67, 61, 23, 1, 17, 161, 12, 2, 3, 16, 1, 161, 13, 66, 23, 1089, 5, 68, 177, 27, 38, 161, 17, 12, 2, 3, 16], [6, 76, 15, 1974, 1975, 4, 1976, 288, 11, 704, 1, 2911, 486, 1977, 2912, 1, 1974, 486, 1975, 4, 1976, 288, 13, 15, 11, 970, 1, 331, 75, 5, 1309, 63, 5, 83], [35, 190, 1, 1978, 201, 877, 1979, 2, 3, 201, 1, 483, 1130, 5, 1, 2913, 655, 1932, 631, 8, 105, 2914, 5, 1, 1978, 201, 159, 62, 2915, 35, 4, 2916, 792, 1979, 2, 3, 201], [1291, 4, 2917, 649, 1980, 13, 7, 2918, 980, 2919, 11, 352, 2920, 498, 197, 68, 231, 5, 290, 7, 2921, 1737, 13, 649, 1980, 498, 197], [1981, 222, 1982, 6, 15, 1, 1982, 249, 1983, 2, 3, 117, 257, 251, 9, 1901, 1745, 11, 111, 30, 61, 23, 493, 1311, 386, 1984, 1985, 4, 995, 2922, 192, 95, 757, 7, 195, 222, 10, 1, 1981, 1983, 2, 3, 117, 257, 251, 9, 111, 30, 8, 1, 75, 5, 1494, 2923], [54, 94, 12, 40, 98, 13, 7, 22, 5, 85, 306, 8, 1011, 394, 21, 1, 208, 5, 1, 203, 449, 6, 15, 7, 388, 5, 1, 28, 176, 11, 1, 391, 877, 23, 70, 56, 35, 94, 42, 351, 1189, 5, 306, 21, 1, 54, 22, 12, 40], [6, 76, 196, 179, 7, 1660, 433, 340, 319, 1, 1007, 32, 830, 61, 23, 1, 1233, 132, 153, 26, 1661, 340, 5, 829, 2, 3, 64, 829, 2, 3, 64, 509, 7, 82, 319, 261, 133, 13, 830, 9, 1794, 1, 209, 30, 269, 61, 23, 30, 73], [1986, 13, 1987, 728, 31, 68, 345, 322, 62, 643, 1, 91, 113, 114, 127, 34, 1, 30, 157, 13, 1988, 26, 1, 75, 5, 149, 8, 42, 98, 2924, 65, 5, 157, 360, 1, 91, 113, 114, 127, 42, 270, 34, 112, 63, 34, 194, 8, 72, 149, 95, 72, 365], [1, 28, 15, 11, 1, 58, 138, 8, 48, 263, 622, 1710, 21, 1711, 491, 512, 4, 1, 54, 22, 5, 203, 845, 208, 12, 40, 1274, 54, 94, 12, 40, 98, 13, 7, 22, 5, 85, 306, 8, 1011, 394, 21, 1, 208, 5, 1, 203, 449], [11, 1, 41, 19, 6, 15, 1, 102, 20, 96, 25, 9, 348, 7, 100, 41, 19, 23, 1, 209, 140, 5, 1, 54, 22, 717, 14, 844, 1989, 1246, 14, 116, 2925, 19, 11, 51, 1795, 49, 6, 18, 7, 1050, 116, 291, 41, 19, 50, 10, 1, 102, 20, 96, 25], [118, 15, 26, 1990, 2, 3, 16, 1, 1690, 183, 240, 253, 332, 354, 5, 1361, 695, 419, 21, 2926, 539, 1090, 1991, 2927, 4, 2928, 2929, 14, 507, 1984, 9, 337, 2930, 11, 195, 215, 6, 18, 1, 2931, 1361, 695, 803, 26, 1990, 2, 3, 16, 11, 1304, 332, 1361, 1135, 1090, 989, 1991, 924, 2932, 2933, 505, 4], [1, 255, 1362, 231, 1363, 1364, 104, 1, 54, 22, 12, 40, 11, 48, 265, 6, 18, 1, 54, 22, 12, 40], [11, 41, 343, 6, 50, 7, 1220, 100, 116, 291, 371, 19, 23, 1, 209, 386, 44, 140, 5, 1, 46, 825, 10, 102, 96, 25, 8, 269, 9, 337, 1, 1677, 5, 111, 29, 6, 69, 100, 41, 65, 11, 111, 41, 10, 102, 96, 25], [328, 15, 1, 1206, 164, 122, 266, 267, 2, 3, 64, 11, 1, 164, 1, 361, 67, 414, 10, 1, 266, 556, 164, 122, 267, 2, 3, 64], [1, 215, 33, 223, 10, 332, 92, 530, 79, 2, 3, 25, 1, 99, 65, 67, 50, 10, 1, 92, 20, 253, 79, 2, 3, 25], [1, 54, 22, 12, 40, 13, 66, 21, 1, 208, 5, 1, 203, 541, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40], [11, 41, 343, 6, 50, 7, 1220, 100, 116, 291, 371, 19, 23, 1, 209, 386, 44, 140, 5, 1, 46, 825, 10, 102, 96, 25, 1, 41, 19, 13, 7, 100, 102, 96, 25, 19, 50, 10, 822, 14, 155, 116, 173, 4, 559, 567], [1, 582, 832, 46, 22, 33, 564, 21, 1, 54, 310, 12, 40, 6, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40], [6, 18, 1, 39, 120, 878, 171, 11, 186, 198, 86, 2, 3, 53, 6, 76, 182, 51, 63, 10, 39, 120, 86, 2, 3, 53], [51, 5, 24, 65, 32, 50, 10, 1992, 455, 2, 3, 974, 6, 50, 24, 628, 238, 56, 35, 49, 420, 863, 8, 193, 131, 14, 1992, 455, 2, 3, 974], [6, 18, 59, 47, 4, 45, 43, 14, 124, 107, 106, 9, 723, 126, 1993, 15, 1, 59, 108, 47, 4, 45, 43, 9, 657, 1, 30, 132], [6, 32, 2934, 14, 60, 1248, 31, 1994, 1995, 2, 3, 88, 1, 636, 13, 402, 14, 2935, 31, 175, 8, 1994, 1995, 2, 3, 88], [328, 32, 61, 23, 91, 113, 42, 1257, 643, 1, 1258, 34, 72, 63, 194, 8, 72, 149, 114, 127, 625, 1518, 360, 1, 60, 91, 113, 42, 270, 34, 63, 945, 8, 1, 139, 149, 237, 9, 95, 72, 157, 114, 127], [124, 575, 19, 13, 7, 2936, 529, 526, 19, 376, 19, 627, 2, 3, 43, 11, 298, 7, 575, 19, 627, 2, 3, 43, 13, 15, 11, 2937, 30, 2938], [1, 1091, 763, 162, 33, 50, 14, 404, 316, 4, 134, 25, 31, 24, 1222, 6, 18, 404, 14, 7, 226, 162, 316, 4, 134, 25], [1, 144, 32, 118, 181, 4, 721, 10, 1, 17, 229, 192, 182, 4, 224, 26, 186, 119, 10, 1, 148, 135, 110, 1, 22, 33, 1947, 21, 2939, 9, 2940, 133, 413, 1228, 1288, 476, 67, 192, 214, 4, 1, 22, 33, 182, 10, 148, 135, 110], [11, 231, 357, 510, 2, 3, 37, 7, 1359, 164, 511, 851, 48, 2941, 231, 1, 357, 510, 2, 3, 37, 511, 2942, 11, 48, 128], [6, 18, 1, 177, 17, 20, 12, 2, 3, 16, 9, 121, 7, 60, 27, 38, 29, 42, 1720, 848, 9, 344, 63, 1272, 8, 1, 17, 126, 2943, 17, 20, 12, 2, 3, 16, 13, 15, 9, 69, 7, 126, 61, 38, 29, 14, 1, 85, 28, 870, 587], [8, 24, 58, 6, 18, 1, 490, 152, 557, 2, 3, 90, 9, 972, 1, 2944, 304, 14, 900, 94, 349, 4, 900, 94, 2945, 6, 18, 1, 490, 99, 152, 557, 2, 3, 90, 31, 98, 13, 1163, 9, 133, 215, 558, 14, 390, 1164, 5, 93, 4, 306], [1, 582, 832, 46, 22, 33, 564, 21, 1, 54, 310, 12, 40, 11, 48, 265, 6, 18, 1, 54, 22, 12, 40], [1, 1365, 1366, 22, 1996, 2, 3, 43, 13, 7, 22, 5, 133, 1856, 5, 655, 2946, 1365, 1366, 22, 1, 1365, 1366, 22, 1996, 2, 3, 43, 13, 2947, 2948, 2949], [6, 18, 317, 134, 36, 11, 1997, 1, 879, 5, 1998, 18, 317, 314, 134, 36, 9, 337, 421, 2950, 659, 296, 1032], [1, 216, 459, 13, 1, 2951, 2952, 1999, 569, 2, 3, 64, 1, 62, 13, 236, 5, 1, 130, 143, 64, 877, 569, 2, 3, 64], [1, 1849, 144, 6, 18, 32, 54, 12, 40, 4, 1, 2953, 2954, 48, 265, 6, 18, 1, 54, 22, 12, 40], [6, 976, 48, 427, 28, 11, 46, 31, 138, 8, 455, 2, 3, 560, 6, 76, 15, 374, 2000, 368, 427, 28, 455, 2, 3, 560], [6, 18, 1, 39, 120, 878, 171, 11, 186, 198, 86, 2, 3, 53, 6, 76, 182, 51, 63, 10, 39, 120, 86, 2, 3, 53], [48, 13, 7, 625, 314, 2001, 23, 1, 91, 113, 5, 157, 2002, 34, 142, 5, 308, 2003, 142, 5, 157, 1092, 114, 127, 625, 157, 418, 910, 23, 1, 91, 113, 42, 1668, 34, 63, 1017, 8, 7, 72, 75, 5, 149, 32, 76, 72, 8, 157, 114, 127], [2004, 475, 2, 3, 117, 397, 493, 332, 30, 418, 23, 595, 4, 738, 4, 2955, 34, 345, 30, 418, 1149, 595, 4, 2956, 2, 3, 117, 337, 332, 1057, 11, 1264, 30, 418, 4, 741, 454, 2957, 11, 576, 595, 4, 738, 49, 545, 76, 2005, 30, 1045], [1367, 296, 29, 6, 225, 7, 212, 5, 1368, 1369, 9, 1, 258, 70, 35, 1001, 17, 12, 2, 3, 16, 6, 192, 18, 1, 126, 303, 1722, 8, 1, 17, 70, 56, 35, 29, 12, 2, 3, 16, 9, 219, 7, 126, 193, 42, 1723, 206, 1191], [11, 46, 99, 163, 6, 15, 404, 152, 316, 4, 134, 156, 6, 15, 404, 9, 424, 24, 2958, 99, 11, 199, 316, 4, 134, 156], [1, 30, 73, 33, 50, 10, 59, 47, 4, 45, 43, 14, 1, 580, 2959, 30, 73, 33, 153, 26, 464, 59, 47, 4, 45, 43], [1, 22, 13, 118, 593, 10, 7, 30, 73, 370, 47, 4, 45, 43, 1, 30, 73, 33, 153, 26, 464, 59, 47, 4, 45, 43], [6, 262, 77, 80, 14, 604, 411, 31, 138, 8, 12, 36, 6, 15, 77, 80, 11, 281, 70, 89, 12, 36], [6, 262, 77, 80, 14, 604, 411, 31, 138, 26, 12, 36, 6, 15, 77, 80, 11, 281, 70, 89, 12, 36], [526, 897, 875, 2006, 2007, 4, 2008, 37, 13, 458, 11, 78, 4, 2960, 897, 875, 2006, 2007, 4, 2008, 37, 32, 145, 5, 1, 70, 1875, 78, 2961], [14, 1, 46, 999, 5, 1, 17, 20, 12, 2, 3, 16, 6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 302], [6, 69, 11, 1023, 1719, 10, 520, 729, 437, 847, 1, 518, 285, 469, 519, 64, 4, 2962, 2963, 2009, 106, 2964, 10, 1, 2965, 410, 410, 11, 677, 106, 4, 754, 754, 11, 178, 4, 69, 1, 19, 10, 729, 437, 847, 1329, 14, 78, 2010], [48, 2966, 1, 2967, 5, 482, 2, 3, 64, 34, 41, 19, 4, 375, 299, 65, 254, 95, 2968, 2969, 23, 35, 2970, 439, 482, 2, 3, 64, 95, 97, 7, 300, 35, 2971, 401, 34, 2972, 254, 23, 427, 144], [11, 48, 265, 6, 18, 1, 54, 22, 12, 40, 6, 337, 24, 82, 26, 949, 5, 1, 54, 22, 12, 40], [6, 976, 48, 427, 28, 11, 46, 31, 138, 8, 455, 2, 3, 560, 6, 76, 15, 374, 2000, 368, 427, 28, 455, 2, 3, 560], [24, 161, 13, 7, 1333, 161, 72, 9, 12, 2, 3, 43, 126, 303, 33, 109, 280, 12, 2, 3, 43], [11, 1, 44, 572, 4, 841, 49, 6, 15, 85, 46, 28, 21, 1, 54, 4, 872, 2011, 144, 31, 350, 31, 1, 2012, 22, 2013, 2, 3, 64, 8, 24, 2014, 1789, 6, 50, 68, 38, 29, 10, 254, 46, 28, 21, 1, 2012, 22, 2013, 2, 3, 64], [1, 30, 73, 33, 50, 10, 59, 47, 4, 45, 43, 14, 1, 580, 2973, 73, 33, 233, 14, 59, 47, 4, 45, 43, 11, 105, 49], [542, 334, 234, 931, 543, 4, 544, 387, 887, 1, 2015, 334, 5, 7, 133, 26, 7, 420, 189, 618, 137, 189, 1486, 31, 268, 8, 426, 83, 1, 861, 1342, 9, 24, 167, 351, 5, 1911, 5, 137, 242, 8, 1, 308, 5, 542, 334, 234, 543, 4, 544, 387], [1, 183, 15, 11, 1, 58, 516, 8, 48, 263, 129, 123, 1855, 26, 2016, 2, 3, 88, 1, 205, 1951, 95, 123, 15, 2974, 11, 231, 26, 2975, 2, 3, 57, 294, 2016, 2, 3, 88, 2976, 24, 81, 13, 332, 8, 255, 5, 1, 1064], [7, 914, 28, 261, 11, 48, 13, 1, 54, 22, 12, 40, 11, 48, 265, 6, 18, 1, 54, 22, 12, 40], [1, 255, 1362, 231, 1363, 1364, 104, 1, 54, 22, 12, 40, 6, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40], [870, 362, 2, 3, 25, 15, 7, 880, 2977, 9, 2004, 210, 7, 168, 287, 5, 111, 136, 2978, 2979, 205, 257, 142, 93, 9, 554, 1, 2980, 1, 2981, 62, 362, 2, 3, 25, 15, 7, 880, 238, 309, 9, 19, 111, 136, 2982, 1093, 1, 287, 11, 1, 136, 21, 1, 418, 5, 124, 2983], [1, 118, 261, 13, 1, 502, 43, 159, 62, 854, 596, 278, 597, 4, 74, 598, 43, 4, 1, 391, 261, 13, 1, 36, 1114, 355, 879, 303, 927, 36, 2017, 13, 855, 21, 1, 743, 159, 62, 596, 278, 597, 4, 74, 598, 43], [6, 15, 1, 17, 20, 12, 2, 3, 16, 9, 121, 7, 126, 61, 56, 35, 29, 14, 7, 1286, 100, 371, 50, 23, 1, 209, 140, 5, 24, 2984, 48, 265, 6, 18, 1, 17, 20, 9, 121, 7, 27, 70, 235, 29, 12, 2, 3, 16, 14, 46, 28, 21, 1, 35, 62, 5, 1, 1345, 88, 877, 2985, 2, 3, 94], [6, 492, 750, 68, 1237, 82, 61, 23, 1999, 2986, 260, 21, 1, 259, 286, 216, 563, 2, 3, 156, 6, 216, 35, 693, 574, 1, 259, 222, 563, 2, 3, 156], [6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25, 6, 15, 1, 226, 99, 81, 14, 107, 106, 4, 256, 416, 14, 326, 200, 105, 169, 190, 92, 79, 2, 3, 25], [393, 1, 743, 159, 62, 596, 278, 597, 4, 74, 598, 43, 13, 68, 426, 83, 495, 188, 11, 1, 2018, 1094, 1095, 4, 1094, 1095, 2019, 13, 855, 21, 1, 743, 159, 62, 596, 278, 597, 4, 74, 598, 43], [881, 6, 874, 181, 4, 182, 1, 133, 8, 512, 4, 353, 10, 39, 120, 20, 1370, 86, 2, 3, 53, 6, 76, 182, 51, 63, 10, 39, 120, 86, 2, 3, 53], [1, 144, 32, 118, 181, 4, 721, 10, 1, 17, 229, 192, 182, 4, 224, 26, 186, 119, 10, 1, 148, 135, 110, 51, 2020, 67, 182, 4, 828, 10, 148, 135, 110], [6, 18, 518, 519, 64, 9, 285, 1, 106, 648, 2021, 76, 18, 518, 519, 64, 9, 651, 1, 78, 5, 42, 13, 7, 852, 82, 9, 69, 1, 238, 875], [1, 118, 261, 13, 1, 502, 43, 159, 62, 854, 596, 278, 597, 4, 74, 598, 43, 4, 1, 391, 261, 13, 1, 36, 1114, 355, 879, 303, 927, 36, 2017, 13, 855, 21, 1, 743, 159, 62, 596, 278, 597, 4, 74, 598, 43], [6, 15, 7, 57, 2022, 5, 512, 94, 42, 33, 119, 224, 4, 182, 10, 1, 148, 135, 110, 51, 2020, 67, 182, 4, 828, 10, 148, 135, 110], [439, 510, 2, 3, 88, 2987, 30, 178, 26, 646, 2, 3, 25, 11, 2988, 2015, 4, 683, 93, 11, 576, 2023, 2989, 2, 3, 88, 214, 189, 669, 9, 2023, 1096], [393, 1, 743, 159, 62, 596, 278, 597, 4, 74, 598, 43, 13, 68, 426, 83, 495, 188, 11, 1, 2018, 1094, 1095, 4, 1094, 1095, 2019, 13, 855, 21, 1, 743, 159, 62, 596, 278, 597, 4, 74, 598, 43], [6, 232, 24, 58, 23, 54, 12, 40, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 541, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40], [6, 214, 77, 80, 12, 36, 9, 216, 70, 89, 228, 410, 5, 24, 65, 555, 9, 7, 1628, 15, 77, 80, 11, 281, 70, 89, 12, 36], [6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 1317, 17, 20, 12, 2, 3, 16, 13, 15, 9, 69, 7, 126, 61, 38, 29, 14, 1, 85, 28, 870, 587], [1, 70, 89, 147, 13, 76, 196, 179, 14, 141, 77, 80, 82, 14, 228, 488, 297, 12, 36, 6, 15, 77, 80, 11, 281, 70, 89, 12, 36], [48, 19, 13, 1273, 26, 168, 816, 19, 702, 2, 3, 2024, 145, 5, 1, 2990, 381, 5, 1704, 7, 566, 11, 142, 2991, 211, 358, 13, 1, 168, 816, 19, 702, 2, 3, 2024], [1, 28, 33, 1049, 283, 1236, 1065, 4, 2992, 1065, 8, 7, 72, 2993, 31, 1, 28, 15, 26, 1016, 4, 638, 165, 6, 95, 15, 1, 1236, 28, 509, 8, 1016, 4, 638, 165, 94], [1, 2994, 13, 61, 23, 1, 2995, 284, 2996, 52, 661, 37, 1, 391, 103, 1542, 2997, 13, 1, 2998, 2999, 103, 11, 3000, 3001, 189, 175, 8, 1, 3002, 661, 37], [145, 13, 7, 1051, 41, 19, 66, 10, 102, 96, 25, 4, 50, 206, 7, 155, 213, 5, 1, 247, 22, 8, 42, 613, 98, 13, 1295, 14, 124, 384, 856, 98, 3003, 4, 796, 5, 7, 155, 116, 291, 170, 951, 41, 19, 32, 233, 23, 1, 44, 140, 5, 1, 46, 28, 10, 102, 96, 25, 325], [1, 30, 73, 33, 153, 26, 464, 59, 47, 4, 45, 43, 101, 30, 132, 67, 153, 26, 464, 59, 8, 105, 398, 4, 817, 10, 1, 356, 370, 47, 4, 45, 43], [491, 11, 44, 63, 8, 1, 257, 1371, 32, 174, 21, 7, 1261, 30, 73, 5, 125, 21, 1, 54, 85, 22, 12, 40, 8, 269, 9, 554, 1, 3004, 5, 1, 30, 132, 1, 358, 67, 1295, 283, 7, 1254, 3005, 3006, 14, 1305, 85, 28, 21, 1, 54, 22, 12, 40], [51, 1, 41, 65, 371, 15, 8, 24, 58, 32, 2025, 155, 116, 291, 1372, 50, 10, 102, 96, 2, 3, 88, 1, 41, 65, 32, 407, 10, 1, 102, 20, 96, 2, 3, 88, 14, 155, 116, 173], [8, 51, 58, 6, 18, 1, 99, 184, 14, 1039, 406, 487, 1318, 81, 169, 8, 1, 202, 152, 204, 2, 3, 57, 11, 51, 58, 6, 18, 7, 3007, 184, 31, 175, 8, 202, 204, 2, 3, 57, 20], [6, 18, 1, 39, 52, 9, 1294, 7, 2026, 11, 111, 136, 74, 71, 2, 3, 37, 6, 15, 1, 375, 55, 52, 8, 1, 39, 70, 372, 41, 52, 744, 74, 71, 2, 3, 37, 9, 210, 276, 11, 1, 28, 456], [599, 6, 181, 1, 44, 140, 5, 51, 1373, 11, 41, 343, 10, 1, 60, 632, 5, 1, 17, 20, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16, 6, 18, 17, 325, 12, 2, 3, 16, 9, 424, 48, 19, 26, 726, 1, 261, 4, 209, 1026, 9, 104, 3008, 645], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [1, 118, 1925, 78, 61, 29, 13, 138, 8, 2027, 2, 3, 156, 9, 972, 826, 6, 15, 7, 1929, 5, 1, 861, 1477, 128, 138, 8, 2027, 2, 3, 156], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [6, 15, 2028, 537, 39, 191, 74, 71, 2, 3, 37, 9, 219, 868, 5, 1, 1331, 4, 15, 254, 7, 388, 5, 2029, 5, 7, 2030, 219, 389, 191, 10, 39, 52, 74, 71, 2, 3, 37, 4, 18, 124, 537, 55, 773], [6, 860, 31, 7, 2031, 22, 54, 717, 12, 40, 14, 2032, 85, 2033, 232, 24, 58, 23, 54, 12, 40, 7, 307, 85, 22, 42, 13, 138, 8, 741, 8, 185, 1810], [6, 15, 1, 375, 55, 52, 8, 1, 39, 70, 372, 41, 52, 744, 74, 71, 2, 3, 37, 9, 210, 276, 11, 1, 28, 2034, 24, 58, 6, 15, 1, 39, 52, 74, 71, 2, 3, 37, 9, 348, 55, 276], [1, 1279, 5, 1, 1886, 13, 1374, 9, 402, 1375, 5, 1, 486, 4, 1, 402, 1097, 5, 1, 940, 31, 524, 8, 468, 2, 3, 16, 1, 503, 403, 13, 15, 9, 262, 51, 5, 1, 1376, 476, 468, 2, 3, 16], [1, 91, 113, 5, 157, 114, 127, 13, 7, 3009, 128, 11, 1655, 446, 3010, 13, 7, 625, 314, 2001, 23, 1, 91, 113, 5, 157, 2002, 34, 142, 5, 308, 2003, 142, 5, 157, 1092, 114, 127], [599, 6, 181, 1, 44, 140, 5, 51, 1373, 11, 41, 343, 10, 1, 60, 632, 5, 1, 17, 20, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [145, 13, 21, 475, 2, 3, 117, 1, 1284, 5, 30, 1045, 13, 1918, 471, 1, 93, 32, 1, 3011, 93, 174, 21, 30, 658, 8, 1, 139, 592, 31, 475, 2, 3, 117], [1, 3012, 67, 3013, 459, 3014, 23, 24, 115, 11, 1, 249, 1126, 4, 130, 142, 562, 745, 2, 3, 36, 11, 7, 654, 5, 63, 249, 586, 7, 3015, 5, 562, 5, 1, 130, 142, 745, 2, 3, 36], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 24, 101, 13, 7, 27, 235, 29, 50, 10, 1, 17, 20, 12, 2, 3, 16], [1, 70, 89, 147, 13, 76, 196, 179, 14, 141, 77, 80, 82, 14, 228, 488, 297, 12, 36, 6, 216, 89, 5, 188, 10, 77, 80, 190, 228, 410, 12, 36], [6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16, 1367, 296, 29, 6, 225, 7, 212, 5, 1368, 1369, 9, 1, 258, 70, 35, 1001, 17, 12, 2, 3, 16], [6, 15, 7, 57, 2022, 5, 512, 94, 42, 33, 119, 224, 4, 182, 10, 1, 148, 135, 110, 131, 51, 44, 28, 32, 119, 224, 4, 473, 10, 1, 148, 135, 110], [1, 133, 33, 442, 10, 2035, 425, 9, 1351, 2036, 4, 192, 181, 14, 1, 39, 632, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [6, 232, 24, 58, 23, 54, 12, 40, 7, 307, 85, 22, 42, 13, 138, 8, 741, 8, 185, 3016, 12, 40, 13, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 449], [6, 18, 92, 79, 2, 3, 25, 1, 56, 78, 403, 11, 553, 11, 1216, 1, 332, 3017, 199, 175, 8, 553, 14, 1, 92, 56, 78, 403, 79, 2, 3, 25, 13, 15, 11, 51, 215, 65], [1, 592, 328, 67, 3018, 13, 72, 9, 2005, 1, 1249, 1287, 138, 26, 409, 2, 3, 87, 8, 48, 263, 6, 18, 1, 2037, 22, 26, 409, 2, 3, 87], [6, 15, 1, 375, 55, 52, 8, 1, 39, 70, 372, 41, 52, 744, 74, 71, 2, 3, 37, 9, 210, 276, 11, 1, 28, 2034, 24, 58, 6, 15, 1, 39, 52, 74, 71, 2, 3, 37, 9, 348, 55, 276], [6, 155, 1, 81, 5, 1, 3019, 3020, 152, 253, 5, 788, 2, 3, 201, 6, 76, 225, 24, 30, 178, 14, 1, 2038, 30, 178, 5, 788, 2, 3, 201, 1018, 833, 1352, 2039, 1877], [1, 39, 55, 52, 74, 71, 2, 3, 37, 13, 15, 11, 430, 93, 21, 1, 55, 160, 727, 15, 1, 375, 55, 52, 8, 1, 39, 70, 372, 41, 52, 744, 74, 71, 2, 3, 37, 9, 210, 276, 11, 1, 28, 456], [1, 118, 167, 23, 48, 996, 33, 233, 3021, 8, 1, 3022, 718, 387, 1, 735, 213, 33, 224, 374, 10, 7, 171, 1112, 23, 1, 44, 171, 138, 8, 718, 387], [1, 503, 403, 13, 15, 9, 262, 51, 5, 1, 1376, 476, 468, 2, 3, 16, 1, 161, 13, 175, 14, 402, 712, 230, 548, 997, 10, 60, 476, 169, 8, 1, 503, 998, 468, 2, 3, 16], [11, 24, 58, 6, 15, 637, 694, 3023, 21, 1, 1355, 22, 652, 3024, 11, 3025, 58, 6, 15, 1, 3026, 2040, 85, 22, 652, 57, 42, 13, 2040, 3027, 3028, 3029, 108, 3030], [1, 3031, 1279, 13, 153, 26, 865, 1, 38, 20, 17, 12, 2, 3, 16, 206, 2041, 605, 14, 3032, 308, 3033, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [1, 503, 403, 13, 15, 9, 262, 51, 5, 1, 1376, 476, 468, 2, 3, 16, 1, 161, 13, 175, 14, 402, 712, 230, 548, 997, 10, 60, 476, 169, 8, 1, 503, 998, 468, 2, 3, 16], [6, 860, 31, 7, 2031, 22, 54, 717, 12, 40, 14, 2032, 85, 2033, 232, 24, 58, 23, 54, 12, 40, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 449], [6, 76, 225, 24, 30, 178, 14, 1, 2038, 30, 178, 5, 788, 2, 3, 201, 1018, 833, 1352, 2039, 3034, 2, 3, 201, 15, 3035, 9, 3036, 30, 178, 571, 1, 280, 608], [1, 1935, 211, 41, 4, 195, 13, 68, 3037, 1342, 5, 3038, 409, 4, 588, 90, 1, 556, 3039, 5, 3040, 879, 1377, 5, 42, 13, 3041, 129, 3042, 68, 2042, 8, 195, 115, 409, 4, 588, 90, 482, 117], [158, 168, 668, 99, 32, 145, 5, 1, 1266, 163, 61, 23, 532, 3043, 1137, 587, 26, 521, 521, 165, 172, 163, 32, 61, 23, 7, 2043, 19, 158, 168, 56, 99, 425, 521, 165], [8, 3044, 4, 3045, 53, 6, 794, 14, 1816, 3046, 10, 1, 907, 169, 38, 29, 17, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [9, 607, 1, 106, 5, 1, 19, 6, 1201, 1, 746, 315, 31, 1, 46, 995, 10, 1, 243, 487, 103, 244, 4, 245, 53, 11, 487, 6, 984, 1, 243, 425, 3047, 103, 244, 4, 245, 53, 9, 285, 106], [11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16, 33, 15, 11, 30, 73, 4, 126, 35, 1749, 67, 407, 21, 789, 26, 1, 17, 29, 12, 2, 3, 16], [1, 133, 33, 442, 10, 2035, 425, 9, 1351, 2036, 4, 192, 181, 14, 1, 39, 632, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [1, 62, 516, 23, 662, 13, 9, 723, 2044, 2045, 4, 1098, 87, 795, 617, 1, 93, 176, 11, 1, 3048, 3049, 62, 3050, 4, 840, 3051, 3052, 60, 11, 1903, 478, 164, 13, 176, 8, 1, 2044, 511, 2045, 4, 1098, 87], [11, 105, 49, 1, 15, 46, 28, 13, 21, 1, 3053, 213, 5, 1, 54, 22, 12, 40, 4, 1, 872, 2011, 1514, 261, 5, 239, 28, 15, 8, 1, 58, 13, 1, 54, 310, 12, 40], [6, 18, 1, 39, 52, 9, 1294, 7, 2026, 11, 111, 136, 74, 71, 2, 3, 37, 6, 15, 1, 375, 55, 52, 8, 1, 39, 70, 372, 41, 52, 744, 74, 71, 2, 3, 37, 9, 210, 276, 11, 1, 28, 456], [6, 15, 2028, 537, 39, 191, 74, 71, 2, 3, 37, 9, 219, 868, 5, 1, 1331, 4, 15, 254, 7, 388, 5, 2029, 5, 7, 2030, 219, 389, 191, 10, 39, 52, 74, 71, 2, 3, 37, 4, 18, 124, 537, 55, 773], [6, 274, 44, 28, 4, 864, 298, 28, 10, 1, 39, 120, 20, 86, 2, 3, 53, 94, 881, 6, 874, 181, 4, 182, 1, 133, 8, 512, 4, 353, 10, 39, 120, 20, 1370, 86, 2, 3, 53], [1, 143, 3054, 8, 1253, 436, 129, 123, 23, 1997, 879, 318, 3055, 10, 3056, 31, 350, 31, 355, 134, 4, 510, 43, 3057, 134, 4, 510, 43, 129, 123, 458, 31, 7, 60, 143, 314, 8, 413, 436, 558], [6, 223, 101, 58, 11, 126, 61, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 51, 58, 67, 196, 179, 10, 1, 177, 38, 20, 17, 12, 2, 3, 16, 8, 124, 60, 1652, 580], [1, 142, 369, 13, 662, 1, 291, 767, 189, 162, 768, 97, 8, 2046, 2, 3, 25, 6, 635, 396, 1, 291, 767, 3058, 768, 97, 8, 2046, 2, 3, 25, 9, 707, 1, 142, 771, 7, 1348, 189, 287], [6, 274, 44, 28, 4, 864, 298, 28, 10, 1, 39, 120, 20, 86, 2, 3, 53, 94, 881, 6, 874, 181, 4, 182, 1, 133, 8, 512, 4, 353, 10, 39, 120, 20, 1370, 86, 2, 3, 53], [1, 39, 55, 52, 74, 71, 2, 3, 37, 13, 15, 11, 430, 93, 21, 1, 55, 160, 727, 15, 1, 375, 55, 52, 8, 1, 39, 70, 372, 41, 52, 744, 74, 71, 2, 3, 37, 9, 210, 276, 11, 1, 28, 456], [9, 3059, 1, 1287, 5, 1, 97, 82, 6, 18, 1, 258, 27, 29, 4, 685, 27, 29, 175, 8, 17, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16, 1, 17, 20, 12, 2, 3, 16, 13, 15, 9, 69, 7, 126, 61, 38, 29, 14, 1, 85, 28, 870, 587], [1367, 296, 29, 6, 225, 7, 212, 5, 1368, 1369, 9, 1, 258, 70, 35, 1001, 17, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [599, 6, 181, 1, 44, 140, 5, 51, 1373, 11, 41, 343, 10, 1, 60, 632, 5, 1, 17, 20, 12, 2, 3, 16, 6, 274, 4, 737, 1, 28, 14, 1, 60, 229, 21, 1, 17, 20, 12, 2, 3, 16], [393, 98, 13, 76, 2047, 34, 1, 1378, 5, 663, 1379, 8, 1, 46, 75, 13, 72, 9, 1, 145, 516, 26, 463, 2, 3, 87, 4, 2048, 9, 8, 185, 2049, 2, 3, 87, 871, 34, 1338, 5, 1, 1029, 354, 8, 1, 1339, 22, 94, 67, 663], [31, 7, 78, 103, 6, 882, 7, 434, 99, 295, 87, 42, 13, 68, 1148, 5, 893, 3060, 69, 7, 3061, 184, 6, 18, 1, 99, 78, 103, 21, 1, 99, 1326, 152, 295, 87, 3062, 51, 3063, 93, 283, 68, 1374, 75, 5, 3064, 3065], [2050, 3066, 32, 1869, 3067, 3068, 6, 15, 1, 3069, 21, 1066, 2, 3, 90, 9, 860, 1, 255, 3070, 2050, 3071, 93, 11, 111, 5, 1, 1229, 3072, 1, 143, 3073, 5, 1066, 2, 3, 90, 226, 65, 50, 23, 625, 93, 32, 15, 9, 1301, 1, 3074, 5, 3075, 3076, 11, 1708, 1307], [1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16, 48, 29, 13, 1, 17, 161, 12, 2, 3, 16, 14, 7, 35, 4, 7, 41, 19, 50, 14, 559, 1022, 1337, 205, 602, 1, 2051, 28, 176, 26, 1, 159, 62, 942], [188, 23, 1305, 3077, 4, 2052, 44, 73, 508, 34, 24, 19, 3078, 1078, 2053, 341, 7, 432, 634, 3079, 5, 716, 19, 94, 901, 2, 3, 88, 24, 73, 19, 13, 61, 23, 7, 708, 647, 5, 716, 19, 94, 319, 1, 73, 377, 13, 254, 3080, 26, 112, 106, 4, 228, 1943, 901, 2, 3, 88], [1, 167, 509, 8, 1099, 2, 3, 197, 34, 13, 61, 23, 1, 7, 324, 955, 3081, 30, 299, 3082, 8, 269, 9, 1836, 1, 1908, 495, 3083, 1332, 299, 515, 8, 1099, 2, 3, 197, 13, 268, 9, 104, 7, 2054, 957, 5, 7, 327, 710, 873, 1030, 8, 42, 1, 30, 299, 513, 32, 1380, 8, 363, 5, 708], [24, 391, 82, 13, 61, 23, 1, 953, 238, 309, 41, 19, 1176, 128, 9, 78, 30, 178, 5, 457, 2, 3, 747, 4, 457, 2, 3, 1100, 10, 1, 883, 3084, 2, 3, 747, 97, 7, 2055, 1381, 19, 883, 170, 42, 2056, 9, 2057, 215, 5, 7, 30, 61, 23, 810, 30, 8, 1, 139, 136], [1, 2058, 22, 13, 7, 155, 4, 3085, 213, 5, 471, 22, 1296, 4, 2059, 2060, 8, 269, 9, 2061, 1, 1839, 587, 26, 171, 1, 2058, 22, 13, 15, 31, 1, 46, 4, 3086, 22, 351, 5, 7, 388, 5, 1, 471, 22, 3087, 63, 14, 327, 602, 3088, 1848, 1296, 4, 2059, 2060, 4, 98, 129, 123, 3089, 4, 1846], [1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16, 51, 172, 93, 32, 1792, 21, 17, 12, 2, 3, 16], [44, 361, 67, 51, 660, 10, 1, 39, 720, 483, 20, 86, 2, 3, 53, 6, 76, 182, 51, 63, 10, 39, 120, 86, 2, 3, 53], [1, 106, 32, 407, 26, 600, 378, 10, 1, 382, 81, 383, 87, 494, 6, 15, 1, 1910, 81, 8, 382, 383, 87], [1, 3090, 13, 7, 226, 99, 557, 2, 3, 90, 175, 14, 3091, 557, 2, 3, 90, 7, 403, 11, 390, 99, 226, 215, 13, 15, 11, 81], [6, 18, 518, 519, 64, 9, 285, 1, 106, 648, 2021, 18, 7, 520, 729, 437, 847, 1329, 14, 1, 518, 285, 469, 519, 64, 396, 256, 3092, 1316, 754, 754, 11, 713, 633, 1277, 4, 2009, 1, 1185, 435], [6, 69, 7, 158, 168, 56, 99, 884, 4, 521, 165, 23, 1, 616, 176, 11, 2062, 6, 18, 158, 168, 56, 99, 884, 4, 521, 165, 11, 48, 265], [435, 1343, 13, 196, 179, 10, 1092, 1382, 1079, 57, 250, 1136, 61, 23, 259, 32, 192, 3093, 10, 1092, 1382, 1079, 57], [1101, 3094, 1, 18, 5, 3095, 8, 1, 2063, 5, 242, 23, 1, 207, 1101, 626, 1101, 97, 1, 704, 82, 11, 293, 2063, 1101, 626], [6, 882, 1, 726, 5, 362, 2, 3, 64, 238, 875, 32, 118, 15, 8, 48, 62, 8, 362, 2, 3, 64], [1, 1219, 169, 122, 59, 33, 15, 9, 341, 1, 3096, 47, 4, 45, 43, 6, 15, 1, 59, 108, 47, 4, 45, 43, 9, 657, 1, 30, 132], [748, 81, 749, 16, 6, 50, 7, 531, 171, 10, 748, 83, 749, 16], [1, 376, 184, 15, 8, 24, 58, 461, 1, 103, 138, 8, 2064, 2, 3, 288, 1, 376, 184, 15, 8, 1, 58, 8, 185, 253, 461, 1, 29, 497, 8, 2064, 2, 3, 288, 4, 98, 1822, 790, 215, 26, 3097, 111, 30, 1401, 145, 5, 1, 736], [6, 260, 1050, 1372, 14, 155, 116, 173, 630, 4, 45, 165, 1, 41, 19, 13, 7, 100, 14, 819, 4, 116, 173, 630, 4, 45, 165], [362, 2, 3, 25, 1009, 861, 9, 24, 209, 3098, 3099, 9, 3100, 3101, 21, 362, 2, 3, 25, 11, 48, 62], [6, 18, 1076, 199, 1914, 14, 1915, 349, 4, 158, 168, 199, 1115, 14, 68, 753, 162, 21, 92, 79, 2, 3, 25, 6, 18, 1, 158, 168, 668, 81, 8, 1, 92, 20, 79, 2, 3, 25, 9, 262, 199, 1115, 23, 111, 250, 75, 14, 1401, 753, 669, 4, 106, 2137], [31, 11, 1, 1002, 3102, 98, 13, 2065, 9, 3103, 6, 3104, 9, 18, 7, 55, 213, 5, 1, 567, 103, 5, 742, 4, 1098, 36, 11, 478, 7, 55, 213, 5, 1, 567, 103, 8, 742, 4, 1098, 36, 13, 15, 9, 681, 8, 68, 3105, 592, 1, 846, 389, 1204, 4, 124, 3106, 8, 7, 160, 189, 8, 7, 3107], [1023, 1, 89, 395, 67, 109, 10, 1, 77, 80, 82, 12, 36, 6, 15, 77, 80, 11, 281, 70, 89, 12, 36], [8, 48, 167, 6, 939, 23, 78, 14, 158, 168, 668, 489, 521, 165, 791, 6, 18, 158, 168, 56, 99, 884, 4, 521, 165, 11, 48, 265], [6, 15, 148, 135, 110, 9, 210, 7, 3108, 654, 11, 111, 3109, 3110, 15, 1, 1205, 148, 135, 110, 9, 986, 522, 1611], [9, 1340, 11, 48, 515, 1742, 333, 21, 441, 130, 115, 2066, 2, 3, 339, 3111, 65, 414, 18, 5, 441, 130, 115, 3112, 2066, 2, 3, 339], [1, 29, 33, 50, 23, 1, 44, 4, 867, 236, 5, 1, 54, 22, 213, 131, 12, 40, 8, 1747, 163, 50, 23, 485, 28, 154, 104, 214, 635, 9, 1042, 724, 206, 1, 44, 140, 5, 1, 54, 22, 12, 40, 15, 11, 46, 4, 281, 38], [6, 18, 226, 489, 21, 490, 4, 489, 14, 753, 162, 21, 404, 316, 4, 134, 25, 1, 1091, 763, 162, 33, 50, 14, 404, 316, 4, 134, 25], [8, 846, 27, 70, 56, 35, 49, 290, 31, 17, 83, 12, 2, 3, 16, 1, 35, 19, 13, 524, 8, 363, 5, 126, 218, 3113, 174, 21, 7, 3114, 13, 7, 60, 27, 56, 35, 19, 12, 2, 3, 16], [6, 69, 7, 158, 168, 56, 99, 884, 4, 521, 165, 23, 1, 616, 176, 11, 2062, 6, 18, 158, 168, 56, 99, 884, 4, 521, 165, 11, 48, 265], [1, 30, 73, 33, 153, 26, 464, 59, 47, 4, 45, 43, 1, 30, 73, 13, 417, 26, 59, 47, 4, 45, 43, 1, 1097, 538, 13, 15], [11, 46, 99, 163, 6, 15, 404, 152, 316, 4, 134, 156, 1, 99, 81, 15, 33, 404, 316, 4, 134, 156, 4, 107, 106, 67, 15, 2067, 1383, 369, 162, 3115, 435, 5, 83, 4, 7, 3116, 793, 5, 1, 1210, 5, 1, 212, 5, 924], [6, 18, 1, 60, 73, 122, 59, 47, 4, 45, 43, 9, 30, 341, 1, 85, 500, 15, 59, 47, 4, 45, 43, 9, 341, 1, 63, 8, 1, 22], [6, 18, 1, 60, 73, 122, 59, 47, 4, 45, 43, 9, 30, 341, 1, 85, 500, 15, 1, 59, 108, 47, 4, 45, 43, 9, 657, 1, 30, 132], [8, 1, 217, 137, 187, 772, 336, 2, 3, 90, 1, 812, 13, 618, 3117, 3118, 1319, 1, 75, 5, 137, 1038, 8, 24, 167, 13, 725, 11, 44, 6, 18, 3119, 1038, 247, 8, 1, 217, 137, 187, 772, 336, 2, 3, 90, 11, 305, 6, 18, 3120, 3121, 3122], [202, 204, 2, 3, 57, 33, 15, 9, 396, 78, 381, 9, 174, 3123, 99, 81, 5, 202, 204, 2, 3, 57, 33, 15, 9, 121, 1, 3124, 19], [6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 3125, 33, 233, 14, 7, 1348, 122, 176, 14, 1, 17, 20, 12, 2, 3, 16], [6, 18, 1, 60, 73, 122, 59, 47, 4, 45, 43, 9, 30, 341, 1, 85, 500, 15, 1, 59, 108, 47, 4, 45, 43, 9, 657, 1, 30, 132], [6, 15, 1, 59, 108, 47, 4, 45, 43, 9, 657, 1, 30, 1993, 109, 30, 73, 10, 59, 47, 4, 45, 43, 14, 1, 107, 356, 73, 538, 82], [1, 41, 19, 15, 13, 1, 100, 22, 21, 367, 1090, 2068, 2, 3, 25, 6, 15, 367, 1090, 698, 2068, 2, 3, 25, 31, 1, 710, 22], [44, 361, 67, 51, 660, 10, 1, 39, 720, 483, 20, 86, 2, 3, 53, 6, 76, 182, 51, 63, 10, 39, 120, 86, 2, 3, 53], [11, 231, 475, 2, 3, 117, 555, 471, 658, 1346, 178, 4, 1347, 178, 8, 595, 4, 738, 2069, 337, 1346, 30, 178, 14, 3126, 326, 4, 869, 2070, 31, 350, 31, 1347, 30, 178, 14, 326, 4, 869, 2070, 34, 32, 587, 8, 475, 2, 3, 117, 4, 154, 104, 1928, 662, 253], [91, 418, 1902, 68, 1048, 26, 124, 1384, 3127, 1, 308, 1666, 3128, 5, 157, 670, 9, 42, 145, 1397, 1398, 7, 30, 26, 1, 1110, 98, 1111, 813, 814, 48, 13, 1823, 618, 91, 379, 319, 7, 30, 294, 7, 3129, 8, 24, 957, 13, 609, 26, 1, 1110, 98, 1111, 813, 814], [448, 2, 3, 88, 18, 3130, 9, 121, 3131, 3132, 238, 309, 13, 72, 9, 34, 5, 448, 2, 3, 88], [6, 18, 1, 60, 73, 122, 59, 47, 4, 45, 43, 9, 30, 341, 1, 85, 500, 15, 59, 47, 4, 45, 43, 9, 341, 1, 63, 8, 1, 22], [1, 671, 34, 13, 15, 9, 3133, 1, 379, 8, 1, 1814, 902, 13, 406, 594, 379, 866, 277, 2, 3, 40, 157, 287, 4, 1375, 8, 1, 380, 1085, 23, 1, 1173, 330, 5, 406, 594, 379, 866, 277, 2, 3, 40], [11, 275, 1525, 831, 4, 954, 6, 15, 54, 22, 83, 12, 40, 11, 48, 265, 6, 18, 1, 54, 22, 12, 40], [1, 44, 133, 33, 181, 10, 1, 30, 274, 1612, 21, 272, 273, 2, 3, 57, 1, 1940, 32, 419, 21, 1, 3134, 501, 176, 26, 1, 272, 20, 273, 2, 3, 57], [6, 18, 1, 17, 161, 12, 2, 3, 16, 31, 7, 2071, 38, 161, 3135, 1, 29, 138, 3136, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [791, 1, 136, 2072, 183, 2073, 4, 1402, 90, 2065, 31, 3137, 13, 15, 11, 3138, 3139, 19, 46, 3140, 145, 666, 13, 9, 396, 98, 9, 1, 41, 19, 61, 2072, 82, 2073, 4, 1402, 90], [18, 1, 39, 52, 74, 71, 2, 3, 37, 9, 219, 7, 75, 5, 191, 21, 111, 3141, 219, 389, 191, 10, 39, 52, 74, 71, 2, 3, 37, 4, 18, 124, 537, 55, 773], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16], [8, 3142, 1, 161, 129, 9, 1224, 696, 495, 9, 853, 98, 3143, 12, 36, 9, 853, 1, 3144, 103, 1963, 696, 495, 13, 1, 60, 821, 495, 82, 12, 36], [1, 261, 5, 239, 28, 15, 8, 1, 58, 13, 1, 54, 310, 12, 40, 1, 54, 28, 75, 351, 5, 1921, 125, 5, 1, 305, 236, 5, 1, 54, 22, 12, 40], [1, 3145, 164, 677, 13, 61, 23, 1, 3146, 806, 497, 234, 2074, 2, 3, 740, 24, 128, 13, 61, 23, 1, 2052, 603, 3147, 919, 1189, 8, 2074, 2, 3, 740], [8, 48, 263, 6, 18, 1, 2037, 22, 26, 409, 2, 3, 87, 172, 163, 95, 123, 15, 8, 601, 167, 26, 409, 2, 3, 87], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 6, 121, 7, 101, 534, 3148, 29, 10, 1, 17, 38, 29, 12, 2, 3, 16], [51, 24, 49, 32, 664, 14, 7, 60, 27, 29, 66, 14, 17, 12, 2, 3, 16, 6, 15, 1, 17, 20, 12, 2, 3, 16, 9, 69, 60, 27, 49, 14, 107, 739], [7, 649, 128, 9, 1093, 142, 13, 9, 993, 1, 212, 5, 649, 63, 665, 740, 48, 812, 142, 216, 13, 1112, 26, 1, 784, 5, 1, 665, 103, 665, 740], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [1, 39, 55, 52, 74, 71, 2, 3, 37, 13, 15, 11, 430, 93, 21, 1, 55, 160, 2075, 24, 58, 6, 15, 1, 39, 52, 74, 71, 2, 3, 37, 9, 348, 55, 276], [6, 274, 4, 737, 1, 28, 14, 1, 60, 229, 21, 1, 17, 20, 12, 2, 3, 16, 6, 424, 1382, 4, 1102, 83, 4, 635, 18, 1102, 94, 21, 17, 12, 2, 3, 16], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 6, 274, 4, 737, 1, 28, 14, 1, 60, 229, 21, 1, 17, 20, 12, 2, 3, 16], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 24, 101, 13, 7, 27, 235, 29, 50, 10, 1, 17, 20, 12, 2, 3, 16], [11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16, 1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16], [6, 274, 4, 737, 1, 28, 14, 1, 60, 229, 21, 1, 17, 20, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [8, 1319, 3149, 2, 3, 37, 939, 23, 390, 1187, 4, 976, 7, 3150, 2, 3, 37, 18, 3151, 8, 966, 14, 7, 3152, 52, 4, 1341], [6, 18, 271, 199, 14, 517, 349, 175, 10, 1, 92, 20, 79, 2, 3, 25, 423, 271, 1314, 6, 18, 271, 199, 14, 517, 315, 798, 4, 1315, 79, 2, 3, 25], [786, 1, 126, 193, 240, 1916, 1, 512, 22, 33, 473, 10, 1, 148, 135, 110, 131, 51, 44, 28, 32, 119, 224, 4, 473, 10, 1, 148, 135, 110], [6, 18, 271, 199, 14, 517, 349, 175, 10, 1, 92, 20, 79, 2, 3, 25, 6, 18, 1, 1844, 366, 312, 184, 8, 445, 14, 1, 107, 302, 79, 2, 3, 25], [1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16, 105, 49, 32, 61, 23, 1, 17, 38, 20, 12, 2, 3, 16, 4, 823, 31, 461], [11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16, 6, 274, 4, 737, 1, 28, 14, 1, 60, 229, 21, 1, 17, 20, 12, 2, 3, 16], [6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16, 6, 15, 1, 17, 20, 12, 2, 3, 16, 9, 69, 60, 27, 49, 14, 107, 739], [6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [54, 12, 40, 13, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 541, 219, 24, 1344, 146, 21, 1, 1913, 540, 5, 1, 54, 22, 213, 170, 12, 40], [11, 51, 389, 1067, 6, 15, 1, 628, 39, 55, 287, 74, 71, 2, 3, 37, 8, 24, 58, 6, 15, 1, 39, 52, 74, 71, 2, 3, 37, 9, 348, 55, 276], [1, 39, 55, 52, 74, 71, 2, 3, 37, 13, 15, 11, 430, 93, 21, 1, 55, 160, 2075, 24, 58, 6, 15, 1, 39, 52, 74, 71, 2, 3, 37, 9, 348, 55, 276], [6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16, 51, 24, 49, 32, 664, 14, 7, 60, 27, 29, 66, 14, 17, 12, 2, 3, 16], [6, 18, 1, 746, 315, 369, 4, 520, 150, 151, 2, 3, 25, 9, 651, 106, 6, 18, 150, 151, 2, 3, 25, 14, 1, 331, 78, 166, 75, 9, 778], [6, 18, 1, 746, 315, 369, 4, 520, 150, 151, 2, 3, 25, 9, 651, 106, 6, 69, 1, 324, 1058, 811, 10, 1059, 1060, 315, 83, 14, 150, 151, 2, 3, 25], [6, 18, 1, 150, 82, 151, 2, 3, 25, 9, 374, 285, 1, 78, 166, 11, 111, 1845, 18, 1, 746, 315, 369, 4, 520, 150, 151, 2, 3, 25, 9, 651, 106], [362, 2, 3, 25, 3153, 10, 880, 1103, 3154, 4, 964, 3155, 11, 1344, 3156, 2, 3, 25, 18, 880, 1103, 11, 195, 115, 23, 1, 136, 472], [94, 35, 19, 17, 13, 7, 258, 20, 11, 27, 38, 49, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [94, 35, 19, 17, 13, 7, 258, 20, 11, 27, 38, 49, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [94, 35, 19, 17, 13, 7, 258, 20, 11, 27, 38, 49, 12, 2, 3, 16, 24, 101, 13, 7, 27, 235, 29, 50, 10, 1, 17, 20, 12, 2, 3, 16], [11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [51, 24, 49, 32, 664, 14, 7, 60, 27, 29, 66, 14, 17, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [11, 51, 389, 1067, 6, 15, 1, 628, 39, 55, 287, 74, 71, 2, 3, 37, 8, 24, 58, 6, 15, 1, 39, 52, 74, 71, 2, 3, 37, 9, 348, 55, 276], [154, 104, 397, 26, 3157, 1, 3158, 23, 7, 46, 22, 2076, 87, 1, 106, 154, 104, 1782, 407, 21, 7, 187, 31, 268, 26, 2076, 87], [1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [94, 35, 19, 17, 13, 7, 258, 20, 11, 27, 38, 49, 12, 2, 3, 16, 51, 24, 49, 32, 664, 14, 7, 60, 27, 29, 66, 14, 17, 12, 2, 3, 16], [8, 48, 128, 6, 15, 1, 2077, 108, 61, 23, 1, 748, 81, 749, 16, 2077, 13, 7, 595, 29, 34, 13, 66, 23, 1089, 5, 1, 748, 749, 16], [94, 35, 19, 17, 13, 7, 258, 20, 11, 27, 38, 49, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [172, 3159, 1263, 3160, 23, 1, 3161, 293, 31, 15, 8, 249, 338, 2, 3, 339, 2078, 154, 447, 547, 1, 2079, 5, 731, 31, 8, 1, 249, 511, 338, 2, 3, 339], [2054, 3162, 5, 3163, 32, 919, 8, 1, 257, 3164, 5, 1, 249, 257, 399, 338, 2, 3, 339, 2078, 154, 447, 547, 1, 2079, 5, 731, 31, 8, 1, 249, 511, 338, 2, 3, 339], [6, 69, 24, 19, 23, 7, 388, 5, 1, 1971, 1972, 425, 22, 463, 2, 3, 57, 6, 15, 1, 1219, 169, 3165, 22, 463, 2, 3, 57], [786, 1, 118, 159, 62, 23, 2080, 1350, 1385, 1046, 2081, 2, 3, 40, 33, 3166, 8, 40, 1377, 655, 129, 123, 233, 23, 1797, 145, 154, 1000, 1385, 211, 372, 41, 3167, 523, 944, 123, 655, 23, 3168, 1380, 8, 372, 41, 1, 3169, 2080, 1350, 1385, 1046, 3170, 2081, 2, 3, 40, 3171, 3172, 2042, 8, 1, 304], [6, 15, 2082, 676, 1102, 2083, 4, 932, 64, 11, 3173, 3174, 1, 49, 10, 2082, 676, 1102, 2083, 4, 932, 64], [6, 15, 1, 382, 20, 383, 87, 11, 78, 532, 1328, 65, 14, 1198, 3175, 11, 51, 24, 610, 18, 1, 382, 152, 383, 87, 11, 58], [6, 18, 1, 81, 176, 26, 748, 325, 749, 16, 11, 105, 46, 4, 215, 2069, 18, 748, 749, 16, 31, 68, 81, 5, 531], [1386, 261, 14, 209, 275, 26, 10, 59, 47, 4, 45, 43, 30, 132, 67, 417, 10, 59, 47, 4, 45, 43], [6, 15, 3176, 134, 36, 11, 143, 5, 1998, 1075, 48, 873, 13, 327, 641, 14, 1, 317, 143, 314, 15, 11, 436, 134, 36], [83, 14, 94, 349, 10, 150, 151, 2, 3, 25, 6, 882, 428, 78, 591, 106, 10, 150, 151, 2, 3, 25], [6, 15, 202, 204, 2, 3, 57, 11, 51, 24, 215, 3177, 204, 2, 3, 57, 42, 240, 1, 81, 5, 51, 493, 530, 33, 15, 8, 24, 678], [119, 198, 33, 109, 14, 148, 135, 110, 148, 13, 7, 70, 405, 3178, 119, 171, 135, 110], [30, 132, 23, 1, 85, 22, 32, 109, 10, 59, 47, 4, 45, 43, 14, 1, 3179, 3180, 85, 22, 13, 593, 10, 59, 47, 4, 45, 43], [6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25, 215, 481, 1, 92, 553, 152, 79, 2, 3, 25], [1, 257, 1371, 28, 33, 373, 10, 1, 498, 4, 885, 584, 52, 498, 4, 885, 36, 98, 1085, 23, 1, 1387, 584, 52, 498, 4, 885, 36], [11, 3181, 6, 15, 1, 39, 3182, 2084, 4, 86, 84, 982, 6, 3183, 51, 611, 14, 248, 119, 680, 6, 18, 1, 39, 119, 171, 2084, 4, 86, 84], [6, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40, 51, 1, 28, 1009, 21, 54, 12, 40], [48, 28, 13, 236, 5, 1, 552, 22, 392, 2, 3, 88, 1, 46, 28, 787, 26, 1, 62, 942, 622, 21, 1, 552, 22, 392, 2, 3, 88, 42, 240, 621, 453, 26, 692, 5, 44, 31, 7, 1515, 41, 4, 13, 3184, 26], [1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16, 223, 10, 1, 17, 27, 161, 12, 2, 3, 16], [6, 15, 1, 30, 73, 660, 26, 59, 47, 4, 45, 84, 179, 5, 68, 716, 19, 3185, 13, 1, 376, 73, 19, 5, 47, 4, 45, 84], [11, 48, 265, 6, 18, 1, 54, 22, 12, 40, 51, 1, 28, 1009, 21, 54, 12, 40], [30, 73, 13, 109, 10, 59, 47, 4, 45, 43, 1386, 261, 14, 209, 275, 26, 10, 59, 47, 4, 45, 43], [30, 73, 33, 233, 14, 59, 47, 4, 45, 43, 11, 105, 2085, 73, 13, 109, 14, 59, 47, 4, 45, 43], [1, 143, 188, 67, 176, 26, 1, 942, 10, 248, 143, 999, 8, 553, 392, 4, 1104, 64, 1, 65, 67, 397, 10, 3186, 1388, 392, 4, 1104, 64], [8, 1336, 167, 6, 154, 492, 3187, 345, 381, 5, 3188, 215, 4, 1977, 1161, 1, 3189, 5, 1, 355, 857, 842, 82, 509, 26, 2086, 2, 3, 64, 2086, 2, 3, 64, 587, 7, 857, 61, 82, 4, 68, 355, 3190, 842, 82], [1386, 261, 14, 209, 275, 26, 10, 59, 47, 4, 45, 43, 30, 73, 13, 233, 10, 59, 47, 4, 45, 43], [6, 223, 70, 89, 395, 11, 259, 211, 24, 606, 1664, 29, 1, 101, 4, 1, 493, 1665, 49, 10, 141, 77, 80, 12, 36, 14, 507, 6, 15, 77, 80, 11, 281, 70, 89, 12, 36], [1, 30, 73, 33, 153, 26, 464, 59, 47, 4, 45, 43, 30, 73, 13, 109, 14, 59, 47, 4, 45, 43], [30, 73, 33, 233, 14, 59, 47, 4, 45, 43, 11, 105, 2085, 73, 13, 109, 10, 59, 47, 4, 45, 43], [11, 1, 234, 5, 346, 347, 84, 9, 104, 514, 6, 95, 97, 7, 300, 384, 5, 734, 4, 1, 653, 301, 515, 8, 1047, 14, 248, 478, 301, 3191, 185, 131, 6, 714, 7, 1459, 9, 1, 1275, 3192, 5, 346, 347, 84, 4, 3193, 3194, 2071, 730, 125, 8, 3195, 643, 24, 115], [8, 269, 9, 225, 24, 82, 9, 7, 350, 3196, 126, 101, 6, 714, 7, 82, 34, 3197, 1272, 26, 3198, 1, 1105, 431, 21, 68, 376, 73, 19, 499, 2, 3, 197, 8, 24, 58, 6, 3199, 105, 1811, 4, 7, 3200, 3201, 1, 1002, 61, 23, 30, 132, 21, 716, 19, 83, 4, 1, 3202, 23, 132, 21, 68, 376, 19, 499, 2, 3, 197], [6, 1765, 1, 693, 5, 491, 9, 44, 21, 298, 4, 292, 10, 3203, 35, 1091, 2010, 3204, 2087, 2, 3, 37, 42, 1199, 1933, 1, 406, 212, 5, 3205, 688, 1, 212, 5, 3206, 6, 15, 7, 155, 35, 1091, 166, 3207, 42, 562, 1, 212, 5, 3208, 1349, 9, 3209, 145, 136, 283, 810, 2087, 2, 3, 37], [2088, 226, 1223, 2089, 129, 439, 123, 214, 9, 2090, 8, 1039, 805, 256, 1880, 781, 4, 3210, 36, 48, 129, 3211, 1, 18, 5, 3212, 1194, 513, 648, 3213, 9, 2061, 1, 534, 3214, 304, 3215, 8, 1, 1252, 128, 6, 262, 412, 2090, 206, 1, 1124, 5, 1, 3216, 4, 3217, 163, 8, 68, 2088, 226, 1223, 2089, 3218], [11, 34, 265, 6, 18, 1, 30, 3219, 62, 97, 26, 457, 2, 3, 747, 42, 562, 1, 623, 23, 3220, 1015, 252, 3221, 13, 1, 30, 34, 13, 72, 9, 941, 8, 1, 139, 3222, 2, 3, 747, 97, 7, 2055, 1381, 19, 883, 170, 42, 2056, 9, 2057, 215, 5, 7, 30, 61, 23, 810, 30, 8, 1, 139, 136], [1, 22, 13, 118, 593, 10, 7, 30, 73, 370, 47, 4, 45, 43, 7, 720, 837, 5, 613, 433, 29, 13, 1, 126, 193, 42, 240, 239, 126, 218, 174, 21, 7, 239, 22, 1040, 30, 73, 47, 4, 45, 43], [11, 434, 6, 18, 1, 99, 496, 2091, 295, 37, 42, 3223, 7, 1357, 3224, 168, 34, 3225, 1, 212, 5, 3226, 218, 8, 1, 46, 1617, 18, 1, 99, 496, 81, 295, 37, 5, 434, 99, 8, 48, 263], [106, 32, 3227, 10, 150, 151, 2, 3, 25, 14, 7, 78, 166, 5, 3228, 65, 67, 50, 10, 150, 151, 2, 3, 25, 14, 68, 331, 863, 78, 166, 5, 3229, 42, 6, 3230, 3231, 14, 7, 3232, 5, 1023, 480, 3233], [1, 171, 6, 18, 13, 429, 180, 84, 7, 529, 526, 629, 171, 42, 33, 50, 23, 1, 960, 735, 22, 1805, 1806, 1807, 3234, 13, 224, 14, 429, 180, 84, 50, 23, 1, 583, 1235], [195, 222, 5, 1, 3235, 3236, 5, 1, 1642, 3237, 6, 50, 68, 517, 3238, 271, 199, 21, 490, 557, 2, 3, 90, 10, 1, 28, 803, 21, 1, 3239, 3240, 50, 1, 163, 10, 1, 490, 81, 557, 2, 3, 90, 5, 271, 3241, 3242], [11, 305, 44, 6, 76, 95, 7, 29, 61, 23, 17, 12, 2, 3, 16, 11, 296, 6, 76, 508, 1, 235, 286, 5, 1, 126, 61, 1177, 4, 3243, 49, 42, 32, 61, 23, 1, 177, 3244, 1252, 12, 2, 3, 16], [6, 18, 1, 577, 119, 578, 1217, 5, 579, 2, 3, 64, 11, 231, 579, 2, 3, 64, 121, 576, 119, 3245, 11, 3246, 394, 10, 1, 429, 171, 180, 84, 14, 68, 2092, 623, 5, 3247], [6, 76, 728, 68, 1812, 5, 24, 128, 4, 34, 5, 2093, 4, 1302, 53, 6, 851, 7, 1860, 290, 34, 24, 128, 3248, 326, 5, 1, 479, 545, 3249, 7, 13, 642, 124, 793, 4, 1, 205, 326, 5, 1, 479, 6, 18, 1, 792, 5, 2093, 4, 1302, 53, 3250], [145, 592, 9, 972, 48, 304, 13, 9, 18, 7, 162, 369, 34, 13, 3251, 11, 709, 483, 764, 290, 31, 1, 189, 162, 451, 4, 690, 156, 11, 70, 3252, 1083, 13, 72, 9, 1, 1083, 11, 1, 189, 162, 8, 451, 4, 690, 156], [6, 18, 243, 244, 4, 245, 201, 11, 1718, 14, 331, 78, 166, 5, 1519, 69, 14, 1, 243, 527, 244, 4, 245, 201, 7, 78, 166, 5, 1122, 676, 415, 5, 326, 4, 444, 14, 528, 769, 214, 9, 1, 529, 677], [6, 121, 24, 433, 49, 8, 7, 60, 592, 10, 1, 17, 29, 102, 11, 41, 1813, 96, 25, 4, 60, 257, 299, 19, 6, 18, 102, 131, 96, 25, 11, 1093, 1, 209, 41, 19, 222], [1, 1751, 3253, 11, 24, 19, 13, 1, 1381, 14, 1985, 378, 3254, 995, 5, 457, 2, 3, 1100, 24, 19, 13, 68, 3255, 5, 1, 683, 1559, 5, 63, 3256, 19, 5, 457, 2, 3, 1100, 7, 82, 11, 78, 168, 418, 5, 63, 61, 23, 248, 91, 149], [6, 952, 7, 300, 2094, 1096, 19, 31, 1, 391, 238, 19, 10, 7, 684, 799, 446, 687, 1106, 309, 1107, 4, 1108, 329, 6, 421, 178, 26, 46, 7, 2095, 2096, 446, 687, 1106, 309, 1107, 4, 1108, 329, 10, 98, 31, 68, 2097, 23, 1, 2098, 190, 2099, 21, 24, 22], [1, 1678, 434, 837, 13, 68, 99, 434, 19, 175, 14, 1243, 7, 158, 168, 1679, 78, 122, 295, 37, 1, 3257, 5, 68, 99, 496, 19, 13, 34, 98, 13, 650, 2053, 4, 98, 129, 123, 268, 9, 104, 1132, 11, 7, 1180, 5, 434, 783, 295, 37], [1986, 13, 1987, 728, 31, 68, 345, 322, 62, 643, 1, 91, 113, 114, 127, 34, 1, 30, 157, 13, 1988, 26, 1, 75, 5, 149, 8, 42, 98, 3258, 9, 1, 91, 113, 5, 157, 114, 127, 63, 34, 194, 8, 72, 149, 237, 9, 104, 766, 72], [6, 232, 24, 58, 23, 54, 12, 40, 7, 307, 85, 22, 42, 13, 138, 8, 741, 8, 185, 3259, 58, 67, 196, 179, 23, 1, 54, 12, 40, 22, 42, 13, 7, 22, 733, 15, 8, 38, 4, 34, 129, 123, 15, 8, 697, 235, 143, 2100], [11, 1, 41, 19, 6, 15, 1, 102, 20, 96, 25, 9, 348, 7, 100, 41, 19, 23, 1, 209, 140, 5, 1, 54, 22, 717, 14, 844, 1989, 1246, 14, 116, 3260, 41, 19, 13, 7, 100, 102, 96, 25, 19, 50, 10, 822, 14, 155, 116, 173, 4, 559, 567], [1, 391, 310, 13, 3261, 26, 1, 1054, 22, 278, 2, 3, 43, 131, 42, 240, 84, 1308, 21, 2101, 7, 1197, 5, 3262, 125, 1054, 278, 2, 3, 43, 13, 7, 310, 5, 84, 655, 1308, 459, 21, 1, 495, 188, 5, 2101, 399, 10, 1689, 3263, 363, 352, 3264, 3265, 4, 3266, 3267], [6, 952, 7, 300, 2094, 1096, 19, 31, 1, 391, 238, 19, 10, 7, 684, 799, 446, 687, 1106, 309, 1107, 4, 1108, 329, 6, 421, 178, 26, 46, 7, 2095, 2096, 446, 687, 1106, 309, 1107, 4, 1108, 329, 10, 98, 31, 68, 2097, 23, 1, 2098, 190, 2099, 21, 24, 22], [1, 136, 585, 85, 28, 13, 118, 593, 10, 59, 8, 105, 3268, 4, 3269, 398, 1631, 26, 1, 3270, 5, 1286, 3271, 705, 47, 4, 45, 43, 101, 30, 132, 67, 153, 26, 464, 59, 8, 105, 398, 4, 817, 10, 1, 356, 370, 47, 4, 45, 43], [51, 1, 41, 65, 371, 15, 8, 24, 58, 32, 2025, 155, 116, 291, 1372, 50, 10, 102, 96, 2, 3, 88, 11, 1, 41, 19, 6, 15, 51, 427, 691, 4, 1, 275, 1065, 5, 1, 85, 691, 4, 50, 7, 100, 41, 19, 14, 155, 116, 173, 10, 102, 96, 2, 3, 88], [328, 32, 61, 23, 91, 113, 42, 1257, 643, 1, 1258, 34, 72, 63, 194, 8, 72, 149, 114, 127, 675, 1783, 95, 728, 1293, 1784, 26, 1785, 1, 207, 1786, 26, 1, 91, 113, 42, 270, 34, 63, 1017, 8, 72, 149, 237, 9, 95, 72, 365, 114, 127], [393, 6, 849, 1, 54, 22, 717, 12, 40, 617, 98, 13, 733, 15, 8, 1, 235, 3272, 11, 572, 3273, 58, 67, 196, 179, 23, 1, 54, 12, 40, 22, 42, 13, 7, 22, 733, 15, 8, 38, 4, 34, 129, 123, 15, 8, 697, 235, 143, 2100], [11, 1, 683, 1312, 6, 18, 1, 367, 207, 313, 100, 22, 180, 4, 246, 37, 42, 240, 450, 11, 698, 21, 800, 571, 9, 1313, 153, 21, 206, 83, 1061, 30, 3274, 344, 11, 30, 467, 6, 18, 1, 800, 28, 21, 1, 367, 1062, 310, 180, 4, 246, 37], [26, 3275, 513, 23, 1, 2102, 30, 3276, 72, 9, 34, 138, 8, 1099, 2, 3, 197, 1, 3277, 128, 3278, 327, 852, 545, 1, 513, 32, 214, 1, 212, 1, 118, 13, 7, 1356, 5, 1, 3279, 161, 138, 8, 1099, 2, 3, 197], [3280, 2, 3, 64, 750, 7, 3281, 2103, 2104, 3282, 330, 9, 1872, 1, 191, 771, 3283, 2, 3, 64, 97, 7, 700, 128, 9, 2103, 2104, 78, 11, 293, 303, 42, 3284, 3285, 51, 1, 125, 8, 306, 4, 51, 795, 8, 804, 1888, 11], [6, 95, 1820, 2105, 34, 61, 23, 346, 347, 84, 24, 234, 154, 104, 7, 514, 234, 5, 1883, 3286, 26, 3287, 1, 300, 384, 734, 4, 1, 653, 301, 1775, 6, 1776, 1, 112, 301, 513, 5, 346, 347, 84, 4, 1777, 1, 653, 301, 515], [11, 1, 683, 1312, 6, 18, 1, 367, 207, 313, 100, 22, 180, 4, 246, 37, 42, 240, 450, 11, 698, 21, 800, 571, 9, 1313, 153, 21, 206, 83, 1061, 30, 3288, 15, 1, 836, 450, 21, 1, 207, 313, 100, 22, 180, 4, 246, 37, 9, 535, 1, 467, 5, 18, 5, 111, 624, 843], [317, 134, 36, 13, 7, 75, 5, 143, 975, 15, 11, 355, 3289, 1075, 48, 873, 13, 327, 641, 14, 1, 317, 143, 314, 15, 11, 436, 134, 36], [662, 6, 732, 1, 106, 5, 1, 60, 27, 35, 19, 12, 2, 3, 16, 1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16], [1, 582, 832, 46, 22, 33, 564, 21, 1, 54, 310, 12, 40, 1, 49, 11, 1, 44, 572, 35, 558, 67, 50, 23, 1, 1334, 54, 22, 12, 40], [6, 1389, 1, 200, 8, 105, 1390, 21, 1391, 9, 420, 191, 462, 426, 83, 10, 1, 39, 1392, 42, 1074, 1393, 354, 5, 420, 191, 83, 74, 71, 2, 3, 37, 6, 18, 1, 39, 52, 14, 39, 191, 74, 71, 2, 3, 37], [6, 1389, 1, 200, 8, 105, 1390, 21, 1391, 9, 420, 191, 462, 426, 83, 10, 1, 39, 1392, 42, 1074, 1393, 354, 5, 420, 191, 83, 74, 71, 2, 3, 6, 18, 1, 39, 52, 14, 39, 191, 74, 71, 2, 3, 37], [6, 18, 102, 131, 96, 25, 11, 1093, 1, 209, 41, 19, 3290, 18, 7, 100, 371, 50, 23, 1, 353, 22, 4, 18, 102, 96, 25, 11, 371, 3291, 648, 282], [1, 255, 1362, 231, 1363, 1364, 104, 1, 54, 22, 12, 40, 1, 29, 33, 50, 23, 1, 44, 4, 867, 236, 5, 1, 54, 22, 213, 131, 12, 40], [6, 18, 1, 39, 52, 14, 39, 191, 74, 71, 2, 3, 37, 6, 1389, 1, 200, 8, 105, 1390, 21, 1391, 9, 420, 191, 462, 426, 83, 10, 1, 39, 1392, 42, 1074, 1393, 354, 5, 420, 191, 83, 74, 71, 2, 3, 37], [589, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16, 662, 6, 732, 1, 106, 5, 1, 60, 27, 35, 19, 12, 2, 3, 16], [1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16, 1, 17, 161, 12, 2, 3, 16, 33, 15, 11, 1293, 3292, 4, 3293, 1190], [1, 3294, 3295, 247, 22, 6, 15, 13, 7, 647, 5, 1, 275, 187, 294, 3296, 2106, 2, 3, 43, 48, 3297, 9, 7, 300, 213, 5, 1, 275, 187, 2106, 2, 3, 43], [523, 129, 123, 7, 390, 1306, 5, 167, 23, 195, 115, 190, 413, 950, 5, 1170, 409, 4, 588, 90, 11, 7, 612, 3298, 5, 1, 1183, 5, 195, 115, 462, 409, 4, 588, 90], [48, 62, 930, 13, 881, 138, 8, 1, 62, 497, 263, 639, 2, 3, 53, 7, 514, 497, 5, 1, 46, 4, 147, 691, 154, 104, 289, 190, 1, 62, 497, 263, 639, 2, 3, 53], [1, 101, 385, 104, 417, 26, 1, 17, 38, 20, 12, 2, 3, 16, 1229, 24, 101, 13, 1, 38, 20, 17, 12, 2, 3, 16, 232, 206, 2041, 605, 3299, 602, 30, 605], [137, 334, 8, 436, 542, 334, 234, 931, 543, 4, 544, 387, 887, 1, 137, 8, 7, 566, 8, 1, 827, 5, 7, 189, 426, 83, 1863, 242, 32, 771, 137, 242, 524, 26, 542, 334, 234, 543, 4, 544, 387], [98, 1085, 23, 1, 1387, 584, 52, 498, 4, 885, 36, 98, 3300, 584, 1259, 21, 1, 1387, 52, 498, 4, 885, 36, 4, 1707, 257, 1135, 4, 1240, 940, 283, 130, 3301, 1380, 26, 3302, 3303], [6, 18, 1335, 2107, 2, 3, 36, 9, 160, 372, 41, 283, 7, 2108, 3304, 3305, 372, 41, 133, 9, 2108, 827, 6, 121, 23, 1, 108, 152, 1335, 2107, 2, 3, 36], [1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16, 6, 18, 1, 17, 27, 35, 29, 12, 2, 3, 16, 9, 424, 24, 65], [6, 262, 77, 80, 14, 604, 411, 31, 138, 8, 12, 36, 6, 76, 871, 364, 311, 297, 3306, 460, 23, 507, 1330, 5, 77, 80, 14, 3307, 12, 36], [1, 1268, 236, 5, 28, 622, 21, 846, 4, 1269, 583, 1270, 5, 1, 54, 28, 75, 12, 40, 1, 54, 22, 12, 40, 13, 66, 21, 1, 208, 5, 1, 203, 449], [11, 1394, 6, 18, 1, 883, 9, 69, 30, 178, 23, 1, 54, 1394, 22, 12, 40, 94, 325, 3308, 3309, 6, 18, 1, 54, 22, 12, 40, 31, 281, 28], [1, 2109, 3310, 401, 2110, 2, 3, 87, 13, 1, 1383, 11, 1, 3311, 1384, 2109, 2110, 2, 3, 3312, 13, 68, 401, 7, 330, 4, 7, 422, 1384, 11, 352, 41, 3313, 1478, 4, 764], [6, 50, 7, 100, 41, 19, 23, 1, 917, 185, 5, 1, 44, 353, 22, 1133, 480, 63, 10, 1, 614, 20, 615, 87, 14, 1, 155, 116, 173, 342, 4, 474, 197, 105, 41, 65, 18, 155, 116, 173, 342, 4, 474, 197], [6, 15, 59, 47, 4, 45, 43, 1161, 14, 1, 3314, 705, 9, 3315, 1, 46, 1890, 3316, 374, 593, 1, 305, 236, 9, 111, 5, 1, 3317, 14, 1, 59, 122, 47, 4, 45, 43], [24, 323, 1208, 128, 11, 412, 221, 4, 322, 5, 1307, 443, 4, 438, 64, 412, 221, 4, 322, 5, 689, 3318, 1, 221, 188, 11, 105, 1, 1207, 443, 4, 438, 64, 4, 1, 3319, 128], [409, 2, 3, 87, 225, 1, 286, 5, 493, 1153, 15, 56, 78, 65, 366, 312, 532, 1328, 4, 3320, 2, 3, 87, 18, 56, 78, 381, 1186, 99, 4, 913, 9, 1000, 1311, 23, 694, 695], [2111, 2, 3, 64, 1263, 3321, 23, 1, 3322, 3323, 147, 31, 68, 1968, 3324, 751, 9, 1, 29, 5, 2111, 2, 3, 64, 31, 1, 1254, 3325, 29], [6, 214, 1, 366, 312, 894, 576, 78, 103, 21, 1, 202, 56, 78, 403, 204, 2, 3, 57, 366, 312, 4, 405, 189, 65, 67, 66, 14, 1, 202, 204, 2, 3, 57, 152, 11, 405, 200, 6, 15, 1, 3326, 81], [6, 492, 3327, 9, 1151, 3328, 3329, 3330, 5, 1, 3331, 1316, 17, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16, 6, 225, 1, 19, 659, 1, 17, 27, 35, 29, 12, 2, 3, 16, 214, 9, 1063, 645], [98, 129, 7, 1377, 1128, 2092, 136, 1298, 602, 217, 298, 187, 3332, 3333, 225, 9, 1907, 742, 2, 3, 40, 6, 192, 719, 8, 327, 741, 7, 1960, 298, 22, 1, 217, 298, 187, 742, 2, 3, 40], [1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16, 1, 118, 112, 1088, 32, 60, 49, 10, 408, 294, 1360, 50, 10, 17, 12, 2, 3, 16], [1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16, 1, 118, 112, 1088, 32, 60, 49, 10, 408, 294, 1360, 50, 10, 17, 12, 2, 3, 16], [1, 444, 166, 33, 75, 9, 778, 4, 1, 19, 33, 50, 574, 150, 151, 2, 3, 25, 6, 18, 1, 746, 315, 369, 4, 520, 150, 151, 2, 3, 25, 9, 651, 106], [1, 444, 166, 33, 75, 9, 778, 4, 1, 19, 33, 50, 574, 150, 151, 2, 3, 25, 6, 18, 150, 151, 2, 3, 25, 14, 1, 331, 78, 166, 75, 9, 778], [6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16, 6, 225, 1, 19, 659, 1, 17, 27, 35, 29, 12, 2, 3, 16, 214, 9, 1063, 645], [328, 944, 268, 34, 1, 217, 137, 187, 485, 2112, 336, 2, 3, 90, 137, 242, 32, 3334, 217, 137, 187, 485, 336, 2, 3, 90, 3335, 697, 293, 354, 34, 32, 1691, 9, 3336, 1700, 3337, 4, 1623], [328, 944, 268, 34, 1, 217, 137, 187, 485, 2112, 336, 2, 3, 90, 137, 242, 32, 3338, 4, 1865, 90, 15, 137, 242, 5, 1, 217, 137, 187, 336, 2, 3, 90, 31, 7, 250], [661, 2, 3, 40, 714, 7, 1754, 11, 46, 2043, 65, 11, 55, 227, 1, 673, 5, 227, 19, 67, 509, 8, 661, 2, 3, 40, 4, 661, 4, 3339, 37], [6, 1354, 1, 160, 200, 8, 357, 283, 389, 191, 10, 39, 120, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [94, 35, 19, 17, 13, 7, 258, 20, 11, 27, 38, 49, 12, 2, 3, 16, 1, 161, 13, 66, 23, 1089, 5, 68, 177, 27, 38, 161, 17, 12, 2, 3, 16], [11, 1, 357, 28, 456, 139, 3340, 588, 2, 3, 88, 13, 1, 254, 250, 11, 3341, 3342, 2113, 1096, 5, 1, 39, 826, 29, 588, 2, 3, 88, 13, 15, 11, 1, 357, 28, 456], [98, 33, 145, 5, 1, 606, 1067, 8, 1, 502, 159, 62, 57, 2114, 2, 3, 57, 3343, 13, 15, 31, 1, 298, 28, 75, 8, 1, 502, 57, 159, 62, 2114, 2, 3, 57], [6, 1354, 1, 160, 200, 8, 357, 283, 389, 191, 10, 39, 120, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [188, 32, 516, 23, 1, 147, 28, 10, 1358, 260, 14, 1, 502, 1388, 392, 4, 1104, 64, 6, 871, 1358, 286, 3344, 10, 1, 2051, 1388, 21, 1, 159, 62, 392, 4, 1104, 64], [1, 818, 807, 26, 1, 985, 1609, 371, 8, 1, 572, 3345, 75, 33, 3346, 1610, 3347, 153, 14, 1, 60, 985, 41, 19, 14, 819, 4, 116, 173, 630, 4, 3348, 188, 5, 48, 340, 1010, 8, 193, 94, 42, 881, 1021, 172, 65, 14, 1, 280, 1088, 83, 3349, 3350, 4, 94, 7, 985, 371, 14, 116, 173, 131, 630, 4, 45], [6, 3351, 1, 3352, 304, 31, 7, 199, 62, 26, 359, 99, 65, 14, 68, 3353, 1053, 4, 7, 2067, 1383, 369, 162, 10, 1, 404, 20, 316, 4, 134, 25, 6, 18, 404, 316, 4, 134, 25, 31, 1, 99, 122], [393, 98, 13, 76, 2047, 34, 1, 1378, 5, 663, 1379, 8, 1, 46, 75, 13, 72, 9, 1, 145, 516, 26, 463, 2, 3, 87, 4, 2048, 9, 8, 185, 2049, 2, 3, 87, 76, 3354, 179, 34, 1, 941, 1378, 5, 663, 1379, 190, 838, 472, 325, 2105, 34, 675, 5, 789, 32, 3355, 3356, 3357, 3358, 294, 650, 3359, 63], [1290, 290, 31, 172, 32, 3360, 9, 1340, 11, 8, 7, 3361, 130, 234, 5, 3362, 1251, 290, 31, 1, 145, 97, 8, 2115, 2, 3, 1084, 1, 255, 1145, 5, 1, 130, 994, 9, 3363, 129, 123, 1, 3364, 3365, 128, 97, 8, 2115, 2, 3, 1084], [51, 172, 299, 65, 32, 484, 10, 17, 12, 2, 3, 16, 590, 34, 1, 238, 19, 3366, 68, 1022, 3367, 1341, 608, 185, 3368, 751, 9, 34, 19, 31, 17, 2116, 1281, 98, 33, 50, 10, 1, 17, 20, 12, 2, 3, 16], [423, 3369, 362, 2, 3, 25, 97, 9, 18, 880, 1103, 11, 3370, 3371, 5, 195, 1042, 3372, 360, 1, 3373, 5, 168, 1375, 97, 26, 362, 2, 3, 25, 590, 34, 6, 657, 318, 1333, 1103, 11, 594], [6, 69, 7, 1076, 199, 19, 92, 79, 2, 3, 25, 11, 111, 1134, 4, 147, 10, 361, 21, 1, 1746, 31, 46, 934, 24, 29, 13, 7, 226, 19, 407, 10, 1076, 199, 31, 175, 8, 1, 92, 20, 79, 2, 3, 25], [6, 360, 1, 1671, 8, 646, 2, 3, 25, 10, 1, 1649, 5, 3374, 178, 31, 679, 9, 7, 1922, 238, 1706, 18, 3375, 646, 2, 3, 25, 9, 535, 1, 3376, 4, 3377, 868, 5, 68, 477, 2113], [48, 13, 1374, 9, 68, 99, 14, 1, 3378, 1169, 93, 31, 8, 561, 2, 3, 90, 1, 3379, 5, 257, 93, 154, 76, 104, 3380, 26, 1, 18, 5, 91, 30, 658, 31, 3381, 26, 561, 2, 3, 90], [51, 58, 32, 223, 10, 1, 17, 27, 38, 29, 12, 2, 3, 16, 14, 7, 532, 126, 1298, 5, 3382, 1, 3383, 5, 35, 693, 1, 65, 32, 50, 848, 10, 7, 27, 35, 29, 12, 2, 3, 16, 34, 15, 1, 642, 3384, 65, 9, 341, 1, 28], [6, 18, 1, 17, 20, 12, 2, 3, 16, 9, 348, 7, 70, 27, 56, 35, 19, 66, 23, 1, 606, 442, 28, 31, 138, 1415, 751, 9, 34, 19, 31, 17, 2116, 1281, 98, 33, 50, 10, 1, 17, 20, 12, 2, 3, 16], [51, 58, 67, 23, 44, 236, 5, 876, 119, 198, 23, 1, 828, 549, 550, 551, 133, 8, 1, 217, 187, 1235, 213, 131, 638, 2, 3, 110, 172, 1057, 67, 397, 8, 58, 23, 1, 217, 187, 638, 2, 3, 110, 14, 1, 3385, 264, 52, 1166, 26], [6, 18, 1, 30, 132, 9, 644, 7, 126, 193, 26, 865, 1, 641, 126, 654, 370, 47, 4, 45, 36, 9, 51, 2117, 18, 1, 1097, 5, 903, 4, 2118, 59, 47, 4, 45, 36, 132, 31, 7, 370, 469, 9, 681, 63, 2119, 585, 9, 111, 205], [6, 407, 7, 685, 235, 19, 11, 1, 69, 3386, 14, 1, 60, 580, 5, 1, 17, 20, 12, 2, 3, 16, 6, 76, 225, 14, 1, 60, 27, 29, 5, 17, 12, 2, 3, 16, 14, 60, 302, 590, 11, 1, 3387, 2120, 42, 6, 75, 9, 869], [7, 941, 1306, 5, 420, 28, 13, 15, 9, 858, 1, 3388, 775, 9, 3389, 1073, 11, 7, 497, 5, 1, 82, 462, 569, 4, 2121, 16, 1175, 1264, 7, 212, 5, 658, 72, 9, 1, 212, 5, 1073, 13, 318, 7, 3390, 11, 914, 188, 569, 4, 2121, 3391], [6, 18, 1, 30, 132, 9, 644, 7, 126, 193, 26, 865, 1, 641, 126, 654, 370, 47, 4, 45, 36, 9, 51, 2117, 18, 1, 1097, 5, 903, 4, 2118, 59, 47, 4, 45, 36, 132, 31, 7, 370, 469, 9, 681, 63, 2119, 585, 9, 111, 205], [83, 11, 7, 296, 60, 133, 6, 15, 1, 453, 540, 5, 1, 1395, 801, 22, 2122, 2123, 84, 1, 143, 22, 13, 7, 388, 5, 68, 1818, 213, 5, 1, 1395, 801, 22, 2122, 7, 869, 480, 30, 3392, 22, 5, 1395, 44, 2123, 84], [6, 3393, 14, 1, 54, 22, 12, 40, 8, 269, 9, 95, 7, 85, 3394, 22, 11, 1394, 4, 3395, 11, 44, 63, 8, 1, 257, 1371, 32, 174, 21, 7, 1261, 30, 73, 5, 125, 21, 1, 54, 85, 22, 12, 40], [1, 3396, 21, 987, 62, 94, 195, 115, 8, 465, 176, 493, 691, 11, 1, 62, 988, 2, 3, 88, 6, 631, 8, 105, 573, 7, 4, 989, 5, 987, 62, 94, 195, 115, 8, 465, 988, 2, 3, 88, 14, 68, 990, 5, 24, 991, 29], [3397, 7, 3398, 583, 143, 10, 1, 17, 29, 12, 2, 3, 16, 31, 7, 1871, 6, 484, 1, 235, 82, 1878, 642, 23, 112, 708, 3399, 147, 24, 82, 6, 223, 112, 2014, 35, 58, 10, 1, 27, 235, 29, 17, 12, 2, 3, 16], [1, 656, 1748, 93, 32, 192, 15, 9, 69, 7, 158, 168, 56, 2091, 295, 87, 9, 496, 1, 261, 358, 1400, 602, 1, 3400, 18, 1, 158, 168, 56, 99, 496, 103, 295, 87, 9, 1301, 7, 496, 269, 11, 111, 501, 5, 3401], [24, 391, 82, 13, 61, 23, 1, 953, 238, 309, 41, 19, 1176, 128, 9, 78, 30, 178, 5, 457, 2, 3, 747, 4, 457, 2, 3, 1100, 10, 1, 883, 3402, 51, 5, 1, 642, 558, 6, 225, 1, 238, 30, 178, 5, 457, 2, 3, 747, 14, 112, 168, 3403, 105, 61, 23, 815, 450, 4, 660, 26, 60, 91, 1057], [6, 396, 1, 529, 526, 19, 376, 1105, 1297, 4, 1, 3404, 103, 3405, 165, 9, 210, 1, 3406, 3407, 18, 1105, 103, 1105, 1297, 9, 3408, 1, 3409, 136], [6, 871, 259, 563, 2, 3, 156, 5, 35, 29, 619, 460, 659, 1, 1332, 44, 3410, 216, 35, 693, 574, 1, 259, 222, 563, 2, 3, 156], [98, 13, 7, 60, 27, 56, 35, 19, 12, 2, 3, 16, 51, 24, 49, 32, 664, 14, 7, 60, 27, 29, 66, 14, 17, 12, 2, 3, 16], [393, 6, 15, 17, 20, 31, 27, 296, 12, 2, 3, 16, 1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [324, 142, 13, 260, 10, 1, 3411, 431, 142, 745, 2, 3, 36, 1, 2124, 5, 142, 211, 112, 72, 63, 13, 1158, 10, 249, 745, 2, 3, 36], [393, 6, 15, 17, 20, 31, 27, 296, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [98, 13, 7, 60, 27, 56, 35, 19, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [94, 35, 19, 17, 13, 7, 258, 20, 11, 27, 38, 49, 12, 2, 3, 16, 393, 6, 15, 17, 20, 31, 27, 296, 12, 2, 3, 16], [3412, 98, 13, 460, 26, 249, 142, 152, 745, 2, 3, 36, 1, 2124, 5, 142, 211, 112, 72, 63, 13, 1158, 10, 249, 745, 2, 3, 36], [6, 18, 100, 41, 65, 14, 116, 839, 96, 2, 3, 88, 1, 41, 65, 32, 407, 10, 1, 102, 20, 96, 2, 3, 88, 14, 155, 116, 173], [6, 882, 428, 78, 591, 106, 10, 150, 151, 2, 3, 25, 6, 18, 1, 746, 315, 369, 4, 520, 150, 151, 2, 3, 25, 9, 651, 106], [6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25, 6, 15, 99, 908, 21, 92, 79, 2, 3, 25, 4, 794, 14, 7, 212, 5, 163], [6, 882, 428, 78, 591, 106, 10, 150, 151, 2, 3, 25, 6, 69, 1, 324, 1058, 811, 10, 1059, 1060, 315, 83, 14, 150, 151, 2, 3, 25], [393, 6, 15, 17, 20, 31, 27, 296, 12, 2, 3, 16, 6, 15, 1, 17, 20, 12, 2, 3, 16, 9, 69, 60, 27, 49, 14, 107, 739], [54, 12, 40, 13, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 541, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40], [6, 3413, 1, 2125, 65, 561, 4, 451, 117, 11, 7, 1336, 678, 3414, 3415, 11, 1, 19, 83, 8, 561, 4, 451, 117, 42, 13, 7, 2125, 19], [6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25, 791, 6, 15, 1, 60, 437, 1930, 1053, 8, 1, 92, 20, 253, 79, 2, 3, 25], [11, 44, 4, 275, 7, 19, 33, 50, 10, 1, 54, 22, 12, 40, 11, 48, 265, 6, 18, 1, 54, 22, 12, 40], [1897, 6, 508, 34, 24, 19, 3416, 1, 258, 49, 5, 2126, 2, 3, 201, 23, 1025, 28, 3417, 439, 2126, 2, 3, 201, 97, 7, 238, 1632, 19, 11, 48, 304, 10, 7, 300, 28, 75, 11, 46, 4, 3418, 258, 286, 23, 1, 3419, 558], [6, 18, 271, 199, 14, 517, 349, 175, 10, 1, 92, 20, 79, 2, 3, 25, 6, 18, 1, 445, 81, 5, 256, 416, 79, 2, 3, 25], [6, 18, 1, 300, 859, 620, 247, 22, 1052, 90, 11, 51, 24, 3420, 1, 1185, 1444, 3421, 3422, 21, 7, 390, 75, 5, 2102, 3423, 6, 803, 21, 1, 300, 859, 620, 247, 22, 3424, 1052, 90, 11, 18, 8, 1771, 3425, 5, 1, 340, 3426], [6, 18, 1, 3427, 4, 1619, 908, 176, 26, 92, 213, 2138, 79, 2, 3, 25, 6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25], [6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 1699, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53, 6, 76, 182, 51, 63, 10, 39, 120, 86, 2, 3, 53], [6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 1317, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 1840, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16], [6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 3428, 161, 17, 12, 2, 3, 16, 14, 107, 1912, 590, 11, 1, 3429, 2120, 3430], [250, 824, 33, 109, 10, 3431, 8, 202, 204, 2, 3, 57, 6, 69, 4, 337, 1, 163, 8, 7, 1889, 1041, 10, 202, 204, 2, 3, 57], [6, 18, 1, 158, 168, 668, 81, 8, 1, 92, 20, 79, 2, 3, 25, 9, 262, 199, 1115, 23, 111, 250, 75, 14, 1401, 753, 669, 4, 106, 2137], [439, 523, 129, 123, 7, 1403, 1116, 9, 1404, 1, 603, 2140, 2141, 1, 755, 49, 888, 4, 1405, 1406, 2, 3, 117, 439, 523, 129, 123, 7, 1403, 1116, 9, 1404, 1, 755, 49, 888, 4, 1405, 1406, 2, 3, 117], [1, 118, 145, 13, 1, 1117, 183, 889, 2, 3, 156, 890, 756, 218, 5, 44, 63, 34, 95, 123, 757, 142, 891, 26, 1407, 118, 145, 13, 1, 1117, 183, 889, 2, 3, 156, 42, 240, 756, 218, 5, 44, 63, 34, 95, 123, 757, 142, 891, 26, 1408], [892, 157, 287, 1409, 1410, 2, 3, 88, 13, 7, 130, 671, 319, 1, 157, 5, 7, 136, 13, 2142, 31, 7, 1411, 1412, 2143, 157, 287, 1409, 1410, 2, 3, 88, 13, 7, 130, 671, 2144, 1, 157, 5, 7, 136, 31, 7, 1411, 1412, 758], [6, 262, 77, 80, 14, 604, 411, 31, 138, 8, 12, 36, 6, 262, 77, 80, 14, 604, 411, 31, 138, 26, 12, 36], [98, 13, 15, 9, 158, 130, 320, 8, 264, 44, 251, 146, 380, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 759, 13, 15, 9, 158, 130, 320, 8, 1, 264, 44, 251, 146, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 760], [6, 18, 39, 52, 74, 71, 2, 3, 37, 9, 210, 160, 200, 4, 55, 761, 118, 18, 7, 55, 52, 74, 71, 2, 3, 37, 9, 160, 111, 136, 4, 219, 1, 75, 5, 55, 242, 762, 14, 1, 136], [1413, 2, 3, 288, 750, 68, 2145, 2146, 322, 103, 11, 1118, 893, 1414, 61, 23, 7, 894, 441, 2147, 2148, 1413, 2, 3, 288, 750, 68, 2149, 322, 2150, 11, 2151], [1, 895, 763, 895, 896, 211, 112, 605, 13, 524, 31, 1, 525, 212, 5, 2152, 476, 2153, 2154, 4, 2155, 895, 763, 2156, 68, 2157, 5, 1, 142, 211, 112, 605, 895, 896], [6, 18, 1, 17, 20, 12, 2, 3, 16, 9, 348, 7, 70, 27, 56, 35, 19, 66, 23, 1, 606, 442, 28, 31, 138, 1415, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [6, 262, 77, 80, 14, 604, 411, 31, 138, 8, 12, 36, 6, 262, 77, 80, 14, 604, 411, 31, 138, 26, 12, 36], [98, 13, 15, 9, 158, 130, 320, 8, 264, 44, 251, 146, 380, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 759, 13, 15, 9, 158, 130, 320, 8, 1, 264, 44, 251, 146, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 760], [98, 13, 15, 9, 158, 130, 320, 8, 1, 44, 264, 146, 380, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 759, 13, 15, 9, 158, 130, 320, 8, 1, 264, 44, 251, 146, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 760], [98, 13, 15, 9, 158, 130, 320, 8, 264, 44, 146, 380, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 759, 13, 15, 9, 158, 130, 320, 8, 1, 264, 44, 251, 146, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 760], [98, 13, 15, 9, 158, 130, 320, 8, 1, 44, 264, 146, 380, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 759, 13, 15, 9, 158, 130, 320, 8, 1, 264, 44, 251, 146, 277, 4, 321, 84, 241, 76, 8, 205, 146, 440, 252, 760], [6, 121, 672, 24, 323, 526, 897, 61, 128, 11, 412, 324, 221, 4, 322, 443, 4, 438, 64, 6, 121, 672, 24, 323, 128, 11, 412, 324, 221, 4, 322, 443, 4, 438, 64], [673, 1119, 99, 4, 2158, 154, 104, 289, 8, 898, 4, 899, 36, 673, 1119, 99, 4, 2159, 154, 104, 289, 8, 898, 4, 899, 36], [6, 607, 1, 106, 10, 7, 1416, 608, 14, 900, 83, 1417, 349, 1418, 4, 674, 16, 6, 607, 1, 106, 10, 7, 1416, 2160, 608, 14, 900, 83, 1417, 349, 1418, 4, 674, 16], [6, 18, 1, 1419, 161, 1420, 901, 2, 3, 117, 253, 4, 121, 902, 10, 124, 81, 5, 1, 1421, 1422, 303, 82, 138, 8, 1423, 2161, 903, 35, 6, 18, 1, 1419, 161, 1420, 901, 2, 3, 117, 253, 4, 121, 902, 10, 124, 81, 5, 1, 1421, 1422, 303, 82, 138, 8, 1423, 16], [48, 13, 609, 31, 1, 91, 113, 114, 904, 48, 13, 609, 31, 1, 91, 113, 8, 905, 114, 904], [51, 24, 65, 31, 350, 31, 1, 52, 138, 8, 906, 43, 32, 232, 254, 2162, 65, 31, 350, 31, 1, 52, 138, 8, 906, 43, 32, 232, 254, 2163], [11, 605, 675, 290, 162, 1120, 1424, 14, 413, 764, 8, 765, 1425, 4, 765, 905, 898, 4, 899, 36, 11, 605, 7, 2164, 5, 290, 162, 1120, 1424, 14, 675, 764, 8, 765, 1425, 4, 765, 905, 898, 4, 899, 36], [1, 411, 5, 1, 766, 291, 767, 189, 162, 768, 13, 414, 169, 26, 68, 1121, 213, 5, 1426, 108, 170, 1427, 37, 1, 411, 5, 1, 766, 291, 767, 189, 162, 768, 13, 414, 169, 26, 68, 1121, 213, 5, 1426, 108, 325, 1427, 37], [6, 69, 14, 1, 243, 527, 244, 4, 245, 201, 7, 78, 166, 5, 1122, 676, 415, 5, 326, 4, 444, 14, 528, 769, 214, 9, 1, 529, 2165, 69, 14, 1, 243, 527, 244, 4, 245, 201, 7, 78, 166, 5, 1122, 676, 415, 5, 326, 4, 444, 14, 528, 769, 214, 9, 1, 529, 677], [8, 24, 1123, 678, 6, 18, 1, 907, 169, 908, 8, 202, 1428, 4, 1429, 40, 8, 24, 1123, 678, 6, 18, 1, 907, 169, 81, 5, 99, 8, 202, 1428, 4, 1429, 40], [327, 439, 1430, 4, 1431, 117, 770, 34, 1, 1432, 5, 1, 1124, 1433, 26, 2166, 530, 1125, 1, 1434, 5, 7, 1435, 322, 2167, 1430, 4, 1431, 117, 770, 34, 1, 1432, 5, 1, 1124, 1433, 26, 2168, 530, 1125, 1, 1434, 5, 7, 1435, 322, 128], [328, 32, 61, 23, 1, 91, 113, 114, 904, 4, 26, 1436, 190, 7, 75, 5, 477, 1437, 1438, 478, 1439, 95, 7, 72, 377, 328, 1440, 9, 909, 1441, 2169, 381, 910, 23, 1, 91, 113, 114, 904, 4, 26, 1436, 190, 7, 75, 5, 477, 1437, 1438, 478, 1439, 95, 7, 72, 377, 1440, 9, 909, 1441, 477], [30, 73, 13, 109, 10, 59, 47, 4, 45, 43, 30, 73, 13, 233, 10, 59, 47, 4, 45, 43], [30, 73, 13, 109, 10, 59, 47, 4, 45, 43, 30, 73, 13, 233, 10, 59, 47, 4, 45, 43], [6, 15, 382, 108, 383, 87, 11, 531, 610, 15, 382, 108, 383, 87, 11, 2170, 58], [1, 2171, 351, 5, 7, 1126, 5, 1442, 130, 1127, 524, 11, 206, 1443, 611, 14, 7, 532, 911, 5, 1444, 1445, 2, 3, 329, 1, 2172, 2173, 351, 5, 7, 1126, 5, 1442, 130, 1127, 524, 11, 206, 1443, 611, 14, 7, 532, 911, 5, 533, 1445, 2, 3, 329], [1, 612, 1446, 13, 176, 8, 1, 1128, 213, 5, 1, 263, 278, 2, 3, 88, 7, 612, 1446, 23, 1, 188, 13, 176, 8, 1, 1128, 213, 5, 1, 263, 278, 2, 3, 88], [1, 118, 145, 13, 1, 2174, 756, 131, 183, 889, 2, 3, 156, 890, 756, 218, 5, 44, 63, 34, 95, 123, 757, 142, 891, 26, 1407, 118, 145, 13, 1, 1117, 183, 889, 2, 3, 156, 42, 240, 756, 218, 5, 44, 63, 34, 95, 123, 757, 142, 891, 26, 1408], [7, 330, 11, 352, 534, 115, 4, 534, 215, 129, 123, 97, 8, 912, 2, 3, 37, 241, 252, 352, 143, 48, 13, 76, 7, 479, 1129, 2175, 330, 11, 352, 534, 115, 129, 123, 97, 8, 912, 2, 3, 37, 241, 31, 613, 352, 143, 48, 13, 76, 7, 479, 1129, 62], [913, 184, 13, 7, 914, 231, 5, 48, 1130, 915, 2, 3, 37, 913, 184, 13, 68, 231, 5, 48, 1130, 915, 2, 3, 37], [771, 172, 2176, 1447, 13, 145, 5, 1, 1448, 4, 1449, 1450, 5, 133, 61, 1451, 1452, 8, 1, 207, 772, 134, 2, 3, 16, 1447, 13, 145, 5, 1, 2177, 1448, 4, 1449, 1450, 5, 133, 61, 1451, 1452, 8, 1, 207, 772, 134, 2, 3, 16], [11, 599, 6, 15, 1131, 279, 115, 4, 221, 11, 292, 916, 2, 3, 57, 42, 13, 145, 5, 1, 255, 1132, 292, 599, 2178, 48, 265, 6, 18, 1131, 279, 115, 4, 221, 11, 292, 916, 2, 3, 57, 42, 13, 145, 5, 1, 255, 1132, 292, 599, 2179], [6, 50, 7, 100, 41, 19, 23, 1, 917, 185, 5, 1, 44, 353, 22, 1133, 480, 63, 10, 1, 614, 20, 615, 87, 14, 1, 155, 116, 2180, 100, 41, 19, 33, 50, 23, 1, 917, 185, 5, 1, 44, 353, 22, 1133, 480, 63, 10, 1, 614, 20, 615, 87, 14, 155, 116, 173], [9, 535, 130, 384, 4, 2181, 6, 69, 112, 99, 918, 163, 10, 99, 918, 1453, 2, 3, 36, 9, 535, 130, 354, 4, 2182, 6, 69, 112, 99, 918, 163, 10, 99, 918, 1453, 2, 3, 36], [6, 18, 1, 445, 81, 5, 256, 416, 79, 2, 3, 25, 6, 18, 1, 445, 81, 5, 99, 79, 2, 3, 25], [111, 446, 8, 1, 679, 133, 385, 104, 919, 26, 124, 920, 4, 119, 680, 8, 1, 280, 773, 2183, 10, 536, 774, 536, 87, 111, 446, 8, 1, 679, 133, 13, 919, 26, 124, 920, 4, 119, 680, 10, 536, 774, 536, 87], [68, 103, 1, 1454, 82, 1455, 1456, 154, 681, 1457, 9, 1, 1458, 1134, 304, 8, 921, 2184, 103, 1, 1454, 82, 1455, 1456, 129, 123, 97, 34, 154, 681, 7, 1459, 9, 1, 1458, 1134, 304, 8, 921, 479], [6, 18, 537, 600, 378, 922, 4, 923, 36, 9, 1460, 1, 106, 5, 1, 19, 4, 1, 441, 2185, 1135, 4, 775, 2186, 11, 616, 617, 1461, 28, 924, 2187, 18, 537, 600, 378, 922, 4, 923, 36, 9, 1460, 1, 106, 5, 1, 19, 617, 1461, 28, 924, 600, 378, 13, 7, 526, 2188, 2189, 2190, 82, 42, 1462, 2191, 2192], [6, 18, 1, 39, 55, 52, 71, 2, 3, 37, 6, 18, 1, 39, 52, 14, 39, 191, 74, 71, 2, 3, 37], [776, 1136, 32, 1463, 10, 1464, 1137, 1465, 4, 1466, 117, 1136, 32, 1463, 10, 1464, 1137, 1465, 4, 1466, 117], [355, 136, 73, 5, 1, 46, 28, 33, 176, 26, 1467, 305, 4, 1, 777, 132, 5, 1, 63, 8, 1, 147, 28, 67, 417, 26, 246, 47, 4, 1468, 45, 47, 4, 45, 43, 355, 136, 73, 5, 1, 46, 28, 33, 176, 26, 1467, 305, 4, 1, 777, 132, 5, 1, 63, 8, 1, 2193, 4, 147, 28, 67, 417, 26, 246, 47, 4, 1468, 45, 47, 4, 45, 43], [6, 18, 1, 150, 527, 151, 2, 3, 25, 14, 331, 78, 166, 75, 9, 1469, 18, 150, 151, 2, 3, 25, 14, 1, 331, 78, 166, 75, 9, 778], [192, 6, 1138, 30, 73, 10, 59, 47, 4, 45, 43, 14, 1, 107, 356, 73, 538, 1139, 109, 30, 73, 10, 59, 47, 4, 45, 43, 14, 1, 107, 356, 73, 538, 82], [11, 231, 1470, 134, 4, 1471, 156, 779, 9, 1472, 332, 418, 5, 1, 139, 130, 293, 386, 72, 55, 2194, 231, 1470, 134, 4, 1471, 156, 779, 9, 1472, 332, 418, 5, 1, 139, 130, 293, 10, 91, 142, 5, 55, 2195], [1, 164, 33, 109, 447, 10, 1, 266, 164, 122, 267, 2, 3, 64, 1, 164, 33, 109, 10, 1, 266, 94, 122, 267, 2, 3, 64], [29, 97, 26, 448, 2, 3, 37, 481, 7, 1473, 128, 9, 216, 136, 2196, 72, 130, 142, 216, 97, 26, 448, 2, 3, 37, 481, 7, 1473, 128, 9, 216, 136, 142], [48, 22, 240, 925, 1474, 926, 247, 11, 333, 1475, 539, 1476, 1140, 926, 4, 248, 2197, 8, 326, 306, 419, 21, 1, 780, 540, 5, 1, 357, 22, 927, 2, 3, 2198, 351, 5, 326, 306, 419, 21, 1, 780, 540, 5, 1, 357, 22, 927, 2, 3, 25, 14, 1141, 1474, 926, 247, 11, 333, 1475, 539, 1476, 1140, 926, 4, 248, 1477], [51, 1478, 1142, 31, 679, 1, 22, 358, 442, 14, 7, 186, 171, 253, 4, 928, 52, 170, 1479, 4, 781, 156, 51, 682, 1142, 31, 679, 1, 22, 358, 442, 14, 7, 186, 171, 94, 4, 928, 52, 131, 1479, 4, 781, 156], [21, 1, 1480, 167, 5, 1481, 165, 683, 142, 129, 123, 15, 11, 1143, 11, 7, 684, 2199, 1, 1480, 167, 5, 1481, 165, 1143, 21, 1109, 144, 129, 123, 1482, 11, 7, 684, 479], [54, 12, 40, 13, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 541, 232, 24, 58, 23, 54, 12, 40, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 449], [1144, 35, 143, 5, 125, 14, 1483, 115, 33, 118, 97, 8, 482, 2, 3, 117, 1484, 33, 618, 1144, 8, 482, 2, 3, 117], [11, 359, 1, 101, 38, 29, 6, 15, 1, 177, 38, 20, 17, 12, 2, 3, 16, 8, 124, 60, 929, 359, 24, 38, 49, 1, 177, 38, 20, 17, 12, 2, 3, 16, 33, 15, 8, 124, 60, 930], [542, 334, 234, 931, 543, 4, 544, 387, 145, 5, 1, 255, 1145, 1485, 5, 137, 887, 306, 26, 420, 685, 782, 618, 137, 200, 2200, 334, 234, 931, 543, 4, 544, 387, 145, 5, 1, 255, 1145, 1485, 5, 137, 2201, 7, 189, 287, 5, 7, 137, 609, 31, 7, 137, 189, 1486], [8, 335, 1, 1487, 82, 8, 932, 2, 3, 37, 11, 126, 193, 173, 13, 76, 2202, 335, 1, 1487, 82, 932, 2, 3, 37, 11, 126, 193, 173, 33, 76, 15], [2203, 41, 686, 1488, 4, 1489, 1490, 1491, 40, 13, 61, 23, 1, 666, 34, 483, 783, 154, 104, 933, 26, 1492, 5, 933, 934, 5, 1, 304, 1493, 8, 2204, 41, 686, 1488, 4, 1489, 1490, 1491, 40, 13, 61, 23, 1, 666, 34, 483, 783, 154, 104, 933, 26, 1492, 5, 933, 934, 5, 1, 304, 8, 687], [6, 688, 70, 89, 5, 286, 667, 10, 1146, 1147, 935, 84, 6, 484, 1, 89, 5, 667, 10, 1146, 1147, 935, 84], [11, 1148, 56, 35, 235, 49, 154, 936, 21, 46, 23, 125, 174, 21, 85, 294, 1109, 358, 1494, 21, 1, 207, 1495, 4, 1496, 40, 8, 335, 56, 35, 235, 49, 154, 104, 1149, 26, 46, 23, 125, 174, 21, 85, 294, 1109, 358, 1497, 21, 1, 207, 1495, 4, 1496, 40], [83, 14, 94, 349, 10, 150, 151, 2, 3, 25, 83, 349, 10, 150, 151, 2, 3, 25], [2205, 937, 1, 2206, 19, 2207, 2208, 1150, 1, 1498, 1499, 2, 3, 165, 222, 13, 1, 525, 212, 5, 1500, 211, 689, 9, 104, 1501, 294, 1502, 545, 938, 1, 619, 9, 7, 886, 2209, 1498, 222, 1499, 2, 3, 165, 450, 1, 525, 212, 5, 1500, 211, 689, 9, 104, 1501, 294, 1502, 545, 938, 7, 29, 2210, 9, 7, 886, 60, 2211, 75], [8, 1, 308, 5, 48, 263, 6, 385, 104, 2212, 23, 1, 388, 189, 2213, 162, 138, 8, 451, 4, 690, 87, 42, 546, 23, 7, 1503, 784, 34, 937, 318, 1151, 9, 1504, 2214, 385, 939, 23, 1, 389, 189, 162, 138, 8, 451, 4, 690, 87, 42, 546, 23, 7, 1503, 784, 34, 937, 318, 1151, 9, 1504, 2215, 940], [54, 12, 40, 13, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 2216, 54, 22, 12, 40, 13, 66, 21, 1, 208, 5, 1, 203, 449], [6, 95, 15, 1505, 7, 1152, 108, 122, 9, 547, 1506, 1507, 4, 548, 1508, 57, 1, 452, 33, 175, 10, 1505, 7, 1152, 108, 122, 9, 547, 1506, 1507, 4, 548, 1508, 57], [6, 15, 1, 139, 147, 75, 15, 8, 448, 2, 3, 36, 11, 24, 281, 170, 6, 15, 1, 139, 147, 28, 31, 8, 448, 2, 3, 36], [1, 217, 137, 187, 485, 336, 2, 3, 90, 13, 7, 390, 22, 247, 14, 137, 242, 1509, 1, 549, 550, 551, 236, 5, 1, 217, 2217, 137, 187, 1, 217, 137, 187, 485, 13, 7, 22, 5, 549, 550, 551, 2218, 247, 14, 137, 242, 336, 2, 3, 90], [6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25, 6, 15, 7, 785, 81, 5, 1, 92, 152, 79, 2, 3, 25], [786, 1, 1153, 15, 30, 142, 691, 1154, 7, 941, 212, 5, 30, 218, 6, 76, 18, 1, 1510, 183, 1511, 2, 3, 64, 5, 1512, 30, 218, 1155, 21, 63, 34, 194, 190, 1156, 1513, 620, 8, 7, 390, 207, 1514, 391, 13, 1, 1510, 183, 1511, 2, 3, 64, 5, 1512, 63, 218, 1155, 21, 63, 34, 194, 190, 1156, 1513, 620, 8, 7, 390, 207, 22], [1, 46, 28, 5, 1, 159, 62, 13, 1, 552, 22, 392, 2, 3, 88, 42, 240, 621, 453, 26, 692, 5, 44, 1, 46, 28, 787, 26, 1, 62, 942, 622, 21, 1, 552, 22, 392, 2, 3, 88, 42, 240, 621, 453, 26, 692, 5, 44, 31, 7, 1515, 41], [51, 29, 81, 33, 233, 10, 553, 4, 1, 177, 56, 78, 20, 92, 79, 2, 3, 25, 51, 5, 1, 56, 78, 33, 233, 10, 92, 79, 2, 3, 25], [98, 129, 123, 268, 34, 7, 1516, 75, 5, 2219, 154, 104, 15, 9, 1157, 554, 161, 623, 11, 413, 783, 8, 483, 906, 4, 943, 288, 98, 129, 123, 684, 1158, 8, 483, 34, 7, 1516, 75, 5, 1457, 21, 7, 161, 154, 104, 2220, 294, 2221, 8, 269, 9, 554, 1, 623, 8, 413, 783, 906, 4, 943, 288], [8, 1, 88, 29, 6, 944, 15, 1159, 486, 1517, 2, 3, 117, 8, 24, 29, 6, 15, 1, 195, 486, 176, 26, 1159, 1517, 2, 3, 117], [393, 6, 76, 225, 1, 693, 5, 1, 624, 126, 178, 14, 30, 178, 788, 2, 3, 25, 26, 1160, 789, 31, 93, 8, 7, 531, 61, 790, 2222, 76, 555, 1, 693, 5, 1, 624, 126, 178, 14, 1, 2223, 178, 26, 1160, 789, 31, 93, 788, 2, 3, 25, 1161, 14, 1, 101, 93, 8, 1, 531, 171], [172, 381, 32, 61, 23, 1, 91, 113, 42, 270, 34, 63, 945, 8, 1, 139, 149, 237, 9, 95, 72, 157, 114, 127, 625, 1518, 360, 1, 60, 91, 113, 42, 270, 34, 63, 945, 8, 1, 139, 149, 237, 9, 95, 72, 157, 114, 127], [51, 361, 67, 233, 10, 1, 266, 556, 164, 122, 267, 2, 3, 64, 1, 361, 67, 414, 10, 1, 266, 556, 164, 122, 267, 2, 3, 64], [555, 9, 249, 1162, 626, 523, 32, 946, 241, 76, 454, 2224, 9, 249, 1162, 626, 523, 32, 946, 31, 350, 31, 2225, 667], [11, 46, 6, 18, 243, 244, 4, 245, 201, 11, 487, 14, 68, 331, 78, 166, 5, 1519, 18, 243, 244, 4, 245, 201, 11, 487, 14, 331, 78, 166, 5, 488], [11, 24, 184, 6, 18, 489, 791, 1, 490, 99, 108, 152, 557, 2, 3, 90, 42, 13, 1163, 9, 133, 215, 558, 14, 390, 1164, 5, 93, 4, 390, 2226, 6, 18, 1, 490, 99, 152, 557, 2, 3, 90, 31, 98, 13, 1163, 9, 133, 215, 558, 14, 390, 1164, 5, 93, 4, 306], [1, 118, 112, 58, 2227, 1, 792, 5, 1, 195, 5, 694, 695, 8, 1, 39, 195, 187, 362, 2, 3, 2228, 118, 112, 58, 2229, 2230, 1, 195, 5, 694, 695, 362, 2, 3, 88], [6, 15, 254, 1, 947, 948, 232, 386, 559, 1520, 1521, 31, 233, 26, 455, 2, 3, 560, 14, 696, 415, 5, 170, 170, 1165, 1522, 1, 1523, 1524, 15, 254, 1, 947, 948, 232, 386, 559, 1520, 1521, 31, 233, 26, 455, 2, 3, 560, 14, 696, 415, 5, 533, 107, 793], [6, 15, 254, 1, 947, 948, 232, 455, 2, 3, 560, 14, 696, 415, 5, 170, 170, 1165, 1522, 1, 1523, 1524, 15, 254, 1, 947, 948, 232, 455, 2, 3, 560, 14, 696, 415, 5, 533, 107, 793], [1, 888, 279, 755, 29, 33, 1166, 11, 7, 2231, 5, 394, 539, 1525, 1526, 36, 1, 888, 279, 755, 29, 13, 7, 2232, 755, 29, 1166, 11, 697, 394, 1526, 36], [1, 126, 1167, 67, 421, 26, 949, 5, 1168, 30, 132, 153, 14, 59, 47, 4, 45, 43, 1, 126, 193, 33, 421, 2233, 1168, 30, 132, 153, 14, 59, 47, 4, 45, 43], [30, 73, 13, 109, 10, 59, 47, 4, 45, 43, 30, 73, 13, 109, 14, 59, 47, 4, 45, 43], [6, 794, 14, 697, 950, 5, 1169, 1170, 10, 422, 28, 4, 280, 561, 2, 3, 2234, 561, 2, 3, 90, 6, 76, 794, 14, 10, 112, 456, 5, 1169, 795, 14, 332, 950, 5, 1170], [1171, 2, 3, 16, 216, 1, 936, 21, 250, 1527, 31, 1, 1172, 8, 1, 78, 1528, 14, 250, 2235, 2, 3, 16, 337, 936, 21, 250, 1527, 8, 363, 5, 1, 1172, 8, 78, 1528], [6, 66, 7, 155, 116, 291, 100, 41, 19, 10, 1, 44, 140, 5, 1, 46, 28, 4, 109, 796, 14, 102, 96, 25, 46, 4, 796, 5, 7, 155, 116, 291, 170, 951, 41, 19, 32, 233, 23, 1, 44, 140, 5, 1, 46, 28, 10, 102, 96, 25], [7, 1173, 2236, 115, 154, 104, 289, 8, 1529, 4, 627, 36, 1, 1173, 1174, 154, 104, 289, 8, 1529, 4, 627, 36], [1, 118, 19, 6, 952, 13, 61, 23, 1, 953, 238, 309, 41, 19, 5, 457, 2, 3, 117, 1, 19, 31, 138, 1175, 2237, 13, 1530, 9, 1, 953, 238, 309, 41, 19, 1176, 5, 457, 2, 3, 117], [1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16, 1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [1, 184, 58, 67, 196, 179, 10, 1, 1531, 108, 295, 288, 169, 190, 1532, 14, 7, 921, 162, 94, 2238, 184, 2239, 67, 196, 179, 10, 1, 1531, 108, 295, 288, 169, 190, 1532, 14, 1, 107, 226, 162, 11, 1, 60, 250, 143], [1, 46, 28, 5, 1, 159, 62, 13, 1, 552, 22, 392, 2, 3, 88, 42, 240, 621, 453, 26, 692, 5, 1533, 46, 28, 11, 1, 62, 13, 21, 1, 552, 22, 392, 2, 3, 88, 68, 2240, 310, 5, 621, 453, 26, 2241, 692, 5, 44], [2242, 1534, 2, 3, 88, 13, 1, 1535, 907, 169, 257, 251, 11, 2243, 1534, 2, 3, 88, 13, 1, 255, 2244, 220, 257, 251, 11, 954], [11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16, 11, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [955, 797, 1177, 49, 232, 1536, 8, 1537, 479, 956, 2, 3, 37, 955, 290, 1177, 49, 232, 1536, 8, 1537, 479, 956, 2, 3, 37], [1, 100, 209, 41, 19, 33, 50, 10, 102, 96, 25, 6, 50, 68, 44, 100, 41, 19, 10, 102, 96, 25], [11, 51, 58, 6, 15, 1, 17, 38, 29, 12, 2, 3, 16, 11, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [6, 337, 24, 82, 23, 1, 280, 28, 456, 423, 2245, 422, 75, 5, 1, 357, 28, 176, 26, 1, 1538, 159, 62, 1539, 2, 3, 64, 423, 2246, 147, 75, 5, 1, 357, 28, 176, 26, 1, 1538, 159, 62, 1539, 2, 3, 64], [6, 18, 1, 17, 27, 35, 29, 12, 2, 3, 16, 9, 424, 24, 2247, 18, 1, 258, 27, 56, 35, 29, 17, 12, 2, 3, 16, 9, 262, 24, 56, 35, 58], [241, 6, 1178, 459, 1540, 5, 1, 46, 28, 15, 8, 448, 2, 3, 36, 31, 24, 46, 28, 4, 1, 1541, 31, 1, 422, 28, 31, 268, 8, 193, 170, 786, 1, 46, 28, 15, 8, 448, 2, 3, 36, 13, 1530, 31, 1, 2248, 5, 24, 46, 4, 422, 28, 6, 1542, 98, 31, 2249, 8, 193, 425], [1, 259, 222, 562, 1, 1543, 5, 698, 206, 51, 699, 9, 253, 8, 24, 957, 14, 1179, 9, 7, 296, 35, 14, 7, 798, 11, 799, 491, 563, 2, 3, 156, 259, 222, 48, 222, 562, 1, 1543, 5, 800, 2250, 2251, 4, 2252, 14, 1179, 9, 7, 958, 75, 5, 296, 491, 14, 7, 798, 11, 2253, 799, 125, 563, 2, 3, 156], [1, 46, 28, 5, 1, 159, 62, 13, 1, 552, 22, 392, 2, 3, 88, 42, 240, 621, 453, 26, 692, 5, 1533, 46, 28, 176, 11, 1, 62, 13, 7, 388, 5, 1, 552, 2254, 22, 392, 2, 3, 88, 42, 2255, 621, 453, 8, 44, 26, 2256, 190, 1, 801, 2257, 5, 2258], [147, 28, 33, 564, 21, 1, 220, 1544, 801, 22, 1545, 4, 1546, 36, 1547, 959, 7, 1180, 5, 1548, 4, 21, 105, 1, 960, 4, 453, 1549, 5, 1, 1181, 459, 1, 183, 5, 2259, 4, 2260, 88, 42, 33, 564, 21, 1, 220, 1544, 801, 22, 1547, 1545, 4, 1546, 36, 959, 7, 1180, 5, 1548, 4, 21, 105, 1, 960, 4, 453, 1549, 5, 1, 22], [6, 216, 70, 89, 10, 364, 311, 297, 260, 14, 141, 77, 80, 12, 36, 1, 70, 89, 395, 10, 364, 311, 802, 32, 460, 14, 141, 77, 80, 12, 36], [6, 18, 1, 44, 540, 5, 1, 1550, 40, 293, 303, 183, 961, 2, 3, 37, 6, 337, 24, 293, 303, 29, 23, 1, 44, 540, 5, 1, 1550, 40, 22, 961, 2, 3, 37], [48, 28, 33, 803, 11, 1, 53, 565, 1551, 962, 2, 3, 53, 4, 351, 5, 1552, 136, 218, 14, 1553, 11, 46, 963, 31, 7, 422, 2261, 2262, 2263, 804, 1182, 13, 21, 62, 83, 5, 1, 53, 565, 1551, 962, 2, 3, 53, 4, 351, 5, 1552, 247, 136, 218, 14, 1553, 11, 46, 963, 31, 7, 422, 75], [1, 227, 19, 15, 11, 2264, 227, 13, 7, 964, 805, 256, 1183, 1554, 1555, 2, 3, 16, 268, 8, 426, 325, 24, 700, 227, 19, 13, 1, 964, 805, 256, 1183, 1554, 1555, 2, 3, 16, 268, 8, 426, 94], [441, 1556, 1557, 701, 13, 7, 806, 19, 42, 1558, 7, 566, 19, 702, 965, 31, 7, 1184, 528, 5, 441, 775, 966, 5, 441, 775, 2265, 1556, 1557, 701, 13, 7, 806, 19, 42, 1558, 7, 566, 19, 2266, 31, 7, 1559, 5, 63, 702, 965, 31, 7, 1184, 528, 5, 441, 775], [91, 113, 234, 114, 127, 967, 34, 63, 34, 194, 8, 1, 139, 308, 237, 9, 95, 72, 1560, 492, 461, 1, 91, 113, 114, 127, 42, 270, 34, 63, 34, 194, 8, 1, 139, 149, 237, 9, 95, 72, 365], [91, 113, 234, 114, 127, 967, 34, 63, 34, 194, 8, 1, 139, 308, 237, 9, 95, 72, 1560, 492, 461, 1, 91, 113, 114, 127, 42, 270, 34, 63, 34, 194, 8, 1, 139, 149, 237, 9, 95, 72, 365], [8, 57, 1561, 968, 968, 2, 3, 57, 15, 1562, 163, 23, 447, 247, 28, 42, 2267, 2268, 5, 2269, 57, 1561, 968, 968, 2, 3, 57, 15, 1562, 163, 23, 447, 247, 28, 4, 807, 925, 2270, 623], [6, 66, 7, 100, 41, 19, 23, 1, 44, 140, 5, 703, 10, 102, 96, 25, 6, 66, 7, 155, 116, 291, 100, 41, 19, 10, 1, 44, 140, 5, 1, 46, 28, 4, 109, 796, 14, 102, 96, 25], [1, 1185, 493, 65, 32, 51, 366, 312, 163, 50, 23, 1, 367, 207, 313, 100, 22, 969, 367, 22, 180, 4, 246, 37, 1, 205, 65, 32, 50, 23, 1563, 44, 28, 1, 367, 207, 313, 100, 22, 969, 367, 180, 4, 246, 37, 14, 1, 366, 312, 1186, 103], [6, 396, 704, 1564, 2, 3, 90, 23, 1, 30, 1565, 26, 447, 970, 494, 1187, 11, 2271, 4, 892, 63, 462, 193, 494, 6, 192, 396, 704, 1564, 2, 3, 90, 23, 1, 2272, 4, 2273, 1565, 26, 970, 494, 1187, 11, 2274, 4, 2275, 611, 4, 2276, 462, 193, 83], [172, 381, 32, 61, 23, 1, 91, 113, 42, 270, 34, 63, 945, 8, 1, 139, 149, 237, 9, 95, 72, 157, 114, 127, 98, 492, 461, 1, 91, 113, 114, 127, 42, 270, 34, 63, 34, 194, 8, 1, 139, 149, 237, 9, 95, 72, 365], [172, 188, 2277, 1, 936, 5, 10, 971, 61, 93, 4, 2278, 1, 113, 34, 971, 61, 93, 808, 7, 700, 75, 5, 892, 93, 34, 1566, 1, 777, 459, 93, 451, 84, 24, 113, 13, 34, 1, 971, 61, 93, 808, 7, 700, 75, 5, 892, 93, 34, 1566, 1, 777, 459, 93, 21, 451, 84, 4, 1, 971, 61, 93, 385, 1157, 554], [1567, 1568, 4, 1569, 1570, 129, 1571, 30, 2279, 1568, 4, 1569, 1570, 13, 75, 5, 1571, 30, 218], [1, 89, 395, 67, 109, 10, 1, 77, 80, 82, 12, 36, 70, 89, 395, 32, 109, 10, 77, 80, 12, 36], [1, 107, 1572, 495, 103, 13, 1188, 567, 956, 4, 1573, 16, 1, 495, 13, 809, 196, 179, 10, 1, 1188, 567, 103, 956, 4, 1573, 16], [368, 28, 13, 1189, 15, 9, 972, 1, 304, 5, 28, 1574, 1575, 4, 1576, 25, 368, 28, 254, 2280, 1, 304, 5, 28, 1574, 1575, 4, 1576, 25], [51, 58, 67, 196, 179, 10, 1, 177, 38, 20, 17, 12, 2, 3, 16, 51, 1, 58, 32, 196, 179, 8, 17, 20, 12, 2, 3, 16], [118, 6, 396, 705, 9, 535, 212, 4, 1577, 61, 23, 30, 1190, 249, 338, 339, 4, 186, 2281, 192, 396, 705, 9, 535, 212, 4, 1577, 11, 1, 1191, 61, 23, 30, 1190, 30, 2282, 338, 339, 4, 119, 568], [496, 99, 295, 87, 13, 7, 82, 61, 23, 158, 168, 668, 489, 11, 42, 6, 18, 254, 226, 669, 9, 2283, 973, 2284, 48, 62, 6, 18, 2285, 295, 87, 42, 13, 7, 82, 61, 23, 158, 168, 668, 489], [7, 327, 612, 497, 5, 1, 62, 154, 104, 289, 8, 1192, 2, 3, 974, 7, 2286, 497, 5, 1, 22, 4, 975, 154, 104, 289, 8, 2287, 497, 263, 1192, 2, 3, 974], [7, 189, 162, 369, 13, 7, 1578, 162, 1579, 288, 524, 206, 218, 5, 2288, 162, 2289, 1120, 32, 1578, 669, 1579, 288, 524, 206, 218, 5, 200], [6, 121, 672, 24, 323, 128, 11, 412, 324, 221, 4, 322, 443, 4, 438, 64, 11, 221, 4, 322, 6, 121, 672, 24, 323, 167, 443, 4, 438, 64], [1580, 314, 134, 36, 13, 15, 11, 1, 143, 6, 15, 1, 2290, 143, 314, 134, 36], [6, 15, 382, 108, 383, 87, 11, 531, 610, 15, 382, 20, 383, 87, 11, 531, 81], [6, 976, 7, 1193, 330, 14, 1194, 78, 4, 1581, 282, 9, 424, 1, 412, 19, 627, 4, 498, 25, 24, 412, 227, 19, 2291, 7, 1193, 330, 14, 1194, 78, 4, 1581, 282, 627, 4, 498, 25, 1121, 21, 7, 1195, 1193, 227, 19], [1, 164, 33, 109, 10, 1, 266, 94, 122, 267, 2, 3, 64, 1, 361, 67, 414, 10, 1, 266, 556, 164, 122, 267, 2, 3, 64], [11, 359, 1, 30, 73, 65, 6, 18, 977, 674, 4, 499, 90, 9, 121, 1, 30, 73, 65, 6, 15, 1, 977, 152, 674, 4, 499, 90], [1, 1196, 5, 1, 164, 33, 397, 10, 1, 1582, 978, 1583, 197, 6, 397, 164, 1196, 26, 10, 1, 1582, 978, 1583, 197], [98, 492, 461, 1, 91, 113, 114, 127, 42, 270, 34, 63, 34, 194, 8, 1, 139, 149, 237, 9, 95, 72, 979, 65, 5, 157, 360, 1, 91, 113, 114, 127, 42, 270, 34, 112, 63, 34, 194, 8, 72, 149, 95, 72, 365], [48, 183, 13, 1584, 5, 2292, 2293, 5, 125, 21, 1, 1585, 340, 340, 83, 8, 1586, 2, 3, 87, 11, 7, 1197, 5, 2294, 2295, 183, 13, 1584, 5, 1587, 136, 2296, 21, 58, 94, 1585, 4, 131, 2297, 2298, 8, 1586, 2, 3, 87, 11, 7, 1197, 5, 2299, 125], [1, 89, 395, 67, 109, 10, 1, 77, 80, 82, 12, 36, 7, 70, 89, 147, 33, 109, 10, 1, 77, 80, 82, 12, 36], [6, 18, 1, 60, 73, 122, 59, 47, 4, 45, 43, 9, 30, 341, 1, 85, 500, 18, 1, 59, 122, 47, 4, 45, 43, 9, 341, 63, 8, 24, 85, 144], [6, 18, 1, 60, 73, 122, 59, 47, 4, 45, 43, 9, 30, 341, 1, 85, 500, 18, 1, 59, 122, 47, 4, 45, 43, 9, 341, 63, 8, 24, 85, 144], [1, 669, 32, 1588, 10, 1198, 980, 199, 1589, 1590, 4, 1591, 37, 1198, 980, 199, 1589, 1590, 4, 1591, 37], [9, 2300, 48, 569, 2, 3, 57, 15, 1592, 1593, 14, 84, 1594, 9, 981, 1595, 946, 5, 63, 23, 7, 22, 5, 1199, 1596, 2301, 810, 167, 7, 22, 5, 1199, 1596, 2302, 33, 15, 26, 569, 2, 3, 57, 9, 981, 1595, 946, 5, 1, 63, 8, 1, 147, 456, 10, 1, 1592, 1593, 23, 84, 1594], [6, 216, 70, 89, 10, 364, 311, 297, 260, 14, 141, 77, 80, 12, 36, 6, 752, 89, 10, 141, 77, 80, 12, 36], [6, 18, 1, 39, 55, 52, 342, 4, 86, 53, 190, 48, 811, 4, 95, 318, 794, 14, 2303, 8, 48, 167, 6, 18, 1, 39, 238, 55, 52, 342, 4, 86, 53], [982, 7, 1597, 33, 181, 4, 983, 283, 706, 570, 2, 3, 53, 7, 279, 115, 122, 11, 292, 2304, 570, 2, 3, 53, 13, 7, 279, 115, 4, 221, 122, 5, 292], [1598, 165, 129, 97, 7, 704, 82, 11, 30, 812, 221, 1598, 165, 97, 290, 7, 82, 11, 30, 812, 221, 42, 6, 751, 9, 31, 427, 704], [6, 76, 501, 1, 323, 258, 286, 21, 7, 1599, 38, 29, 1200, 2, 3, 53, 14, 1, 259, 5, 2305, 76, 501, 1, 188, 21, 38, 19, 1200, 2, 3, 53, 31, 7, 2306], [6, 15, 243, 244, 4, 245, 53, 14, 7, 78, 166, 5, 1600, 18, 1, 243, 244, 4, 245, 53, 103, 9, 1201, 1, 1601, 5, 1, 315, 11, 228, 2307, 4, 228, 2308, 14, 7, 78, 166, 5, 494], [91, 379, 13, 61, 23, 1, 666, 34, 813, 814, 8, 205, 63, 1, 157, 5, 7, 30, 13, 601, 9, 1, 149, 98, 2128, 2129, 2130, 1396, 2131, 1397, 1398, 7, 30, 26, 1, 1110, 98, 1111, 813, 814], [8, 269, 9, 707, 1, 628, 257, 142, 369, 984, 8, 1, 1601, 2309, 4, 768, 1602, 7, 815, 30, 816, 13, 1603, 571, 1, 91, 115, 5, 1, 1202, 22, 463, 2, 3, 57, 1, 815, 30, 816, 13, 1603, 571, 1, 91, 115, 5, 1, 1202, 22, 463, 2, 3, 57], [6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25, 6, 15, 1, 92, 20, 9, 69, 24, 163, 79, 2, 3, 25], [1, 35, 19, 33, 50, 26, 59, 47, 4, 45, 43, 4, 1, 629, 33, 50, 26, 1, 2310, 70, 41, 343, 20, 2311, 1604, 4, 1605, 329, 1, 528, 2312, 13, 260, 10, 7, 708, 629, 41, 19, 34, 33, 50, 10, 1, 2313, 41, 343, 20, 1604, 4, 1605, 329], [48, 13, 7, 2314, 5, 1, 1606, 2315, 8, 1607, 4, 1608, 110, 48, 13, 72, 9, 1, 1606, 2316, 8, 1607, 4, 1608, 110], [6, 15, 60, 163, 169, 8, 92, 152, 79, 2, 3, 25, 6, 15, 7, 785, 81, 5, 1, 92, 152, 79, 2, 3, 25], [6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25, 11, 1, 226, 271, 199, 81, 6, 15, 92, 79, 2, 3, 25], [11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16, 393, 6, 15, 17, 20, 31, 27, 296, 12, 2, 3, 16], [253, 30, 132, 32, 417, 26, 1203, 1, 28, 8, 105, 398, 14, 59, 170, 4, 817, 1, 112, 50, 132, 47, 4, 45, 43, 101, 30, 132, 67, 153, 26, 464, 59, 8, 105, 398, 4, 817, 10, 1, 356, 370, 47, 4, 45, 43], [1, 818, 807, 26, 1, 425, 951, 1609, 371, 8, 1, 572, 2317, 422, 75, 33, 2318, 1610, 2319, 153, 14, 7, 60, 985, 41, 19, 14, 819, 4, 116, 173, 630, 4, 45, 165, 1, 41, 19, 13, 7, 100, 14, 819, 4, 116, 173, 630, 4, 45, 165], [6, 15, 60, 163, 169, 8, 92, 152, 79, 2, 3, 25, 6, 15, 1, 92, 20, 9, 69, 24, 163, 79, 2, 3, 25], [70, 56, 35, 13, 809, 109, 10, 27, 49, 12, 2, 3, 16, 98, 13, 7, 60, 27, 56, 35, 19, 12, 2, 3, 16], [249, 338, 2, 3, 339, 13, 68, 428, 685, 257, 399, 42, 240, 130, 333, 1119, 44, 2320, 249, 428, 257, 399, 338, 2, 3, 339], [6, 18, 1, 445, 81, 5, 256, 416, 79, 2, 3, 25, 6, 15, 7, 785, 81, 5, 1, 92, 152, 79, 2, 3, 25], [6, 986, 1, 1204, 5, 111, 522, 14, 148, 135, 110, 6, 15, 1, 1205, 148, 135, 110, 9, 986, 522, 1611], [24, 133, 686, 481, 1, 372, 41, 20, 272, 273, 2, 3, 57, 186, 198, 33, 2321, 10, 1, 372, 41, 20, 272, 273, 2, 3, 57], [328, 15, 1, 1206, 164, 122, 266, 267, 2, 3, 64, 11, 1, 164, 1, 164, 33, 109, 447, 10, 1, 266, 164, 122, 267, 2, 3, 64], [24, 29, 631, 8, 987, 62, 94, 195, 115, 8, 465, 988, 2, 3, 88, 6, 631, 8, 105, 573, 7, 4, 989, 5, 987, 62, 94, 195, 115, 8, 465, 988, 2, 3, 88, 14, 68, 990, 5, 24, 991, 29], [1, 44, 133, 33, 181, 10, 1, 30, 274, 1612, 21, 272, 273, 2, 3, 57, 6, 1613, 1, 133, 10, 1, 107, 1614, 21, 272, 273, 2, 3, 57], [1, 1615, 67, 373, 10, 1, 39, 120, 108, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [70, 56, 35, 13, 809, 109, 10, 27, 49, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [1, 44, 140, 33, 181, 10, 1, 17, 20, 12, 2, 3, 16, 1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16], [6, 50, 68, 44, 100, 41, 19, 10, 102, 96, 25, 6, 66, 7, 629, 41, 19, 14, 116, 173, 10, 102, 20, 96, 25], [51, 5, 1, 133, 28, 21, 2322, 33, 181, 10, 1, 272, 632, 273, 2, 3, 57, 6, 1613, 1, 133, 10, 1, 107, 1614, 21, 272, 273, 2, 3, 57], [24, 56, 35, 49, 32, 50, 10, 17, 131, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16, 11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16], [6, 121, 672, 24, 323, 128, 11, 412, 324, 221, 4, 322, 443, 4, 438, 64, 1207, 1616, 24, 323, 1208, 128, 11, 324, 221, 443, 4, 438, 64], [6, 15, 243, 244, 4, 245, 53, 14, 7, 78, 166, 5, 2323, 6, 18, 243, 244, 4, 245, 53, 14, 7, 78, 166, 5, 2324, 4, 7, 676, 415, 5, 1587], [11, 51, 58, 6, 15, 1, 17, 38, 29, 12, 2, 3, 16, 11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16], [1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16, 1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16], [11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16, 11, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16, 118, 6, 15, 1, 17, 20, 12, 2, 3, 16, 11, 70, 56, 35], [11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16, 1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [6, 820, 35, 65, 10, 1, 27, 17, 12, 2, 3, 16, 38, 1209, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [6, 15, 429, 180, 84, 50, 23, 1, 992, 46, 1617, 984, 1, 429, 171, 180, 84, 42, 33, 50, 23, 1, 2325, 502, 46, 28], [1, 1615, 67, 373, 10, 1, 39, 120, 108, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [6, 820, 35, 65, 10, 1, 27, 17, 12, 2, 3, 16, 38, 1209, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [1, 446, 467, 993, 13, 2326, 14, 1, 1210, 566, 467, 8, 1, 147, 310, 702, 4, 1618, 387, 1619, 13, 7, 60, 70, 82, 34, 2327, 1, 467, 5, 7, 446, 8, 7, 709, 566, 14, 124, 1210, 566, 467, 8, 710, 18, 702, 4, 1618, 387], [6, 2328, 1, 70, 89, 5, 667, 8, 222, 14, 68, 821, 1211, 147, 344, 1620, 965, 2329, 7, 454, 2330, 8, 2331, 2332, 1621, 70, 89, 5, 1, 711, 8, 633, 83, 222, 11, 112, 994, 574, 68, 821, 1211, 147, 1620, 965], [7, 330, 11, 352, 534, 115, 4, 534, 215, 129, 123, 97, 8, 912, 2, 3, 37, 4, 7, 612, 115, 5, 1, 153, 188, 129, 123, 196, 2333, 330, 11, 352, 534, 115, 129, 123, 97, 8, 912, 2, 3, 37, 241, 31, 613, 352, 143, 48, 13, 76, 7, 479, 1129, 62], [11, 231, 316, 2, 3, 57, 289, 34, 1, 528, 5, 2334, 358, 13, 318, 1622, 7, 914, 2335, 5, 352, 2336, 2, 3, 57, 2337, 34, 145, 1623, 13, 34, 1, 995, 369, 5, 996, 65, 937, 318, 1622, 1624, 350, 14, 352, 1212], [23, 1, 298, 140, 6, 15, 1, 279, 2338, 138, 8, 1625, 2, 3, 57, 50, 23, 1, 46, 28, 5, 1626, 2339, 9, 262, 30, 575, 4, 119, 198, 4, 15, 1, 2340, 2341, 29, 1625, 2, 3, 57, 50, 23, 1, 46, 28, 33, 15, 9, 262, 30, 575, 4, 198, 4, 1, 101, 52, 33, 15, 9, 160, 1, 125, 8, 1, 353, 22], [223, 10, 1, 17, 27, 161, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [223, 10, 1, 17, 27, 161, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 223, 10, 1, 17, 27, 161, 12, 2, 3, 16], [192, 1, 1213, 28, 33, 109, 11, 1214, 119, 198, 227, 1215, 4, 1627, 10, 39, 120, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [6, 214, 77, 80, 12, 36, 9, 216, 70, 89, 228, 410, 5, 24, 65, 555, 9, 7, 1628, 216, 89, 5, 188, 10, 77, 80, 190, 228, 410, 12, 36], [48, 29, 481, 1, 2342, 1629, 401, 138, 26, 1630, 2, 3, 201, 359, 23, 167, 26, 2343, 2, 3, 53, 6, 1631, 1, 1629, 401, 14, 1632, 97, 26, 1630, 2, 3, 201], [1, 41, 19, 13, 7, 100, 102, 96, 25, 19, 50, 10, 822, 14, 155, 116, 173, 4, 559, 1633, 66, 7, 629, 41, 19, 14, 116, 173, 10, 102, 20, 96, 25], [402, 712, 230, 548, 1634, 15, 8, 24, 19, 32, 823, 14, 503, 468, 2, 3, 16, 1, 161, 13, 175, 14, 402, 712, 230, 548, 997, 10, 60, 476, 169, 8, 1, 503, 998, 468, 2, 3, 16], [402, 712, 230, 548, 1634, 15, 8, 24, 19, 32, 823, 14, 503, 468, 2, 3, 16, 1, 161, 13, 175, 14, 402, 712, 230, 548, 997, 10, 60, 476, 169, 8, 1, 503, 998, 468, 2, 3, 16], [192, 1, 1213, 28, 33, 109, 11, 1214, 119, 198, 227, 1215, 4, 1627, 10, 39, 120, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [98, 13, 7, 2344, 5, 1, 19, 97, 26, 1635, 2, 3, 57, 24, 118, 101, 13, 2345, 7, 2346, 576, 184, 61, 23, 1, 167, 5, 1635, 2, 3, 57], [6, 18, 92, 79, 2, 3, 25, 1, 56, 78, 403, 11, 553, 11, 1216, 1, 332, 2347, 15, 1, 92, 56, 78, 403, 79, 2, 3, 25, 11, 105, 1216, 24, 215, 65, 4, 1636, 70, 250, 824], [6, 18, 1, 577, 119, 578, 1217, 5, 579, 2, 3, 64, 48, 8, 1637, 546, 23, 1, 139, 1218, 250, 19, 809, 1638, 21, 7, 159, 186, 119, 287, 290, 31, 1, 577, 119, 578, 5, 579, 2, 3, 64], [6, 18, 1, 577, 119, 578, 1217, 5, 579, 2, 3, 64, 48, 8, 1637, 546, 23, 1, 139, 1218, 250, 19, 809, 1638, 21, 7, 159, 186, 119, 287, 290, 31, 1, 577, 119, 578, 5, 579, 2, 3, 64], [1, 531, 13, 50, 10, 2348, 21, 1, 280, 1218, 682, 423, 706, 13, 7, 1219, 169, 122, 11, 279, 115, 4, 221, 5, 2349, 4, 2350, 133, 570, 2, 3, 2351, 570, 2, 3, 53, 13, 7, 279, 115, 4, 221, 122, 5, 292], [11, 41, 343, 6, 50, 7, 1220, 100, 116, 291, 371, 19, 23, 1, 209, 386, 44, 140, 5, 1, 46, 825, 10, 102, 96, 25, 6, 66, 7, 155, 630, 45, 291, 100, 41, 19, 10, 1, 44, 140, 5, 1, 46, 28, 4, 109, 796, 14, 102, 96, 25, 325], [14, 1, 46, 999, 5, 1, 17, 20, 12, 2, 3, 16, 6, 442, 1, 46, 144, 14, 229, 1221, 8, 1, 17, 20, 12, 2, 3, 16], [1, 30, 73, 33, 50, 10, 59, 47, 4, 45, 43, 14, 1, 580, 356, 73, 538, 1139, 109, 30, 73, 10, 59, 47, 4, 45, 43, 14, 1, 107, 356, 73, 538, 82], [6, 18, 226, 489, 21, 490, 4, 489, 14, 753, 162, 21, 404, 316, 4, 134, 25, 31, 24, 1222, 6, 18, 404, 14, 7, 226, 162, 316, 4, 134, 25], [6, 547, 1, 685, 2352, 8, 363, 5, 7, 1639, 29, 1640, 2, 3, 165, 6, 547, 24, 964, 1223, 103, 31, 7, 1639, 29, 1640, 2, 3, 165], [6, 18, 39, 52, 74, 71, 2, 3, 37, 9, 210, 160, 200, 4, 55, 761, 18, 1, 39, 55, 52, 74, 71, 2, 3, 37, 11, 430, 55, 431, 4, 186, 93], [8, 48, 167, 6, 18, 1, 39, 238, 55, 52, 342, 4, 86, 53, 8, 269, 9, 1000, 1, 2353, 2354, 6, 1224, 39, 52, 342, 4, 86, 53], [6, 18, 39, 52, 74, 71, 2, 3, 37, 9, 210, 160, 200, 4, 55, 761, 76, 210, 1, 55, 160, 5, 1, 125, 10, 1, 39, 52, 74, 71, 2, 3, 37], [1, 78, 103, 15, 8, 24, 826, 1001, 13, 1225, 1641, 581, 1, 1226, 184, 15, 8, 1, 58, 13, 1, 1225, 405, 189, 184, 1641, 581], [6, 18, 39, 52, 74, 71, 2, 3, 37, 9, 210, 160, 200, 4, 55, 761, 18, 1, 39, 55, 52, 74, 71, 2, 3, 37, 11, 430, 55, 431, 4, 186, 93], [91, 379, 462, 504, 4, 2355, 57, 11, 68, 2127, 13, 61, 23, 1, 1642, 34, 63, 34, 194, 8, 72, 149, 237, 9, 104, 766, 601, 114, 127, 1, 1002, 34, 13, 1, 255, 432, 546, 23, 1, 91, 113, 34, 2356, 1227, 1, 666, 34, 63, 14, 72, 157, 237, 9, 194, 8, 72, 149, 114, 127], [248, 167, 13, 236, 5, 1, 258, 292, 279, 171, 706, 570, 2, 3, 53, 706, 570, 2, 3, 53, 13, 7, 279, 115, 4, 221, 122, 5, 292], [6, 18, 39, 52, 74, 71, 2, 3, 37, 9, 210, 160, 200, 4, 55, 761, 76, 210, 1, 55, 160, 5, 1, 125, 10, 1, 39, 52, 74, 71, 2, 3, 37], [1572, 461, 1, 634, 128, 9, 27, 35, 47, 4, 45, 36, 8, 42, 1, 405, 469, 129, 1, 2357, 226, 827, 1643, 1644, 505, 713, 505, 633, 2358, 634, 128, 9, 27, 35, 47, 4, 45, 36, 635, 65, 1, 1645, 35, 377, 1646, 713, 83, 1647, 1648, 713, 505, 633, 83, 319, 505, 13, 1, 209, 1228], [46, 28, 32, 61, 23, 7, 1649, 5, 1229, 828, 44, 144, 94, 21, 1, 1003, 399, 1004, 84, 105, 2359, 144, 32, 169, 21, 1, 1003, 399, 1004, 84], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16], [100, 41, 65, 5, 1650, 4, 44, 67, 50, 10, 102, 96, 25, 6, 66, 7, 100, 41, 19, 23, 1, 44, 140, 5, 703, 10, 102, 96, 25], [6, 15, 1, 293, 215, 183, 5, 1, 565, 117, 62, 344, 1005, 2, 3, 117, 6, 397, 24, 19, 23, 7, 130, 293, 215, 62, 565, 117, 62, 344, 1005, 2, 3, 117], [6, 192, 232, 30, 73, 14, 59, 47, 4, 45, 43, 8, 105, 398, 14, 1, 107, 106, 15, 8, 1651, 109, 30, 73, 10, 59, 47, 4, 45, 43, 14, 1, 107, 356, 73, 538, 82], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 11, 51, 58, 6, 15, 1, 17, 38, 29, 12, 2, 3, 16], [1, 89, 395, 67, 109, 10, 1, 77, 80, 82, 12, 36, 1, 70, 89, 395, 10, 364, 311, 802, 32, 460, 14, 141, 77, 80, 12, 36], [51, 58, 67, 196, 179, 10, 1, 177, 38, 20, 17, 12, 2, 3, 16, 8, 124, 60, 1652, 580, 1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16], [172, 125, 95, 192, 104, 983, 283, 68, 2360, 264, 52, 1230, 1653, 84, 14, 380, 2361, 125, 32, 983, 283, 1, 1230, 264, 52, 1653, 84, 14, 1, 2362, 2363], [1, 41, 19, 13, 7, 100, 102, 96, 25, 19, 50, 10, 822, 14, 155, 116, 173, 4, 559, 2364, 100, 209, 41, 19, 33, 50, 10, 102, 96, 25], [504, 2, 3, 64, 714, 7, 1231, 103, 11, 1654, 411, 241, 1, 2365, 5, 1, 1654, 19, 4, 124, 1231, 103, 9, 2366, 13, 2367, 4, 2368, 6, 714, 68, 103, 11, 1655, 2369, 2370, 9, 1, 1231, 103, 5, 504, 2, 3, 64, 241, 1232, 9, 2371, 4, 424], [91, 113, 234, 114, 127, 967, 34, 63, 34, 194, 8, 1, 139, 308, 237, 9, 95, 72, 1656, 1657, 1658, 8, 1006, 1659, 72, 308, 5, 112, 363, 8, 1, 470, 4, 363, 34, 194, 8, 1, 139, 149, 237, 9, 95, 72, 365, 114, 127], [91, 113, 234, 114, 127, 967, 34, 63, 34, 194, 8, 1, 139, 308, 237, 9, 95, 72, 1656, 1657, 1658, 8, 1006, 1659, 72, 308, 5, 112, 363, 8, 1, 470, 4, 363, 34, 194, 8, 1, 139, 149, 237, 9, 95, 72, 365, 114, 127], [6, 18, 1, 39, 55, 52, 71, 2, 3, 37, 6, 118, 18, 7, 55, 52, 74, 71, 2, 3, 37, 9, 160, 111, 136, 4, 219, 1, 75, 5, 55, 242, 762, 14, 1, 136], [6, 76, 2372, 1, 340, 5, 829, 2, 3, 64, 23, 48, 2373, 76, 196, 179, 7, 1660, 433, 340, 319, 1, 1007, 32, 830, 61, 23, 1, 1233, 132, 153, 26, 1661, 340, 5, 829, 2, 3, 64], [6, 820, 35, 65, 10, 1, 27, 17, 12, 2, 3, 16, 38, 2374, 10, 1, 17, 27, 161, 12, 2, 3, 16], [119, 198, 33, 109, 14, 148, 135, 110, 1, 147, 75, 33, 224, 14, 1, 275, 148, 135, 110], [6, 15, 1, 1662, 715, 2, 3, 16, 11, 227, 2375, 1, 227, 2376, 1008, 15, 1662, 715, 2, 3, 16, 213, 2377], [215, 481, 1, 92, 553, 152, 79, 2, 3, 25, 6, 15, 7, 785, 81, 5, 1, 92, 152, 79, 2, 3, 25], [6, 18, 1, 39, 55, 52, 71, 2, 3, 37, 172, 93, 67, 153, 10, 1, 39, 52, 94, 71, 2, 3, 37], [6, 15, 243, 244, 4, 245, 53, 14, 7, 78, 166, 5, 1600, 15, 243, 31, 1, 527, 244, 4, 245, 53], [30, 73, 13, 109, 10, 59, 47, 4, 45, 43, 48, 13, 233, 10, 716, 19, 83, 471, 2, 3, 581, 4, 59, 47, 4, 45, 43], [1, 147, 75, 33, 224, 14, 1, 275, 148, 135, 110, 11, 1, 275, 140, 1, 148, 135, 110, 33, 15], [11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16, 223, 10, 1, 17, 27, 161, 12, 2, 3, 16], [119, 198, 33, 109, 14, 148, 135, 110, 119, 198, 13, 109, 10, 1, 1663, 189, 171, 135, 110], [11, 359, 1, 30, 73, 65, 6, 18, 977, 674, 4, 499, 90, 11, 30, 132, 6, 15, 977, 674, 4, 499, 90], [1, 831, 28, 13, 419, 21, 1, 54, 22, 12, 40, 51, 1, 28, 1009, 21, 54, 12, 40], [1, 2378, 22, 13, 54, 717, 12, 40, 1, 831, 28, 13, 419, 21, 1, 54, 22, 12, 40], [91, 65, 5, 157, 360, 1, 91, 113, 114, 127, 42, 270, 34, 112, 63, 34, 194, 8, 72, 149, 95, 72, 979, 142, 546, 23, 1, 91, 113, 34, 72, 363, 1010, 8, 72, 149, 114, 127], [223, 10, 1, 17, 27, 161, 12, 2, 3, 16, 1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16], [54, 94, 12, 40, 98, 13, 7, 22, 5, 85, 306, 8, 1011, 394, 21, 1, 208, 5, 1, 203, 449, 8, 24, 235, 58, 6, 2379, 275, 283, 572, 4, 18, 1, 280, 144, 9, 607, 24, 35, 49, 423, 54, 22, 12, 40, 1, 54, 85, 22, 13, 219], [6, 223, 70, 89, 395, 11, 259, 211, 24, 606, 1664, 29, 1, 101, 4, 1, 493, 1665, 49, 10, 141, 77, 80, 12, 36, 14, 507, 70, 89, 13, 484, 23, 1, 259, 314, 10, 141, 77, 80, 12, 36, 14, 699, 507, 4, 228, 410], [1, 44, 140, 33, 181, 10, 1, 17, 20, 12, 2, 3, 16, 1, 101, 385, 104, 417, 26, 1, 17, 38, 20, 12, 2, 3, 16], [6, 820, 35, 65, 10, 1, 27, 17, 12, 2, 3, 16, 38, 1209, 18, 1, 17, 27, 35, 29, 12, 2, 3, 16, 9, 424, 24, 65], [1, 101, 385, 104, 417, 26, 1, 17, 38, 20, 12, 2, 3, 16, 11, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [1, 1399, 636, 13, 402, 10, 1012, 1013, 333, 718, 4, 1014, 339, 7, 432, 216, 5, 48, 2380, 13, 1012, 1013, 333, 2381, 718, 4, 1014, 339], [1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16, 1, 1015, 32, 637, 10, 7, 27, 29, 66, 10, 17, 12, 2, 3, 16, 1, 1234, 75], [1, 2382, 198, 129, 123, 414, 14, 1, 189, 171, 135, 110, 119, 198, 13, 109, 10, 1, 1663, 189, 171, 135, 110], [11, 1, 2383, 5, 119, 568, 6, 18, 1, 1205, 148, 135, 110, 11, 105, 6, 18, 148, 135, 110, 14, 41, 1666, 65, 386, 275, 19, 11, 275, 306, 44, 11, 44, 306], [1, 831, 28, 13, 419, 21, 1, 54, 22, 12, 40, 1, 54, 22, 12, 40, 13, 66, 21, 1, 208, 5, 1, 203, 449], [51, 361, 67, 233, 10, 1, 266, 556, 164, 122, 267, 2, 3, 64, 1, 164, 33, 109, 10, 1, 266, 94, 122, 267, 2, 3, 64], [1, 582, 832, 46, 22, 33, 564, 21, 1, 54, 310, 12, 40, 1, 831, 28, 13, 419, 21, 1, 54, 22, 12, 40], [6, 15, 429, 180, 84, 50, 23, 1, 992, 46, 2384, 13, 224, 14, 429, 180, 84, 50, 23, 1, 583, 1235], [11, 51, 58, 6, 15, 1, 17, 38, 29, 12, 2, 3, 16, 11, 2385, 6, 76, 95, 7, 29, 61, 23, 17, 12, 2, 3, 16], [112, 1236, 28, 456, 95, 123, 1667, 1227, 26, 1016, 4, 638, 165, 68, 1237, 287, 11, 2386, 129, 123, 1667, 1227, 26, 1016, 4, 638, 165], [105, 5, 24, 49, 67, 61, 23, 1, 17, 161, 12, 2, 3, 16, 1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16], [11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16, 1, 101, 385, 104, 417, 26, 1, 17, 38, 20, 12, 2, 3, 16], [625, 157, 418, 910, 23, 1, 91, 113, 42, 1668, 34, 63, 1017, 8, 7, 72, 75, 5, 149, 32, 76, 72, 8, 157, 114, 127, 91, 65, 5, 157, 360, 1, 91, 113, 114, 127, 42, 270, 34, 112, 63, 34, 194, 8, 72, 149, 95, 72, 365], [439, 1669, 2, 3, 53, 97, 68, 345, 78, 103, 11, 374, 1203, 125, 8, 7, 566, 14, 2387, 2388, 2389, 1669, 2, 3, 53, 97, 7, 1670, 345, 128, 11, 1203, 2390, 340, 2391, 14, 762, 133, 1671, 1018, 833, 903, 2392], [7, 91, 142, 19, 13, 823, 61, 23, 1, 91, 113, 114, 127, 63, 34, 194, 8, 1, 139, 149, 237, 9, 2393, 72, 979, 65, 5, 157, 360, 1, 91, 113, 114, 127, 42, 270, 34, 112, 63, 34, 194, 8, 72, 149, 95, 72, 365], [48, 263, 1238, 1, 673, 5, 24, 29, 34, 631, 8, 1, 573, 7, 5, 834, 62, 835, 195, 115, 8, 465, 639, 2, 3, 53, 8, 1, 280, 6, 385, 719, 1, 29, 14, 42, 6, 631, 8, 1, 1239, 2394, 215, 573, 5, 195, 115, 8, 465, 62, 835, 5, 565, 53, 639, 2, 3, 53], [6, 15, 1, 27, 19, 17, 12, 2, 3, 16, 11, 1, 58, 14, 51, 1, 60, 302, 539, 7, 375, 299, 19, 4, 7, 100, 41, 2395, 33, 66, 14, 1, 17, 20, 12, 2, 3, 16, 10, 1, 2396, 60, 720, 93, 539, 7, 100, 41, 19], [1240, 1241, 146, 584, 13, 7, 603, 671, 34, 887, 105, 1, 1019, 4, 379, 5, 41, 1020, 197, 1240, 1241, 146, 584, 1020, 197, 13, 7, 375, 146, 671, 34, 129, 123, 15, 11, 105, 2397, 2398, 389, 227, 4, 130, 227], [6, 15, 1, 256, 1672, 81, 5, 92, 20, 79, 2, 3, 25, 14, 326, 2399, 15, 1, 226, 99, 81, 14, 107, 106, 4, 256, 416, 14, 326, 200, 105, 169, 190, 92, 79, 2, 3, 25], [1, 97, 19, 1673, 1, 701, 330, 5, 1674, 2, 3, 43, 1, 1675, 5, 1674, 2, 3, 43, 1021, 701, 14, 2400, 4, 1184, 836, 65, 10, 1, 818, 5, 1, 19], [6, 15, 1, 46, 185, 5, 1, 183, 21, 1676, 2, 3, 25, 1676, 2, 3, 25, 176, 7, 183, 5, 828, 616, 1242, 1141, 2401, 5, 616, 1155, 21, 145, 709, 2402, 2403, 2404, 117], [100, 41, 65, 5, 1650, 4, 44, 67, 50, 10, 102, 96, 25, 8, 269, 9, 337, 1, 1677, 5, 111, 29, 6, 69, 100, 41, 65, 11, 111, 41, 10, 102, 96, 25], [1, 144, 32, 118, 181, 4, 721, 10, 1, 17, 229, 192, 182, 4, 224, 26, 186, 119, 10, 1, 148, 135, 110, 1, 22, 33, 192, 374, 224, 14, 186, 333, 10, 148, 135, 110], [1, 70, 89, 147, 13, 76, 196, 179, 14, 141, 77, 80, 82, 14, 228, 488, 297, 12, 36, 1, 722, 13, 640, 454, 670, 9, 141, 77, 80, 147, 12, 36], [1, 1678, 434, 837, 13, 68, 99, 434, 19, 175, 14, 1243, 7, 158, 168, 1679, 78, 122, 295, 37, 1, 1222, 13, 175, 31, 7, 434, 837, 50, 14, 1243, 295, 37], [1, 70, 89, 147, 13, 76, 196, 179, 14, 141, 77, 80, 82, 14, 228, 488, 297, 12, 36, 7, 70, 89, 147, 33, 109, 10, 1, 77, 80, 82, 12, 36], [6, 18, 1, 272, 20, 1680, 4, 273, 87, 9, 219, 1, 1244, 2405, 21, 111, 1681, 18, 1, 2406, 136, 2407, 21, 272, 1680, 4, 273, 87, 9, 262, 105, 136, 4, 30, 575, 23, 111, 133, 1245], [6, 2408, 1, 261, 1246, 9, 42, 111, 209, 2409, 2410, 13, 255, 585, 14, 1, 82, 5, 1682, 2, 3, 201, 6, 681, 48, 82, 586, 68, 1022, 1023, 259, 2132, 42, 13, 641, 14, 1, 2411, 8, 1682, 2, 3, 201], [6, 910, 23, 1, 1247, 585, 257, 130, 251, 97, 26, 1683, 2, 3, 1684, 9, 262, 2412, 709, 1, 2413, 5, 48, 263, 13, 7, 300, 345, 2414, 128, 9, 1685, 61, 23, 1, 1247, 585, 251, 2415, 587, 26, 1683, 2, 3, 1684], [9, 1157, 554, 1, 333, 303, 1248, 7, 22, 618, 1686, 129, 123, 247, 11, 1024, 1249, 4, 124, 603, 1687, 8, 1250, 306, 1688, 2, 3, 90, 1, 1686, 22, 13, 7, 447, 247, 22, 11, 1024, 4, 1249, 1689, 838, 472, 4, 248, 603, 1687, 136, 472, 1688, 2, 3, 90], [1, 452, 5, 826, 1251, 1221, 8, 1, 2416, 1252, 13, 2417, 61, 23, 1, 39, 2418, 2419, 2420, 29, 588, 2, 3, 88, 6, 396, 1, 39, 826, 1251, 29, 588, 2, 3, 88], [6, 232, 24, 58, 23, 54, 12, 40, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 2421, 94, 12, 40, 98, 13, 7, 22, 5, 85, 306, 8, 1011, 394, 21, 1, 208, 5, 1, 203, 449], [6, 15, 1, 92, 79, 2, 3, 25, 81, 5, 172, 163, 14, 248, 107, 435, 302, 11, 24, 610, 15, 7, 785, 81, 5, 1, 92, 152, 79, 2, 3, 25], [6, 15, 1, 92, 79, 2, 3, 25, 81, 5, 172, 163, 14, 248, 107, 435, 302, 11, 24, 610, 15, 1, 92, 20, 9, 69, 24, 163, 79, 2, 3, 25], [355, 1253, 436, 1690, 779, 190, 970, 1, 1691, 333, 21, 1025, 358, 23, 1, 139, 996, 9, 723, 7, 1692, 915, 156, 355, 133, 436, 779, 9, 374, 723, 7, 799, 4, 2422, 1692, 5, 1254, 294, 1025, 358, 915, 156], [589, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16, 11, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [6, 15, 1, 92, 79, 2, 3, 25, 81, 5, 172, 163, 14, 248, 107, 435, 302, 11, 24, 610, 15, 1, 92, 79, 2, 3, 25, 81, 5, 1693, 4, 1, 1694, 20], [589, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16, 11, 1, 27, 38, 29, 6, 458, 1, 17, 20, 12, 2, 3, 16], [6, 69, 163, 11, 111, 5, 1, 642, 250, 354, 4, 11, 1, 583, 250, 75, 23, 1, 46, 75, 5, 111, 22, 10, 1, 107, 580, 5, 1, 366, 312, 81, 5, 202, 204, 2, 3, 57, 6, 69, 256, 416, 163, 1255, 156, 10, 202, 204, 2, 3, 57, 11, 111, 1695, 4, 76, 11, 1, 412, 19], [8, 335, 1, 22, 33, 473, 10, 1, 148, 1256, 135, 110, 83, 1, 22, 13, 473, 4, 224, 26, 186, 23, 105, 1026, 10, 1, 148, 135, 110], [328, 32, 61, 23, 91, 113, 42, 1257, 643, 1, 1258, 34, 72, 63, 194, 8, 72, 149, 114, 127, 91, 65, 5, 157, 360, 1, 91, 113, 114, 127, 42, 270, 34, 112, 63, 34, 194, 8, 72, 149, 95, 72, 365], [6, 15, 1, 139, 147, 75, 15, 8, 448, 2, 3, 36, 11, 24, 281, 170, 241, 6, 1178, 459, 1540, 5, 1, 46, 28, 15, 8, 448, 2, 3, 36, 31, 24, 46, 28, 4, 1, 1541, 31, 1, 422, 28, 31, 268, 8, 193, 170], [6, 820, 35, 65, 10, 1, 27, 17, 12, 2, 3, 16, 38, 2423, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16], [6, 15, 1027, 93, 61, 23, 1028, 2, 3, 40, 11, 231, 1028, 2, 3, 40, 2133, 1, 18, 5, 1027, 93, 26, 10, 600, 378], [11, 359, 1, 101, 38, 29, 6, 15, 1, 177, 38, 20, 17, 12, 2, 3, 16, 8, 124, 60, 929, 51, 58, 6, 15, 1, 17, 38, 29, 12, 2, 3, 16], [11, 35, 6, 18, 17, 12, 2, 3, 16, 14, 375, 299, 1695, 4, 1, 97, 19, 14, 441, 1259, 2424, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [9, 909, 2425, 724, 6, 644, 7, 501, 5, 991, 724, 420, 8, 1, 217, 137, 187, 336, 2, 3, 2426, 8, 335, 6, 508, 34, 1, 441, 287, 2427, 350, 14, 1, 2428, 5, 137, 724, 8, 1, 217, 137, 187, 336, 2, 3, 90], [9, 644, 41, 65, 4, 216, 818, 6, 18, 614, 615, 87, 14, 1696, 155, 116, 839, 342, 4, 474, 197, 4, 14, 7, 725, 1029, 105, 41, 65, 18, 155, 116, 173, 342, 4, 474, 197], [6, 18, 600, 378, 9, 707, 1, 1697, 5, 699, 4, 840, 2429, 179, 1, 1260, 106, 922, 4, 923, 36, 6, 18, 1, 600, 378, 61, 701, 922, 4, 923, 36], [2430, 13, 318, 7, 2431, 774, 21, 1, 292, 999, 31, 13, 11, 231, 1, 536, 774, 536, 36, 83, 292, 774, 13, 509, 8, 1, 536, 1030, 536, 36], [9, 644, 41, 65, 4, 216, 818, 6, 18, 614, 615, 87, 14, 1696, 155, 116, 839, 342, 4, 474, 197, 4, 14, 7, 725, 1029, 105, 41, 65, 18, 155, 116, 173, 342, 4, 474, 197], [6, 121, 672, 24, 323, 526, 897, 61, 128, 11, 412, 324, 221, 4, 322, 443, 4, 438, 64, 1207, 1616, 24, 323, 1208, 128, 11, 324, 221, 443, 4, 438, 64], [6, 18, 1, 2432, 238, 894, 41, 19, 401, 5, 1698, 2, 3, 88, 31, 268, 8, 426, 253, 6, 360, 1, 238, 309, 401, 5, 1698, 2, 3, 88, 10, 112, 529, 2433, 5, 2434, 226, 2435], [1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16, 6, 50, 7, 212, 5, 841, 38, 49, 10, 1, 17, 20, 12, 2, 3, 16, 8, 124, 107, 726], [11, 359, 1, 101, 38, 29, 6, 15, 1, 177, 38, 20, 17, 12, 2, 3, 16, 8, 124, 60, 929, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 38, 491, 7, 27, 38, 29, 66, 10, 1, 17, 20, 12, 2, 3, 16, 4, 1, 958, 582, 183, 590, 1, 125, 8, 1, 147, 75, 33, 15, 9, 637, 1], [280, 251, 310, 4, 842, 7, 38, 19, 11, 2436, 33, 50, 10, 1, 17, 20, 12, 2, 3, 16, 10, 124, 101, 1699, 50, 7, 19, 10, 17, 20, 12, 2, 3, 16, 23, 1, 46, 28, 31, 24, 101, 29], [145, 1261, 128, 9, 143, 13, 317, 134, 4, 510, 43, 42, 13, 1700, 61, 23, 1031, 815, 211, 355, 4, 352, 2437, 82, 317, 134, 4, 510, 43, 13, 61, 23, 1031, 2438, 211, 1, 2439, 4, 2440, 1032], [6, 18, 1, 207, 313, 100, 22, 180, 4, 246, 37, 9, 981, 1, 41, 19, 222, 11, 7, 1681, 15, 1, 836, 450, 21, 1, 207, 313, 100, 22, 180, 4, 246, 37, 9, 535, 1, 467, 5, 18, 5, 111, 624, 843], [1, 2441, 1701, 61, 195, 115, 62, 13, 7, 2442, 5, 834, 62, 253, 1702, 2, 3, 53, 48, 263, 1238, 1, 128, 5, 1, 130, 2443, 511, 2444, 9, 62, 253, 5, 565, 53, 1701, 61, 195, 115, 1702, 2, 3, 53], [1, 57, 2445, 483, 159, 62, 278, 2, 3, 57, 2446, 190, 430, 2447, 319, 145, 5, 1, 477, 354, 33, 2448, 2449, 1262, 2450, 57, 159, 62, 2451, 23, 477, 303, 278, 2, 3, 57, 1263, 23, 477, 354, 5, 2452, 973], [1, 522, 308, 19, 1033, 11, 1264, 1265, 276, 1034, 4, 86, 87, 33, 1, 118, 345, 128, 9, 2453, 7, 2454, 101, 1, 1033, 13, 7, 806, 19, 11, 1, 345, 1703, 5, 1266, 1265, 276, 206, 645, 5, 186, 119, 568, 1034, 4, 86, 87], [1, 1033, 13, 7, 806, 19, 11, 1, 345, 1703, 5, 1266, 1265, 276, 206, 645, 5, 186, 119, 568, 1034, 4, 86, 87, 1, 666, 5, 1704, 7, 522, 26, 124, 2455, 4, 7, 332, 784, 5, 308, 13, 15, 26, 1, 1033, 345, 227, 19, 1034, 4, 86, 87], [6, 18, 1, 81, 176, 26, 1705, 2, 3, 201, 2456, 254, 1, 55, 276, 34, 32, 983, 9, 248, 1706, 18, 1, 55, 189, 684, 2457, 687, 309, 2458, 97, 26, 1705, 2, 3, 201, 2459, 2460, 248, 107, 55, 52, 14, 24, 213, 34, 1707, 1708, 63], [54, 12, 40, 13, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 541, 18, 1, 275, 44, 85, 22, 844, 533, 480, 125, 21, 1, 22, 5, 203, 845, 208, 12, 40, 31, 1, 28, 23, 42, 1267, 13, 109, 9, 133], [1, 39, 55, 52, 74, 71, 2, 3, 37, 13, 15, 11, 430, 93, 21, 1, 55, 160, 727, 15, 1, 39, 55, 52, 74, 71, 2, 3, 37, 9, 160, 1, 125, 34, 1154, 7, 624, 1024, 1709, 4, 174, 1, 280, 93, 21, 1, 55, 160, 200], [1, 39, 55, 52, 74, 71, 2, 3, 37, 13, 15, 11, 430, 93, 21, 1, 55, 160, 727, 15, 1, 39, 55, 52, 74, 71, 2, 3, 37, 9, 160, 1, 125, 34, 1154, 7, 624, 1024, 1709, 4, 174, 1, 280, 93, 21, 1, 55, 160, 200], [1, 1268, 236, 5, 28, 622, 21, 846, 4, 1269, 583, 1270, 5, 1, 54, 28, 75, 12, 40, 1, 28, 15, 11, 1, 58, 138, 8, 48, 263, 622, 1710, 21, 1711, 491, 512, 4, 1, 54, 22, 5, 203, 845, 208, 12, 40], [6, 225, 1, 97, 19, 9, 24, 81, 5, 1, 1712, 19, 138, 8, 646, 2, 3, 25, 214, 9, 1713, 198, 1, 19, 401, 268, 8, 426, 83, 13, 7, 2461, 647, 5, 1, 1271, 401, 5, 646, 2, 3, 25], [98, 13, 7, 27, 29, 66, 10, 1, 17, 20, 12, 2, 3, 16, 4, 2462, 10, 254, 1, 442, 400, 721, 85, 28, 176, 11, 1, 159, 2463, 491, 7, 27, 38, 29, 66, 10, 1, 17, 20, 12, 2, 3, 16, 4, 1, 958, 582, 183, 590, 1, 125, 8, 1, 147, 75, 33, 15, 9, 637, 1, 159, 62], [8, 48, 185, 6, 118, 1714, 1, 1247, 189, 19, 5, 1715, 2, 3, 90, 4, 952, 7, 700, 2464, 1715, 2, 3, 90, 1, 2465, 19, 68, 819, 211, 1, 836, 19, 4, 1, 1716, 19, 33, 76, 728, 545, 227, 700, 125, 42, 2466, 7, 1717, 286], [1, 54, 22, 12, 40, 13, 66, 21, 1, 208, 5, 1, 203, 541, 18, 1, 275, 44, 85, 22, 844, 533, 480, 125, 21, 1, 22, 5, 203, 845, 208, 12, 40, 31, 1, 28, 23, 42, 1267, 13, 109, 9, 133], [1, 54, 22, 12, 40, 13, 66, 21, 1, 208, 5, 1, 203, 541, 18, 1, 275, 44, 85, 22, 844, 533, 480, 125, 21, 1, 22, 5, 203, 845, 208, 12, 40, 31, 1, 28, 23, 42, 1267, 13, 109, 9, 133], [6, 18, 243, 244, 4, 245, 201, 11, 1718, 14, 331, 78, 166, 5, 2467, 19, 33, 50, 648, 326, 1719, 10, 1, 2468, 647, 5, 729, 437, 847, 243, 244, 4, 245, 201, 14, 68, 331, 78, 166, 5, 488], [6, 18, 1, 177, 17, 20, 12, 2, 3, 16, 9, 121, 7, 60, 27, 38, 29, 42, 1720, 848, 9, 344, 63, 1272, 8, 1, 17, 126, 2469, 18, 1, 17, 27, 35, 29, 12, 2, 3, 16, 9, 424, 24, 65], [6, 15, 1, 2470, 122, 8, 59, 47, 4, 45, 43, 9, 607, 1, 30, 2471, 76, 15, 59, 30, 73, 122, 47, 4, 45, 43, 23, 1, 139, 1721, 4, 803, 2472, 2473, 9, 1, 73, 5, 2474, 2475, 8, 2476, 4, 44], [6, 192, 18, 1, 126, 303, 1722, 8, 1, 17, 70, 56, 35, 29, 12, 2, 3, 16, 9, 219, 7, 126, 193, 42, 1723, 206, 1191, 6, 18, 1, 17, 27, 35, 29, 12, 2, 3, 16, 9, 424, 24, 65], [126, 218, 32, 174, 21, 2477, 132, 153, 14, 59, 47, 4, 45, 43, 126, 218, 67, 174, 21, 1724, 30, 132, 4, 1725, 421, 26, 59, 47, 4, 45, 43, 10, 1, 966, 5, 705, 356, 4, 2478], [791, 6, 121, 2479, 1, 1726, 1727, 2480, 15, 26, 1728, 2, 3, 117, 11, 343, 465, 2481, 42, 385, 104, 24, 1035, 2482, 167, 13, 1273, 26, 1, 1726, 376, 128, 5, 1728, 2, 3, 117, 1274, 1, 19, 6, 751, 9, 31, 1, 1727, 376, 2483, 1274, 4, 6, 849, 48, 24, 1035, 101], [252, 346, 347, 84, 24, 115, 76, 586, 1, 139, 850, 11, 413, 730, 125, 290, 31, 1, 1729, 1730, 842, 1731, 8, 2484, 1, 301, 513, 8, 253, 325, 4, 344, 6, 154, 808, 7, 1232, 850, 9, 413, 730, 125, 539, 1, 1275, 1732, 9, 1, 115, 5, 346, 347, 84], [6, 750, 7, 2485, 708, 4, 2486, 345, 19, 1112, 26, 1, 427, 402, 636, 1733, 1734, 19, 97, 8, 1735, 4, 1736, 64, 42, 6, 1113, 9, 1, 2487, 1276, 642, 6, 1113, 1, 1734, 19, 97, 8, 1735, 4, 1736, 64, 9, 239, 4, 307, 302, 26, 2488, 1, 112, 427, 682, 9, 18, 7, 159, 1737], [24, 167, 13, 76, 601, 9, 1036, 4, 1037, 40, 319, 1, 142, 211, 1, 63, 23, 1, 431, 1738, 112, 731, 8, 1, 55, 758, 13, 15, 9, 1739, 7, 162, 2489, 55, 431, 13, 1, 1740, 431, 211, 1, 112, 731, 8, 7, 55, 160, 758, 4, 129, 123, 268, 9, 104, 1741, 11, 293, 303, 1036, 4, 1037, 40], [1273, 26, 323, 167, 6, 1742, 7, 467, 993, 5, 2490, 137, 1038, 42, 67, 289, 9, 104, 1, 255, 649, 959, 1, 2491, 22, 1743, 2, 3, 25, 2492, 5, 1, 542, 93, 1, 137, 1038, 42, 32, 289, 9, 104, 1, 255, 2493, 8, 24, 58, 2494, 14, 797, 289, 8, 1, 2495, 2496, 22, 1743, 2, 3, 25], [1, 2497, 286, 950, 67, 807, 10, 7, 1039, 406, 487, 103, 11, 46, 7, 158, 168, 184, 10, 921, 669, 1744, 626, 193, 344, 9, 193, 533, 508, 1, 2498, 633, 2499, 1745, 153, 1040, 494, 1041, 10, 1039, 406, 487, 103, 1277, 1744, 626, 11, 46, 7, 158, 168, 2500], [8, 48, 1746, 5, 48, 263, 6, 1714, 601, 167, 1, 381, 11, 111, 29, 4, 58, 4, 188, 11, 111, 573, 10, 1, 28, 176, 26, 834, 62, 835, 195, 115, 8, 2501, 263, 1238, 1, 673, 5, 24, 29, 34, 631, 8, 1, 573, 7, 5, 834, 62, 835, 195, 115, 8, 465, 639, 2, 3, 53], [8, 1747, 163, 50, 23, 485, 28, 154, 104, 214, 635, 9, 1042, 724, 206, 1, 44, 140, 5, 1, 54, 22, 12, 40, 15, 11, 46, 4, 281, 2502, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40], [8, 1, 982, 185, 6, 1278, 732, 343, 5, 1748, 1749, 8, 7, 1599, 376, 73, 19, 499, 2, 3, 197, 47, 4, 45, 1750, 6, 1278, 732, 1, 376, 61, 30, 73, 65, 499, 197, 47, 4, 45, 1750], [48, 1279, 13, 640, 454, 190, 228, 410, 670, 9, 77, 80, 147, 12, 36, 1, 722, 13, 640, 454, 670, 9, 141, 77, 80, 147, 12, 36], [1751, 14, 2503, 1752, 4, 1753, 36, 1280, 434, 381, 32, 2504, 1, 255, 733, 15, 345, 128, 11, 2505, 2506, 1, 345, 128, 1280, 434, 381, 32, 258, 1752, 4, 1753, 36], [1281, 5, 24, 2507, 14, 1, 202, 152, 204, 2, 3, 57, 6, 851, 48, 122, 11, 81, 2508, 6, 18, 1, 202, 204, 2, 3, 57, 81, 5, 1, 708, 2509, 11, 24, 58], [444, 1043, 2, 3, 53, 13, 175, 14, 7, 444, 166, 5, 769, 9, 1044, 1, 19, 21, 2510, 1043, 2, 3, 53, 13, 7, 650, 852, 349, 1754, 9, 1044, 2511, 5, 7, 309], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 11, 24, 38, 58, 6, 18, 1, 17, 20, 12, 2, 3, 16], [31, 1276, 8, 185, 131, 6, 153, 191, 21, 1, 619, 5, 1, 39, 52, 74, 71, 4, 86, 90, 6, 853, 18, 5, 55, 160, 333, 21, 1, 39, 55, 52, 74, 71, 4, 86, 90], [6, 274, 4, 1282, 51, 5, 1, 144, 10, 1283, 787, 14, 17, 12, 2, 3, 16, 6, 442, 1, 46, 144, 14, 229, 1221, 8, 1, 17, 20, 12, 2, 3, 16], [9, 210, 172, 6, 18, 1, 39, 55, 52, 74, 71, 2, 3, 37, 4, 1, 1755, 73, 21, 185, 1756, 18, 1, 39, 55, 52, 74, 71, 2, 3, 37, 11, 430, 55, 431, 4, 186, 93], [1, 2512, 65, 32, 50, 10, 243, 527, 244, 4, 245, 53, 14, 68, 331, 78, 166, 5, 2513, 106, 32, 2514, 26, 243, 527, 244, 4, 245, 53, 14, 1, 78, 166, 488], [1, 1149, 132, 2515, 7, 1172, 5, 193, 344, 685, 375, 299, 19, 1757, 4, 86, 90, 145, 5, 1, 27, 49, 2516, 2517, 7, 375, 299, 19, 1757, 4, 86, 90], [1, 70, 89, 395, 10, 364, 311, 802, 32, 460, 14, 141, 77, 80, 12, 36, 7, 70, 89, 147, 33, 109, 10, 1, 77, 80, 82, 12, 36], [9, 210, 172, 6, 18, 1, 39, 55, 52, 74, 71, 2, 3, 37, 4, 1, 1755, 73, 21, 185, 1756, 18, 1, 39, 55, 52, 74, 71, 2, 3, 37, 11, 430, 55, 431, 4, 186, 93], [6, 18, 1, 150, 103, 151, 2, 3, 25, 9, 651, 1, 805, 2518, 2519, 5, 1, 500, 18, 428, 78, 9, 69, 19, 106, 591, 1, 106, 10, 1, 150, 103, 151, 2, 3, 25], [1, 205, 13, 21, 1758, 1759, 2, 3, 53, 1, 1284, 5, 30, 1045, 13, 2520, 18, 1, 2521, 2522, 30, 178, 21, 1758, 1759, 2, 3, 53], [6, 18, 1, 150, 527, 151, 2, 3, 25, 14, 331, 78, 166, 75, 9, 1469, 18, 428, 78, 9, 69, 19, 106, 591, 1, 106, 10, 1, 150, 103, 151, 2, 3, 25], [6, 340, 14, 1, 27, 70, 56, 35, 20, 17, 12, 2, 3, 16, 8, 269, 9, 69, 7, 1285, 44, 29, 4, 9, 508, 1, 1760, 5, 1, 1761, 1762, 15, 1, 17, 20, 12, 2, 3, 16, 9, 121, 7, 126, 61, 56, 35, 29, 14, 7, 1286, 100, 371, 50, 23, 1, 209, 140, 5, 24, 825], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 1, 38, 49, 67, 66, 10, 1, 17, 20, 12, 2, 3, 16], [6, 360, 1, 784, 8, 504, 2, 3, 64, 5, 2523, 76, 18, 172, 2524, 2525, 9, 348, 1025, 65, 11, 1, 103, 5, 504, 2, 3, 64], [1, 41, 19, 13, 7, 100, 102, 96, 25, 19, 50, 10, 822, 14, 155, 116, 173, 4, 559, 1633, 50, 68, 44, 100, 41, 19, 10, 102, 96, 25], [31, 1276, 8, 185, 131, 6, 153, 191, 21, 1, 619, 5, 1, 39, 52, 74, 71, 4, 86, 90, 6, 853, 18, 5, 55, 160, 333, 21, 1, 39, 55, 52, 74, 71, 4, 86, 90], [76, 6, 337, 23, 1, 1046, 236, 5, 1, 1182, 183, 962, 2, 3, 53, 4, 508, 34, 24, 128, 2526, 9, 2527, 391, 13, 1, 1046, 236, 5, 1, 1182, 183, 962, 2, 3, 53], [24, 19, 129, 7, 1763, 334, 1764, 2, 3, 581, 14, 112, 2528, 111, 686, 7, 136, 8, 2529, 323, 167, 18, 136, 343, 14, 7, 1763, 334, 1764, 2, 3, 581], [1, 2530, 2531, 2532, 5, 1, 22, 461, 1, 854, 198, 1030, 961, 2, 3, 156, 1, 1127, 15, 9, 69, 1, 854, 171, 32, 855, 635, 21, 1, 854, 198, 1030, 961, 2, 3, 2533], [6, 76, 210, 1, 55, 160, 5, 1, 125, 10, 1, 39, 52, 74, 71, 2, 3, 37, 9, 1765, 1, 1287, 5, 1288, 23, 55, 227, 6, 1224, 1, 39, 55, 52, 131, 71, 2, 3, 37], [1, 1399, 636, 13, 402, 10, 1012, 1013, 333, 718, 4, 1014, 339, 6, 644, 2534, 815, 636, 1766, 613, 2535, 8, 1, 636, 13, 1, 1012, 1013, 333, 211, 1, 112, 63, 718, 4, 1014, 339], [6, 219, 1289, 1290, 10, 112, 381, 1767, 1768, 1769, 4, 1770, 88, 4, 2536, 612, 1771, 8, 2537, 253, 76, 462, 2538, 2539, 219, 1290, 21, 2540, 10, 1767, 1768, 1769, 4, 1770, 88, 4, 24, 97, 2541, 29], [444, 1043, 2, 3, 53, 13, 175, 14, 7, 444, 166, 5, 769, 9, 1044, 1, 19, 21, 2542, 1, 619, 5, 1, 2543, 677, 13, 2544, 9, 7, 444, 677, 1043, 2, 3, 53], [1, 1772, 377, 33, 407, 26, 1, 77, 82, 504, 165, 1, 77, 378, 82, 586, 7, 592, 11, 1773, 1291, 7, 378, 377, 11, 7, 978, 545, 1, 377, 13, 318, 609, 504, 165], [1, 70, 89, 147, 13, 76, 196, 179, 14, 141, 77, 80, 82, 14, 228, 488, 297, 12, 36, 1, 89, 281, 13, 109, 26, 141, 77, 80, 12, 36], [1, 77, 378, 82, 586, 7, 592, 11, 1773, 1291, 7, 378, 377, 11, 7, 978, 545, 1, 377, 13, 318, 609, 504, 165, 1, 1772, 377, 33, 407, 26, 1, 77, 82, 504, 165], [11, 1292, 6, 18, 1, 1250, 28, 21, 1774, 652, 57, 11, 841, 58, 6, 15, 1, 1774, 85, 22, 652, 57, 42, 32, 1292, 358, 21, 1, 203, 2545, 2546], [1, 235, 58, 67, 196, 179, 10, 1, 60, 634, 27, 38, 20, 17, 12, 2, 3, 16, 1, 35, 29, 13, 50, 10, 1, 350, 609, 17, 20, 12, 2, 3, 16], [11, 1, 234, 5, 346, 347, 84, 9, 104, 514, 6, 95, 97, 7, 300, 384, 5, 734, 4, 1, 653, 301, 515, 8, 1047, 14, 248, 478, 301, 1775, 6, 1776, 1, 112, 301, 513, 5, 346, 347, 84, 4, 1777, 1, 653, 301, 515], [131, 14, 172, 200, 725, 1, 767, 1778, 14, 1179, 9, 106, 32, 260, 574, 1, 1779, 571, 782, 103, 1780, 4, 1781, 197, 1778, 32, 260, 1782, 574, 1779, 571, 334, 1780, 4, 1781, 197], [675, 1783, 95, 728, 1293, 1784, 26, 1785, 1, 207, 1786, 26, 1, 91, 113, 42, 270, 34, 63, 1017, 8, 72, 149, 237, 9, 95, 72, 365, 114, 127, 91, 65, 5, 157, 360, 1, 91, 113, 114, 127, 42, 270, 34, 112, 63, 34, 194, 8, 72, 149, 95, 72, 365], [6, 15, 202, 204, 2, 3, 57, 11, 51, 24, 215, 610, 18, 1, 202, 20, 204, 2, 3, 57, 11, 24, 576, 78, 58], [239, 144, 1, 22, 15, 8, 1, 280, 58, 13, 1, 628, 1787, 1048, 22, 1788, 2, 3, 87, 1, 58, 32, 196, 179, 23, 7, 388, 5, 1, 628, 1787, 1048, 22, 2547, 1788, 2, 3, 87, 31, 98, 13, 15, 11, 1, 2548, 28, 2549, 1789, 5, 1, 2550, 143, 2551], [118, 6, 15, 1, 17, 20, 12, 2, 3, 16, 11, 70, 56, 1790, 17, 20, 12, 2, 3, 16, 13, 15, 11, 126, 303], [810, 85, 22, 13, 1, 2552, 307, 85, 22, 1791, 2, 3, 37, 22, 1791, 2, 3, 37], [1, 85, 22, 13, 593, 10, 59, 47, 4, 45, 43, 48, 13, 233, 10, 716, 19, 83, 471, 2, 3, 581, 4, 59, 47, 4, 45, 43], [6, 1175, 2553, 2554, 31, 7, 130, 136, 215, 62, 8, 7, 1271, 401, 2555, 1, 2556, 1271, 19, 5, 278, 53, 7, 647, 5, 646, 2, 3, 25, 6, 225, 1, 97, 19, 9, 24, 81, 5, 1, 1712, 19, 138, 8, 646, 2, 3, 25, 214, 9, 1713, 198], [51, 172, 93, 32, 1792, 21, 17, 12, 2, 3, 16, 51, 1, 58, 32, 196, 179, 8, 17, 20, 12, 2, 3, 16], [11, 48, 265, 6, 18, 1, 54, 22, 12, 40, 11, 24, 58, 6, 18, 2557, 125, 21, 54, 12, 40, 11, 111, 41, 654, 280, 1, 628, 930, 5, 652, 53], [1, 306, 67, 118, 374, 1049, 4, 181, 494, 4, 192, 328, 67, 186, 224, 26, 429, 171, 180, 84, 42, 33, 50, 23, 1, 2558, 502, 46, 28, 1, 2559, 28, 33, 224, 10, 429, 180, 84, 10, 7, 19, 50, 23, 1, 549, 550, 551], [2560, 829, 2, 3, 64, 830, 261, 63, 61, 23, 30, 73, 1793, 6, 1125, 299, 261, 2561, 2, 3, 64, 509, 7, 82, 319, 261, 133, 13, 830, 9, 1794, 1, 209, 30, 269, 61, 23, 30, 73], [11, 41, 343, 6, 18, 1, 44, 353, 22, 14, 100, 371, 175, 14, 1, 102, 20, 96, 25, 41, 19, 11, 51, 1795, 49, 6, 18, 7, 1050, 116, 291, 41, 19, 50, 10, 1, 102, 20, 96, 25], [11, 24, 701, 908, 6, 18, 382, 383, 87, 11, 46, 239, 996, 65, 6, 18, 382, 20, 383, 87], [6, 18, 1, 60, 1796, 75, 5, 55, 795, 74, 71, 2, 3, 37, 6, 18, 1, 39, 52, 14, 39, 191, 74, 71, 2, 3, 37], [982, 6, 337, 1797, 350, 1, 973, 562, 97, 8, 1171, 2, 3, 16, 1624, 14, 722, 8, 286, 4, 722, 8, 78, 2562, 76, 225, 164, 2563, 8, 363, 5, 1, 78, 166, 72, 9, 1171, 2, 3, 16, 590, 34, 6, 707, 4, 225, 1, 532, 722, 8, 1, 78, 166], [1019, 8, 2564, 13, 247, 280, 1, 55, 61, 671, 15, 8, 1, 2565, 55, 187, 42, 33, 76, 15, 8, 1, 305, 992, 22, 1798, 2, 3, 329, 11, 1, 305, 58, 6, 15, 1, 992, 22, 1798, 2, 3, 329], [6, 15, 1, 92, 79, 2, 3, 25, 81, 5, 1693, 4, 1, 1694, 2566, 18, 1, 445, 81, 5, 99, 79, 2, 3, 25], [6, 15, 1, 92, 20, 9, 69, 24, 163, 79, 2, 3, 25, 11, 1799, 6, 15, 92, 79, 2, 3, 25, 9, 69, 163], [1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16, 6, 50, 7, 60, 17, 101, 12, 2, 3, 16, 23, 1, 139, 46, 28, 4, 15, 1, 139, 253, 951, 41, 19, 9, 1294, 2567], [172, 93, 67, 153, 10, 1, 39, 52, 94, 71, 2, 3, 37, 6, 18, 1, 39, 52, 14, 39, 191, 74, 71, 2, 3, 37], [6, 66, 7, 100, 41, 19, 23, 1, 44, 140, 5, 703, 10, 102, 96, 25, 145, 13, 7, 1051, 41, 19, 66, 10, 102, 96, 25, 4, 50, 206, 7, 155, 213, 5, 1, 247, 22, 8, 42, 613, 98, 13, 1295, 14, 124, 384, 856, 98, 477], [8, 1, 205, 140, 1, 275, 22, 13, 186, 119, 224, 26, 10, 148, 122, 135, 110, 11, 2568, 133, 14, 186, 4, 2569, 2570, 1, 22, 13, 473, 4, 224, 26, 186, 23, 105, 1026, 10, 1, 148, 135, 110], [1, 1800, 1019, 857, 1801, 2, 3, 110, 1, 1800, 1019, 857, 1801, 2, 3, 110, 471, 22, 2571, 4, 1296, 1297, 129, 123, 15, 31, 7, 296, 22, 8, 675, 765, 764], [6, 986, 1, 1204, 5, 111, 522, 14, 148, 135, 110, 119, 198, 33, 109, 14, 1, 148, 135, 110], [14, 1, 102, 20, 96, 25, 11, 41, 343, 6, 15, 1, 102, 20, 96, 25, 11, 60, 1031, 343, 14, 68, 1031, 1298, 5, 170], [6, 1278, 732, 1, 376, 61, 30, 73, 65, 499, 197, 47, 4, 45, 84, 47, 13, 1, 376, 73, 19, 5, 47, 4, 45, 84], [6, 18, 1, 39, 52, 14, 39, 191, 71, 2, 3, 37, 172, 93, 67, 153, 10, 1, 39, 52, 94, 71, 2, 3, 37], [6, 18, 1, 39, 52, 14, 39, 191, 74, 71, 2, 3, 37, 6, 18, 1, 60, 1796, 75, 5, 55, 795, 74, 71, 2, 3, 37], [51, 1, 1032, 32, 397, 10, 317, 134, 36, 11, 1, 436, 62, 6, 225, 188, 10, 317, 134, 36], [1, 516, 311, 297, 67, 407, 10, 77, 80, 12, 36, 6, 752, 89, 10, 141, 77, 80, 12, 36], [6, 175, 2572, 10, 1, 1802, 403, 1803, 2, 3, 117, 1804, 260, 10, 1, 355, 2573, 2574, 5, 1802, 1803, 2, 3, 117, 42, 1299, 7, 1300, 2575], [11, 1, 226, 271, 199, 81, 6, 15, 92, 79, 2, 3, 25, 11, 1799, 6, 15, 92, 79, 2, 3, 25, 9, 69, 163], [1, 171, 6, 18, 13, 429, 180, 84, 7, 529, 526, 629, 171, 42, 33, 50, 23, 1, 960, 735, 22, 1805, 1806, 1807, 2576, 28, 33, 224, 10, 429, 180, 84, 10, 7, 19, 50, 23, 1, 549, 550, 551], [6, 95, 15, 1, 81, 138, 8, 1808, 4, 1809, 288, 14, 405, 200, 5, 911, 725, 9, 1810, 2577, 1, 78, 103, 6, 15, 1300, 2578, 14, 2579, 1811, 163, 42, 2580, 68, 1812, 5, 405, 200, 5, 725, 911, 1808, 4, 1809, 288], [6, 66, 7, 100, 41, 19, 23, 1, 44, 140, 5, 703, 10, 102, 96, 25, 6, 121, 24, 433, 49, 8, 7, 60, 592, 10, 1, 17, 29, 102, 11, 41, 1813, 96, 25, 4, 60, 257, 299, 19], [1814, 406, 594, 379, 2581, 31, 236, 5, 1, 583, 264, 2582, 1, 380, 76, 1815, 169, 7, 2583, 287, 5, 2584, 379, 8, 1, 773, 5, 406, 594, 2585, 619, 1, 146, 2586, 612, 130, 418, 8, 1, 827, 5, 406, 594, 379, 277, 2, 3, 40], [252, 1, 2587, 159, 62, 1, 16, 159, 62, 2588, 23, 55, 227, 4, 779, 190, 2589, 258, 56, 78, 530, 214, 9, 48, 62, 715, 2, 3, 16, 145, 5, 1, 118, 2590, 190, 42, 470, 990, 33, 1816, 33, 1, 16, 502, 159, 62, 23, 55, 227, 715, 2, 3, 16], [252, 346, 347, 84, 24, 115, 76, 586, 1, 139, 850, 11, 413, 730, 125, 290, 31, 1, 1729, 1730, 842, 1731, 8, 2591, 252, 346, 347, 84, 24, 115, 154, 76, 1301, 34, 1, 730, 136, 2592, 13, 1817, 1793, 2593, 13, 1818], [1, 126, 193, 13, 174, 21, 7, 239, 133, 585, 23, 1, 30, 472, 10, 856, 59, 47, 4, 45, 43, 7, 720, 837, 5, 613, 433, 29, 13, 1, 126, 193, 42, 240, 239, 126, 218, 174, 21, 7, 239, 22, 1040, 30, 73, 47, 4, 45, 43], [367, 207, 313, 180, 4, 246, 37, 129, 123, 15, 9, 688, 446, 2594, 42, 13, 15, 31, 7, 216, 5, 1, 2595, 5, 1, 2596, 367, 207, 313, 28, 180, 4, 246, 37, 129, 123, 268, 9, 2597, 7, 1400, 222, 955, 48, 28, 33, 318, 169, 648, 1, 2598, 5, 48, 655], [6, 118, 18, 7, 55, 52, 74, 71, 2, 3, 37, 9, 160, 111, 136, 4, 219, 1, 75, 5, 55, 242, 762, 14, 1, 136, 7, 514, 75, 5, 52, 568, 4, 1, 82, 15, 9, 858, 21, 7, 522, 9, 7, 1819, 55, 146, 154, 104, 289, 8, 74, 71, 2, 3, 37], [6, 118, 18, 7, 55, 52, 74, 71, 2, 3, 37, 9, 160, 111, 136, 4, 219, 1, 75, 5, 55, 242, 762, 14, 1, 136, 7, 514, 75, 5, 52, 568, 4, 1, 82, 15, 9, 858, 21, 7, 522, 9, 7, 1819, 55, 146, 154, 104, 289, 8, 74, 71, 2, 3, 37], [172, 320, 808, 68, 1237, 241, 1820, 327, 2599, 850, 9, 1, 2600, 5, 1302, 2, 3, 37, 2601, 328, 2602, 1821, 886, 1259, 11, 1, 2603, 5, 2604, 2, 3, 37, 2605, 34, 60, 285, 1822, 2606, 602, 2607, 285, 42, 328, 2608, 9, 1, 2609, 34, 1, 1002, 1823, 285, 2610, 7, 886, 1174, 414, 848, 5, 1821, 1824], [6, 340, 14, 1, 27, 70, 56, 35, 20, 17, 12, 2, 3, 16, 8, 269, 9, 69, 7, 1285, 44, 29, 4, 9, 508, 1, 1760, 5, 1, 1761, 1762, 714, 7, 29, 34, 2611, 7, 710, 17, 70, 56, 35, 29, 12, 2, 3, 16, 4, 2612, 98, 9, 1, 1250, 470], [11, 24, 22, 6, 1178, 459, 358, 21, 1, 2613, 185, 5, 1, 300, 859, 620, 22, 1052, 90, 21, 1, 1303, 2614, 859, 620, 351, 5, 963, 256, 125, 21, 1, 300, 859, 620, 22, 1052, 90, 1303, 16, 6, 459, 254, 125, 34, 2615, 190, 1156, 145, 736, 656, 670, 9, 1, 2616], [328, 15, 1, 1206, 164, 122, 266, 267, 2, 3, 64, 11, 1, 164, 6, 849, 1, 280, 354, 5, 2617, 2134, 1, 266, 122, 267, 2, 3, 64, 33, 15, 11, 164], [51, 144, 67, 419, 21, 1, 1003, 399, 1004, 84, 1, 143, 75, 33, 2618, 5, 2619, 2620, 21, 1, 471, 2621, 28, 5, 1, 1003, 399, 1004, 84], [1, 29, 421, 616, 67, 397, 10, 317, 562, 134, 36, 1, 1032, 21, 1, 642, 103, 11, 1, 2622, 67, 397, 61, 23, 317, 975, 134, 36], [1, 1825, 843, 824, 62, 13, 9, 860, 1, 766, 861, 30, 9, 7, 209, 21, 7, 501, 5, 1304, 2623, 1826, 4, 1827, 329, 1, 1825, 843, 183, 1826, 4, 1827, 329, 351, 5, 1828, 1226, 63, 11, 111, 5, 42, 253, 1829, 63, 32, 617, 4, 1, 62, 13, 9, 860, 1, 1829, 30, 255, 72, 9, 1, 1226], [6, 688, 24, 93, 10, 1, 102, 20, 96, 25, 83, 6, 15, 1, 102, 20, 96, 25, 9, 121, 51, 41, 65, 15, 8, 48, 167, 386, 105, 11, 28, 824, 4, 11, 1, 235, 49, 15, 11, 1830, 143], [6, 18, 1, 445, 81, 5, 256, 416, 79, 2, 3, 25, 1, 1053, 15, 13, 7, 256, 416, 1053, 8, 1, 81, 176, 26, 92, 79, 2, 3, 25], [11, 48, 265, 6, 18, 1, 54, 22, 12, 40, 1, 70, 857, 11, 48, 62, 33, 174, 21, 1, 1305, 54, 325, 22, 12, 40], [1, 70, 89, 395, 10, 364, 311, 802, 32, 460, 14, 141, 77, 80, 12, 36, 6, 15, 77, 80, 11, 281, 70, 89, 12, 36], [6, 192, 232, 30, 73, 14, 59, 47, 4, 45, 43, 8, 105, 398, 14, 1, 107, 106, 15, 8, 1651, 15, 1, 59, 108, 47, 4, 45, 43, 9, 657, 1, 30, 132], [6, 66, 7, 100, 41, 19, 23, 1, 44, 140, 5, 703, 10, 102, 96, 25, 6, 688, 24, 93, 10, 1, 102, 20, 96, 25], [6, 15, 59, 47, 4, 45, 43, 9, 341, 1, 63, 8, 1, 1181, 192, 232, 30, 73, 14, 59, 47, 4, 45, 43, 8, 105, 398, 14, 1, 107, 106, 15, 8, 17], [1, 126, 1167, 67, 421, 26, 949, 5, 1168, 30, 132, 153, 14, 59, 47, 4, 45, 43, 1, 30, 73, 33, 153, 26, 464, 59, 47, 4, 45, 43], [1831, 2, 3, 88, 214, 1832, 776, 19, 9, 607, 4, 707, 2624, 2625, 8, 248, 1833, 1834, 30, 575, 2626, 1, 2627, 5, 1833, 1834, 298, 30, 575, 1, 1832, 776, 128, 97, 26, 1831, 2, 3, 88, 1, 2628, 793, 4, 1, 2629, 2630, 2631, 1, 1835, 5], [561, 2, 3, 90, 95, 97, 9, 18, 30, 658, 31, 93, 9, 554, 1280, 70, 55, 227, 11, 44, 4, 2632, 269, 9, 1836, 1, 1306, 5, 247, 28, 9, 69, 7, 55, 52, 561, 2, 3, 90, 15, 30, 658, 260, 21, 2633, 28, 31, 93, 11, 46, 7, 52], [327, 439, 570, 2, 3, 53, 417, 706, 7, 29, 11, 279, 115, 4, 221, 5, 292, 48, 29, 154, 104, 15, 9, 554, 1, 623, 5, 1837, 1838, 29, 2634, 1, 1721, 32, 1213, 14, 1, 279, 115, 4, 221, 29, 706, 570, 2, 3, 53, 34, 2635, 7, 649, 2636, 5, 1837, 1839], [6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 1840, 359, 24, 38, 49, 1, 177, 38, 20, 17, 12, 2, 3, 16, 33, 15, 8, 124, 60, 930], [423, 366, 1841, 6, 18, 1842, 647, 14, 1843, 173, 435, 83, 79, 2, 3, 25, 6, 18, 1, 1844, 366, 312, 184, 8, 445, 14, 1, 107, 302, 79, 2, 3, 25], [6, 18, 150, 151, 2, 3, 25, 14, 1, 331, 78, 166, 75, 9, 2637, 18, 428, 78, 9, 69, 19, 106, 591, 1, 106, 10, 1, 150, 103, 151, 2, 3, 25], [1, 235, 58, 67, 196, 179, 10, 1, 60, 634, 27, 38, 20, 17, 12, 2, 3, 16, 1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [6, 18, 1, 150, 82, 151, 2, 3, 25, 9, 374, 285, 1, 78, 166, 11, 111, 1845, 18, 428, 78, 9, 69, 19, 106, 591, 1, 106, 10, 1, 150, 103, 151, 2, 3, 25], [771, 1, 991, 1846, 144, 1, 1847, 22, 338, 2, 3, 110, 13, 145, 5, 1, 255, 733, 2638, 1847, 22, 338, 2, 3, 110, 13, 145, 5, 1, 2639, 2640, 169, 447, 1848, 144, 11, 1685], [6, 18, 1, 490, 158, 168, 56, 99, 316, 4, 134, 25, 184, 11, 46, 4, 232, 2641, 1041, 11, 2642, 2643, 158, 168, 56, 316, 4, 134, 25, 184, 13, 50, 23, 1, 169, 46, 28], [6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16, 6, 274, 4, 1282, 51, 5, 1, 144, 10, 1283, 787, 14, 17, 12, 2, 3, 16, 83], [6, 66, 7, 862, 433, 19, 21, 1, 239, 470, 22, 10, 1, 17, 20, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [1, 2644, 32, 374, 421, 574, 30, 73, 10, 59, 47, 4, 45, 84, 23, 85, 2645, 192, 414, 18, 5, 1, 59, 108, 47, 4, 45, 84, 9, 262, 30, 73, 23, 1, 85, 144], [523, 32, 112, 1849, 994, 9, 686, 2646, 28, 1288, 4, 470, 990, 1850, 88, 523, 32, 112, 994, 97, 8, 1, 2647, 9, 2648, 48, 304, 1850, 88], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 6, 50, 7, 19, 10, 17, 20, 12, 2, 3, 16, 23, 1, 46, 28, 31, 24, 101, 29], [1, 62, 5, 2649, 689, 9, 1292, 1307, 8, 1152, 133, 4, 938, 172, 689, 9, 7, 804, 863, 33, 439, 97, 8, 1851, 1852, 143, 1853, 88, 1854, 2, 3, 88, 6, 289, 1, 1535, 5, 290, 22, 9, 104, 1, 183, 21, 1, 62, 9, 909, 2650, 689, 8, 2651, 133, 2652, 2653, 26, 1851, 1852, 143, 1853, 2654, 8, 88, 1854, 2, 3, 88], [6, 66, 7, 862, 433, 19, 21, 1, 239, 470, 22, 10, 1, 17, 20, 12, 2, 3, 16, 6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16], [6, 76, 18, 2655, 916, 2, 3, 57, 9, 2656, 4, 274, 1, 292, 140, 5, 1, 1181, 18, 1, 1131, 152, 916, 2, 3, 57, 9, 2657, 1, 920, 4, 1, 279, 93, 5, 1, 113, 4, 296, 35], [105, 46, 4, 281, 28, 2658, 5, 2659, 1308, 174, 21, 1, 1054, 22, 278, 2, 3, 90, 1, 28, 176, 11, 1, 159, 62, 13, 1855, 21, 1, 1054, 22, 278, 2, 3, 90], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 6, 66, 7, 862, 433, 19, 21, 1, 239, 470, 22, 10, 1, 17, 20, 12, 2, 3, 16], [9, 219, 24, 186, 119, 93, 6, 118, 680, 1, 1856, 10, 1, 272, 119, 171, 273, 2, 3, 57, 6, 15, 1, 943, 171, 176, 26, 272, 11, 24, 119, 198, 273, 2, 3, 57], [1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16, 6, 274, 4, 1282, 51, 5, 1, 144, 10, 1283, 787, 14, 17, 12, 2, 3, 16, 83], [6, 50, 7, 19, 10, 17, 20, 12, 2, 3, 16, 23, 1, 46, 28, 31, 24, 101, 2660, 101, 13, 7, 27, 235, 29, 50, 10, 1, 17, 20, 12, 2, 3, 16], [11, 1309, 4, 147, 1055, 6, 15, 1857, 1858, 1055, 21, 1, 1310, 279, 399, 1056, 2, 3, 165, 11, 44, 6, 15, 1, 1310, 399, 1056, 2, 3, 165, 9, 864, 44, 63, 283, 2661], [192, 6, 1138, 30, 73, 10, 59, 47, 4, 45, 43, 14, 1, 107, 356, 73, 538, 1139, 192, 232, 30, 73, 14, 59, 47, 4, 45, 43, 8, 105, 398, 14, 1, 107, 106, 15, 8, 17], [409, 2, 3, 87, 95, 516, 1, 2662, 5, 865, 56, 78, 1057, 9, 1, 2663, 2664, 2, 3, 87, 18, 56, 78, 381, 1186, 99, 4, 913, 9, 1000, 1311, 23, 694, 695], [6, 69, 1, 324, 1058, 811, 10, 1059, 1060, 315, 83, 14, 150, 151, 2, 3, 25, 6, 18, 428, 78, 9, 69, 19, 106, 591, 1, 106, 10, 1, 150, 103, 151, 2, 3, 25], [11, 1, 683, 1312, 6, 18, 1, 367, 207, 313, 100, 22, 180, 4, 246, 37, 42, 240, 450, 11, 698, 21, 800, 571, 9, 1313, 153, 21, 206, 83, 1061, 30, 2665, 1062, 22, 180, 4, 246, 37, 13, 7, 183, 1242, 5, 1, 450, 11, 698, 153, 21, 83, 1061, 494, 533, 63, 5, 44, 207, 133, 1859, 9, 7, 525, 1835, 1860, 772], [6, 66, 7, 862, 433, 19, 21, 1, 239, 470, 22, 10, 1, 17, 20, 12, 2, 3, 16, 1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16], [423, 366, 1841, 6, 18, 1842, 647, 14, 1843, 173, 435, 83, 79, 2, 3, 25, 423, 271, 1314, 6, 18, 271, 199, 14, 517, 315, 798, 4, 1315, 79, 2, 3, 25], [6, 66, 7, 862, 433, 19, 21, 1, 239, 470, 22, 10, 1, 17, 20, 12, 2, 3, 16, 6, 274, 4, 737, 1, 28, 14, 1, 60, 229, 21, 1, 17, 20, 12, 2, 3, 16], [68, 2666, 155, 2667, 1050, 41, 19, 13, 407, 10, 1, 102, 20, 96, 2, 3, 88, 1, 41, 19, 13, 7, 60, 100, 19, 407, 21, 1, 427, 28, 10, 155, 116, 173, 1018, 567, 865, 102, 1248, 96, 2, 3, 88], [11, 1563, 28, 697, 2668, 853, 18, 5, 1, 207, 313, 100, 22, 969, 1062, 180, 4, 246, 37, 6, 15, 1, 836, 450, 21, 1, 207, 313, 100, 22, 180, 4, 246, 37, 9, 535, 1, 467, 5, 18, 5, 111, 624, 843], [6, 216, 70, 89, 10, 364, 311, 297, 260, 14, 141, 77, 80, 12, 36, 1, 1861, 2669, 1862, 364, 311, 297, 2670, 9, 1, 1035, 29, 10, 1, 141, 77, 80, 82, 12, 36], [83, 1, 65, 32, 823, 10, 1225, 405, 189, 163, 31, 175, 1316, 202, 204, 2, 3, 57, 14, 107, 435, 1317, 202, 1318, 81, 5, 99, 204, 2, 3, 57, 33, 15, 31, 184, 14, 107, 435, 302], [98, 492, 461, 1, 91, 113, 114, 127, 42, 270, 34, 63, 34, 194, 8, 1, 139, 149, 237, 9, 95, 72, 979, 142, 546, 23, 1, 91, 113, 34, 72, 363, 1010, 8, 72, 149, 114, 127], [849, 1, 112, 934, 2671, 564, 21, 1, 217, 137, 187, 485, 336, 2, 3, 90, 5, 7, 1863, 4, 7, 1319, 293, 1864, 2672, 4, 1865, 90, 15, 137, 242, 5, 1, 217, 137, 187, 336, 2, 3, 90, 31, 7, 250], [1143, 13, 61, 23, 1, 91, 113, 114, 127, 2673, 34, 63, 14, 72, 157, 95, 72, 1697, 959, 394, 91, 142, 546, 23, 1, 91, 113, 34, 72, 363, 1010, 8, 72, 149, 114, 127], [6, 18, 1, 259, 222, 31, 1035, 2674, 42, 13, 2675, 23, 1, 422, 75, 10, 1, 1866, 1867, 103, 1868, 2, 3, 87, 1, 487, 13, 233, 10, 1, 1866, 1867, 103, 21, 1, 1244, 2676, 2677, 1868, 2, 3, 87], [1, 293, 792, 62, 5, 2678, 386, 13, 2679, 4, 1869, 332, 21, 205, 130, 293, 792, 62, 252, 1870, 62, 344, 1005, 2, 3, 57, 1, 1870, 62, 344, 183, 13, 7, 733, 15, 1871, 11, 293, 215, 1005, 2, 3, 57], [56, 35, 29, 302, 6, 15, 7, 27, 70, 56, 35, 19, 31, 175, 8, 1, 17, 20, 12, 2, 3, 16, 11, 56, 1320, 225, 1, 19, 659, 1, 17, 27, 35, 29, 12, 2, 3, 16, 214, 9, 1063, 645], [6, 18, 1, 866, 320, 34, 32, 660, 26, 1, 264, 44, 251, 146, 380, 321, 84, 1, 58, 32, 196, 179, 23, 7, 2680, 2681, 264, 146, 11, 44, 1, 2682, 44, 251, 146, 380, 277, 4, 321, 84], [8, 111, 957, 1, 722, 5, 2683, 2684, 38, 206, 1, 101, 38, 13, 640, 454, 1196, 5, 2685, 10, 77, 80, 12, 36, 8, 51, 1064, 188, 32, 640, 454, 2686, 280, 1, 654, 77, 80, 12, 36], [9, 1872, 2687, 1321, 1140, 462, 231, 94, 6, 219, 7, 501, 5, 2688, 611, 42, 547, 359, 1065, 856, 2689, 294, 2690, 21, 1, 710, 1873, 486, 1874, 2, 3, 896, 6, 219, 7, 501, 890, 925, 2691, 1875, 611, 21, 249, 4, 7, 501, 890, 925, 963, 611, 34, 547, 2692, 1876, 21, 1, 710, 1873, 486, 1874, 2, 3, 896], [1, 2693, 287, 228, 33, 7, 60, 2694, 1877, 1733, 10, 1, 139, 93, 31, 661, 2, 3, 40, 1, 212, 5, 93, 174, 21, 1, 2695, 46, 75, 33, 1150, 2696, 2697, 10, 1, 250, 75, 1878, 26, 661, 2, 3, 40], [8, 24, 167, 252, 1879, 2, 3, 117, 6, 76, 849, 1, 137, 575, 62, 31, 7, 790, 1322, 2698, 9, 1, 167, 5, 1879, 2, 3, 117, 24, 863, 19, 481, 805, 256, 1880, 83, 9, 607, 7, 790, 1322, 19], [6, 18, 1, 28, 34, 67, 2699, 4, 442, 26, 1066, 2, 3, 90, 169, 11, 2700, 8, 248, 1881, 428, 2701, 583, 673, 5, 1, 1123, 2702, 28, 1414, 4, 599, 154, 104, 289, 8, 1066, 2, 3, 90, 4, 1, 1881, 2703], [6, 50, 1882, 55, 1067, 11, 425, 394, 10, 1, 1323, 159, 62, 691, 1068, 4, 1069, 37, 292, 867, 735, 1285, 1324, 4, 2704, 1882, 227, 58, 1304, 394, 21, 1, 502, 1766, 159, 62, 32, 15, 867, 735, 1324, 4, 954, 1068, 4, 1069, 37], [24, 301, 513, 61, 23, 1, 300, 384, 734, 32, 2705, 14, 797, 5, 346, 347, 84, 23, 1, 145, 777, 4, 32, 2706, 9, 2707, 14, 1, 1883, 211, 2708, 4, 868, 23, 1, 205, 2709, 1, 234, 5, 346, 347, 84, 9, 104, 514, 6, 95, 97, 7, 300, 384, 5, 734, 4, 1, 653, 301, 515, 8, 1047, 14, 248, 478, 301, 515], [1, 780, 146, 2710, 1, 1884, 549, 550, 551, 780, 187, 125, 638, 2, 3, 110, 1, 128, 509, 8, 48, 263, 13, 7, 118, 1116, 9, 1885, 848, 729, 227, 49, 61, 23, 2711, 2712, 2713, 902, 9, 1, 1884, 549, 550, 551, 969, 780], [640, 454, 188, 752, 14, 141, 77, 80, 12, 36, 11, 259, 4, 1114, 32, 2135, 14, 2136, 228, 754, 4, 228, 2714, 752, 89, 10, 141, 77, 80, 12, 36], [8, 281, 6, 15, 525, 312, 1070, 282, 1071, 4, 1072, 36, 1188, 567, 4, 1, 1886, 790, 19, 1200, 2, 3, 25, 1325, 1, 35, 14, 525, 312, 1070, 1071, 4, 1072, 36], [1325, 1, 35, 14, 525, 312, 1070, 1071, 4, 1072, 36, 589, 6, 385, 225, 112, 405, 940, 1, 649, 532, 2715, 858, 405, 469, 4, 1, 525, 312, 1070, 2716, 405, 469, 1071, 4, 1072, 36], [6, 18, 7, 2717, 1118, 893, 19, 1887, 16, 6, 121, 23, 7, 1262, 1118, 893, 19, 1887, 16, 34, 1888, 124, 2718, 23, 30, 142, 8, 7, 168, 816], [6, 15, 1889, 1041, 75, 1, 311, 802, 9, 364, 4, 15, 1, 2719, 608, 11, 2720, 143, 134, 36, 1580, 314, 134, 36, 13, 15, 11, 1, 143], [1, 471, 22, 224, 14, 249, 1073, 338, 2, 3, 581, 1, 1073, 8, 249, 32, 2721, 670, 9, 1, 467, 28, 8, 1, 447, 224, 251, 2722, 1890, 338, 2, 3, 581], [31, 68, 81, 6, 18, 99, 1326, 295, 288, 31, 7, 576, 184, 11, 2723, 4, 2724, 6, 851, 158, 168, 668, 10, 99, 1326, 295, 288], [11, 35, 1167, 1, 17, 29, 12, 2, 3, 16, 31, 350, 31, 1891, 2725, 19, 2726, 17, 1892, 1891, 1892, 2727, 190, 2728, 2729, 225, 1, 19, 659, 1, 17, 27, 35, 29, 12, 2, 3, 16, 214, 9, 1063, 645], [1, 1893, 776, 5, 1894, 2, 3, 57, 2730, 206, 2731, 2732, 21, 1, 1895, 26, 2733, 51, 1896, 2734, 4, 2735, 1893, 776, 1482, 1897, 26, 1894, 2, 3, 57, 1493, 1, 2736, 1895, 2737, 226, 2738, 973, 8, 1, 415, 5, 1, 2739], [6, 719, 68, 1327, 9, 1, 259, 222, 563, 2, 3, 156, 34, 385, 2740, 172, 2741, 6, 662, 719, 7, 226, 1327, 9, 1, 2742, 222, 563, 2, 3, 156, 42, 1462, 290, 7, 2743], [6, 214, 1, 366, 312, 894, 576, 78, 103, 21, 1, 202, 56, 78, 403, 204, 2, 3, 57, 6, 223, 58, 10, 1260, 366, 312, 184, 175, 8, 1, 202, 20, 204, 2, 3, 57], [6, 1201, 1, 2744, 1328, 315, 10, 2745, 487, 4, 1, 243, 285, 469, 244, 4, 245, 53, 1, 309, 13, 50, 10, 1329, 14, 2746, 1898, 10, 1, 243, 285, 469, 244, 4, 245, 53], [70, 89, 8, 259, 222, 711, 33, 460, 26, 10, 141, 77, 80, 12, 36, 70, 89, 5, 1, 711, 211, 49, 13, 260, 14, 141, 77, 80, 12, 36, 228, 410, 83, 2747, 1330], [6, 216, 70, 89, 10, 364, 311, 297, 260, 14, 141, 77, 80, 12, 36, 11, 51, 188, 6, 260, 248, 311, 297, 228, 410, 26, 949, 5, 77, 80, 12, 36], [131, 31, 1331, 6, 1142, 51, 568, 34, 858, 9, 1899, 8, 1, 577, 680, 2748, 21, 579, 2, 3, 64, 8, 1, 119, 680, 472, 6, 2749, 15, 1, 577, 578, 97, 26, 579, 2, 3, 64, 8, 938, 1332, 568, 283, 577, 2750], [6, 18, 1, 866, 320, 34, 32, 660, 26, 1, 264, 44, 251, 146, 380, 321, 84, 662, 6, 18, 83, 1, 2751, 146, 52, 344, 4, 94, 1, 264, 44, 251, 146, 277, 4, 321, 84, 4, 1230, 52], [6, 216, 70, 89, 10, 364, 311, 297, 260, 14, 141, 77, 80, 12, 36, 11, 70, 89, 281, 6, 18, 7, 141, 77, 80, 82, 97, 8, 12, 36], [23, 130, 2752, 1322, 2753, 4, 1900, 87, 118, 509, 7, 29, 61, 23, 7, 70, 184, 42, 13, 50, 23, 7, 2754, 144, 2755, 4, 1900, 87, 67, 1, 118, 9, 719, 7, 70, 29, 50, 23, 1, 28, 21, 1, 2756, 511, 9, 374, 1901, 130, 1876], [6, 396, 1, 729, 437, 847, 103, 14, 1898, 4, 1, 518, 285, 469, 519, 64, 6, 18, 7, 520, 415, 5, 869, 4, 18, 518, 519, 64, 31, 1, 437, 285, 82], [6, 15, 1, 226, 99, 81, 14, 107, 106, 4, 256, 416, 14, 326, 200, 105, 169, 190, 92, 79, 2, 3, 25, 6, 18, 1, 445, 81, 5, 99, 79, 2, 3, 25], [6, 147, 1, 70, 89, 5, 667, 211, 413, 235, 49, 10, 1, 77, 80, 82, 12, 36, 70, 89, 395, 32, 109, 10, 77, 80, 12, 36], [8, 709, 1, 52, 1299, 1, 1195, 227, 103, 715, 36, 83, 1, 52, 1299, 1, 1195, 103, 715, 36, 4, 98, 492, 1815, 18, 5, 7, 1333, 4, 7, 2757], [6, 18, 518, 519, 64, 9, 285, 1, 106, 648, 2758, 1, 46, 608, 6, 18, 518, 519, 64, 9, 285, 19, 106, 14, 7, 520, 415, 1828], [6, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40, 1, 49, 11, 1, 44, 572, 35, 558, 67, 50, 23, 1, 1334, 54, 22, 12, 40], [584, 13, 7, 375, 234, 5, 146, 1020, 156, 1, 679, 5, 1, 1335, 29, 13, 7, 389, 115, 8, 1, 827, 5, 7, 1174, 5, 2759, 1241, 146, 584, 1020, 156], [1, 188, 11, 2760, 4, 1484, 95, 870, 123, 516, 8, 482, 2, 3, 117, 131, 1144, 35, 143, 5, 125, 14, 1483, 115, 33, 118, 97, 8, 482, 2, 3, 117], [1, 130, 287, 13, 406, 594, 379, 277, 2, 3, 40, 1, 380, 1074, 406, 594, 379, 866, 277, 2, 3, 40, 320, 42, 32, 2761, 782, 34, 2762, 1902, 1903, 478, 242, 4, 205, 28], [1904, 2763, 13, 1, 305, 2764, 9, 2765, 1905, 4, 1906, 329, 1336, 167, 2766, 104, 9, 1113, 1, 305, 183, 26, 1160, 1022, 1337, 9, 1, 2767, 11, 1148, 1904, 1905, 4, 1906, 329], [463, 2, 3, 87, 871, 34, 1338, 5, 1, 1029, 354, 8, 1, 1339, 22, 94, 67, 663, 463, 2, 3, 87, 2768, 1, 1907, 480, 63, 305, 1339, 872, 22, 4, 2769, 34, 663, 1340, 11, 1338, 5, 1, 30, 354, 241, 254, 325, 5, 1, 1908, 838, 993, 14, 2770, 5, 663], [105, 41, 65, 18, 155, 116, 173, 342, 4, 474, 197, 100, 41, 65, 32, 50, 206, 1, 1909, 5, 1, 46, 28, 10, 614, 615, 87, 14, 155, 116, 839, 342, 4, 474, 197], [6, 15, 382, 383, 87, 11, 48, 340, 4, 1, 139, 2771, 2772, 15, 8, 1, 340, 516, 8, 193, 131, 494, 6, 15, 1, 1910, 81, 8, 382, 383, 87], [6, 752, 89, 10, 141, 77, 80, 12, 36, 1, 70, 89, 147, 13, 76, 196, 179, 14, 141, 77, 80, 82, 14, 228, 488, 297, 12, 36], [589, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16, 393, 6, 15, 17, 20, 31, 27, 296, 12, 2, 3, 16], [11, 359, 1, 101, 38, 29, 6, 15, 1, 177, 38, 20, 17, 12, 2, 3, 16, 8, 124, 60, 2773, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 302], [48, 188, 8, 1, 130, 2774, 2775, 182, 9, 2776, 10, 1, 39, 120, 1256, 86, 2, 3, 53, 6, 76, 182, 51, 63, 10, 39, 120, 86, 2, 3, 53], [6, 15, 60, 163, 169, 8, 92, 152, 79, 2, 3, 25, 131, 6, 15, 271, 199, 2777, 226, 99, 770, 1141, 1, 139, 188, 11, 1, 1233, 1341, 1, 81, 33, 419, 21, 92, 152, 79, 2, 3, 25], [542, 334, 234, 543, 4, 544, 387, 2778, 9, 1, 118, 2779, 861, 1342, 9, 24, 167, 351, 5, 1911, 5, 137, 242, 8, 1, 308, 5, 542, 334, 234, 543, 4, 544, 387], [105, 41, 65, 18, 155, 116, 173, 342, 4, 474, 197, 100, 41, 65, 32, 50, 206, 1, 1909, 5, 1, 46, 28, 10, 614, 615, 87, 14, 155, 116, 839, 342, 4, 474, 197], [6, 69, 1, 324, 1058, 811, 10, 1059, 1060, 315, 14, 150, 151, 2, 3, 25, 83, 10, 150, 151, 2, 3, 25], [11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16, 51, 24, 35, 49, 32, 61, 23, 17, 12, 2, 3, 16, 4, 60, 682, 11, 46, 4, 1343, 1, 65], [6, 18, 317, 222, 31, 24, 143, 314, 134, 36, 14, 60, 1912, 344, 6, 1075, 48, 873, 13, 327, 641, 14, 1, 317, 143, 314, 15, 11, 436, 134, 36], [11, 359, 1, 101, 38, 29, 6, 15, 1, 177, 38, 20, 17, 12, 2, 3, 16, 8, 124, 60, 929, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16], [6, 18, 1, 54, 85, 22, 12, 40, 31, 1, 825, 21, 42, 9, 219, 1, 2780, 2781, 239, 2782, 219, 24, 1344, 146, 21, 1, 1913, 540, 5, 1, 54, 22, 213, 170, 12, 40], [6, 18, 1076, 199, 1914, 14, 1915, 349, 4, 158, 168, 199, 1115, 14, 68, 753, 162, 21, 92, 79, 2, 3, 25, 423, 271, 1314, 6, 18, 271, 199, 14, 517, 315, 798, 4, 1315, 79, 2, 3, 25], [51, 28, 15, 8, 24, 58, 32, 874, 721, 4, 274, 10, 1, 120, 20, 86, 2, 3, 53, 6, 274, 44, 28, 4, 864, 298, 28, 10, 1, 39, 120, 20, 86, 2, 3, 53], [786, 1, 126, 193, 240, 1916, 1, 512, 22, 33, 473, 10, 1, 148, 135, 110, 83, 1, 22, 13, 473, 4, 224, 26, 186, 23, 105, 1026, 10, 1, 148, 135, 110], [11, 48, 265, 6, 15, 1, 271, 199, 184, 21, 92, 14, 517, 2783, 79, 2, 3, 25, 83, 6, 18, 271, 199, 14, 517, 349, 175, 10, 1, 92, 20, 79, 2, 3, 25], [51, 24, 49, 32, 664, 14, 7, 60, 27, 29, 66, 14, 17, 12, 2, 3, 16, 24, 56, 35, 49, 48, 1303, 32, 7, 2784, 21, 24, 323, 17, 12, 2, 3, 16, 61, 49, 21, 2785], [6, 147, 24, 975, 8, 1, 726, 5, 1, 1345, 57, 143, 62, 1917, 2, 3, 57, 1, 555, 49, 32, 397, 23, 1, 2786, 1150, 872, 35, 62, 5, 1345, 57, 1917, 2, 3, 57], [172, 163, 95, 123, 15, 8, 601, 167, 26, 409, 2, 3, 87, 6, 397, 24, 82, 14, 694, 732, 358, 34, 67, 15, 8, 409, 2, 3, 87], [145, 13, 21, 475, 2, 3, 117, 1, 1284, 5, 30, 1045, 13, 1918, 231, 475, 2, 3, 117, 770, 34, 1, 1919, 1920, 11, 30, 178, 13, 2787], [30, 73, 10, 59, 20, 47, 4, 45, 84, 1, 107, 580, 31, 169, 8, 46, 229, 11, 2788, 73, 13, 109, 26, 59, 47, 4, 45, 84, 8, 105, 398, 14, 1, 107, 726], [6, 289, 34, 10, 150, 151, 2, 3, 25, 9, 285, 1, 106, 13, 650, 852, 83, 10, 150, 151, 2, 3, 25], [1, 217, 137, 187, 485, 336, 2, 3, 90, 13, 7, 390, 22, 247, 14, 137, 242, 1509, 1, 549, 550, 551, 236, 5, 1, 217, 2789, 5, 1, 255, 1741, 1337, 11, 137, 724, 8, 44, 13, 1, 217, 137, 187, 336, 2, 3, 90], [1, 29, 33, 50, 23, 1, 44, 4, 867, 236, 5, 1, 54, 22, 213, 131, 12, 40, 1, 54, 28, 75, 351, 5, 1921, 125, 5, 1, 305, 236, 5, 1, 54, 22, 12, 40], [11, 231, 475, 2, 3, 117, 555, 471, 658, 1346, 178, 4, 1347, 178, 8, 595, 4, 738, 2790, 2, 3, 117, 214, 30, 178, 9, 738, 4, 736, 656, 1077, 595], [6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16, 8, 709, 6, 18, 17, 12, 2, 3, 16, 9, 69, 7, 427, 27, 235, 29, 23, 1, 2791, 22], [1896, 2792, 23, 48, 29, 95, 268, 34, 1, 256, 416, 1255, 156, 1078, 205, 199, 530, 252, 158, 168, 199, 294, 1922, 2793, 2794, 29, 11, 1, 391, 62, 13, 61, 23, 256, 416, 1255, 156], [6, 50, 7, 212, 5, 841, 38, 49, 10, 1, 17, 20, 12, 2, 3, 16, 8, 124, 107, 1923, 15, 1, 17, 20, 12, 2, 3, 16, 9, 69, 60, 27, 49, 14, 107, 739], [423, 2795, 627, 2, 3, 201, 97, 9, 18, 928, 1924, 238, 875, 9, 19, 112, 868, 2796, 2, 3, 201, 2797, 7, 928, 1924, 238, 309, 4, 2798, 1925, 286], [51, 28, 15, 8, 24, 58, 32, 874, 721, 4, 274, 10, 1, 120, 20, 86, 2, 3, 53, 6, 274, 44, 28, 4, 864, 298, 28, 10, 1, 39, 120, 20, 86, 2, 3, 53], [1, 1268, 236, 5, 28, 622, 21, 846, 4, 1269, 2799, 1270, 5, 1, 54, 183, 12, 40, 1, 261, 5, 239, 28, 15, 8, 1, 58, 13, 1, 54, 310, 12, 40], [6, 1075, 48, 873, 13, 327, 641, 14, 1, 317, 143, 314, 15, 11, 436, 134, 36, 317, 134, 36, 13, 1, 1670, 355, 314, 1153, 15, 9, 337, 1, 133, 436, 188], [589, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16, 6, 15, 1, 17, 20, 12, 2, 3, 16, 9, 69, 60, 27, 49, 14, 107, 739], [589, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16, 1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [1, 161, 2800, 11, 1, 606, 35, 617, 7, 75, 5, 65, 2801, 840, 505, 1008, 83, 2802, 2803, 83, 633, 1277, 83, 26, 2804, 1, 634, 250, 222, 47, 4, 45, 36, 505, 1008, 83, 1643, 1644, 386, 1008, 83, 840, 2805, 634, 128, 9, 27, 35, 47, 4, 45, 36, 635, 65, 1, 1645, 35, 377, 1646, 713, 83, 1647, 1648, 713, 505, 633, 83, 319, 505, 13, 1, 209, 1228], [1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16, 6, 15, 1, 17, 20, 12, 2, 3, 16, 9, 69, 60, 27, 49, 14, 107, 739], [1, 2806, 21, 1, 1202, 32, 400, 4, 473, 10, 148, 135, 110, 31, 176, 26, 1, 2807, 51, 44, 28, 32, 119, 224, 4, 473, 10, 1, 148, 135, 110], [9, 2808, 1, 2809, 5, 782, 6, 1321, 876, 171, 1, 44, 353, 22, 1926, 4, 1927, 43, 1, 629, 209, 41, 19, 13, 50, 21, 1, 917, 540, 5, 44, 353, 22, 1926, 4, 1927, 43], [101, 30, 132, 67, 153, 26, 464, 59, 8, 105, 398, 4, 817, 10, 1, 356, 370, 47, 4, 45, 43, 35, 65, 67, 50, 206, 1, 239, 28, 34, 33, 374, 593, 10, 59, 47, 4, 45, 43, 8, 105, 398, 4, 1, 2810, 370, 33, 15, 9, 2811], [1, 188, 2812, 8, 193, 131, 32, 153, 14, 1, 1318, 184, 50, 10, 1, 202, 403, 204, 2, 3, 57, 23, 24, 1928, 565, 88, 422, 4, 46, 144, 2813, 2814, 720, 19, 13, 7, 405, 189, 184, 50, 23, 1, 2815, 85, 46, 28, 10, 202, 204, 2, 3, 57], [6, 50, 7, 212, 5, 841, 38, 49, 10, 1, 17, 20, 12, 2, 3, 16, 8, 124, 107, 1923, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [1, 286, 5, 24, 103, 13, 555, 14, 1, 221, 623, 153, 14, 7, 1929, 5, 1, 665, 103, 131, 665, 740, 42, 1325, 1, 157, 5, 68, 2816, 30, 26, 2817, 34, 30, 812, 221, 13, 109, 10, 1, 665, 103, 665, 740], [6, 18, 1, 445, 81, 5, 99, 79, 2, 3, 25, 6, 18, 7, 2818, 92, 79, 2, 3, 25, 81, 5, 256, 1672, 4, 489, 8, 553, 31, 350, 31, 7, 650, 852, 437, 1930, 200, 81, 21, 1, 2819, 2820], [11, 231, 475, 2, 3, 117, 770, 34, 1, 1919, 1920, 11, 30, 178, 13, 62, 1348, 475, 2, 3, 117, 214, 30, 178, 9, 738, 4, 736, 656, 1077, 595], [2821, 481, 272, 273, 2, 3, 57, 11, 119, 198, 4, 1215, 7, 1931, 11, 1931, 1838, 4, 249, 1162, 626, 9, 2822, 1, 2823, 6, 15, 1, 943, 171, 176, 26, 272, 11, 24, 119, 198, 273, 2, 3, 57], [51, 24, 35, 49, 32, 61, 23, 17, 12, 2, 3, 16, 4, 60, 682, 11, 46, 4, 1343, 1, 2824, 24, 49, 32, 664, 14, 7, 60, 27, 29, 66, 14, 17, 12, 2, 3, 16], [1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16, 6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16], [589, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16, 11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16], [6, 2825, 2826, 2827, 129, 655, 1932, 11, 905, 11, 1636, 1, 2828, 23, 1, 1602, 5, 1, 362, 2, 3, 88, 2829, 984, 1, 258, 195, 2830, 26, 362, 2, 3, 88, 11, 48, 265], [98, 129, 123, 268, 8, 323, 167, 23, 293, 303, 34, 1, 1740, 55, 431, 211, 833, 112, 731, 1933, 1, 333, 1934, 9, 2831, 7, 1935, 211, 789, 1036, 4, 1037, 40, 24, 167, 13, 76, 601, 9, 1036, 4, 1037, 40, 319, 1, 142, 211, 1, 63, 23, 1, 431, 1738, 112, 731, 8, 1, 55, 758, 13, 15, 9, 1739, 7, 162, 369], [172, 530, 67, 15, 9, 2832, 8, 1, 1, 1048, 472, 62, 573, 7, 4, 1239, 472, 62, 573, 989, 5, 1, 834, 62, 835, 195, 115, 8, 465, 639, 2, 3, 1, 29, 34, 153, 1, 606, 286, 8, 1, 195, 115, 190, 1239, 472, 62, 5, 565, 88, 1192, 2, 3, 88, 4, 53, 639, 2, 3, 53, 1497, 465, 9, 121, 2833, 2834], [27, 38, 29, 7, 60, 2835, 2836, 27, 38, 29, 33, 66, 10, 1, 220, 261, 17, 20, 12, 2, 3, 16, 14, 106, 75, 72, 9, 797, 5, 2837, 2838, 491, 7, 27, 38, 29, 66, 10, 1, 17, 20, 12, 2, 3, 16, 4, 1, 958, 582, 183, 590, 1, 125, 8, 1, 147, 75, 33, 15, 9, 637, 1], [11, 1, 234, 5, 346, 347, 84, 9, 104, 514, 6, 95, 97, 7, 300, 384, 5, 734, 4, 1, 653, 301, 515, 8, 1047, 14, 248, 478, 301, 2839, 1, 301, 513, 8, 253, 325, 4, 344, 6, 154, 808, 7, 1232, 850, 9, 413, 730, 125, 539, 1, 1275, 1732, 9, 1, 115, 5, 346, 347, 84], [51, 603, 361, 1349, 11, 93, 119, 1007, 325, 276, 32, 21, 39, 120, 86, 2, 3, 53, 236, 5, 876, 198, 4, 736, 656, 1077, 11, 1936, 1937, 93, 32, 196, 179, 10, 39, 120, 86, 2, 3, 53], [103, 170, 1862, 7, 1938, 103, 11, 1, 1289, 619, 1939, 2, 3, 37, 9, 121, 7, 52, 6, 18, 7, 1289, 184, 9, 821, 1, 2840, 4, 396, 1, 1938, 2841, 103, 1939, 2, 3, 37, 11, 435, 411], [1, 62, 13, 236, 5, 1, 130, 143, 64, 877, 569, 2, 3, 64, 130, 1350, 142, 2842, 13, 1, 62, 5, 2843, 1, 142, 5, 7, 654, 5, 125, 23, 7, 1885, 21, 83, 9, 170, 569, 2, 3, 64], [6, 15, 1, 139, 164, 2844, 31, 1079, 2, 3, 16, 6, 76, 18, 2845, 9, 225, 659, 1, 2846, 99, 82, 4, 99, 101, 5, 1079, 2, 3, 16], [1, 39, 52, 83, 71, 2, 3, 37, 33, 15, 9, 723, 51, 55, 2847, 139, 2848, 5, 980, 33, 214, 9, 1, 217, 187, 10, 1, 39, 2849, 29, 9, 723, 55, 361, 74, 71, 2, 3, 37], [83, 1040, 1214, 6, 2850, 4, 920, 616, 4, 1351, 1940, 21, 111, 1597, 10, 1, 272, 20, 273, 2, 3, 57, 11, 1941, 6, 1049, 4, 181, 1, 28, 10, 272, 273, 2, 3, 57], [56, 35, 29, 302, 6, 15, 7, 27, 70, 56, 35, 19, 31, 175, 8, 1, 17, 20, 12, 2, 3, 16, 11, 56, 1320, 18, 1, 17, 27, 35, 29, 12, 2, 3, 16, 9, 424, 24, 65], [1, 126, 193, 13, 174, 21, 7, 239, 133, 585, 23, 1, 30, 472, 10, 856, 59, 47, 4, 45, 43, 1, 22, 13, 118, 593, 10, 7, 30, 73, 370, 47, 4, 45, 43], [48, 401, 13, 650, 72, 9, 1, 330, 5, 1080, 1081, 4, 1082, 36, 131, 2851, 13, 7, 310, 5, 108, 682, 11, 372, 41, 686, 61, 23, 1, 2852, 1080, 330, 1081, 4, 1082, 36], [2853, 10, 1159, 6, 18, 1, 486, 128, 97, 26, 1942, 4, 482, 36, 8, 269, 9, 644, 1, 257, 1352, 804, 636, 2854, 1943, 6, 18, 1, 195, 486, 421, 26, 1942, 4, 482, 36], [1944, 6, 1945, 18, 1, 39, 120, 20, 131, 86, 2, 3, 53, 9, 1946, 111, 838, 11, 1, 136, 218, 8, 1, 143, 500, 18, 1, 39, 120, 878, 171, 11, 186, 198, 86, 2, 3, 53], [26, 726, 699, 2855, 4, 2856, 11, 51, 2857, 1, 1300, 162, 154, 104, 1947, 9, 1, 162, 97, 8, 451, 4, 690, 156, 1, 1083, 13, 72, 9, 1, 1083, 11, 1, 189, 162, 8, 451, 4, 690, 156], [94, 6, 484, 1, 711, 8, 286, 11, 70, 89, 10, 68, 821, 1211, 608, 935, 84, 14, 2858, 1330, 6, 688, 70, 89, 5, 286, 667, 10, 1146, 1147, 935, 84], [1944, 6, 1945, 18, 1, 39, 120, 20, 131, 86, 2, 3, 53, 9, 1946, 111, 838, 11, 1, 136, 218, 8, 1, 143, 500, 18, 1, 39, 120, 878, 171, 11, 186, 198, 86, 2, 3, 53], [51, 603, 361, 1349, 11, 93, 119, 1007, 325, 276, 32, 21, 39, 120, 86, 2, 3, 53, 236, 5, 876, 198, 4, 736, 656, 1077, 11, 1936, 1937, 93, 32, 196, 179, 10, 39, 120, 86, 2, 3, 53], [8, 1948, 4, 1949, 36, 1, 324, 5, 758, 61, 2859, 33, 15, 9, 496, 7, 75, 5, 125, 8, 2860, 2861, 1253, 2862, 33, 76, 1, 19, 15, 9, 496, 125, 8, 1948, 4, 1949, 36], [89, 33, 484, 10, 7, 141, 77, 12, 36, 14, 507, 1353, 2863, 89, 13, 484, 23, 1, 259, 314, 10, 141, 77, 80, 12, 36, 14, 699, 507, 4, 228, 410], [11, 1309, 4, 147, 1055, 6, 15, 1857, 1858, 1055, 21, 1, 1310, 279, 399, 1056, 2, 3, 165, 9, 2864, 48, 216, 6, 260, 1, 2865, 142, 211, 63, 4, 248, 279, 2866, 21, 1, 2867, 399, 1056, 2, 3, 165], [6, 15, 102, 96, 25, 9, 348, 1051, 41, 65, 14, 116, 173, 23, 1, 209, 140, 5, 1, 239, 46, 2868, 6, 15, 1, 102, 20, 96, 25, 9, 121, 51, 41, 65, 15, 8, 48, 167, 386, 105, 11, 28, 824, 4, 11, 1, 235, 49, 15, 11, 1830, 143], [6, 66, 7, 100, 41, 19, 23, 1, 44, 140, 5, 703, 10, 102, 96, 25, 6, 15, 102, 96, 25, 9, 348, 1051, 41, 65, 14, 116, 173, 23, 1, 209, 140, 5, 1, 239, 46, 144], [68, 707, 5, 1, 2869, 5, 7, 2870, 1165, 7, 477, 1859, 33, 260, 206, 1, 247, 44, 353, 2871, 22, 1950, 2, 3, 64, 8, 1, 872, 1861, 6, 508, 1, 1396, 5, 7, 388, 5, 247, 44, 353, 1950, 2, 3, 64], [6, 18, 1, 207, 313, 100, 22, 180, 4, 246, 37, 9, 981, 1, 41, 19, 222, 11, 7, 2872, 1, 2873, 13, 318, 169, 833, 327, 6, 18, 1, 207, 313, 100, 22, 180, 4, 246, 37, 9, 219, 1, 367, 763, 250], [56, 35, 29, 302, 6, 15, 7, 27, 70, 56, 35, 19, 31, 175, 8, 1, 17, 20, 12, 2, 3, 16, 11, 56, 1790, 17, 38, 20, 12, 2, 3, 16, 586, 7, 514, 70, 35, 29, 2874, 643, 1, 2875, 2876], [56, 35, 29, 302, 6, 15, 7, 27, 70, 56, 35, 19, 31, 175, 8, 1, 17, 20, 12, 2, 3, 16, 11, 56, 1320, 50, 7, 19, 10, 17, 20, 12, 2, 3, 16, 23, 1, 46, 28, 31, 24, 101, 29], [7, 1245, 13, 7, 406, 2877, 334, 1242, 5, 2878, 1951, 5, 63, 1952, 2, 3, 37, 1, 1245, 1042, 578, 13, 7, 2879, 213, 5, 2880, 578, 1952, 2, 3, 37], [2881, 7, 388, 5, 1567, 183, 14, 1953, 30, 218, 338, 4, 1954, 1084, 6, 18, 7, 75, 5, 1953, 30, 218, 21, 7, 678, 196, 179, 26, 338, 4, 1954, 1084], [6, 153, 1955, 1212, 10, 1, 266, 164, 122, 267, 2, 3, 64, 21, 112, 1956, 131, 51, 361, 67, 233, 10, 1, 266, 556, 164, 122, 267, 2, 3, 64], [6, 1354, 1, 160, 200, 8, 357, 283, 389, 191, 10, 39, 120, 86, 2, 3, 53, 6, 18, 1, 39, 120, 878, 171, 11, 186, 198, 86, 2, 3, 53], [437, 1957, 370, 9, 1044, 1, 1958, 437, 304, 1959, 88, 11, 1, 1958, 437, 304, 1244, 2882, 154, 104, 807, 26, 1957, 1, 1804, 1959, 88], [6, 153, 1955, 1212, 10, 1, 266, 164, 122, 267, 2, 3, 64, 21, 112, 1956, 131, 1, 361, 67, 414, 10, 1, 266, 556, 164, 122, 267, 2, 3, 64], [6, 192, 719, 8, 327, 741, 7, 1960, 298, 22, 1, 217, 298, 187, 742, 2, 3, 40, 11, 298, 6, 18, 1, 217, 298, 187, 213, 2883, 1626, 742, 2, 3, 40], [8, 269, 9, 1621, 70, 89, 5, 1, 153, 188, 6, 18, 1, 141, 77, 80, 82, 12, 36, 42, 2884, 1, 528, 2885, 34, 7, 460, 711, 11, 70, 89, 281, 6, 18, 7, 141, 77, 80, 82, 97, 8, 12, 36], [70, 89, 8, 259, 222, 711, 33, 460, 26, 10, 141, 77, 80, 12, 36, 89, 33, 484, 10, 7, 141, 77, 12, 36, 14, 507, 1353, 1961], [1, 1675, 29, 1085, 23, 1, 2886, 5, 1, 29, 138, 8, 1962, 4, 781, 2887, 1, 2888, 184, 481, 7, 1588, 29, 359, 23, 167, 138, 8, 4, 1962, 4, 781, 2889], [11, 2890, 1, 1934, 2891, 6, 18, 1, 1062, 22, 425, 180, 4, 246, 37, 31, 7, 1963, 1327, 6, 18, 1716, 450, 21, 1, 207, 313, 22, 180, 4, 246, 37], [126, 218, 67, 174, 21, 1724, 30, 132, 4, 1725, 421, 26, 59, 47, 4, 45, 43, 10, 1, 966, 5, 705, 356, 4, 2892, 132, 67, 417, 10, 59, 47, 4, 45, 43], [6, 1964, 51, 5, 24, 58, 8, 202, 204, 2, 3, 57, 10, 271, 2893, 204, 2, 3, 57, 42, 240, 1, 81, 5, 51, 493, 530, 33, 15, 8, 24, 678], [105, 144, 67, 174, 21, 1, 220, 85, 22, 1355, 652, 64, 1, 85, 28, 67, 419, 21, 1355, 652, 64, 42, 586, 1334, 144, 14, 164], [6, 1964, 51, 5, 24, 58, 8, 202, 204, 2, 3, 57, 10, 271, 2894, 223, 58, 10, 1260, 366, 312, 184, 175, 8, 1, 202, 20, 204, 2, 3, 57], [1, 582, 832, 46, 22, 33, 564, 21, 1, 54, 310, 12, 40, 1, 58, 939, 23, 35, 21, 305, 9, 44, 10, 1, 54, 28, 12, 40], [384, 29, 1673, 1, 384, 29, 34, 13, 66, 283, 1, 1080, 425, 330, 1081, 4, 1082, 36, 48, 401, 13, 650, 72, 9, 1, 330, 5, 1080, 1081, 4, 1082, 36], [11, 205, 394, 6, 18, 1, 144, 414, 169, 11, 1, 1323, 159, 62, 1068, 4, 1069, 37, 6, 18, 1, 1323, 1068, 4, 1069, 37, 377, 28, 21, 2895, 332, 394, 292, 2896, 735, 2897, 1324, 572, 4, 954], [193, 170, 1021, 24, 299, 19, 14, 7, 1356, 5, 1, 299, 19, 97, 8, 1086, 4, 1087, 57, 6, 1965, 34, 24, 19, 1078, 1, 19, 97, 8, 1086, 4, 1087, 57, 8, 51, 1064], [193, 170, 1021, 24, 299, 19, 14, 7, 1356, 5, 1, 299, 19, 97, 8, 1086, 4, 1087, 57, 6, 1965, 34, 24, 19, 1078, 1, 19, 97, 8, 1086, 4, 1087, 57, 8, 51, 1064], [172, 188, 2898, 797, 617, 8, 2899, 2, 3, 43, 319, 1, 1357, 162, 2900, 2901, 1717, 1358, 286, 602, 1, 1966, 2902, 2, 3, 43, 95, 268, 1, 1966, 162, 9, 104, 2903, 8, 1967, 4, 1, 1357, 162, 8, 1967, 131, 319, 840, 4, 699, 32, 1, 212, 5, 2904, 8, 200, 1006, 83, 4, 1006, 94, 1864], [11, 231, 357, 510, 2, 3, 37, 7, 1359, 164, 511, 851, 48, 2905, 48, 1968, 7, 1262, 1359, 164, 2906, 618, 1, 357, 511, 510, 2, 3, 37, 33, 2907], [31, 11, 1969, 35, 6, 18, 1, 39, 52, 74, 71, 2, 3, 37, 9, 210, 44, 1970, 727, 18, 39, 52, 74, 71, 2, 3, 37, 9, 210, 160, 200, 4, 55, 242], [11, 105, 44, 4, 305, 6, 15, 1, 1321, 876, 171, 148, 135, 110, 9, 210, 2908, 11, 305, 28, 1138, 6, 15, 1, 148, 135, 110, 632], [6, 69, 24, 19, 23, 7, 388, 5, 1, 1971, 1972, 425, 22, 463, 2, 3, 57, 6, 66, 7, 804, 863, 1899, 94, 1824, 83, 10, 1, 2909, 22, 463, 2, 3, 57], [1, 1015, 32, 637, 10, 7, 27, 29, 66, 10, 17, 12, 2, 3, 16, 1, 1234, 1973, 118, 112, 1088, 32, 60, 49, 10, 408, 294, 1360, 50, 10, 17, 12, 2, 3, 16], [1, 1015, 32, 637, 10, 7, 27, 29, 66, 10, 17, 12, 2, 3, 16, 1, 1234, 1973, 161, 13, 66, 23, 1089, 5, 68, 177, 27, 38, 161, 17, 12, 2, 3, 16], [6, 216, 70, 89, 10, 364, 311, 297, 260, 14, 141, 77, 80, 12, 36, 89, 33, 484, 10, 7, 141, 77, 12, 36, 14, 507, 1353, 1961], [31, 11, 1969, 35, 6, 18, 1, 39, 52, 74, 71, 2, 3, 37, 9, 210, 44, 1970, 727, 18, 39, 52, 74, 71, 2, 3, 37, 9, 210, 160, 200, 4, 55, 242], [63, 67, 2910, 4, 182, 10, 1, 249, 1256, 8, 1, 272, 94, 20, 273, 2, 3, 57, 11, 1941, 6, 1049, 4, 181, 1, 28, 10, 272, 273, 2, 3, 57], [44, 125, 32, 373, 283, 55, 782, 26, 39, 52, 71, 2, 3, 37, 1, 1817, 242, 32, 51, 1, 537, 191, 660, 26, 1, 39, 55, 52, 71, 2, 3, 37], [105, 5, 24, 49, 67, 61, 23, 1, 17, 161, 12, 2, 3, 16, 1, 161, 13, 66, 23, 1089, 5, 68, 177, 27, 38, 161, 17, 12, 2, 3, 16], [6, 76, 15, 1974, 1975, 4, 1976, 288, 11, 704, 1, 2911, 486, 1977, 2912, 1, 1974, 486, 1975, 4, 1976, 288, 13, 15, 11, 970, 1, 331, 75, 5, 1309, 63, 5, 83], [35, 190, 1, 1978, 201, 877, 1979, 2, 3, 201, 1, 483, 1130, 5, 1, 2913, 655, 1932, 631, 8, 105, 2914, 5, 1, 1978, 201, 159, 62, 2915, 35, 4, 2916, 792, 1979, 2, 3, 201], [1291, 4, 2917, 649, 1980, 13, 7, 2918, 980, 2919, 11, 352, 2920, 498, 197, 68, 231, 5, 290, 7, 2921, 1737, 13, 649, 1980, 498, 197], [1981, 222, 1982, 6, 15, 1, 1982, 249, 1983, 2, 3, 117, 257, 251, 9, 1901, 1745, 11, 111, 30, 61, 23, 493, 1311, 386, 1984, 1985, 4, 995, 2922, 192, 95, 757, 7, 195, 222, 10, 1, 1981, 1983, 2, 3, 117, 257, 251, 9, 111, 30, 8, 1, 75, 5, 1494, 2923], [54, 94, 12, 40, 98, 13, 7, 22, 5, 85, 306, 8, 1011, 394, 21, 1, 208, 5, 1, 203, 449, 6, 15, 7, 388, 5, 1, 28, 176, 11, 1, 391, 877, 23, 70, 56, 35, 94, 42, 351, 1189, 5, 306, 21, 1, 54, 22, 12, 40], [6, 76, 196, 179, 7, 1660, 433, 340, 319, 1, 1007, 32, 830, 61, 23, 1, 1233, 132, 153, 26, 1661, 340, 5, 829, 2, 3, 64, 829, 2, 3, 64, 509, 7, 82, 319, 261, 133, 13, 830, 9, 1794, 1, 209, 30, 269, 61, 23, 30, 73], [1986, 13, 1987, 728, 31, 68, 345, 322, 62, 643, 1, 91, 113, 114, 127, 34, 1, 30, 157, 13, 1988, 26, 1, 75, 5, 149, 8, 42, 98, 2924, 65, 5, 157, 360, 1, 91, 113, 114, 127, 42, 270, 34, 112, 63, 34, 194, 8, 72, 149, 95, 72, 365], [1, 28, 15, 11, 1, 58, 138, 8, 48, 263, 622, 1710, 21, 1711, 491, 512, 4, 1, 54, 22, 5, 203, 845, 208, 12, 40, 1274, 54, 94, 12, 40, 98, 13, 7, 22, 5, 85, 306, 8, 1011, 394, 21, 1, 208, 5, 1, 203, 449], [11, 1, 41, 19, 6, 15, 1, 102, 20, 96, 25, 9, 348, 7, 100, 41, 19, 23, 1, 209, 140, 5, 1, 54, 22, 717, 14, 844, 1989, 1246, 14, 116, 2925, 19, 11, 51, 1795, 49, 6, 18, 7, 1050, 116, 291, 41, 19, 50, 10, 1, 102, 20, 96, 25], [118, 15, 26, 1990, 2, 3, 16, 1, 1690, 183, 240, 253, 332, 354, 5, 1361, 695, 419, 21, 2926, 539, 1090, 1991, 2927, 4, 2928, 2929, 14, 507, 1984, 9, 337, 2930, 11, 195, 215, 6, 18, 1, 2931, 1361, 695, 803, 26, 1990, 2, 3, 16, 11, 1304, 332, 1361, 1135, 1090, 989, 1991, 924, 2932, 2933, 505, 4], [1, 255, 1362, 231, 1363, 1364, 104, 1, 54, 22, 12, 40, 11, 48, 265, 6, 18, 1, 54, 22, 12, 40], [11, 41, 343, 6, 50, 7, 1220, 100, 116, 291, 371, 19, 23, 1, 209, 386, 44, 140, 5, 1, 46, 825, 10, 102, 96, 25, 8, 269, 9, 337, 1, 1677, 5, 111, 29, 6, 69, 100, 41, 65, 11, 111, 41, 10, 102, 96, 25], [328, 15, 1, 1206, 164, 122, 266, 267, 2, 3, 64, 11, 1, 164, 1, 361, 67, 414, 10, 1, 266, 556, 164, 122, 267, 2, 3, 64], [1, 215, 33, 223, 10, 332, 92, 530, 79, 2, 3, 25, 1, 99, 65, 67, 50, 10, 1, 92, 20, 253, 79, 2, 3, 25], [1, 54, 22, 12, 40, 13, 66, 21, 1, 208, 5, 1, 203, 541, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40], [11, 41, 343, 6, 50, 7, 1220, 100, 116, 291, 371, 19, 23, 1, 209, 386, 44, 140, 5, 1, 46, 825, 10, 102, 96, 25, 1, 41, 19, 13, 7, 100, 102, 96, 25, 19, 50, 10, 822, 14, 155, 116, 173, 4, 559, 567], [1, 582, 832, 46, 22, 33, 564, 21, 1, 54, 310, 12, 40, 6, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40], [6, 18, 1, 39, 120, 878, 171, 11, 186, 198, 86, 2, 3, 53, 6, 76, 182, 51, 63, 10, 39, 120, 86, 2, 3, 53], [51, 5, 24, 65, 32, 50, 10, 1992, 455, 2, 3, 974, 6, 50, 24, 628, 238, 56, 35, 49, 420, 863, 8, 193, 131, 14, 1992, 455, 2, 3, 974], [6, 18, 59, 47, 4, 45, 43, 14, 124, 107, 106, 9, 723, 126, 1993, 15, 1, 59, 108, 47, 4, 45, 43, 9, 657, 1, 30, 132], [6, 32, 2934, 14, 60, 1248, 31, 1994, 1995, 2, 3, 88, 1, 636, 13, 402, 14, 2935, 31, 175, 8, 1994, 1995, 2, 3, 88], [328, 32, 61, 23, 91, 113, 42, 1257, 643, 1, 1258, 34, 72, 63, 194, 8, 72, 149, 114, 127, 625, 1518, 360, 1, 60, 91, 113, 42, 270, 34, 63, 945, 8, 1, 139, 149, 237, 9, 95, 72, 157, 114, 127], [124, 575, 19, 13, 7, 2936, 529, 526, 19, 376, 19, 627, 2, 3, 43, 11, 298, 7, 575, 19, 627, 2, 3, 43, 13, 15, 11, 2937, 30, 2938], [1, 1091, 763, 162, 33, 50, 14, 404, 316, 4, 134, 25, 31, 24, 1222, 6, 18, 404, 14, 7, 226, 162, 316, 4, 134, 25], [1, 144, 32, 118, 181, 4, 721, 10, 1, 17, 229, 192, 182, 4, 224, 26, 186, 119, 10, 1, 148, 135, 110, 1, 22, 33, 1947, 21, 2939, 9, 2940, 133, 413, 1228, 1288, 476, 67, 192, 214, 4, 1, 22, 33, 182, 10, 148, 135, 110], [11, 231, 357, 510, 2, 3, 37, 7, 1359, 164, 511, 851, 48, 2941, 231, 1, 357, 510, 2, 3, 37, 511, 2942, 11, 48, 128], [6, 18, 1, 177, 17, 20, 12, 2, 3, 16, 9, 121, 7, 60, 27, 38, 29, 42, 1720, 848, 9, 344, 63, 1272, 8, 1, 17, 126, 2943, 17, 20, 12, 2, 3, 16, 13, 15, 9, 69, 7, 126, 61, 38, 29, 14, 1, 85, 28, 870, 587], [8, 24, 58, 6, 18, 1, 490, 152, 557, 2, 3, 90, 9, 972, 1, 2944, 304, 14, 900, 94, 349, 4, 900, 94, 2945, 6, 18, 1, 490, 99, 152, 557, 2, 3, 90, 31, 98, 13, 1163, 9, 133, 215, 558, 14, 390, 1164, 5, 93, 4, 306], [1, 582, 832, 46, 22, 33, 564, 21, 1, 54, 310, 12, 40, 11, 48, 265, 6, 18, 1, 54, 22, 12, 40], [1, 1365, 1366, 22, 1996, 2, 3, 43, 13, 7, 22, 5, 133, 1856, 5, 655, 2946, 1365, 1366, 22, 1, 1365, 1366, 22, 1996, 2, 3, 43, 13, 2947, 2948, 2949], [6, 18, 317, 134, 36, 11, 1997, 1, 879, 5, 1998, 18, 317, 314, 134, 36, 9, 337, 421, 2950, 659, 296, 1032], [1, 216, 459, 13, 1, 2951, 2952, 1999, 569, 2, 3, 64, 1, 62, 13, 236, 5, 1, 130, 143, 64, 877, 569, 2, 3, 64], [1, 1849, 144, 6, 18, 32, 54, 12, 40, 4, 1, 2953, 2954, 48, 265, 6, 18, 1, 54, 22, 12, 40], [6, 976, 48, 427, 28, 11, 46, 31, 138, 8, 455, 2, 3, 560, 6, 76, 15, 374, 2000, 368, 427, 28, 455, 2, 3, 560], [6, 18, 1, 39, 120, 878, 171, 11, 186, 198, 86, 2, 3, 53, 6, 76, 182, 51, 63, 10, 39, 120, 86, 2, 3, 53], [48, 13, 7, 625, 314, 2001, 23, 1, 91, 113, 5, 157, 2002, 34, 142, 5, 308, 2003, 142, 5, 157, 1092, 114, 127, 625, 157, 418, 910, 23, 1, 91, 113, 42, 1668, 34, 63, 1017, 8, 7, 72, 75, 5, 149, 32, 76, 72, 8, 157, 114, 127], [2004, 475, 2, 3, 117, 397, 493, 332, 30, 418, 23, 595, 4, 738, 4, 2955, 34, 345, 30, 418, 1149, 595, 4, 2956, 2, 3, 117, 337, 332, 1057, 11, 1264, 30, 418, 4, 741, 454, 2957, 11, 576, 595, 4, 738, 49, 545, 76, 2005, 30, 1045], [1367, 296, 29, 6, 225, 7, 212, 5, 1368, 1369, 9, 1, 258, 70, 35, 1001, 17, 12, 2, 3, 16, 6, 192, 18, 1, 126, 303, 1722, 8, 1, 17, 70, 56, 35, 29, 12, 2, 3, 16, 9, 219, 7, 126, 193, 42, 1723, 206, 1191], [11, 46, 99, 163, 6, 15, 404, 152, 316, 4, 134, 156, 6, 15, 404, 9, 424, 24, 2958, 99, 11, 199, 316, 4, 134, 156], [1, 30, 73, 33, 50, 10, 59, 47, 4, 45, 43, 14, 1, 580, 2959, 30, 73, 33, 153, 26, 464, 59, 47, 4, 45, 43], [1, 22, 13, 118, 593, 10, 7, 30, 73, 370, 47, 4, 45, 43, 1, 30, 73, 33, 153, 26, 464, 59, 47, 4, 45, 43], [6, 262, 77, 80, 14, 604, 411, 31, 138, 8, 12, 36, 6, 15, 77, 80, 11, 281, 70, 89, 12, 36], [6, 262, 77, 80, 14, 604, 411, 31, 138, 26, 12, 36, 6, 15, 77, 80, 11, 281, 70, 89, 12, 36], [526, 897, 875, 2006, 2007, 4, 2008, 37, 13, 458, 11, 78, 4, 2960, 897, 875, 2006, 2007, 4, 2008, 37, 32, 145, 5, 1, 70, 1875, 78, 2961], [14, 1, 46, 999, 5, 1, 17, 20, 12, 2, 3, 16, 6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 302], [6, 69, 11, 1023, 1719, 10, 520, 729, 437, 847, 1, 518, 285, 469, 519, 64, 4, 2962, 2963, 2009, 106, 2964, 10, 1, 2965, 410, 410, 11, 677, 106, 4, 754, 754, 11, 178, 4, 69, 1, 19, 10, 729, 437, 847, 1329, 14, 78, 2010], [48, 2966, 1, 2967, 5, 482, 2, 3, 64, 34, 41, 19, 4, 375, 299, 65, 254, 95, 2968, 2969, 23, 35, 2970, 439, 482, 2, 3, 64, 95, 97, 7, 300, 35, 2971, 401, 34, 2972, 254, 23, 427, 144], [11, 48, 265, 6, 18, 1, 54, 22, 12, 40, 6, 337, 24, 82, 26, 949, 5, 1, 54, 22, 12, 40], [6, 976, 48, 427, 28, 11, 46, 31, 138, 8, 455, 2, 3, 560, 6, 76, 15, 374, 2000, 368, 427, 28, 455, 2, 3, 560], [24, 161, 13, 7, 1333, 161, 72, 9, 12, 2, 3, 43, 126, 303, 33, 109, 280, 12, 2, 3, 43], [11, 1, 44, 572, 4, 841, 49, 6, 15, 85, 46, 28, 21, 1, 54, 4, 872, 2011, 144, 31, 350, 31, 1, 2012, 22, 2013, 2, 3, 64, 8, 24, 2014, 1789, 6, 50, 68, 38, 29, 10, 254, 46, 28, 21, 1, 2012, 22, 2013, 2, 3, 64], [1, 30, 73, 33, 50, 10, 59, 47, 4, 45, 43, 14, 1, 580, 2973, 73, 33, 233, 14, 59, 47, 4, 45, 43, 11, 105, 49], [542, 334, 234, 931, 543, 4, 544, 387, 887, 1, 2015, 334, 5, 7, 133, 26, 7, 420, 189, 618, 137, 189, 1486, 31, 268, 8, 426, 83, 1, 861, 1342, 9, 24, 167, 351, 5, 1911, 5, 137, 242, 8, 1, 308, 5, 542, 334, 234, 543, 4, 544, 387], [1, 183, 15, 11, 1, 58, 516, 8, 48, 263, 129, 123, 1855, 26, 2016, 2, 3, 88, 1, 205, 1951, 95, 123, 15, 2974, 11, 231, 26, 2975, 2, 3, 57, 294, 2016, 2, 3, 88, 2976, 24, 81, 13, 332, 8, 255, 5, 1, 1064], [7, 914, 28, 261, 11, 48, 13, 1, 54, 22, 12, 40, 11, 48, 265, 6, 18, 1, 54, 22, 12, 40], [1, 255, 1362, 231, 1363, 1364, 104, 1, 54, 22, 12, 40, 6, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40], [870, 362, 2, 3, 25, 15, 7, 880, 2977, 9, 2004, 210, 7, 168, 287, 5, 111, 136, 2978, 2979, 205, 257, 142, 93, 9, 554, 1, 2980, 1, 2981, 62, 362, 2, 3, 25, 15, 7, 880, 238, 309, 9, 19, 111, 136, 2982, 1093, 1, 287, 11, 1, 136, 21, 1, 418, 5, 124, 2983], [1, 118, 261, 13, 1, 502, 43, 159, 62, 854, 596, 278, 597, 4, 74, 598, 43, 4, 1, 391, 261, 13, 1, 36, 1114, 355, 879, 303, 927, 36, 2017, 13, 855, 21, 1, 743, 159, 62, 596, 278, 597, 4, 74, 598, 43], [6, 15, 1, 17, 20, 12, 2, 3, 16, 9, 121, 7, 126, 61, 56, 35, 29, 14, 7, 1286, 100, 371, 50, 23, 1, 209, 140, 5, 24, 2984, 48, 265, 6, 18, 1, 17, 20, 9, 121, 7, 27, 70, 235, 29, 12, 2, 3, 16, 14, 46, 28, 21, 1, 35, 62, 5, 1, 1345, 88, 877, 2985, 2, 3, 94], [6, 492, 750, 68, 1237, 82, 61, 23, 1999, 2986, 260, 21, 1, 259, 286, 216, 563, 2, 3, 156, 6, 216, 35, 693, 574, 1, 259, 222, 563, 2, 3, 156], [6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25, 6, 15, 1, 226, 99, 81, 14, 107, 106, 4, 256, 416, 14, 326, 200, 105, 169, 190, 92, 79, 2, 3, 25], [393, 1, 743, 159, 62, 596, 278, 597, 4, 74, 598, 43, 13, 68, 426, 83, 495, 188, 11, 1, 2018, 1094, 1095, 4, 1094, 1095, 2019, 13, 855, 21, 1, 743, 159, 62, 596, 278, 597, 4, 74, 598, 43], [881, 6, 874, 181, 4, 182, 1, 133, 8, 512, 4, 353, 10, 39, 120, 20, 1370, 86, 2, 3, 53, 6, 76, 182, 51, 63, 10, 39, 120, 86, 2, 3, 53], [1, 144, 32, 118, 181, 4, 721, 10, 1, 17, 229, 192, 182, 4, 224, 26, 186, 119, 10, 1, 148, 135, 110, 51, 2020, 67, 182, 4, 828, 10, 148, 135, 110], [6, 18, 518, 519, 64, 9, 285, 1, 106, 648, 2021, 76, 18, 518, 519, 64, 9, 651, 1, 78, 5, 42, 13, 7, 852, 82, 9, 69, 1, 238, 875], [1, 118, 261, 13, 1, 502, 43, 159, 62, 854, 596, 278, 597, 4, 74, 598, 43, 4, 1, 391, 261, 13, 1, 36, 1114, 355, 879, 303, 927, 36, 2017, 13, 855, 21, 1, 743, 159, 62, 596, 278, 597, 4, 74, 598, 43], [6, 15, 7, 57, 2022, 5, 512, 94, 42, 33, 119, 224, 4, 182, 10, 1, 148, 135, 110, 51, 2020, 67, 182, 4, 828, 10, 148, 135, 110], [439, 510, 2, 3, 88, 2987, 30, 178, 26, 646, 2, 3, 25, 11, 2988, 2015, 4, 683, 93, 11, 576, 2023, 2989, 2, 3, 88, 214, 189, 669, 9, 2023, 1096], [393, 1, 743, 159, 62, 596, 278, 597, 4, 74, 598, 43, 13, 68, 426, 83, 495, 188, 11, 1, 2018, 1094, 1095, 4, 1094, 1095, 2019, 13, 855, 21, 1, 743, 159, 62, 596, 278, 597, 4, 74, 598, 43], [6, 232, 24, 58, 23, 54, 12, 40, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 541, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40], [6, 214, 77, 80, 12, 36, 9, 216, 70, 89, 228, 410, 5, 24, 65, 555, 9, 7, 1628, 15, 77, 80, 11, 281, 70, 89, 12, 36], [6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 1317, 17, 20, 12, 2, 3, 16, 13, 15, 9, 69, 7, 126, 61, 38, 29, 14, 1, 85, 28, 870, 587], [1, 70, 89, 147, 13, 76, 196, 179, 14, 141, 77, 80, 82, 14, 228, 488, 297, 12, 36, 6, 15, 77, 80, 11, 281, 70, 89, 12, 36], [48, 19, 13, 1273, 26, 168, 816, 19, 702, 2, 3, 2024, 145, 5, 1, 2990, 381, 5, 1704, 7, 566, 11, 142, 2991, 211, 358, 13, 1, 168, 816, 19, 702, 2, 3, 2024], [1, 28, 33, 1049, 283, 1236, 1065, 4, 2992, 1065, 8, 7, 72, 2993, 31, 1, 28, 15, 26, 1016, 4, 638, 165, 6, 95, 15, 1, 1236, 28, 509, 8, 1016, 4, 638, 165, 94], [1, 2994, 13, 61, 23, 1, 2995, 284, 2996, 52, 661, 37, 1, 391, 103, 1542, 2997, 13, 1, 2998, 2999, 103, 11, 3000, 3001, 189, 175, 8, 1, 3002, 661, 37], [145, 13, 7, 1051, 41, 19, 66, 10, 102, 96, 25, 4, 50, 206, 7, 155, 213, 5, 1, 247, 22, 8, 42, 613, 98, 13, 1295, 14, 124, 384, 856, 98, 3003, 4, 796, 5, 7, 155, 116, 291, 170, 951, 41, 19, 32, 233, 23, 1, 44, 140, 5, 1, 46, 28, 10, 102, 96, 25, 325], [1, 30, 73, 33, 153, 26, 464, 59, 47, 4, 45, 43, 101, 30, 132, 67, 153, 26, 464, 59, 8, 105, 398, 4, 817, 10, 1, 356, 370, 47, 4, 45, 43], [491, 11, 44, 63, 8, 1, 257, 1371, 32, 174, 21, 7, 1261, 30, 73, 5, 125, 21, 1, 54, 85, 22, 12, 40, 8, 269, 9, 554, 1, 3004, 5, 1, 30, 132, 1, 358, 67, 1295, 283, 7, 1254, 3005, 3006, 14, 1305, 85, 28, 21, 1, 54, 22, 12, 40], [51, 1, 41, 65, 371, 15, 8, 24, 58, 32, 2025, 155, 116, 291, 1372, 50, 10, 102, 96, 2, 3, 88, 1, 41, 65, 32, 407, 10, 1, 102, 20, 96, 2, 3, 88, 14, 155, 116, 173], [8, 51, 58, 6, 18, 1, 99, 184, 14, 1039, 406, 487, 1318, 81, 169, 8, 1, 202, 152, 204, 2, 3, 57, 11, 51, 58, 6, 18, 7, 3007, 184, 31, 175, 8, 202, 204, 2, 3, 57, 20], [6, 18, 1, 39, 52, 9, 1294, 7, 2026, 11, 111, 136, 74, 71, 2, 3, 37, 6, 15, 1, 375, 55, 52, 8, 1, 39, 70, 372, 41, 52, 744, 74, 71, 2, 3, 37, 9, 210, 276, 11, 1, 28, 456], [599, 6, 181, 1, 44, 140, 5, 51, 1373, 11, 41, 343, 10, 1, 60, 632, 5, 1, 17, 20, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16, 6, 18, 17, 325, 12, 2, 3, 16, 9, 424, 48, 19, 26, 726, 1, 261, 4, 209, 1026, 9, 104, 3008, 645], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [1, 118, 1925, 78, 61, 29, 13, 138, 8, 2027, 2, 3, 156, 9, 972, 826, 6, 15, 7, 1929, 5, 1, 861, 1477, 128, 138, 8, 2027, 2, 3, 156], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [6, 15, 2028, 537, 39, 191, 74, 71, 2, 3, 37, 9, 219, 868, 5, 1, 1331, 4, 15, 254, 7, 388, 5, 2029, 5, 7, 2030, 219, 389, 191, 10, 39, 52, 74, 71, 2, 3, 37, 4, 18, 124, 537, 55, 773], [6, 860, 31, 7, 2031, 22, 54, 717, 12, 40, 14, 2032, 85, 2033, 232, 24, 58, 23, 54, 12, 40, 7, 307, 85, 22, 42, 13, 138, 8, 741, 8, 185, 1810], [6, 15, 1, 375, 55, 52, 8, 1, 39, 70, 372, 41, 52, 744, 74, 71, 2, 3, 37, 9, 210, 276, 11, 1, 28, 2034, 24, 58, 6, 15, 1, 39, 52, 74, 71, 2, 3, 37, 9, 348, 55, 276], [1, 1279, 5, 1, 1886, 13, 1374, 9, 402, 1375, 5, 1, 486, 4, 1, 402, 1097, 5, 1, 940, 31, 524, 8, 468, 2, 3, 16, 1, 503, 403, 13, 15, 9, 262, 51, 5, 1, 1376, 476, 468, 2, 3, 16], [1, 91, 113, 5, 157, 114, 127, 13, 7, 3009, 128, 11, 1655, 446, 3010, 13, 7, 625, 314, 2001, 23, 1, 91, 113, 5, 157, 2002, 34, 142, 5, 308, 2003, 142, 5, 157, 1092, 114, 127], [599, 6, 181, 1, 44, 140, 5, 51, 1373, 11, 41, 343, 10, 1, 60, 632, 5, 1, 17, 20, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [145, 13, 21, 475, 2, 3, 117, 1, 1284, 5, 30, 1045, 13, 1918, 471, 1, 93, 32, 1, 3011, 93, 174, 21, 30, 658, 8, 1, 139, 592, 31, 475, 2, 3, 117], [1, 3012, 67, 3013, 459, 3014, 23, 24, 115, 11, 1, 249, 1126, 4, 130, 142, 562, 745, 2, 3, 36, 11, 7, 654, 5, 63, 249, 586, 7, 3015, 5, 562, 5, 1, 130, 142, 745, 2, 3, 36], [368, 38, 6, 15, 1, 85, 22, 185, 131, 9, 69, 68, 506, 27, 38, 29, 10, 1, 17, 20, 12, 2, 3, 16, 24, 101, 13, 7, 27, 235, 29, 50, 10, 1, 17, 20, 12, 2, 3, 16], [1, 70, 89, 147, 13, 76, 196, 179, 14, 141, 77, 80, 82, 14, 228, 488, 297, 12, 36, 6, 216, 89, 5, 188, 10, 77, 80, 190, 228, 410, 12, 36], [6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16, 1367, 296, 29, 6, 225, 7, 212, 5, 1368, 1369, 9, 1, 258, 70, 35, 1001, 17, 12, 2, 3, 16], [6, 15, 7, 57, 2022, 5, 512, 94, 42, 33, 119, 224, 4, 182, 10, 1, 148, 135, 110, 131, 51, 44, 28, 32, 119, 224, 4, 473, 10, 1, 148, 135, 110], [1, 133, 33, 442, 10, 2035, 425, 9, 1351, 2036, 4, 192, 181, 14, 1, 39, 632, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [6, 232, 24, 58, 23, 54, 12, 40, 7, 307, 85, 22, 42, 13, 138, 8, 741, 8, 185, 3016, 12, 40, 13, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 449], [6, 18, 92, 79, 2, 3, 25, 1, 56, 78, 403, 11, 553, 11, 1216, 1, 332, 3017, 199, 175, 8, 553, 14, 1, 92, 56, 78, 403, 79, 2, 3, 25, 13, 15, 11, 51, 215, 65], [1, 592, 328, 67, 3018, 13, 72, 9, 2005, 1, 1249, 1287, 138, 26, 409, 2, 3, 87, 8, 48, 263, 6, 18, 1, 2037, 22, 26, 409, 2, 3, 87], [6, 15, 1, 375, 55, 52, 8, 1, 39, 70, 372, 41, 52, 744, 74, 71, 2, 3, 37, 9, 210, 276, 11, 1, 28, 2034, 24, 58, 6, 15, 1, 39, 52, 74, 71, 2, 3, 37, 9, 348, 55, 276], [6, 155, 1, 81, 5, 1, 3019, 3020, 152, 253, 5, 788, 2, 3, 201, 6, 76, 225, 24, 30, 178, 14, 1, 2038, 30, 178, 5, 788, 2, 3, 201, 1018, 833, 1352, 2039, 1877], [1, 39, 55, 52, 74, 71, 2, 3, 37, 13, 15, 11, 430, 93, 21, 1, 55, 160, 727, 15, 1, 375, 55, 52, 8, 1, 39, 70, 372, 41, 52, 744, 74, 71, 2, 3, 37, 9, 210, 276, 11, 1, 28, 456], [1, 118, 167, 23, 48, 996, 33, 233, 3021, 8, 1, 3022, 718, 387, 1, 735, 213, 33, 224, 374, 10, 7, 171, 1112, 23, 1, 44, 171, 138, 8, 718, 387], [1, 503, 403, 13, 15, 9, 262, 51, 5, 1, 1376, 476, 468, 2, 3, 16, 1, 161, 13, 175, 14, 402, 712, 230, 548, 997, 10, 60, 476, 169, 8, 1, 503, 998, 468, 2, 3, 16], [11, 24, 58, 6, 15, 637, 694, 3023, 21, 1, 1355, 22, 652, 3024, 11, 3025, 58, 6, 15, 1, 3026, 2040, 85, 22, 652, 57, 42, 13, 2040, 3027, 3028, 3029, 108, 3030], [1, 3031, 1279, 13, 153, 26, 865, 1, 38, 20, 17, 12, 2, 3, 16, 206, 2041, 605, 14, 3032, 308, 3033, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [1, 503, 403, 13, 15, 9, 262, 51, 5, 1, 1376, 476, 468, 2, 3, 16, 1, 161, 13, 175, 14, 402, 712, 230, 548, 997, 10, 60, 476, 169, 8, 1, 503, 998, 468, 2, 3, 16], [6, 860, 31, 7, 2031, 22, 54, 717, 12, 40, 14, 2032, 85, 2033, 232, 24, 58, 23, 54, 12, 40, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 449], [6, 76, 225, 24, 30, 178, 14, 1, 2038, 30, 178, 5, 788, 2, 3, 201, 1018, 833, 1352, 2039, 3034, 2, 3, 201, 15, 3035, 9, 3036, 30, 178, 571, 1, 280, 608], [1, 1935, 211, 41, 4, 195, 13, 68, 3037, 1342, 5, 3038, 409, 4, 588, 90, 1, 556, 3039, 5, 3040, 879, 1377, 5, 42, 13, 3041, 129, 3042, 68, 2042, 8, 195, 115, 409, 4, 588, 90, 482, 117], [158, 168, 668, 99, 32, 145, 5, 1, 1266, 163, 61, 23, 532, 3043, 1137, 587, 26, 521, 521, 165, 172, 163, 32, 61, 23, 7, 2043, 19, 158, 168, 56, 99, 425, 521, 165], [8, 3044, 4, 3045, 53, 6, 794, 14, 1816, 3046, 10, 1, 907, 169, 38, 29, 17, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [9, 607, 1, 106, 5, 1, 19, 6, 1201, 1, 746, 315, 31, 1, 46, 995, 10, 1, 243, 487, 103, 244, 4, 245, 53, 11, 487, 6, 984, 1, 243, 425, 3047, 103, 244, 4, 245, 53, 9, 285, 106], [11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16, 33, 15, 11, 30, 73, 4, 126, 35, 1749, 67, 407, 21, 789, 26, 1, 17, 29, 12, 2, 3, 16], [1, 133, 33, 442, 10, 2035, 425, 9, 1351, 2036, 4, 192, 181, 14, 1, 39, 632, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [1, 62, 516, 23, 662, 13, 9, 723, 2044, 2045, 4, 1098, 87, 795, 617, 1, 93, 176, 11, 1, 3048, 3049, 62, 3050, 4, 840, 3051, 3052, 60, 11, 1903, 478, 164, 13, 176, 8, 1, 2044, 511, 2045, 4, 1098, 87], [11, 105, 49, 1, 15, 46, 28, 13, 21, 1, 3053, 213, 5, 1, 54, 22, 12, 40, 4, 1, 872, 2011, 1514, 261, 5, 239, 28, 15, 8, 1, 58, 13, 1, 54, 310, 12, 40], [6, 18, 1, 39, 52, 9, 1294, 7, 2026, 11, 111, 136, 74, 71, 2, 3, 37, 6, 15, 1, 375, 55, 52, 8, 1, 39, 70, 372, 41, 52, 744, 74, 71, 2, 3, 37, 9, 210, 276, 11, 1, 28, 456], [6, 15, 2028, 537, 39, 191, 74, 71, 2, 3, 37, 9, 219, 868, 5, 1, 1331, 4, 15, 254, 7, 388, 5, 2029, 5, 7, 2030, 219, 389, 191, 10, 39, 52, 74, 71, 2, 3, 37, 4, 18, 124, 537, 55, 773], [6, 274, 44, 28, 4, 864, 298, 28, 10, 1, 39, 120, 20, 86, 2, 3, 53, 94, 881, 6, 874, 181, 4, 182, 1, 133, 8, 512, 4, 353, 10, 39, 120, 20, 1370, 86, 2, 3, 53], [1, 143, 3054, 8, 1253, 436, 129, 123, 23, 1997, 879, 318, 3055, 10, 3056, 31, 350, 31, 355, 134, 4, 510, 43, 3057, 134, 4, 510, 43, 129, 123, 458, 31, 7, 60, 143, 314, 8, 413, 436, 558], [6, 223, 101, 58, 11, 126, 61, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 51, 58, 67, 196, 179, 10, 1, 177, 38, 20, 17, 12, 2, 3, 16, 8, 124, 60, 1652, 580], [1, 142, 369, 13, 662, 1, 291, 767, 189, 162, 768, 97, 8, 2046, 2, 3, 25, 6, 635, 396, 1, 291, 767, 3058, 768, 97, 8, 2046, 2, 3, 25, 9, 707, 1, 142, 771, 7, 1348, 189, 287], [6, 274, 44, 28, 4, 864, 298, 28, 10, 1, 39, 120, 20, 86, 2, 3, 53, 94, 881, 6, 874, 181, 4, 182, 1, 133, 8, 512, 4, 353, 10, 39, 120, 20, 1370, 86, 2, 3, 53], [1, 39, 55, 52, 74, 71, 2, 3, 37, 13, 15, 11, 430, 93, 21, 1, 55, 160, 727, 15, 1, 375, 55, 52, 8, 1, 39, 70, 372, 41, 52, 744, 74, 71, 2, 3, 37, 9, 210, 276, 11, 1, 28, 456], [9, 3059, 1, 1287, 5, 1, 97, 82, 6, 18, 1, 258, 27, 29, 4, 685, 27, 29, 175, 8, 17, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16, 1, 17, 20, 12, 2, 3, 16, 13, 15, 9, 69, 7, 126, 61, 38, 29, 14, 1, 85, 28, 870, 587], [1367, 296, 29, 6, 225, 7, 212, 5, 1368, 1369, 9, 1, 258, 70, 35, 1001, 17, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [599, 6, 181, 1, 44, 140, 5, 51, 1373, 11, 41, 343, 10, 1, 60, 632, 5, 1, 17, 20, 12, 2, 3, 16, 6, 274, 4, 737, 1, 28, 14, 1, 60, 229, 21, 1, 17, 20, 12, 2, 3, 16], [393, 98, 13, 76, 2047, 34, 1, 1378, 5, 663, 1379, 8, 1, 46, 75, 13, 72, 9, 1, 145, 516, 26, 463, 2, 3, 87, 4, 2048, 9, 8, 185, 2049, 2, 3, 87, 871, 34, 1338, 5, 1, 1029, 354, 8, 1, 1339, 22, 94, 67, 663], [31, 7, 78, 103, 6, 882, 7, 434, 99, 295, 87, 42, 13, 68, 1148, 5, 893, 3060, 69, 7, 3061, 184, 6, 18, 1, 99, 78, 103, 21, 1, 99, 1326, 152, 295, 87, 3062, 51, 3063, 93, 283, 68, 1374, 75, 5, 3064, 3065], [2050, 3066, 32, 1869, 3067, 3068, 6, 15, 1, 3069, 21, 1066, 2, 3, 90, 9, 860, 1, 255, 3070, 2050, 3071, 93, 11, 111, 5, 1, 1229, 3072, 1, 143, 3073, 5, 1066, 2, 3, 90, 226, 65, 50, 23, 625, 93, 32, 15, 9, 1301, 1, 3074, 5, 3075, 3076, 11, 1708, 1307], [1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16, 48, 29, 13, 1, 17, 161, 12, 2, 3, 16, 14, 7, 35, 4, 7, 41, 19, 50, 14, 559, 1022, 1337, 205, 602, 1, 2051, 28, 176, 26, 1, 159, 62, 942], [188, 23, 1305, 3077, 4, 2052, 44, 73, 508, 34, 24, 19, 3078, 1078, 2053, 341, 7, 432, 634, 3079, 5, 716, 19, 94, 901, 2, 3, 88, 24, 73, 19, 13, 61, 23, 7, 708, 647, 5, 716, 19, 94, 319, 1, 73, 377, 13, 254, 3080, 26, 112, 106, 4, 228, 1943, 901, 2, 3, 88], [1, 167, 509, 8, 1099, 2, 3, 197, 34, 13, 61, 23, 1, 7, 324, 955, 3081, 30, 299, 3082, 8, 269, 9, 1836, 1, 1908, 495, 3083, 1332, 299, 515, 8, 1099, 2, 3, 197, 13, 268, 9, 104, 7, 2054, 957, 5, 7, 327, 710, 873, 1030, 8, 42, 1, 30, 299, 513, 32, 1380, 8, 363, 5, 708], [24, 391, 82, 13, 61, 23, 1, 953, 238, 309, 41, 19, 1176, 128, 9, 78, 30, 178, 5, 457, 2, 3, 747, 4, 457, 2, 3, 1100, 10, 1, 883, 3084, 2, 3, 747, 97, 7, 2055, 1381, 19, 883, 170, 42, 2056, 9, 2057, 215, 5, 7, 30, 61, 23, 810, 30, 8, 1, 139, 136], [1, 2058, 22, 13, 7, 155, 4, 3085, 213, 5, 471, 22, 1296, 4, 2059, 2060, 8, 269, 9, 2061, 1, 1839, 587, 26, 171, 1, 2058, 22, 13, 15, 31, 1, 46, 4, 3086, 22, 351, 5, 7, 388, 5, 1, 471, 22, 3087, 63, 14, 327, 602, 3088, 1848, 1296, 4, 2059, 2060, 4, 98, 129, 123, 3089, 4, 1846], [1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16, 51, 172, 93, 32, 1792, 21, 17, 12, 2, 3, 16], [44, 361, 67, 51, 660, 10, 1, 39, 720, 483, 20, 86, 2, 3, 53, 6, 76, 182, 51, 63, 10, 39, 120, 86, 2, 3, 53], [1, 106, 32, 407, 26, 600, 378, 10, 1, 382, 81, 383, 87, 494, 6, 15, 1, 1910, 81, 8, 382, 383, 87], [1, 3090, 13, 7, 226, 99, 557, 2, 3, 90, 175, 14, 3091, 557, 2, 3, 90, 7, 403, 11, 390, 99, 226, 215, 13, 15, 11, 81], [6, 18, 518, 519, 64, 9, 285, 1, 106, 648, 2021, 18, 7, 520, 729, 437, 847, 1329, 14, 1, 518, 285, 469, 519, 64, 396, 256, 3092, 1316, 754, 754, 11, 713, 633, 1277, 4, 2009, 1, 1185, 435], [6, 69, 7, 158, 168, 56, 99, 884, 4, 521, 165, 23, 1, 616, 176, 11, 2062, 6, 18, 158, 168, 56, 99, 884, 4, 521, 165, 11, 48, 265], [435, 1343, 13, 196, 179, 10, 1092, 1382, 1079, 57, 250, 1136, 61, 23, 259, 32, 192, 3093, 10, 1092, 1382, 1079, 57], [1101, 3094, 1, 18, 5, 3095, 8, 1, 2063, 5, 242, 23, 1, 207, 1101, 626, 1101, 97, 1, 704, 82, 11, 293, 2063, 1101, 626], [6, 882, 1, 726, 5, 362, 2, 3, 64, 238, 875, 32, 118, 15, 8, 48, 62, 8, 362, 2, 3, 64], [1, 1219, 169, 122, 59, 33, 15, 9, 341, 1, 3096, 47, 4, 45, 43, 6, 15, 1, 59, 108, 47, 4, 45, 43, 9, 657, 1, 30, 132], [748, 81, 749, 16, 6, 50, 7, 531, 171, 10, 748, 83, 749, 16], [1, 376, 184, 15, 8, 24, 58, 461, 1, 103, 138, 8, 2064, 2, 3, 288, 1, 376, 184, 15, 8, 1, 58, 8, 185, 253, 461, 1, 29, 497, 8, 2064, 2, 3, 288, 4, 98, 1822, 790, 215, 26, 3097, 111, 30, 1401, 145, 5, 1, 736], [6, 260, 1050, 1372, 14, 155, 116, 173, 630, 4, 45, 165, 1, 41, 19, 13, 7, 100, 14, 819, 4, 116, 173, 630, 4, 45, 165], [362, 2, 3, 25, 1009, 861, 9, 24, 209, 3098, 3099, 9, 3100, 3101, 21, 362, 2, 3, 25, 11, 48, 62], [6, 18, 1076, 199, 1914, 14, 1915, 349, 4, 158, 168, 199, 1115, 14, 68, 753, 162, 21, 92, 79, 2, 3, 25, 6, 18, 1, 158, 168, 668, 81, 8, 1, 92, 20, 79, 2, 3, 25, 9, 262, 199, 1115, 23, 111, 250, 75, 14, 1401, 753, 669, 4, 106, 2137], [31, 11, 1, 1002, 3102, 98, 13, 2065, 9, 3103, 6, 3104, 9, 18, 7, 55, 213, 5, 1, 567, 103, 5, 742, 4, 1098, 36, 11, 478, 7, 55, 213, 5, 1, 567, 103, 8, 742, 4, 1098, 36, 13, 15, 9, 681, 8, 68, 3105, 592, 1, 846, 389, 1204, 4, 124, 3106, 8, 7, 160, 189, 8, 7, 3107], [1023, 1, 89, 395, 67, 109, 10, 1, 77, 80, 82, 12, 36, 6, 15, 77, 80, 11, 281, 70, 89, 12, 36], [8, 48, 167, 6, 939, 23, 78, 14, 158, 168, 668, 489, 521, 165, 791, 6, 18, 158, 168, 56, 99, 884, 4, 521, 165, 11, 48, 265], [6, 15, 148, 135, 110, 9, 210, 7, 3108, 654, 11, 111, 3109, 3110, 15, 1, 1205, 148, 135, 110, 9, 986, 522, 1611], [9, 1340, 11, 48, 515, 1742, 333, 21, 441, 130, 115, 2066, 2, 3, 339, 3111, 65, 414, 18, 5, 441, 130, 115, 3112, 2066, 2, 3, 339], [1, 29, 33, 50, 23, 1, 44, 4, 867, 236, 5, 1, 54, 22, 213, 131, 12, 40, 8, 1747, 163, 50, 23, 485, 28, 154, 104, 214, 635, 9, 1042, 724, 206, 1, 44, 140, 5, 1, 54, 22, 12, 40, 15, 11, 46, 4, 281, 38], [6, 18, 226, 489, 21, 490, 4, 489, 14, 753, 162, 21, 404, 316, 4, 134, 25, 1, 1091, 763, 162, 33, 50, 14, 404, 316, 4, 134, 25], [8, 846, 27, 70, 56, 35, 49, 290, 31, 17, 83, 12, 2, 3, 16, 1, 35, 19, 13, 524, 8, 363, 5, 126, 218, 3113, 174, 21, 7, 3114, 13, 7, 60, 27, 56, 35, 19, 12, 2, 3, 16], [6, 69, 7, 158, 168, 56, 99, 884, 4, 521, 165, 23, 1, 616, 176, 11, 2062, 6, 18, 158, 168, 56, 99, 884, 4, 521, 165, 11, 48, 265], [1, 30, 73, 33, 153, 26, 464, 59, 47, 4, 45, 43, 1, 30, 73, 13, 417, 26, 59, 47, 4, 45, 43, 1, 1097, 538, 13, 15], [11, 46, 99, 163, 6, 15, 404, 152, 316, 4, 134, 156, 1, 99, 81, 15, 33, 404, 316, 4, 134, 156, 4, 107, 106, 67, 15, 2067, 1383, 369, 162, 3115, 435, 5, 83, 4, 7, 3116, 793, 5, 1, 1210, 5, 1, 212, 5, 924], [6, 18, 1, 60, 73, 122, 59, 47, 4, 45, 43, 9, 30, 341, 1, 85, 500, 15, 59, 47, 4, 45, 43, 9, 341, 1, 63, 8, 1, 22], [6, 18, 1, 60, 73, 122, 59, 47, 4, 45, 43, 9, 30, 341, 1, 85, 500, 15, 1, 59, 108, 47, 4, 45, 43, 9, 657, 1, 30, 132], [8, 1, 217, 137, 187, 772, 336, 2, 3, 90, 1, 812, 13, 618, 3117, 3118, 1319, 1, 75, 5, 137, 1038, 8, 24, 167, 13, 725, 11, 44, 6, 18, 3119, 1038, 247, 8, 1, 217, 137, 187, 772, 336, 2, 3, 90, 11, 305, 6, 18, 3120, 3121, 3122], [202, 204, 2, 3, 57, 33, 15, 9, 396, 78, 381, 9, 174, 3123, 99, 81, 5, 202, 204, 2, 3, 57, 33, 15, 9, 121, 1, 3124, 19], [6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 3125, 33, 233, 14, 7, 1348, 122, 176, 14, 1, 17, 20, 12, 2, 3, 16], [6, 18, 1, 60, 73, 122, 59, 47, 4, 45, 43, 9, 30, 341, 1, 85, 500, 15, 1, 59, 108, 47, 4, 45, 43, 9, 657, 1, 30, 132], [6, 15, 1, 59, 108, 47, 4, 45, 43, 9, 657, 1, 30, 1993, 109, 30, 73, 10, 59, 47, 4, 45, 43, 14, 1, 107, 356, 73, 538, 82], [1, 41, 19, 15, 13, 1, 100, 22, 21, 367, 1090, 2068, 2, 3, 25, 6, 15, 367, 1090, 698, 2068, 2, 3, 25, 31, 1, 710, 22], [44, 361, 67, 51, 660, 10, 1, 39, 720, 483, 20, 86, 2, 3, 53, 6, 76, 182, 51, 63, 10, 39, 120, 86, 2, 3, 53], [11, 231, 475, 2, 3, 117, 555, 471, 658, 1346, 178, 4, 1347, 178, 8, 595, 4, 738, 2069, 337, 1346, 30, 178, 14, 3126, 326, 4, 869, 2070, 31, 350, 31, 1347, 30, 178, 14, 326, 4, 869, 2070, 34, 32, 587, 8, 475, 2, 3, 117, 4, 154, 104, 1928, 662, 253], [91, 418, 1902, 68, 1048, 26, 124, 1384, 3127, 1, 308, 1666, 3128, 5, 157, 670, 9, 42, 145, 1397, 1398, 7, 30, 26, 1, 1110, 98, 1111, 813, 814, 48, 13, 1823, 618, 91, 379, 319, 7, 30, 294, 7, 3129, 8, 24, 957, 13, 609, 26, 1, 1110, 98, 1111, 813, 814], [448, 2, 3, 88, 18, 3130, 9, 121, 3131, 3132, 238, 309, 13, 72, 9, 34, 5, 448, 2, 3, 88], [6, 18, 1, 60, 73, 122, 59, 47, 4, 45, 43, 9, 30, 341, 1, 85, 500, 15, 59, 47, 4, 45, 43, 9, 341, 1, 63, 8, 1, 22], [1, 671, 34, 13, 15, 9, 3133, 1, 379, 8, 1, 1814, 902, 13, 406, 594, 379, 866, 277, 2, 3, 40, 157, 287, 4, 1375, 8, 1, 380, 1085, 23, 1, 1173, 330, 5, 406, 594, 379, 866, 277, 2, 3, 40], [11, 275, 1525, 831, 4, 954, 6, 15, 54, 22, 83, 12, 40, 11, 48, 265, 6, 18, 1, 54, 22, 12, 40], [1, 44, 133, 33, 181, 10, 1, 30, 274, 1612, 21, 272, 273, 2, 3, 57, 1, 1940, 32, 419, 21, 1, 3134, 501, 176, 26, 1, 272, 20, 273, 2, 3, 57], [6, 18, 1, 17, 161, 12, 2, 3, 16, 31, 7, 2071, 38, 161, 3135, 1, 29, 138, 3136, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [791, 1, 136, 2072, 183, 2073, 4, 1402, 90, 2065, 31, 3137, 13, 15, 11, 3138, 3139, 19, 46, 3140, 145, 666, 13, 9, 396, 98, 9, 1, 41, 19, 61, 2072, 82, 2073, 4, 1402, 90], [18, 1, 39, 52, 74, 71, 2, 3, 37, 9, 219, 7, 75, 5, 191, 21, 111, 3141, 219, 389, 191, 10, 39, 52, 74, 71, 2, 3, 37, 4, 18, 124, 537, 55, 773], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16], [8, 3142, 1, 161, 129, 9, 1224, 696, 495, 9, 853, 98, 3143, 12, 36, 9, 853, 1, 3144, 103, 1963, 696, 495, 13, 1, 60, 821, 495, 82, 12, 36], [1, 261, 5, 239, 28, 15, 8, 1, 58, 13, 1, 54, 310, 12, 40, 1, 54, 28, 75, 351, 5, 1921, 125, 5, 1, 305, 236, 5, 1, 54, 22, 12, 40], [1, 3145, 164, 677, 13, 61, 23, 1, 3146, 806, 497, 234, 2074, 2, 3, 740, 24, 128, 13, 61, 23, 1, 2052, 603, 3147, 919, 1189, 8, 2074, 2, 3, 740], [8, 48, 263, 6, 18, 1, 2037, 22, 26, 409, 2, 3, 87, 172, 163, 95, 123, 15, 8, 601, 167, 26, 409, 2, 3, 87], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 6, 121, 7, 101, 534, 3148, 29, 10, 1, 17, 38, 29, 12, 2, 3, 16], [51, 24, 49, 32, 664, 14, 7, 60, 27, 29, 66, 14, 17, 12, 2, 3, 16, 6, 15, 1, 17, 20, 12, 2, 3, 16, 9, 69, 60, 27, 49, 14, 107, 739], [7, 649, 128, 9, 1093, 142, 13, 9, 993, 1, 212, 5, 649, 63, 665, 740, 48, 812, 142, 216, 13, 1112, 26, 1, 784, 5, 1, 665, 103, 665, 740], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [1, 39, 55, 52, 74, 71, 2, 3, 37, 13, 15, 11, 430, 93, 21, 1, 55, 160, 2075, 24, 58, 6, 15, 1, 39, 52, 74, 71, 2, 3, 37, 9, 348, 55, 276], [6, 274, 4, 737, 1, 28, 14, 1, 60, 229, 21, 1, 17, 20, 12, 2, 3, 16, 6, 424, 1382, 4, 1102, 83, 4, 635, 18, 1102, 94, 21, 17, 12, 2, 3, 16], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 6, 274, 4, 737, 1, 28, 14, 1, 60, 229, 21, 1, 17, 20, 12, 2, 3, 16], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 24, 101, 13, 7, 27, 235, 29, 50, 10, 1, 17, 20, 12, 2, 3, 16], [11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16, 1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16], [6, 274, 4, 737, 1, 28, 14, 1, 60, 229, 21, 1, 17, 20, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [6, 223, 101, 58, 11, 27, 56, 35, 10, 1, 17, 20, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [8, 1319, 3149, 2, 3, 37, 939, 23, 390, 1187, 4, 976, 7, 3150, 2, 3, 37, 18, 3151, 8, 966, 14, 7, 3152, 52, 4, 1341], [6, 18, 271, 199, 14, 517, 349, 175, 10, 1, 92, 20, 79, 2, 3, 25, 423, 271, 1314, 6, 18, 271, 199, 14, 517, 315, 798, 4, 1315, 79, 2, 3, 25], [786, 1, 126, 193, 240, 1916, 1, 512, 22, 33, 473, 10, 1, 148, 135, 110, 131, 51, 44, 28, 32, 119, 224, 4, 473, 10, 1, 148, 135, 110], [6, 18, 271, 199, 14, 517, 349, 175, 10, 1, 92, 20, 79, 2, 3, 25, 6, 18, 1, 1844, 366, 312, 184, 8, 445, 14, 1, 107, 302, 79, 2, 3, 25], [1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16, 105, 49, 32, 61, 23, 1, 17, 38, 20, 12, 2, 3, 16, 4, 823, 31, 461], [11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16, 6, 274, 4, 737, 1, 28, 14, 1, 60, 229, 21, 1, 17, 20, 12, 2, 3, 16], [6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16, 6, 15, 1, 17, 20, 12, 2, 3, 16, 9, 69, 60, 27, 49, 14, 107, 739], [6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [54, 12, 40, 13, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 541, 219, 24, 1344, 146, 21, 1, 1913, 540, 5, 1, 54, 22, 213, 170, 12, 40], [11, 51, 389, 1067, 6, 15, 1, 628, 39, 55, 287, 74, 71, 2, 3, 37, 8, 24, 58, 6, 15, 1, 39, 52, 74, 71, 2, 3, 37, 9, 348, 55, 276], [1, 39, 55, 52, 74, 71, 2, 3, 37, 13, 15, 11, 430, 93, 21, 1, 55, 160, 2075, 24, 58, 6, 15, 1, 39, 52, 74, 71, 2, 3, 37, 9, 348, 55, 276], [6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16, 51, 24, 49, 32, 664, 14, 7, 60, 27, 29, 66, 14, 17, 12, 2, 3, 16], [6, 18, 1, 746, 315, 369, 4, 520, 150, 151, 2, 3, 25, 9, 651, 106, 6, 18, 150, 151, 2, 3, 25, 14, 1, 331, 78, 166, 75, 9, 778], [6, 18, 1, 746, 315, 369, 4, 520, 150, 151, 2, 3, 25, 9, 651, 106, 6, 69, 1, 324, 1058, 811, 10, 1059, 1060, 315, 83, 14, 150, 151, 2, 3, 25], [6, 18, 1, 150, 82, 151, 2, 3, 25, 9, 374, 285, 1, 78, 166, 11, 111, 1845, 18, 1, 746, 315, 369, 4, 520, 150, 151, 2, 3, 25, 9, 651, 106], [362, 2, 3, 25, 3153, 10, 880, 1103, 3154, 4, 964, 3155, 11, 1344, 3156, 2, 3, 25, 18, 880, 1103, 11, 195, 115, 23, 1, 136, 472], [94, 35, 19, 17, 13, 7, 258, 20, 11, 27, 38, 49, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [94, 35, 19, 17, 13, 7, 258, 20, 11, 27, 38, 49, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [94, 35, 19, 17, 13, 7, 258, 20, 11, 27, 38, 49, 12, 2, 3, 16, 24, 101, 13, 7, 27, 235, 29, 50, 10, 1, 17, 20, 12, 2, 3, 16], [11, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [51, 24, 49, 32, 664, 14, 7, 60, 27, 29, 66, 14, 17, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [11, 51, 389, 1067, 6, 15, 1, 628, 39, 55, 287, 74, 71, 2, 3, 37, 8, 24, 58, 6, 15, 1, 39, 52, 74, 71, 2, 3, 37, 9, 348, 55, 276], [154, 104, 397, 26, 3157, 1, 3158, 23, 7, 46, 22, 2076, 87, 1, 106, 154, 104, 1782, 407, 21, 7, 187, 31, 268, 26, 2076, 87], [1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [94, 35, 19, 17, 13, 7, 258, 20, 11, 27, 38, 49, 12, 2, 3, 16, 51, 24, 49, 32, 664, 14, 7, 60, 27, 29, 66, 14, 17, 12, 2, 3, 16], [8, 48, 128, 6, 15, 1, 2077, 108, 61, 23, 1, 748, 81, 749, 16, 2077, 13, 7, 595, 29, 34, 13, 66, 23, 1089, 5, 1, 748, 749, 16], [94, 35, 19, 17, 13, 7, 258, 20, 11, 27, 38, 49, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [172, 3159, 1263, 3160, 23, 1, 3161, 293, 31, 15, 8, 249, 338, 2, 3, 339, 2078, 154, 447, 547, 1, 2079, 5, 731, 31, 8, 1, 249, 511, 338, 2, 3, 339], [2054, 3162, 5, 3163, 32, 919, 8, 1, 257, 3164, 5, 1, 249, 257, 399, 338, 2, 3, 339, 2078, 154, 447, 547, 1, 2079, 5, 731, 31, 8, 1, 249, 511, 338, 2, 3, 339], [6, 69, 24, 19, 23, 7, 388, 5, 1, 1971, 1972, 425, 22, 463, 2, 3, 57, 6, 15, 1, 1219, 169, 3165, 22, 463, 2, 3, 57], [786, 1, 118, 159, 62, 23, 2080, 1350, 1385, 1046, 2081, 2, 3, 40, 33, 3166, 8, 40, 1377, 655, 129, 123, 233, 23, 1797, 145, 154, 1000, 1385, 211, 372, 41, 3167, 523, 944, 123, 655, 23, 3168, 1380, 8, 372, 41, 1, 3169, 2080, 1350, 1385, 1046, 3170, 2081, 2, 3, 40, 3171, 3172, 2042, 8, 1, 304], [6, 15, 2082, 676, 1102, 2083, 4, 932, 64, 11, 3173, 3174, 1, 49, 10, 2082, 676, 1102, 2083, 4, 932, 64], [6, 15, 1, 382, 20, 383, 87, 11, 78, 532, 1328, 65, 14, 1198, 3175, 11, 51, 24, 610, 18, 1, 382, 152, 383, 87, 11, 58], [6, 18, 1, 81, 176, 26, 748, 325, 749, 16, 11, 105, 46, 4, 215, 2069, 18, 748, 749, 16, 31, 68, 81, 5, 531], [1386, 261, 14, 209, 275, 26, 10, 59, 47, 4, 45, 43, 30, 132, 67, 417, 10, 59, 47, 4, 45, 43], [6, 15, 3176, 134, 36, 11, 143, 5, 1998, 1075, 48, 873, 13, 327, 641, 14, 1, 317, 143, 314, 15, 11, 436, 134, 36], [83, 14, 94, 349, 10, 150, 151, 2, 3, 25, 6, 882, 428, 78, 591, 106, 10, 150, 151, 2, 3, 25], [6, 15, 202, 204, 2, 3, 57, 11, 51, 24, 215, 3177, 204, 2, 3, 57, 42, 240, 1, 81, 5, 51, 493, 530, 33, 15, 8, 24, 678], [119, 198, 33, 109, 14, 148, 135, 110, 148, 13, 7, 70, 405, 3178, 119, 171, 135, 110], [30, 132, 23, 1, 85, 22, 32, 109, 10, 59, 47, 4, 45, 43, 14, 1, 3179, 3180, 85, 22, 13, 593, 10, 59, 47, 4, 45, 43], [6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25, 215, 481, 1, 92, 553, 152, 79, 2, 3, 25], [1, 257, 1371, 28, 33, 373, 10, 1, 498, 4, 885, 584, 52, 498, 4, 885, 36, 98, 1085, 23, 1, 1387, 584, 52, 498, 4, 885, 36], [11, 3181, 6, 15, 1, 39, 3182, 2084, 4, 86, 84, 982, 6, 3183, 51, 611, 14, 248, 119, 680, 6, 18, 1, 39, 119, 171, 2084, 4, 86, 84], [6, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40, 51, 1, 28, 1009, 21, 54, 12, 40], [48, 28, 13, 236, 5, 1, 552, 22, 392, 2, 3, 88, 1, 46, 28, 787, 26, 1, 62, 942, 622, 21, 1, 552, 22, 392, 2, 3, 88, 42, 240, 621, 453, 26, 692, 5, 44, 31, 7, 1515, 41, 4, 13, 3184, 26], [1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16, 223, 10, 1, 17, 27, 161, 12, 2, 3, 16], [6, 15, 1, 30, 73, 660, 26, 59, 47, 4, 45, 84, 179, 5, 68, 716, 19, 3185, 13, 1, 376, 73, 19, 5, 47, 4, 45, 84], [11, 48, 265, 6, 18, 1, 54, 22, 12, 40, 51, 1, 28, 1009, 21, 54, 12, 40], [30, 73, 13, 109, 10, 59, 47, 4, 45, 43, 1386, 261, 14, 209, 275, 26, 10, 59, 47, 4, 45, 43], [30, 73, 33, 233, 14, 59, 47, 4, 45, 43, 11, 105, 2085, 73, 13, 109, 14, 59, 47, 4, 45, 43], [1, 143, 188, 67, 176, 26, 1, 942, 10, 248, 143, 999, 8, 553, 392, 4, 1104, 64, 1, 65, 67, 397, 10, 3186, 1388, 392, 4, 1104, 64], [8, 1336, 167, 6, 154, 492, 3187, 345, 381, 5, 3188, 215, 4, 1977, 1161, 1, 3189, 5, 1, 355, 857, 842, 82, 509, 26, 2086, 2, 3, 64, 2086, 2, 3, 64, 587, 7, 857, 61, 82, 4, 68, 355, 3190, 842, 82], [1386, 261, 14, 209, 275, 26, 10, 59, 47, 4, 45, 43, 30, 73, 13, 233, 10, 59, 47, 4, 45, 43], [6, 223, 70, 89, 395, 11, 259, 211, 24, 606, 1664, 29, 1, 101, 4, 1, 493, 1665, 49, 10, 141, 77, 80, 12, 36, 14, 507, 6, 15, 77, 80, 11, 281, 70, 89, 12, 36], [1, 30, 73, 33, 153, 26, 464, 59, 47, 4, 45, 43, 30, 73, 13, 109, 14, 59, 47, 4, 45, 43], [30, 73, 33, 233, 14, 59, 47, 4, 45, 43, 11, 105, 2085, 73, 13, 109, 10, 59, 47, 4, 45, 43], [11, 1, 234, 5, 346, 347, 84, 9, 104, 514, 6, 95, 97, 7, 300, 384, 5, 734, 4, 1, 653, 301, 515, 8, 1047, 14, 248, 478, 301, 3191, 185, 131, 6, 714, 7, 1459, 9, 1, 1275, 3192, 5, 346, 347, 84, 4, 3193, 3194, 2071, 730, 125, 8, 3195, 643, 24, 115], [8, 269, 9, 225, 24, 82, 9, 7, 350, 3196, 126, 101, 6, 714, 7, 82, 34, 3197, 1272, 26, 3198, 1, 1105, 431, 21, 68, 376, 73, 19, 499, 2, 3, 197, 8, 24, 58, 6, 3199, 105, 1811, 4, 7, 3200, 3201, 1, 1002, 61, 23, 30, 132, 21, 716, 19, 83, 4, 1, 3202, 23, 132, 21, 68, 376, 19, 499, 2, 3, 197], [6, 1765, 1, 693, 5, 491, 9, 44, 21, 298, 4, 292, 10, 3203, 35, 1091, 2010, 3204, 2087, 2, 3, 37, 42, 1199, 1933, 1, 406, 212, 5, 3205, 688, 1, 212, 5, 3206, 6, 15, 7, 155, 35, 1091, 166, 3207, 42, 562, 1, 212, 5, 3208, 1349, 9, 3209, 145, 136, 283, 810, 2087, 2, 3, 37], [2088, 226, 1223, 2089, 129, 439, 123, 214, 9, 2090, 8, 1039, 805, 256, 1880, 781, 4, 3210, 36, 48, 129, 3211, 1, 18, 5, 3212, 1194, 513, 648, 3213, 9, 2061, 1, 534, 3214, 304, 3215, 8, 1, 1252, 128, 6, 262, 412, 2090, 206, 1, 1124, 5, 1, 3216, 4, 3217, 163, 8, 68, 2088, 226, 1223, 2089, 3218], [11, 34, 265, 6, 18, 1, 30, 3219, 62, 97, 26, 457, 2, 3, 747, 42, 562, 1, 623, 23, 3220, 1015, 252, 3221, 13, 1, 30, 34, 13, 72, 9, 941, 8, 1, 139, 3222, 2, 3, 747, 97, 7, 2055, 1381, 19, 883, 170, 42, 2056, 9, 2057, 215, 5, 7, 30, 61, 23, 810, 30, 8, 1, 139, 136], [1, 22, 13, 118, 593, 10, 7, 30, 73, 370, 47, 4, 45, 43, 7, 720, 837, 5, 613, 433, 29, 13, 1, 126, 193, 42, 240, 239, 126, 218, 174, 21, 7, 239, 22, 1040, 30, 73, 47, 4, 45, 43], [11, 434, 6, 18, 1, 99, 496, 2091, 295, 37, 42, 3223, 7, 1357, 3224, 168, 34, 3225, 1, 212, 5, 3226, 218, 8, 1, 46, 1617, 18, 1, 99, 496, 81, 295, 37, 5, 434, 99, 8, 48, 263], [106, 32, 3227, 10, 150, 151, 2, 3, 25, 14, 7, 78, 166, 5, 3228, 65, 67, 50, 10, 150, 151, 2, 3, 25, 14, 68, 331, 863, 78, 166, 5, 3229, 42, 6, 3230, 3231, 14, 7, 3232, 5, 1023, 480, 3233], [1, 171, 6, 18, 13, 429, 180, 84, 7, 529, 526, 629, 171, 42, 33, 50, 23, 1, 960, 735, 22, 1805, 1806, 1807, 3234, 13, 224, 14, 429, 180, 84, 50, 23, 1, 583, 1235], [195, 222, 5, 1, 3235, 3236, 5, 1, 1642, 3237, 6, 50, 68, 517, 3238, 271, 199, 21, 490, 557, 2, 3, 90, 10, 1, 28, 803, 21, 1, 3239, 3240, 50, 1, 163, 10, 1, 490, 81, 557, 2, 3, 90, 5, 271, 3241, 3242], [11, 305, 44, 6, 76, 95, 7, 29, 61, 23, 17, 12, 2, 3, 16, 11, 296, 6, 76, 508, 1, 235, 286, 5, 1, 126, 61, 1177, 4, 3243, 49, 42, 32, 61, 23, 1, 177, 3244, 1252, 12, 2, 3, 16], [6, 18, 1, 577, 119, 578, 1217, 5, 579, 2, 3, 64, 11, 231, 579, 2, 3, 64, 121, 576, 119, 3245, 11, 3246, 394, 10, 1, 429, 171, 180, 84, 14, 68, 2092, 623, 5, 3247], [6, 76, 728, 68, 1812, 5, 24, 128, 4, 34, 5, 2093, 4, 1302, 53, 6, 851, 7, 1860, 290, 34, 24, 128, 3248, 326, 5, 1, 479, 545, 3249, 7, 13, 642, 124, 793, 4, 1, 205, 326, 5, 1, 479, 6, 18, 1, 792, 5, 2093, 4, 1302, 53, 3250], [145, 592, 9, 972, 48, 304, 13, 9, 18, 7, 162, 369, 34, 13, 3251, 11, 709, 483, 764, 290, 31, 1, 189, 162, 451, 4, 690, 156, 11, 70, 3252, 1083, 13, 72, 9, 1, 1083, 11, 1, 189, 162, 8, 451, 4, 690, 156], [6, 18, 243, 244, 4, 245, 201, 11, 1718, 14, 331, 78, 166, 5, 1519, 69, 14, 1, 243, 527, 244, 4, 245, 201, 7, 78, 166, 5, 1122, 676, 415, 5, 326, 4, 444, 14, 528, 769, 214, 9, 1, 529, 677], [6, 121, 24, 433, 49, 8, 7, 60, 592, 10, 1, 17, 29, 102, 11, 41, 1813, 96, 25, 4, 60, 257, 299, 19, 6, 18, 102, 131, 96, 25, 11, 1093, 1, 209, 41, 19, 222], [1, 1751, 3253, 11, 24, 19, 13, 1, 1381, 14, 1985, 378, 3254, 995, 5, 457, 2, 3, 1100, 24, 19, 13, 68, 3255, 5, 1, 683, 1559, 5, 63, 3256, 19, 5, 457, 2, 3, 1100, 7, 82, 11, 78, 168, 418, 5, 63, 61, 23, 248, 91, 149], [6, 952, 7, 300, 2094, 1096, 19, 31, 1, 391, 238, 19, 10, 7, 684, 799, 446, 687, 1106, 309, 1107, 4, 1108, 329, 6, 421, 178, 26, 46, 7, 2095, 2096, 446, 687, 1106, 309, 1107, 4, 1108, 329, 10, 98, 31, 68, 2097, 23, 1, 2098, 190, 2099, 21, 24, 22], [1, 1678, 434, 837, 13, 68, 99, 434, 19, 175, 14, 1243, 7, 158, 168, 1679, 78, 122, 295, 37, 1, 3257, 5, 68, 99, 496, 19, 13, 34, 98, 13, 650, 2053, 4, 98, 129, 123, 268, 9, 104, 1132, 11, 7, 1180, 5, 434, 783, 295, 37], [1986, 13, 1987, 728, 31, 68, 345, 322, 62, 643, 1, 91, 113, 114, 127, 34, 1, 30, 157, 13, 1988, 26, 1, 75, 5, 149, 8, 42, 98, 3258, 9, 1, 91, 113, 5, 157, 114, 127, 63, 34, 194, 8, 72, 149, 237, 9, 104, 766, 72], [6, 232, 24, 58, 23, 54, 12, 40, 7, 307, 85, 22, 42, 13, 138, 8, 741, 8, 185, 3259, 58, 67, 196, 179, 23, 1, 54, 12, 40, 22, 42, 13, 7, 22, 733, 15, 8, 38, 4, 34, 129, 123, 15, 8, 697, 235, 143, 2100], [11, 1, 41, 19, 6, 15, 1, 102, 20, 96, 25, 9, 348, 7, 100, 41, 19, 23, 1, 209, 140, 5, 1, 54, 22, 717, 14, 844, 1989, 1246, 14, 116, 3260, 41, 19, 13, 7, 100, 102, 96, 25, 19, 50, 10, 822, 14, 155, 116, 173, 4, 559, 567], [1, 391, 310, 13, 3261, 26, 1, 1054, 22, 278, 2, 3, 43, 131, 42, 240, 84, 1308, 21, 2101, 7, 1197, 5, 3262, 125, 1054, 278, 2, 3, 43, 13, 7, 310, 5, 84, 655, 1308, 459, 21, 1, 495, 188, 5, 2101, 399, 10, 1689, 3263, 363, 352, 3264, 3265, 4, 3266, 3267], [6, 952, 7, 300, 2094, 1096, 19, 31, 1, 391, 238, 19, 10, 7, 684, 799, 446, 687, 1106, 309, 1107, 4, 1108, 329, 6, 421, 178, 26, 46, 7, 2095, 2096, 446, 687, 1106, 309, 1107, 4, 1108, 329, 10, 98, 31, 68, 2097, 23, 1, 2098, 190, 2099, 21, 24, 22], [1, 136, 585, 85, 28, 13, 118, 593, 10, 59, 8, 105, 3268, 4, 3269, 398, 1631, 26, 1, 3270, 5, 1286, 3271, 705, 47, 4, 45, 43, 101, 30, 132, 67, 153, 26, 464, 59, 8, 105, 398, 4, 817, 10, 1, 356, 370, 47, 4, 45, 43], [51, 1, 41, 65, 371, 15, 8, 24, 58, 32, 2025, 155, 116, 291, 1372, 50, 10, 102, 96, 2, 3, 88, 11, 1, 41, 19, 6, 15, 51, 427, 691, 4, 1, 275, 1065, 5, 1, 85, 691, 4, 50, 7, 100, 41, 19, 14, 155, 116, 173, 10, 102, 96, 2, 3, 88], [328, 32, 61, 23, 91, 113, 42, 1257, 643, 1, 1258, 34, 72, 63, 194, 8, 72, 149, 114, 127, 675, 1783, 95, 728, 1293, 1784, 26, 1785, 1, 207, 1786, 26, 1, 91, 113, 42, 270, 34, 63, 1017, 8, 72, 149, 237, 9, 95, 72, 365, 114, 127], [393, 6, 849, 1, 54, 22, 717, 12, 40, 617, 98, 13, 733, 15, 8, 1, 235, 3272, 11, 572, 3273, 58, 67, 196, 179, 23, 1, 54, 12, 40, 22, 42, 13, 7, 22, 733, 15, 8, 38, 4, 34, 129, 123, 15, 8, 697, 235, 143, 2100], [11, 1, 683, 1312, 6, 18, 1, 367, 207, 313, 100, 22, 180, 4, 246, 37, 42, 240, 450, 11, 698, 21, 800, 571, 9, 1313, 153, 21, 206, 83, 1061, 30, 3274, 344, 11, 30, 467, 6, 18, 1, 800, 28, 21, 1, 367, 1062, 310, 180, 4, 246, 37], [26, 3275, 513, 23, 1, 2102, 30, 3276, 72, 9, 34, 138, 8, 1099, 2, 3, 197, 1, 3277, 128, 3278, 327, 852, 545, 1, 513, 32, 214, 1, 212, 1, 118, 13, 7, 1356, 5, 1, 3279, 161, 138, 8, 1099, 2, 3, 197], [3280, 2, 3, 64, 750, 7, 3281, 2103, 2104, 3282, 330, 9, 1872, 1, 191, 771, 3283, 2, 3, 64, 97, 7, 700, 128, 9, 2103, 2104, 78, 11, 293, 303, 42, 3284, 3285, 51, 1, 125, 8, 306, 4, 51, 795, 8, 804, 1888, 11], [6, 95, 1820, 2105, 34, 61, 23, 346, 347, 84, 24, 234, 154, 104, 7, 514, 234, 5, 1883, 3286, 26, 3287, 1, 300, 384, 734, 4, 1, 653, 301, 1775, 6, 1776, 1, 112, 301, 513, 5, 346, 347, 84, 4, 1777, 1, 653, 301, 515], [11, 1, 683, 1312, 6, 18, 1, 367, 207, 313, 100, 22, 180, 4, 246, 37, 42, 240, 450, 11, 698, 21, 800, 571, 9, 1313, 153, 21, 206, 83, 1061, 30, 3288, 15, 1, 836, 450, 21, 1, 207, 313, 100, 22, 180, 4, 246, 37, 9, 535, 1, 467, 5, 18, 5, 111, 624, 843], [317, 134, 36, 13, 7, 75, 5, 143, 975, 15, 11, 355, 3289, 1075, 48, 873, 13, 327, 641, 14, 1, 317, 143, 314, 15, 11, 436, 134, 36], [662, 6, 732, 1, 106, 5, 1, 60, 27, 35, 19, 12, 2, 3, 16, 1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16], [1, 582, 832, 46, 22, 33, 564, 21, 1, 54, 310, 12, 40, 1, 49, 11, 1, 44, 572, 35, 558, 67, 50, 23, 1, 1334, 54, 22, 12, 40], [6, 1389, 1, 200, 8, 105, 1390, 21, 1391, 9, 420, 191, 462, 426, 83, 10, 1, 39, 1392, 42, 1074, 1393, 354, 5, 420, 191, 83, 74, 71, 2, 3, 37, 6, 18, 1, 39, 52, 14, 39, 191, 74, 71, 2, 3, 37], [6, 1389, 1, 200, 8, 105, 1390, 21, 1391, 9, 420, 191, 462, 426, 83, 10, 1, 39, 1392, 42, 1074, 1393, 354, 5, 420, 191, 83, 74, 71, 2, 3, 6, 18, 1, 39, 52, 14, 39, 191, 74, 71, 2, 3, 37], [6, 18, 102, 131, 96, 25, 11, 1093, 1, 209, 41, 19, 3290, 18, 7, 100, 371, 50, 23, 1, 353, 22, 4, 18, 102, 96, 25, 11, 371, 3291, 648, 282], [1, 255, 1362, 231, 1363, 1364, 104, 1, 54, 22, 12, 40, 1, 29, 33, 50, 23, 1, 44, 4, 867, 236, 5, 1, 54, 22, 213, 131, 12, 40], [6, 18, 1, 39, 52, 14, 39, 191, 74, 71, 2, 3, 37, 6, 1389, 1, 200, 8, 105, 1390, 21, 1391, 9, 420, 191, 462, 426, 83, 10, 1, 39, 1392, 42, 1074, 1393, 354, 5, 420, 191, 83, 74, 71, 2, 3, 37], [589, 6, 69, 27, 56, 35, 65, 14, 24, 132, 10, 1, 432, 17, 20, 12, 2, 3, 16, 662, 6, 732, 1, 106, 5, 1, 60, 27, 35, 19, 12, 2, 3, 16], [1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16, 1, 17, 161, 12, 2, 3, 16, 33, 15, 11, 1293, 3292, 4, 3293, 1190], [1, 3294, 3295, 247, 22, 6, 15, 13, 7, 647, 5, 1, 275, 187, 294, 3296, 2106, 2, 3, 43, 48, 3297, 9, 7, 300, 213, 5, 1, 275, 187, 2106, 2, 3, 43], [523, 129, 123, 7, 390, 1306, 5, 167, 23, 195, 115, 190, 413, 950, 5, 1170, 409, 4, 588, 90, 11, 7, 612, 3298, 5, 1, 1183, 5, 195, 115, 462, 409, 4, 588, 90], [48, 62, 930, 13, 881, 138, 8, 1, 62, 497, 263, 639, 2, 3, 53, 7, 514, 497, 5, 1, 46, 4, 147, 691, 154, 104, 289, 190, 1, 62, 497, 263, 639, 2, 3, 53], [1, 101, 385, 104, 417, 26, 1, 17, 38, 20, 12, 2, 3, 16, 1229, 24, 101, 13, 1, 38, 20, 17, 12, 2, 3, 16, 232, 206, 2041, 605, 3299, 602, 30, 605], [137, 334, 8, 436, 542, 334, 234, 931, 543, 4, 544, 387, 887, 1, 137, 8, 7, 566, 8, 1, 827, 5, 7, 189, 426, 83, 1863, 242, 32, 771, 137, 242, 524, 26, 542, 334, 234, 543, 4, 544, 387], [98, 1085, 23, 1, 1387, 584, 52, 498, 4, 885, 36, 98, 3300, 584, 1259, 21, 1, 1387, 52, 498, 4, 885, 36, 4, 1707, 257, 1135, 4, 1240, 940, 283, 130, 3301, 1380, 26, 3302, 3303], [6, 18, 1335, 2107, 2, 3, 36, 9, 160, 372, 41, 283, 7, 2108, 3304, 3305, 372, 41, 133, 9, 2108, 827, 6, 121, 23, 1, 108, 152, 1335, 2107, 2, 3, 36], [1, 46, 75, 13, 15, 9, 69, 1, 27, 35, 19, 4, 41, 19, 11, 17, 12, 2, 3, 16, 6, 18, 1, 17, 27, 35, 29, 12, 2, 3, 16, 9, 424, 24, 65], [6, 262, 77, 80, 14, 604, 411, 31, 138, 8, 12, 36, 6, 76, 871, 364, 311, 297, 3306, 460, 23, 507, 1330, 5, 77, 80, 14, 3307, 12, 36], [1, 1268, 236, 5, 28, 622, 21, 846, 4, 1269, 583, 1270, 5, 1, 54, 28, 75, 12, 40, 1, 54, 22, 12, 40, 13, 66, 21, 1, 208, 5, 1, 203, 449], [11, 1394, 6, 18, 1, 883, 9, 69, 30, 178, 23, 1, 54, 1394, 22, 12, 40, 94, 325, 3308, 3309, 6, 18, 1, 54, 22, 12, 40, 31, 281, 28], [1, 2109, 3310, 401, 2110, 2, 3, 87, 13, 1, 1383, 11, 1, 3311, 1384, 2109, 2110, 2, 3, 3312, 13, 68, 401, 7, 330, 4, 7, 422, 1384, 11, 352, 41, 3313, 1478, 4, 764], [6, 50, 7, 100, 41, 19, 23, 1, 917, 185, 5, 1, 44, 353, 22, 1133, 480, 63, 10, 1, 614, 20, 615, 87, 14, 1, 155, 116, 173, 342, 4, 474, 197, 105, 41, 65, 18, 155, 116, 173, 342, 4, 474, 197], [6, 15, 59, 47, 4, 45, 43, 1161, 14, 1, 3314, 705, 9, 3315, 1, 46, 1890, 3316, 374, 593, 1, 305, 236, 9, 111, 5, 1, 3317, 14, 1, 59, 122, 47, 4, 45, 43], [24, 323, 1208, 128, 11, 412, 221, 4, 322, 5, 1307, 443, 4, 438, 64, 412, 221, 4, 322, 5, 689, 3318, 1, 221, 188, 11, 105, 1, 1207, 443, 4, 438, 64, 4, 1, 3319, 128], [409, 2, 3, 87, 225, 1, 286, 5, 493, 1153, 15, 56, 78, 65, 366, 312, 532, 1328, 4, 3320, 2, 3, 87, 18, 56, 78, 381, 1186, 99, 4, 913, 9, 1000, 1311, 23, 694, 695], [2111, 2, 3, 64, 1263, 3321, 23, 1, 3322, 3323, 147, 31, 68, 1968, 3324, 751, 9, 1, 29, 5, 2111, 2, 3, 64, 31, 1, 1254, 3325, 29], [6, 214, 1, 366, 312, 894, 576, 78, 103, 21, 1, 202, 56, 78, 403, 204, 2, 3, 57, 366, 312, 4, 405, 189, 65, 67, 66, 14, 1, 202, 204, 2, 3, 57, 152, 11, 405, 200, 6, 15, 1, 3326, 81], [6, 492, 3327, 9, 1151, 3328, 3329, 3330, 5, 1, 3331, 1316, 17, 12, 2, 3, 16, 6, 121, 7, 230, 5, 1, 284, 27, 38, 29, 10, 17, 12, 2, 3, 16], [6, 18, 1, 17, 108, 152, 170, 9, 69, 7, 408, 19, 12, 2, 3, 16, 6, 225, 1, 19, 659, 1, 17, 27, 35, 29, 12, 2, 3, 16, 214, 9, 1063, 645], [98, 129, 7, 1377, 1128, 2092, 136, 1298, 602, 217, 298, 187, 3332, 3333, 225, 9, 1907, 742, 2, 3, 40, 6, 192, 719, 8, 327, 741, 7, 1960, 298, 22, 1, 217, 298, 187, 742, 2, 3, 40], [1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16, 1, 118, 112, 1088, 32, 60, 49, 10, 408, 294, 1360, 50, 10, 17, 12, 2, 3, 16], [1, 144, 32, 400, 4, 466, 10, 229, 21, 1, 17, 20, 12, 2, 3, 16, 1, 118, 112, 1088, 32, 60, 49, 10, 408, 294, 1360, 50, 10, 17, 12, 2, 3, 16], [1, 444, 166, 33, 75, 9, 778, 4, 1, 19, 33, 50, 574, 150, 151, 2, 3, 25, 6, 18, 1, 746, 315, 369, 4, 520, 150, 151, 2, 3, 25, 9, 651, 106], [1, 444, 166, 33, 75, 9, 778, 4, 1, 19, 33, 50, 574, 150, 151, 2, 3, 25, 6, 18, 150, 151, 2, 3, 25, 14, 1, 331, 78, 166, 75, 9, 778], [6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16, 6, 225, 1, 19, 659, 1, 17, 27, 35, 29, 12, 2, 3, 16, 214, 9, 1063, 645], [328, 944, 268, 34, 1, 217, 137, 187, 485, 2112, 336, 2, 3, 90, 137, 242, 32, 3334, 217, 137, 187, 485, 336, 2, 3, 90, 3335, 697, 293, 354, 34, 32, 1691, 9, 3336, 1700, 3337, 4, 1623], [328, 944, 268, 34, 1, 217, 137, 187, 485, 2112, 336, 2, 3, 90, 137, 242, 32, 3338, 4, 1865, 90, 15, 137, 242, 5, 1, 217, 137, 187, 336, 2, 3, 90, 31, 7, 250], [661, 2, 3, 40, 714, 7, 1754, 11, 46, 2043, 65, 11, 55, 227, 1, 673, 5, 227, 19, 67, 509, 8, 661, 2, 3, 40, 4, 661, 4, 3339, 37], [6, 1354, 1, 160, 200, 8, 357, 283, 389, 191, 10, 39, 120, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [94, 35, 19, 17, 13, 7, 258, 20, 11, 27, 38, 49, 12, 2, 3, 16, 1, 161, 13, 66, 23, 1089, 5, 68, 177, 27, 38, 161, 17, 12, 2, 3, 16], [11, 1, 357, 28, 456, 139, 3340, 588, 2, 3, 88, 13, 1, 254, 250, 11, 3341, 3342, 2113, 1096, 5, 1, 39, 826, 29, 588, 2, 3, 88, 13, 15, 11, 1, 357, 28, 456], [98, 33, 145, 5, 1, 606, 1067, 8, 1, 502, 159, 62, 57, 2114, 2, 3, 57, 3343, 13, 15, 31, 1, 298, 28, 75, 8, 1, 502, 57, 159, 62, 2114, 2, 3, 57], [6, 1354, 1, 160, 200, 8, 357, 283, 389, 191, 10, 39, 120, 86, 2, 3, 53, 8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53], [188, 32, 516, 23, 1, 147, 28, 10, 1358, 260, 14, 1, 502, 1388, 392, 4, 1104, 64, 6, 871, 1358, 286, 3344, 10, 1, 2051, 1388, 21, 1, 159, 62, 392, 4, 1104, 64], [1, 818, 807, 26, 1, 985, 1609, 371, 8, 1, 572, 3345, 75, 33, 3346, 1610, 3347, 153, 14, 1, 60, 985, 41, 19, 14, 819, 4, 116, 173, 630, 4, 3348, 188, 5, 48, 340, 1010, 8, 193, 94, 42, 881, 1021, 172, 65, 14, 1, 280, 1088, 83, 3349, 3350, 4, 94, 7, 985, 371, 14, 116, 173, 131, 630, 4, 45], [6, 3351, 1, 3352, 304, 31, 7, 199, 62, 26, 359, 99, 65, 14, 68, 3353, 1053, 4, 7, 2067, 1383, 369, 162, 10, 1, 404, 20, 316, 4, 134, 25, 6, 18, 404, 316, 4, 134, 25, 31, 1, 99, 122], [393, 98, 13, 76, 2047, 34, 1, 1378, 5, 663, 1379, 8, 1, 46, 75, 13, 72, 9, 1, 145, 516, 26, 463, 2, 3, 87, 4, 2048, 9, 8, 185, 2049, 2, 3, 87, 76, 3354, 179, 34, 1, 941, 1378, 5, 663, 1379, 190, 838, 472, 325, 2105, 34, 675, 5, 789, 32, 3355, 3356, 3357, 3358, 294, 650, 3359, 63], [1290, 290, 31, 172, 32, 3360, 9, 1340, 11, 8, 7, 3361, 130, 234, 5, 3362, 1251, 290, 31, 1, 145, 97, 8, 2115, 2, 3, 1084, 1, 255, 1145, 5, 1, 130, 994, 9, 3363, 129, 123, 1, 3364, 3365, 128, 97, 8, 2115, 2, 3, 1084], [51, 172, 299, 65, 32, 484, 10, 17, 12, 2, 3, 16, 590, 34, 1, 238, 19, 3366, 68, 1022, 3367, 1341, 608, 185, 3368, 751, 9, 34, 19, 31, 17, 2116, 1281, 98, 33, 50, 10, 1, 17, 20, 12, 2, 3, 16], [423, 3369, 362, 2, 3, 25, 97, 9, 18, 880, 1103, 11, 3370, 3371, 5, 195, 1042, 3372, 360, 1, 3373, 5, 168, 1375, 97, 26, 362, 2, 3, 25, 590, 34, 6, 657, 318, 1333, 1103, 11, 594], [6, 69, 7, 1076, 199, 19, 92, 79, 2, 3, 25, 11, 111, 1134, 4, 147, 10, 361, 21, 1, 1746, 31, 46, 934, 24, 29, 13, 7, 226, 19, 407, 10, 1076, 199, 31, 175, 8, 1, 92, 20, 79, 2, 3, 25], [6, 360, 1, 1671, 8, 646, 2, 3, 25, 10, 1, 1649, 5, 3374, 178, 31, 679, 9, 7, 1922, 238, 1706, 18, 3375, 646, 2, 3, 25, 9, 535, 1, 3376, 4, 3377, 868, 5, 68, 477, 2113], [48, 13, 1374, 9, 68, 99, 14, 1, 3378, 1169, 93, 31, 8, 561, 2, 3, 90, 1, 3379, 5, 257, 93, 154, 76, 104, 3380, 26, 1, 18, 5, 91, 30, 658, 31, 3381, 26, 561, 2, 3, 90], [51, 58, 32, 223, 10, 1, 17, 27, 38, 29, 12, 2, 3, 16, 14, 7, 532, 126, 1298, 5, 3382, 1, 3383, 5, 35, 693, 1, 65, 32, 50, 848, 10, 7, 27, 35, 29, 12, 2, 3, 16, 34, 15, 1, 642, 3384, 65, 9, 341, 1, 28], [6, 18, 1, 17, 20, 12, 2, 3, 16, 9, 348, 7, 70, 27, 56, 35, 19, 66, 23, 1, 606, 442, 28, 31, 138, 1415, 751, 9, 34, 19, 31, 17, 2116, 1281, 98, 33, 50, 10, 1, 17, 20, 12, 2, 3, 16], [51, 58, 67, 23, 44, 236, 5, 876, 119, 198, 23, 1, 828, 549, 550, 551, 133, 8, 1, 217, 187, 1235, 213, 131, 638, 2, 3, 110, 172, 1057, 67, 397, 8, 58, 23, 1, 217, 187, 638, 2, 3, 110, 14, 1, 3385, 264, 52, 1166, 26], [6, 18, 1, 30, 132, 9, 644, 7, 126, 193, 26, 865, 1, 641, 126, 654, 370, 47, 4, 45, 36, 9, 51, 2117, 18, 1, 1097, 5, 903, 4, 2118, 59, 47, 4, 45, 36, 132, 31, 7, 370, 469, 9, 681, 63, 2119, 585, 9, 111, 205], [6, 407, 7, 685, 235, 19, 11, 1, 69, 3386, 14, 1, 60, 580, 5, 1, 17, 20, 12, 2, 3, 16, 6, 76, 225, 14, 1, 60, 27, 29, 5, 17, 12, 2, 3, 16, 14, 60, 302, 590, 11, 1, 3387, 2120, 42, 6, 75, 9, 869], [7, 941, 1306, 5, 420, 28, 13, 15, 9, 858, 1, 3388, 775, 9, 3389, 1073, 11, 7, 497, 5, 1, 82, 462, 569, 4, 2121, 16, 1175, 1264, 7, 212, 5, 658, 72, 9, 1, 212, 5, 1073, 13, 318, 7, 3390, 11, 914, 188, 569, 4, 2121, 3391], [6, 18, 1, 30, 132, 9, 644, 7, 126, 193, 26, 865, 1, 641, 126, 654, 370, 47, 4, 45, 36, 9, 51, 2117, 18, 1, 1097, 5, 903, 4, 2118, 59, 47, 4, 45, 36, 132, 31, 7, 370, 469, 9, 681, 63, 2119, 585, 9, 111, 205], [83, 11, 7, 296, 60, 133, 6, 15, 1, 453, 540, 5, 1, 1395, 801, 22, 2122, 2123, 84, 1, 143, 22, 13, 7, 388, 5, 68, 1818, 213, 5, 1, 1395, 801, 22, 2122, 7, 869, 480, 30, 3392, 22, 5, 1395, 44, 2123, 84], [6, 3393, 14, 1, 54, 22, 12, 40, 8, 269, 9, 95, 7, 85, 3394, 22, 11, 1394, 4, 3395, 11, 44, 63, 8, 1, 257, 1371, 32, 174, 21, 7, 1261, 30, 73, 5, 125, 21, 1, 54, 85, 22, 12, 40], [1, 3396, 21, 987, 62, 94, 195, 115, 8, 465, 176, 493, 691, 11, 1, 62, 988, 2, 3, 88, 6, 631, 8, 105, 573, 7, 4, 989, 5, 987, 62, 94, 195, 115, 8, 465, 988, 2, 3, 88, 14, 68, 990, 5, 24, 991, 29], [3397, 7, 3398, 583, 143, 10, 1, 17, 29, 12, 2, 3, 16, 31, 7, 1871, 6, 484, 1, 235, 82, 1878, 642, 23, 112, 708, 3399, 147, 24, 82, 6, 223, 112, 2014, 35, 58, 10, 1, 27, 235, 29, 17, 12, 2, 3, 16], [1, 656, 1748, 93, 32, 192, 15, 9, 69, 7, 158, 168, 56, 2091, 295, 87, 9, 496, 1, 261, 358, 1400, 602, 1, 3400, 18, 1, 158, 168, 56, 99, 496, 103, 295, 87, 9, 1301, 7, 496, 269, 11, 111, 501, 5, 3401], [24, 391, 82, 13, 61, 23, 1, 953, 238, 309, 41, 19, 1176, 128, 9, 78, 30, 178, 5, 457, 2, 3, 747, 4, 457, 2, 3, 1100, 10, 1, 883, 3402, 51, 5, 1, 642, 558, 6, 225, 1, 238, 30, 178, 5, 457, 2, 3, 747, 14, 112, 168, 3403, 105, 61, 23, 815, 450, 4, 660, 26, 60, 91, 1057], [6, 396, 1, 529, 526, 19, 376, 1105, 1297, 4, 1, 3404, 103, 3405, 165, 9, 210, 1, 3406, 3407, 18, 1105, 103, 1105, 1297, 9, 3408, 1, 3409, 136], [6, 871, 259, 563, 2, 3, 156, 5, 35, 29, 619, 460, 659, 1, 1332, 44, 3410, 216, 35, 693, 574, 1, 259, 222, 563, 2, 3, 156], [98, 13, 7, 60, 27, 56, 35, 19, 12, 2, 3, 16, 51, 24, 49, 32, 664, 14, 7, 60, 27, 29, 66, 14, 17, 12, 2, 3, 16], [393, 6, 15, 17, 20, 31, 27, 296, 12, 2, 3, 16, 1, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [324, 142, 13, 260, 10, 1, 3411, 431, 142, 745, 2, 3, 36, 1, 2124, 5, 142, 211, 112, 72, 63, 13, 1158, 10, 249, 745, 2, 3, 36], [393, 6, 15, 17, 20, 31, 27, 296, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [98, 13, 7, 60, 27, 56, 35, 19, 12, 2, 3, 16, 6, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [94, 35, 19, 17, 13, 7, 258, 20, 11, 27, 38, 49, 12, 2, 3, 16, 393, 6, 15, 17, 20, 31, 27, 296, 12, 2, 3, 16], [3412, 98, 13, 460, 26, 249, 142, 152, 745, 2, 3, 36, 1, 2124, 5, 142, 211, 112, 72, 63, 13, 1158, 10, 249, 745, 2, 3, 36], [6, 18, 100, 41, 65, 14, 116, 839, 96, 2, 3, 88, 1, 41, 65, 32, 407, 10, 1, 102, 20, 96, 2, 3, 88, 14, 155, 116, 173], [6, 882, 428, 78, 591, 106, 10, 150, 151, 2, 3, 25, 6, 18, 1, 746, 315, 369, 4, 520, 150, 151, 2, 3, 25, 9, 651, 106], [6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25, 6, 15, 99, 908, 21, 92, 79, 2, 3, 25, 4, 794, 14, 7, 212, 5, 163], [6, 882, 428, 78, 591, 106, 10, 150, 151, 2, 3, 25, 6, 69, 1, 324, 1058, 811, 10, 1059, 1060, 315, 83, 14, 150, 151, 2, 3, 25], [393, 6, 15, 17, 20, 31, 27, 296, 12, 2, 3, 16, 6, 15, 1, 17, 20, 12, 2, 3, 16, 9, 69, 60, 27, 49, 14, 107, 739], [54, 12, 40, 13, 7, 307, 85, 22, 174, 21, 1, 208, 5, 1, 203, 541, 15, 1, 44, 140, 5, 1, 54, 22, 12, 40], [6, 3413, 1, 2125, 65, 561, 4, 451, 117, 11, 7, 1336, 678, 3414, 3415, 11, 1, 19, 83, 8, 561, 4, 451, 117, 42, 13, 7, 2125, 19], [6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25, 791, 6, 15, 1, 60, 437, 1930, 1053, 8, 1, 92, 20, 253, 79, 2, 3, 25], [11, 44, 4, 275, 7, 19, 33, 50, 10, 1, 54, 22, 12, 40, 11, 48, 265, 6, 18, 1, 54, 22, 12, 40], [1897, 6, 508, 34, 24, 19, 3416, 1, 258, 49, 5, 2126, 2, 3, 201, 23, 1025, 28, 3417, 439, 2126, 2, 3, 201, 97, 7, 238, 1632, 19, 11, 48, 304, 10, 7, 300, 28, 75, 11, 46, 4, 3418, 258, 286, 23, 1, 3419, 558], [6, 18, 271, 199, 14, 517, 349, 175, 10, 1, 92, 20, 79, 2, 3, 25, 6, 18, 1, 445, 81, 5, 256, 416, 79, 2, 3, 25], [6, 18, 1, 300, 859, 620, 247, 22, 1052, 90, 11, 51, 24, 3420, 1, 1185, 1444, 3421, 3422, 21, 7, 390, 75, 5, 2102, 3423, 6, 803, 21, 1, 300, 859, 620, 247, 22, 3424, 1052, 90, 11, 18, 8, 1771, 3425, 5, 1, 340, 3426], [6, 18, 1, 3427, 4, 1619, 908, 176, 26, 92, 213, 2138, 79, 2, 3, 25, 6, 15, 1, 81, 5, 1, 92, 94, 452, 79, 2, 3, 25], [6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 1699, 66, 27, 56, 35, 49, 10, 1, 220, 108, 20, 17, 12, 2, 3, 16], [8, 335, 1, 28, 33, 181, 182, 4, 373, 10, 39, 120, 86, 2, 3, 53, 6, 76, 182, 51, 63, 10, 39, 120, 86, 2, 3, 53], [6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 1317, 101, 49, 32, 66, 14, 1, 177, 27, 38, 20, 17, 12, 2, 3, 16], [6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 1840, 46, 1, 35, 19, 4, 11, 282, 6, 15, 1, 17, 20, 12, 2, 3, 16], [6, 15, 1, 17, 20, 12, 2, 3, 16, 14, 124, 107, 3428, 161, 17, 12, 2, 3, 16, 14, 107, 1912, 590, 11, 1, 3429, 2120, 3430], [250, 824, 33, 109, 10, 3431, 8, 202, 204, 2, 3, 57, 6, 69, 4, 337, 1, 163, 8, 7, 1889, 1041, 10, 202, 204, 2, 3, 57], [3436, 4, 1402, 90, 750, 68, 656, 3432, 19, 42, 887, 1, 377, 5, 2134, 8, 7, 137, 11, 136, 3437, 4, 217, 117, 1113, 1, 128, 5, 3438, 4, 438, 16, 4, 3439, 7, 3440, 522, 3441, 19, 14, 3442, 93, 1112, 21, 1, 656, 3432, 19], [1, 667, 8, 259, 2132, 32, 2138, 4, 3443, 42, 32, 318, 640, 454, 670, 9, 1, 141, 77, 80, 82, 12, 36, 640, 454, 188, 752, 14, 141, 77, 80, 12, 36, 11, 259, 4, 1114, 32, 2135, 14, 2136, 228, 754, 4, 228, 410], [48, 384, 5, 1027, 250, 33, 318, 15, 26, 1028, 2, 3, 40, 294, 2139, 4, 86, 3444, 231, 1028, 2, 3, 40, 2133, 1, 18, 5, 1027, 93, 26, 10, 600, 378], [48, 384, 5, 1027, 250, 33, 318, 15, 26, 1028, 2, 3, 40, 294, 2139, 4, 86, 3445, 1399, 286, 5, 1, 97, 103, 14, 1027, 93, 13, 1400, 602, 34, 5, 1028, 2, 3, 40, 4, 1109, 14, 34, 5, 2139, 4, 86, 37], [91, 379, 13, 61, 23, 1, 666, 34, 813, 814, 8, 205, 63, 1, 157, 5, 7, 30, 13, 601, 9, 1, 149, 98, 2128, 2129, 2130, 1396, 2131, 1397, 1398, 7, 30, 26, 1, 1110, 98, 1111, 813, 814]]\n"
     ]
    }
   ],
   "source": [
    "encoded_text = token.texts_to_sequences(words) \n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 439,  523,  129, ...,    0,    0,    0],\n",
       "       [   1,  118,  145, ...,  891,   26, 1408],\n",
       "       [ 892,  157,  287, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  48,  384,    5, ...,    0,    0,    0],\n",
       "       [  48,  384,    5, ...,    0,    0,    0],\n",
       "       [  91,  379,   13, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_kata = 50\n",
    "X = pad_sequences(encoded_text, maxlen = max_kata, padding='post')\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buat ngebagi data set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, random_state= 40, test_size = 0.2) #X ; kumpulan array, y : kelasnya "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  11,    1,  226, ...,    0,    0,    0],\n",
       "       [  26,  726,  699, ...,    0,    0,    0],\n",
       "       [   6,  492,  750, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [1904, 2763,   13, ...,    0,    0,    0],\n",
       "       [   6,   15,    1, ...,    0,    0,    0],\n",
       "       [ 118,    6,   15, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "# X_val = np.asarray(X_val)\n",
    "Y_train = np.asarray(Y_train)\n",
    "Y_test = np.asarray(Y_test)\n",
    "# Y_val = np.asarray(Y_val)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out_weights = word_model.syn1neg\n",
    "#print(out_weights)\n",
    "#word_model.wv.most_similar('scripts')\n",
    "#kv = word_model.wv\n",
    "#kv.vectors\n",
    "#out_weights = word_model.syn1neg\n",
    "#print(out_weights)\n",
    "vocab, vectors = word_model.wv.key_to_index, word_model.wv.vectors\n",
    "\n",
    "# get label and vector index.\n",
    "label_index = np.array([(voc[0], voc[1]) for voc in vocab.items()])\n",
    "\n",
    "# init dataframe using embedding vectors and set index as node name\n",
    "tmp =  pd.DataFrame(vectors[label_index[:,1].astype(int)])\n",
    "tmp.index = label_index[:, 0]\n",
    "tmp.to_csv(\"matrix_with_labels_CNN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "adadelta = keras.optimizers.Adadelta(learning_rate=0.001)\n",
    "sgd = keras.optimizers.SGD(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def prec(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(optimasi, output_dim=100, max_length=max_kata, y_dim=6, num_filters=64, filter_sizes = [2,3,4], pooling = 'max', pool_padding = 'valid', dropout=0.5):\n",
    "    \n",
    "    # Input Layer\n",
    "    embed_input = Input(shape=(max_length,))\n",
    "    x = Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1],input_length=max_length,weights=[embedding_matrix],trainable=True)(embed_input)\n",
    "    pooled_outputs = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        conv = Conv1D(num_filters, kernel_size=filter_sizes[i], padding='valid', activation='relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))(x)\n",
    "        if pooling=='max':\n",
    "            conv = MaxPooling1D(pool_size=max_length-filter_sizes[i]+1, strides=1, padding = pool_padding)(conv)\n",
    "        else:\n",
    "            conv = AveragePooling1D(pool_size=max_length-filter_sizes[i]+1, strides=1, padding = pool_padding)(conv)            \n",
    "        pooled_outputs.append(conv)\n",
    "    \n",
    "    merge = concatenate(pooled_outputs)\n",
    "        \n",
    "    x = Flatten()(merge)\n",
    "    dropX = Dropout(0.25)(x)\n",
    "\n",
    "    predictions = Dense(y_dim, activation = 'softmax')(dropX)\n",
    "    model = Model(inputs=embed_input,outputs=predictions)\n",
    "    model.compile(optimizer=optimasi ,loss ='categorical_crossentropy', metrics = ['acc', prec, rec])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_model(log_data,name, epoh):\n",
    "    # summarize history for accuracy\n",
    "    new_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728','#9467bd', '#8c564b', '#e377c2', '#7f7f7f','#bcbd22', '#17becf']\n",
    "    primes = list(range(1, epoh+1))\n",
    "    fig = plt.figure() \n",
    "    fig.set_size_inches(15,5)\n",
    "    plt.rcParams.update({'font.size': 19})\n",
    "    plt.plot(primes,log_data['acc'], label='acc', color=new_colors[0],linewidth=2)\n",
    "    plt.plot(primes,log_data['val_acc'], label='val_acc', color=new_colors[1],linewidth=2)\n",
    "    plt.title('Model accuracy ')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend([name,'val_'+name],loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    fig = plt.figure() \n",
    "    fig.set_size_inches(15,5)\n",
    "    plt.plot(primes,log_data['loss'], label='loss', color=new_colors[0],linewidth=2)\n",
    "    plt.plot(primes,log_data['val_loss'], label='val_loss', color=new_colors[1],linewidth=2)\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend([name,'val_'+name],loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    # summarize history for precision\n",
    "    fig = plt.figure() \n",
    "    fig.set_size_inches(15,5)\n",
    "    plt.plot(primes,log_data['prec'], label='prec', color=new_colors[0],linewidth=2)\n",
    "    plt.plot(primes,log_data['val_prec'], label='val_prec', color=new_colors[1],linewidth=2)\n",
    "    plt.title('Model Precision')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend([name,'val_'+name],loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "    # summarize history for Recall\n",
    "    fig = plt.figure() \n",
    "    fig.set_size_inches(15,5)\n",
    "    plt.plot(primes,log_data['rec'], label='rec', color=new_colors[0],linewidth=2)\n",
    "    plt.plot(primes,log_data['val_rec'], label='val_rec', color=new_colors[1],linewidth=2)\n",
    "    plt.title('Model Recall')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend([name,'val_'+name],loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)       (None, 50, 100)      344600      ['input_13[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_36 (Conv1D)             (None, 49, 64)       12864       ['embedding_12[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_37 (Conv1D)             (None, 48, 64)       19264       ['embedding_12[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_38 (Conv1D)             (None, 47, 64)       25664       ['embedding_12[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_36 (MaxPooling1D  (None, 1, 64)       0           ['conv1d_36[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_37 (MaxPooling1D  (None, 1, 64)       0           ['conv1d_37[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_38 (MaxPooling1D  (None, 1, 64)       0           ['conv1d_38[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 1, 192)       0           ['max_pooling1d_36[0][0]',       \n",
      "                                                                  'max_pooling1d_37[0][0]',       \n",
      "                                                                  'max_pooling1d_38[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_12 (Flatten)           (None, 192)          0           ['concatenate_12[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 192)          0           ['flatten_12[0][0]']             \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 6)            1158        ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 403,550\n",
      "Trainable params: 403,550\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_13 (Embedding)       (None, 50, 100)      344600      ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_39 (Conv1D)             (None, 49, 64)       12864       ['embedding_13[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_40 (Conv1D)             (None, 48, 64)       19264       ['embedding_13[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_41 (Conv1D)             (None, 47, 64)       25664       ['embedding_13[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_39 (MaxPooling1D  (None, 1, 64)       0           ['conv1d_39[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_40 (MaxPooling1D  (None, 1, 64)       0           ['conv1d_40[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_41 (MaxPooling1D  (None, 1, 64)       0           ['conv1d_41[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_13 (Concatenate)   (None, 1, 192)       0           ['max_pooling1d_39[0][0]',       \n",
      "                                                                  'max_pooling1d_40[0][0]',       \n",
      "                                                                  'max_pooling1d_41[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_13 (Flatten)           (None, 192)          0           ['concatenate_13[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 192)          0           ['flatten_13[0][0]']             \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 6)            1158        ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 403,550\n",
      "Trainable params: 403,550\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding_14 (Embedding)       (None, 50, 100)      344600      ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_42 (Conv1D)             (None, 49, 64)       12864       ['embedding_14[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_43 (Conv1D)             (None, 48, 64)       19264       ['embedding_14[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_44 (Conv1D)             (None, 47, 64)       25664       ['embedding_14[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling1d_42 (MaxPooling1D  (None, 1, 64)       0           ['conv1d_42[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_43 (MaxPooling1D  (None, 1, 64)       0           ['conv1d_43[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling1d_44 (MaxPooling1D  (None, 1, 64)       0           ['conv1d_44[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " concatenate_14 (Concatenate)   (None, 1, 192)       0           ['max_pooling1d_42[0][0]',       \n",
      "                                                                  'max_pooling1d_43[0][0]',       \n",
      "                                                                  'max_pooling1d_44[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_14 (Flatten)           (None, 192)          0           ['concatenate_14[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 192)          0           ['flatten_14[0][0]']             \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 6)            1158        ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 403,550\n",
      "Trainable params: 403,550\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "modeladam = model('adam',output_dim=100, max_length=max_kata,y_dim=6,filter_sizes = [2,3,4], pooling = 'max' )\n",
    "modeladadelta = model('adadelta',output_dim=100, max_length=max_kata,y_dim=6,filter_sizes = [2,3,4], pooling = 'max' )\n",
    "modelsgd = model('sgd',output_dim=100, max_length=max_kata,y_dim=6,filter_sizes = [2,3,4], pooling = 'max' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "68/68 [==============================] - 11s 19ms/step - loss: 4.3209 - acc: 0.3867 - prec: 0.3853 - rec: 0.3789 - val_loss: 2.1173 - val_acc: 0.5196 - val_prec: 0.5270 - val_rec: 0.5071\n",
      "Epoch 2/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 2.7191 - acc: 0.5359 - prec: 0.5399 - rec: 0.5316 - val_loss: 1.8827 - val_acc: 0.6093 - val_prec: 0.6341 - val_rec: 0.6079\n",
      "Epoch 3/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 2.0524 - acc: 0.6188 - prec: 0.6258 - rec: 0.6129 - val_loss: 1.6107 - val_acc: 0.6393 - val_prec: 0.6596 - val_rec: 0.6355\n",
      "Epoch 4/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 1.6624 - acc: 0.6842 - prec: 0.6894 - rec: 0.6765 - val_loss: 1.5104 - val_acc: 0.6991 - val_prec: 0.7081 - val_rec: 0.6878\n",
      "Epoch 5/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 1.4217 - acc: 0.7109 - prec: 0.7134 - rec: 0.7058 - val_loss: 1.5801 - val_acc: 0.6972 - val_prec: 0.7106 - val_rec: 0.6828\n",
      "Epoch 6/100\n",
      "68/68 [==============================] - 1s 20ms/step - loss: 1.1979 - acc: 0.7707 - prec: 0.7771 - rec: 0.7692 - val_loss: 1.4094 - val_acc: 0.7271 - val_prec: 0.7381 - val_rec: 0.7164\n",
      "Epoch 7/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 1.0644 - acc: 0.7808 - prec: 0.7865 - rec: 0.7763 - val_loss: 1.3369 - val_acc: 0.7327 - val_prec: 0.7447 - val_rec: 0.7190\n",
      "Epoch 8/100\n",
      "68/68 [==============================] - 2s 24ms/step - loss: 0.9978 - acc: 0.8011 - prec: 0.8126 - rec: 0.7939 - val_loss: 1.4828 - val_acc: 0.7140 - val_prec: 0.7205 - val_rec: 0.7085\n",
      "Epoch 9/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.8408 - acc: 0.8278 - prec: 0.8363 - rec: 0.8241 - val_loss: 1.3746 - val_acc: 0.7178 - val_prec: 0.7346 - val_rec: 0.7080\n",
      "Epoch 10/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.8630 - acc: 0.8066 - prec: 0.8134 - rec: 0.8037 - val_loss: 1.3962 - val_acc: 0.7477 - val_prec: 0.7616 - val_rec: 0.7434\n",
      "Epoch 11/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.7348 - acc: 0.8306 - prec: 0.8335 - rec: 0.8297 - val_loss: 1.3512 - val_acc: 0.7402 - val_prec: 0.7545 - val_rec: 0.7366\n",
      "Epoch 12/100\n",
      "68/68 [==============================] - 1s 20ms/step - loss: 0.7035 - acc: 0.8471 - prec: 0.8521 - rec: 0.8418 - val_loss: 1.3471 - val_acc: 0.7495 - val_prec: 0.7570 - val_rec: 0.7434\n",
      "Epoch 13/100\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.6288 - acc: 0.8812 - prec: 0.8840 - rec: 0.8764 - val_loss: 1.3354 - val_acc: 0.7533 - val_prec: 0.7538 - val_rec: 0.7411\n",
      "Epoch 14/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.5773 - acc: 0.8858 - prec: 0.8878 - rec: 0.8838 - val_loss: 1.2932 - val_acc: 0.7421 - val_prec: 0.7538 - val_rec: 0.7329\n",
      "Epoch 15/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.6128 - acc: 0.8849 - prec: 0.8875 - rec: 0.8793 - val_loss: 1.2639 - val_acc: 0.7626 - val_prec: 0.7782 - val_rec: 0.7532\n",
      "Epoch 16/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.5639 - acc: 0.8932 - prec: 0.8990 - rec: 0.8876 - val_loss: 1.2651 - val_acc: 0.7589 - val_prec: 0.7654 - val_rec: 0.7550\n",
      "Epoch 17/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.5301 - acc: 0.8987 - prec: 0.9026 - rec: 0.8950 - val_loss: 1.2901 - val_acc: 0.7495 - val_prec: 0.7651 - val_rec: 0.7403\n",
      "Epoch 18/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.4925 - acc: 0.9070 - prec: 0.9103 - rec: 0.9052 - val_loss: 1.3197 - val_acc: 0.7346 - val_prec: 0.7413 - val_rec: 0.7300\n",
      "Epoch 19/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.4730 - acc: 0.9116 - prec: 0.9188 - rec: 0.9068 - val_loss: 1.2736 - val_acc: 0.7551 - val_prec: 0.7638 - val_rec: 0.7550\n",
      "Epoch 20/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.4568 - acc: 0.9125 - prec: 0.9183 - rec: 0.9080 - val_loss: 1.2607 - val_acc: 0.7664 - val_prec: 0.7675 - val_rec: 0.7563\n",
      "Epoch 21/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.4305 - acc: 0.9355 - prec: 0.9380 - rec: 0.9337 - val_loss: 1.2746 - val_acc: 0.7626 - val_prec: 0.7707 - val_rec: 0.7489\n",
      "Epoch 22/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.4610 - acc: 0.9236 - prec: 0.9255 - rec: 0.9190 - val_loss: 1.3833 - val_acc: 0.7383 - val_prec: 0.7525 - val_rec: 0.7366\n",
      "Epoch 23/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.4217 - acc: 0.9282 - prec: 0.9351 - rec: 0.9262 - val_loss: 1.2950 - val_acc: 0.7570 - val_prec: 0.7653 - val_rec: 0.7532\n",
      "Epoch 24/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.4333 - acc: 0.9346 - prec: 0.9368 - rec: 0.9309 - val_loss: 1.3115 - val_acc: 0.7626 - val_prec: 0.7700 - val_rec: 0.7476\n",
      "Epoch 25/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.4095 - acc: 0.9337 - prec: 0.9358 - rec: 0.9291 - val_loss: 1.3282 - val_acc: 0.7682 - val_prec: 0.7739 - val_rec: 0.7623\n",
      "Epoch 26/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.4117 - acc: 0.9411 - prec: 0.9411 - rec: 0.9393 - val_loss: 1.3938 - val_acc: 0.7402 - val_prec: 0.7566 - val_rec: 0.7342\n",
      "Epoch 27/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.3871 - acc: 0.9365 - prec: 0.9382 - rec: 0.9338 - val_loss: 1.3303 - val_acc: 0.7421 - val_prec: 0.7539 - val_rec: 0.7366\n",
      "Epoch 28/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.4111 - acc: 0.9392 - prec: 0.9417 - rec: 0.9375 - val_loss: 1.3453 - val_acc: 0.7383 - val_prec: 0.7418 - val_rec: 0.7237\n",
      "Epoch 29/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.3620 - acc: 0.9484 - prec: 0.9493 - rec: 0.9449 - val_loss: 1.3267 - val_acc: 0.7477 - val_prec: 0.7634 - val_rec: 0.7421\n",
      "Epoch 30/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.3765 - acc: 0.9448 - prec: 0.9455 - rec: 0.9403 - val_loss: 1.2802 - val_acc: 0.7645 - val_prec: 0.7774 - val_rec: 0.7605\n",
      "Epoch 31/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.3578 - acc: 0.9549 - prec: 0.9564 - rec: 0.9521 - val_loss: 1.3383 - val_acc: 0.7458 - val_prec: 0.7601 - val_rec: 0.7403\n",
      "Epoch 32/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.3517 - acc: 0.9475 - prec: 0.9529 - rec: 0.9466 - val_loss: 1.3459 - val_acc: 0.7477 - val_prec: 0.7635 - val_rec: 0.7476\n",
      "Epoch 33/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.3275 - acc: 0.9512 - prec: 0.9536 - rec: 0.9493 - val_loss: 1.2791 - val_acc: 0.7495 - val_prec: 0.7708 - val_rec: 0.7513\n",
      "Epoch 34/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.3408 - acc: 0.9540 - prec: 0.9545 - rec: 0.9483 - val_loss: 1.3541 - val_acc: 0.7477 - val_prec: 0.7540 - val_rec: 0.7440\n",
      "Epoch 35/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.3615 - acc: 0.9448 - prec: 0.9481 - rec: 0.9429 - val_loss: 1.3103 - val_acc: 0.7495 - val_prec: 0.7520 - val_rec: 0.7348\n",
      "Epoch 36/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.3653 - acc: 0.9466 - prec: 0.9481 - rec: 0.9437 - val_loss: 1.2586 - val_acc: 0.7477 - val_prec: 0.7584 - val_rec: 0.7458\n",
      "Epoch 37/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.3788 - acc: 0.9530 - prec: 0.9538 - rec: 0.9512 - val_loss: 1.2474 - val_acc: 0.7682 - val_prec: 0.7754 - val_rec: 0.7631\n",
      "Epoch 38/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.3042 - acc: 0.9586 - prec: 0.9614 - rec: 0.9576 - val_loss: 1.2910 - val_acc: 0.7682 - val_prec: 0.7729 - val_rec: 0.7642\n",
      "Epoch 39/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.3331 - acc: 0.9678 - prec: 0.9688 - rec: 0.9678 - val_loss: 1.2808 - val_acc: 0.7607 - val_prec: 0.7677 - val_rec: 0.7587\n",
      "Epoch 40/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.3239 - acc: 0.9567 - prec: 0.9567 - rec: 0.9558 - val_loss: 1.2643 - val_acc: 0.7738 - val_prec: 0.7822 - val_rec: 0.7637\n",
      "Epoch 41/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.3483 - acc: 0.9586 - prec: 0.9612 - rec: 0.9586 - val_loss: 1.2281 - val_acc: 0.7682 - val_prec: 0.7767 - val_rec: 0.7642\n",
      "Epoch 42/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.3304 - acc: 0.9576 - prec: 0.9601 - rec: 0.9575 - val_loss: 1.2268 - val_acc: 0.7626 - val_prec: 0.7720 - val_rec: 0.7605\n",
      "Epoch 43/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.3139 - acc: 0.9678 - prec: 0.9686 - rec: 0.9642 - val_loss: 1.2307 - val_acc: 0.7794 - val_prec: 0.7864 - val_rec: 0.7642\n",
      "Epoch 44/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.3232 - acc: 0.9632 - prec: 0.9649 - rec: 0.9631 - val_loss: 1.1920 - val_acc: 0.7794 - val_prec: 0.7878 - val_rec: 0.7679\n",
      "Epoch 45/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.3379 - acc: 0.9558 - prec: 0.9581 - rec: 0.9530 - val_loss: 1.3347 - val_acc: 0.7477 - val_prec: 0.7587 - val_rec: 0.7458\n",
      "Epoch 46/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.3154 - acc: 0.9632 - prec: 0.9658 - rec: 0.9623 - val_loss: 1.2632 - val_acc: 0.7738 - val_prec: 0.7806 - val_rec: 0.7728\n",
      "Epoch 47/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2961 - acc: 0.9622 - prec: 0.9632 - rec: 0.9605 - val_loss: 1.1998 - val_acc: 0.7757 - val_prec: 0.7801 - val_rec: 0.7550\n",
      "Epoch 48/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.3016 - acc: 0.9622 - prec: 0.9646 - rec: 0.9603 - val_loss: 1.2857 - val_acc: 0.7626 - val_prec: 0.7790 - val_rec: 0.7458\n",
      "Epoch 49/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.3075 - acc: 0.9715 - prec: 0.9724 - rec: 0.9688 - val_loss: 1.2467 - val_acc: 0.7626 - val_prec: 0.7780 - val_rec: 0.7587\n",
      "Epoch 50/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.3050 - acc: 0.9687 - prec: 0.9697 - rec: 0.9669 - val_loss: 1.2204 - val_acc: 0.7645 - val_prec: 0.7669 - val_rec: 0.7508\n",
      "Epoch 51/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.3197 - acc: 0.9622 - prec: 0.9638 - rec: 0.9603 - val_loss: 1.2874 - val_acc: 0.7570 - val_prec: 0.7677 - val_rec: 0.7550\n",
      "Epoch 52/100\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 0.3038 - acc: 0.9687 - prec: 0.9695 - rec: 0.9677 - val_loss: 1.2042 - val_acc: 0.7701 - val_prec: 0.7828 - val_rec: 0.7600\n",
      "Epoch 53/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.3005 - acc: 0.9650 - prec: 0.9654 - rec: 0.9594 - val_loss: 1.2374 - val_acc: 0.7664 - val_prec: 0.7731 - val_rec: 0.7655\n",
      "Epoch 54/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.2771 - acc: 0.9678 - prec: 0.9685 - rec: 0.9666 - val_loss: 1.2330 - val_acc: 0.7664 - val_prec: 0.7773 - val_rec: 0.7526\n",
      "Epoch 55/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.3031 - acc: 0.9576 - prec: 0.9584 - rec: 0.9558 - val_loss: 1.2489 - val_acc: 0.7794 - val_prec: 0.7792 - val_rec: 0.7637\n",
      "Epoch 56/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.3115 - acc: 0.9595 - prec: 0.9618 - rec: 0.9565 - val_loss: 1.2106 - val_acc: 0.7607 - val_prec: 0.7787 - val_rec: 0.7605\n",
      "Epoch 57/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2849 - acc: 0.9669 - prec: 0.9669 - rec: 0.9642 - val_loss: 1.2334 - val_acc: 0.7607 - val_prec: 0.7803 - val_rec: 0.7526\n",
      "Epoch 58/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2721 - acc: 0.9687 - prec: 0.9712 - rec: 0.9666 - val_loss: 1.1963 - val_acc: 0.7701 - val_prec: 0.7763 - val_rec: 0.7587\n",
      "Epoch 59/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2934 - acc: 0.9632 - prec: 0.9650 - rec: 0.9605 - val_loss: 1.1830 - val_acc: 0.7738 - val_prec: 0.7892 - val_rec: 0.7637\n",
      "Epoch 60/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2541 - acc: 0.9779 - prec: 0.9779 - rec: 0.9779 - val_loss: 1.2047 - val_acc: 0.7720 - val_prec: 0.7811 - val_rec: 0.7568\n",
      "Epoch 61/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2682 - acc: 0.9751 - prec: 0.9752 - rec: 0.9752 - val_loss: 1.1863 - val_acc: 0.7682 - val_prec: 0.7783 - val_rec: 0.7605\n",
      "Epoch 62/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.2862 - acc: 0.9687 - prec: 0.9701 - rec: 0.9674 - val_loss: 1.1923 - val_acc: 0.7757 - val_prec: 0.7799 - val_rec: 0.7618\n",
      "Epoch 63/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2606 - acc: 0.9696 - prec: 0.9702 - rec: 0.9676 - val_loss: 1.1888 - val_acc: 0.7738 - val_prec: 0.7805 - val_rec: 0.7673\n",
      "Epoch 64/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.2504 - acc: 0.9687 - prec: 0.9704 - rec: 0.9686 - val_loss: 1.2301 - val_acc: 0.7551 - val_prec: 0.7584 - val_rec: 0.7403\n",
      "Epoch 65/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2650 - acc: 0.9742 - prec: 0.9743 - rec: 0.9732 - val_loss: 1.2081 - val_acc: 0.7664 - val_prec: 0.7789 - val_rec: 0.7466\n",
      "Epoch 66/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2690 - acc: 0.9696 - prec: 0.9705 - rec: 0.9695 - val_loss: 1.1974 - val_acc: 0.7664 - val_prec: 0.7821 - val_rec: 0.7600\n",
      "Epoch 67/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2699 - acc: 0.9724 - prec: 0.9732 - rec: 0.9706 - val_loss: 1.2258 - val_acc: 0.7813 - val_prec: 0.7985 - val_rec: 0.7728\n",
      "Epoch 68/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2762 - acc: 0.9687 - prec: 0.9693 - rec: 0.9660 - val_loss: 1.1263 - val_acc: 0.7589 - val_prec: 0.7689 - val_rec: 0.7532\n",
      "Epoch 69/100\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.2818 - acc: 0.9650 - prec: 0.9668 - rec: 0.9640 - val_loss: 1.1510 - val_acc: 0.7664 - val_prec: 0.7757 - val_rec: 0.7642\n",
      "Epoch 70/100\n",
      "68/68 [==============================] - 1s 20ms/step - loss: 0.2623 - acc: 0.9696 - prec: 0.9697 - rec: 0.9678 - val_loss: 1.1784 - val_acc: 0.7682 - val_prec: 0.7807 - val_rec: 0.7679\n",
      "Epoch 71/100\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.2820 - acc: 0.9641 - prec: 0.9657 - rec: 0.9631 - val_loss: 1.1552 - val_acc: 0.7682 - val_prec: 0.7800 - val_rec: 0.7702\n",
      "Epoch 72/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2684 - acc: 0.9622 - prec: 0.9631 - rec: 0.9596 - val_loss: 1.1442 - val_acc: 0.7832 - val_prec: 0.7949 - val_rec: 0.7642\n",
      "Epoch 73/100\n",
      "68/68 [==============================] - 1s 20ms/step - loss: 0.3004 - acc: 0.9650 - prec: 0.9648 - rec: 0.9623 - val_loss: 1.2085 - val_acc: 0.7720 - val_prec: 0.7828 - val_rec: 0.7642\n",
      "Epoch 74/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.2904 - acc: 0.9632 - prec: 0.9632 - rec: 0.9623 - val_loss: 1.0852 - val_acc: 0.7776 - val_prec: 0.7950 - val_rec: 0.7770\n",
      "Epoch 75/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.2729 - acc: 0.9678 - prec: 0.9688 - rec: 0.9660 - val_loss: 1.1467 - val_acc: 0.7701 - val_prec: 0.7767 - val_rec: 0.7660\n",
      "Epoch 76/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2832 - acc: 0.9622 - prec: 0.9648 - rec: 0.9623 - val_loss: 1.1347 - val_acc: 0.7757 - val_prec: 0.7872 - val_rec: 0.7697\n",
      "Epoch 77/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2615 - acc: 0.9696 - prec: 0.9704 - rec: 0.9677 - val_loss: 1.1416 - val_acc: 0.7682 - val_prec: 0.7818 - val_rec: 0.7642\n",
      "Epoch 78/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.2408 - acc: 0.9751 - prec: 0.9769 - rec: 0.9733 - val_loss: 1.1672 - val_acc: 0.7720 - val_prec: 0.7810 - val_rec: 0.7642\n",
      "Epoch 79/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.2372 - acc: 0.9742 - prec: 0.9759 - rec: 0.9724 - val_loss: 1.1444 - val_acc: 0.7832 - val_prec: 0.7942 - val_rec: 0.7802\n",
      "Epoch 80/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.2499 - acc: 0.9733 - prec: 0.9739 - rec: 0.9714 - val_loss: 1.2155 - val_acc: 0.7682 - val_prec: 0.7755 - val_rec: 0.7581\n",
      "Epoch 81/100\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 0.2419 - acc: 0.9779 - prec: 0.9779 - rec: 0.9761 - val_loss: 1.1728 - val_acc: 0.7738 - val_prec: 0.7868 - val_rec: 0.7605\n",
      "Epoch 82/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2358 - acc: 0.9705 - prec: 0.9722 - rec: 0.9686 - val_loss: 1.1866 - val_acc: 0.7589 - val_prec: 0.7618 - val_rec: 0.7526\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 1s 15ms/step - loss: 0.2547 - acc: 0.9696 - prec: 0.9696 - rec: 0.9688 - val_loss: 1.2487 - val_acc: 0.7570 - val_prec: 0.7683 - val_rec: 0.7471\n",
      "Epoch 84/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2663 - acc: 0.9715 - prec: 0.9715 - rec: 0.9706 - val_loss: 1.2042 - val_acc: 0.7570 - val_prec: 0.7764 - val_rec: 0.7495\n",
      "Epoch 85/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.2645 - acc: 0.9696 - prec: 0.9713 - rec: 0.9678 - val_loss: 1.1188 - val_acc: 0.7701 - val_prec: 0.7819 - val_rec: 0.7660\n",
      "Epoch 86/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2543 - acc: 0.9586 - prec: 0.9586 - rec: 0.9577 - val_loss: 1.1514 - val_acc: 0.7757 - val_prec: 0.7983 - val_rec: 0.7776\n",
      "Epoch 87/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2607 - acc: 0.9641 - prec: 0.9640 - rec: 0.9632 - val_loss: 1.1445 - val_acc: 0.7757 - val_prec: 0.7936 - val_rec: 0.7721\n",
      "Epoch 88/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.2559 - acc: 0.9586 - prec: 0.9593 - rec: 0.9577 - val_loss: 1.1778 - val_acc: 0.7720 - val_prec: 0.7841 - val_rec: 0.7637\n",
      "Epoch 89/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.2509 - acc: 0.9687 - prec: 0.9685 - rec: 0.9685 - val_loss: 1.2608 - val_acc: 0.7607 - val_prec: 0.7685 - val_rec: 0.7500\n",
      "Epoch 90/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2351 - acc: 0.9715 - prec: 0.9724 - rec: 0.9715 - val_loss: 1.1796 - val_acc: 0.7794 - val_prec: 0.7894 - val_rec: 0.7757\n",
      "Epoch 91/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.2506 - acc: 0.9733 - prec: 0.9760 - rec: 0.9724 - val_loss: 1.2204 - val_acc: 0.7738 - val_prec: 0.7741 - val_rec: 0.7673\n",
      "Epoch 92/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2220 - acc: 0.9742 - prec: 0.9752 - rec: 0.9743 - val_loss: 1.2786 - val_acc: 0.7776 - val_prec: 0.7839 - val_rec: 0.7684\n",
      "Epoch 93/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2421 - acc: 0.9678 - prec: 0.9677 - rec: 0.9677 - val_loss: 1.1633 - val_acc: 0.7832 - val_prec: 0.7937 - val_rec: 0.7752\n",
      "Epoch 94/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.2336 - acc: 0.9733 - prec: 0.9743 - rec: 0.9733 - val_loss: 1.1708 - val_acc: 0.7701 - val_prec: 0.7735 - val_rec: 0.7642\n",
      "Epoch 95/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.2299 - acc: 0.9678 - prec: 0.9687 - rec: 0.9669 - val_loss: 1.1628 - val_acc: 0.7776 - val_prec: 0.7883 - val_rec: 0.7697\n",
      "Epoch 96/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2022 - acc: 0.9715 - prec: 0.9724 - rec: 0.9715 - val_loss: 1.1884 - val_acc: 0.7589 - val_prec: 0.7760 - val_rec: 0.7550\n",
      "Epoch 97/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2222 - acc: 0.9715 - prec: 0.9713 - rec: 0.9695 - val_loss: 1.2025 - val_acc: 0.7551 - val_prec: 0.7575 - val_rec: 0.7489\n",
      "Epoch 98/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2413 - acc: 0.9733 - prec: 0.9733 - rec: 0.9724 - val_loss: 1.2581 - val_acc: 0.7738 - val_prec: 0.7836 - val_rec: 0.7715\n",
      "Epoch 99/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.2312 - acc: 0.9696 - prec: 0.9704 - rec: 0.9686 - val_loss: 1.2874 - val_acc: 0.7495 - val_prec: 0.7619 - val_rec: 0.7476\n",
      "Epoch 100/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2434 - acc: 0.9733 - prec: 0.9743 - rec: 0.9733 - val_loss: 1.2317 - val_acc: 0.7607 - val_prec: 0.7718 - val_rec: 0.7513\n",
      "Time taken : 1.96 mins\n"
     ]
    }
   ],
   "source": [
    "epoh = 100\n",
    "log = CSVLogger('cnnadam.log', separator=',', append=False)\n",
    "start_time = time.time()\n",
    "historyadam = modeladam.fit(X_train, Y_train, epochs=epoh, batch_size=16,   validation_split = 0.33, callbacks=[log]) \n",
    "print(f'Time taken : {(time.time() - start_time) / 60:.2f} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "68/68 [==============================] - 2s 13ms/step - loss: 11.6587 - acc: 0.0838 - prec: 0.0823 - rec: 0.0813 - val_loss: 10.9687 - val_acc: 0.0150 - val_prec: 0.0147 - val_rec: 0.0147\n",
      "Epoch 2/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 11.6797 - acc: 0.0709 - prec: 0.0713 - rec: 0.0709 - val_loss: 10.6829 - val_acc: 0.0168 - val_prec: 0.0167 - val_rec: 0.0165\n",
      "Epoch 3/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 11.2848 - acc: 0.0838 - prec: 0.0813 - rec: 0.0800 - val_loss: 10.4125 - val_acc: 0.0224 - val_prec: 0.0221 - val_rec: 0.0221\n",
      "Epoch 4/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 11.5900 - acc: 0.0829 - prec: 0.0829 - rec: 0.0818 - val_loss: 10.1380 - val_acc: 0.0224 - val_prec: 0.0221 - val_rec: 0.0221\n",
      "Epoch 5/100\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 11.0112 - acc: 0.0939 - prec: 0.0924 - rec: 0.0909 - val_loss: 9.8715 - val_acc: 0.0262 - val_prec: 0.0257 - val_rec: 0.0257\n",
      "Epoch 6/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 10.7984 - acc: 0.1013 - prec: 0.0986 - rec: 0.0976 - val_loss: 9.6090 - val_acc: 0.0299 - val_prec: 0.0276 - val_rec: 0.0276\n",
      "Epoch 7/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 10.2783 - acc: 0.1013 - prec: 0.1015 - rec: 0.1003 - val_loss: 9.3548 - val_acc: 0.0336 - val_prec: 0.0314 - val_rec: 0.0312\n",
      "Epoch 8/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 10.4170 - acc: 0.1096 - prec: 0.1109 - rec: 0.1086 - val_loss: 9.1055 - val_acc: 0.0393 - val_prec: 0.0350 - val_rec: 0.0349\n",
      "Epoch 9/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 9.8199 - acc: 0.1133 - prec: 0.1137 - rec: 0.1112 - val_loss: 8.8607 - val_acc: 0.0411 - val_prec: 0.0370 - val_rec: 0.0368\n",
      "Epoch 10/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 9.9648 - acc: 0.1215 - prec: 0.1187 - rec: 0.1179 - val_loss: 8.6156 - val_acc: 0.0486 - val_prec: 0.0478 - val_rec: 0.0478\n",
      "Epoch 11/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 9.7666 - acc: 0.1151 - prec: 0.1133 - rec: 0.1104 - val_loss: 8.3779 - val_acc: 0.0579 - val_prec: 0.0570 - val_rec: 0.0570\n",
      "Epoch 12/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 9.3972 - acc: 0.1243 - prec: 0.1237 - rec: 0.1205 - val_loss: 8.1505 - val_acc: 0.0579 - val_prec: 0.0571 - val_rec: 0.0570\n",
      "Epoch 13/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 9.2588 - acc: 0.1418 - prec: 0.1452 - rec: 0.1419 - val_loss: 7.9303 - val_acc: 0.0617 - val_prec: 0.0610 - val_rec: 0.0607\n",
      "Epoch 14/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 9.0647 - acc: 0.1409 - prec: 0.1426 - rec: 0.1389 - val_loss: 7.7093 - val_acc: 0.0692 - val_prec: 0.0692 - val_rec: 0.0680\n",
      "Epoch 15/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 8.8806 - acc: 0.1436 - prec: 0.1429 - rec: 0.1400 - val_loss: 7.4920 - val_acc: 0.0822 - val_prec: 0.0819 - val_rec: 0.0809\n",
      "Epoch 16/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 8.9359 - acc: 0.1501 - prec: 0.1500 - rec: 0.1465 - val_loss: 7.2813 - val_acc: 0.0879 - val_prec: 0.0855 - val_rec: 0.0846\n",
      "Epoch 17/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 8.7573 - acc: 0.1565 - prec: 0.1579 - rec: 0.1539 - val_loss: 7.0829 - val_acc: 0.0897 - val_prec: 0.0875 - val_rec: 0.0864\n",
      "Epoch 18/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 8.8050 - acc: 0.1473 - prec: 0.1477 - rec: 0.1439 - val_loss: 6.8823 - val_acc: 0.0935 - val_prec: 0.0907 - val_rec: 0.0901\n",
      "Epoch 19/100\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 8.4254 - acc: 0.1565 - prec: 0.1574 - rec: 0.1535 - val_loss: 6.6887 - val_acc: 0.0991 - val_prec: 0.0963 - val_rec: 0.0956\n",
      "Epoch 20/100\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 8.0688 - acc: 0.1602 - prec: 0.1589 - rec: 0.1564 - val_loss: 6.5016 - val_acc: 0.1047 - val_prec: 0.1004 - val_rec: 0.0993\n",
      "Epoch 21/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 7.9418 - acc: 0.1575 - prec: 0.1553 - rec: 0.1517 - val_loss: 6.3199 - val_acc: 0.1140 - val_prec: 0.1102 - val_rec: 0.1085\n",
      "Epoch 22/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 7.9349 - acc: 0.1630 - prec: 0.1603 - rec: 0.1573 - val_loss: 6.1311 - val_acc: 0.1196 - val_prec: 0.1176 - val_rec: 0.1158\n",
      "Epoch 23/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 7.7641 - acc: 0.1722 - prec: 0.1723 - rec: 0.1650 - val_loss: 5.9563 - val_acc: 0.1346 - val_prec: 0.1269 - val_rec: 0.1232\n",
      "Epoch 24/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 7.5955 - acc: 0.1740 - prec: 0.1717 - rec: 0.1657 - val_loss: 5.7869 - val_acc: 0.1402 - val_prec: 0.1334 - val_rec: 0.1305\n",
      "Epoch 25/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 7.2240 - acc: 0.2099 - prec: 0.2112 - rec: 0.2038 - val_loss: 5.6252 - val_acc: 0.1495 - val_prec: 0.1431 - val_rec: 0.1397\n",
      "Epoch 26/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 7.3465 - acc: 0.1952 - prec: 0.1944 - rec: 0.1878 - val_loss: 5.4649 - val_acc: 0.1495 - val_prec: 0.1454 - val_rec: 0.1397\n",
      "Epoch 27/100\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 7.0346 - acc: 0.1998 - prec: 0.1968 - rec: 0.1897 - val_loss: 5.3107 - val_acc: 0.1664 - val_prec: 0.1534 - val_rec: 0.1489\n",
      "Epoch 28/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 7.2157 - acc: 0.1897 - prec: 0.1898 - rec: 0.1824 - val_loss: 5.1571 - val_acc: 0.1701 - val_prec: 0.1617 - val_rec: 0.1562\n",
      "Epoch 29/100\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 7.0714 - acc: 0.1897 - prec: 0.1933 - rec: 0.1849 - val_loss: 5.0150 - val_acc: 0.1701 - val_prec: 0.1718 - val_rec: 0.1636\n",
      "Epoch 30/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 6.8135 - acc: 0.1934 - prec: 0.1931 - rec: 0.1867 - val_loss: 4.8704 - val_acc: 0.1776 - val_prec: 0.1829 - val_rec: 0.1728\n",
      "Epoch 31/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 6.7173 - acc: 0.2099 - prec: 0.2080 - rec: 0.1997 - val_loss: 4.7355 - val_acc: 0.1850 - val_prec: 0.1874 - val_rec: 0.1765\n",
      "Epoch 32/100\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 6.4722 - acc: 0.2348 - prec: 0.2341 - rec: 0.2241 - val_loss: 4.6058 - val_acc: 0.1888 - val_prec: 0.1937 - val_rec: 0.1838\n",
      "Epoch 33/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 6.2583 - acc: 0.2145 - prec: 0.2179 - rec: 0.2088 - val_loss: 4.4794 - val_acc: 0.1907 - val_prec: 0.1976 - val_rec: 0.1838\n",
      "Epoch 34/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 6.5862 - acc: 0.2164 - prec: 0.2165 - rec: 0.2082 - val_loss: 4.3620 - val_acc: 0.1963 - val_prec: 0.2043 - val_rec: 0.1893\n",
      "Epoch 35/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 6.1381 - acc: 0.2422 - prec: 0.2403 - rec: 0.2298 - val_loss: 4.2468 - val_acc: 0.2075 - val_prec: 0.2083 - val_rec: 0.1930\n",
      "Epoch 36/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 6.1012 - acc: 0.2155 - prec: 0.2196 - rec: 0.2085 - val_loss: 4.1299 - val_acc: 0.2187 - val_prec: 0.2178 - val_rec: 0.2004\n",
      "Epoch 37/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 6.1016 - acc: 0.2551 - prec: 0.2503 - rec: 0.2383 - val_loss: 4.0179 - val_acc: 0.2280 - val_prec: 0.2306 - val_rec: 0.2096\n",
      "Epoch 38/100\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 6.2876 - acc: 0.2192 - prec: 0.2179 - rec: 0.2077 - val_loss: 3.9114 - val_acc: 0.2336 - val_prec: 0.2364 - val_rec: 0.2151\n",
      "Epoch 39/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 5.9910 - acc: 0.2394 - prec: 0.2411 - rec: 0.2282 - val_loss: 3.8059 - val_acc: 0.2374 - val_prec: 0.2418 - val_rec: 0.2188\n",
      "Epoch 40/100\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 5.9611 - acc: 0.2136 - prec: 0.2153 - rec: 0.2072 - val_loss: 3.7077 - val_acc: 0.2505 - val_prec: 0.2461 - val_rec: 0.2243\n",
      "Epoch 41/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 5.8257 - acc: 0.2689 - prec: 0.2746 - rec: 0.2622 - val_loss: 3.6129 - val_acc: 0.2542 - val_prec: 0.2547 - val_rec: 0.2279\n",
      "Epoch 42/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 5.7815 - acc: 0.2505 - prec: 0.2518 - rec: 0.2415 - val_loss: 3.5256 - val_acc: 0.2617 - val_prec: 0.2614 - val_rec: 0.2335\n",
      "Epoch 43/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 5.6810 - acc: 0.2366 - prec: 0.2438 - rec: 0.2302 - val_loss: 3.4387 - val_acc: 0.2636 - val_prec: 0.2662 - val_rec: 0.2353\n",
      "Epoch 44/100\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 5.5365 - acc: 0.2606 - prec: 0.2598 - rec: 0.2521 - val_loss: 3.3569 - val_acc: 0.2692 - val_prec: 0.2716 - val_rec: 0.2371\n",
      "Epoch 45/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 5.5669 - acc: 0.2505 - prec: 0.2504 - rec: 0.2395 - val_loss: 3.2764 - val_acc: 0.2729 - val_prec: 0.2752 - val_rec: 0.2408\n",
      "Epoch 46/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 5.3698 - acc: 0.2615 - prec: 0.2630 - rec: 0.2499 - val_loss: 3.1996 - val_acc: 0.2841 - val_prec: 0.2785 - val_rec: 0.2426\n",
      "Epoch 47/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 5.2908 - acc: 0.2698 - prec: 0.2762 - rec: 0.2643 - val_loss: 3.1273 - val_acc: 0.2822 - val_prec: 0.2836 - val_rec: 0.2482\n",
      "Epoch 48/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 5.3284 - acc: 0.2551 - prec: 0.2603 - rec: 0.2467 - val_loss: 3.0556 - val_acc: 0.2841 - val_prec: 0.2840 - val_rec: 0.2500\n",
      "Epoch 49/100\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 5.1928 - acc: 0.2845 - prec: 0.2890 - rec: 0.2753 - val_loss: 2.9894 - val_acc: 0.2822 - val_prec: 0.2872 - val_rec: 0.2500\n",
      "Epoch 50/100\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 5.1868 - acc: 0.2726 - prec: 0.2729 - rec: 0.2634 - val_loss: 2.9230 - val_acc: 0.2916 - val_prec: 0.2967 - val_rec: 0.2574\n",
      "Epoch 51/100\n",
      "68/68 [==============================] - 1s 8ms/step - loss: 4.9974 - acc: 0.2947 - prec: 0.2933 - rec: 0.2790 - val_loss: 2.8684 - val_acc: 0.3009 - val_prec: 0.3040 - val_rec: 0.2647\n",
      "Epoch 52/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 4.9857 - acc: 0.2799 - prec: 0.2803 - rec: 0.2655 - val_loss: 2.8153 - val_acc: 0.3065 - val_prec: 0.3124 - val_rec: 0.2707\n",
      "Epoch 53/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 4.8128 - acc: 0.2947 - prec: 0.3006 - rec: 0.2855 - val_loss: 2.7643 - val_acc: 0.3178 - val_prec: 0.3214 - val_rec: 0.2799\n",
      "Epoch 54/100\n",
      "68/68 [==============================] - 2s 30ms/step - loss: 4.9326 - acc: 0.2781 - prec: 0.2839 - rec: 0.2721 - val_loss: 2.7141 - val_acc: 0.3215 - val_prec: 0.3291 - val_rec: 0.2910\n",
      "Epoch 55/100\n",
      "68/68 [==============================] - 1s 9ms/step - loss: 4.9413 - acc: 0.2983 - prec: 0.2967 - rec: 0.2818 - val_loss: 2.6723 - val_acc: 0.3290 - val_prec: 0.3293 - val_rec: 0.2928\n",
      "Epoch 56/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 4.8498 - acc: 0.2993 - prec: 0.3025 - rec: 0.2899 - val_loss: 2.6285 - val_acc: 0.3346 - val_prec: 0.3240 - val_rec: 0.2855\n",
      "Epoch 57/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 4.8690 - acc: 0.2772 - prec: 0.2801 - rec: 0.2680 - val_loss: 2.5870 - val_acc: 0.3364 - val_prec: 0.3283 - val_rec: 0.2910\n",
      "Epoch 58/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 4.9058 - acc: 0.2827 - prec: 0.2855 - rec: 0.2705 - val_loss: 2.5473 - val_acc: 0.3346 - val_prec: 0.3412 - val_rec: 0.2965\n",
      "Epoch 59/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 4.5970 - acc: 0.3085 - prec: 0.3056 - rec: 0.2908 - val_loss: 2.5115 - val_acc: 0.3346 - val_prec: 0.3452 - val_rec: 0.3038\n",
      "Epoch 60/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 4.8126 - acc: 0.3020 - prec: 0.3048 - rec: 0.2953 - val_loss: 2.4779 - val_acc: 0.3402 - val_prec: 0.3534 - val_rec: 0.3075\n",
      "Epoch 61/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 4.6739 - acc: 0.3057 - prec: 0.3036 - rec: 0.2918 - val_loss: 2.4471 - val_acc: 0.3495 - val_prec: 0.3517 - val_rec: 0.3075\n",
      "Epoch 62/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 4.7475 - acc: 0.3011 - prec: 0.3010 - rec: 0.2845 - val_loss: 2.4209 - val_acc: 0.3533 - val_prec: 0.3560 - val_rec: 0.3112\n",
      "Epoch 63/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 4.6291 - acc: 0.3039 - prec: 0.3078 - rec: 0.2962 - val_loss: 2.3963 - val_acc: 0.3551 - val_prec: 0.3580 - val_rec: 0.3167\n",
      "Epoch 64/100\n",
      "68/68 [==============================] - 2s 24ms/step - loss: 4.5876 - acc: 0.3011 - prec: 0.3033 - rec: 0.2872 - val_loss: 2.3717 - val_acc: 0.3570 - val_prec: 0.3630 - val_rec: 0.3167\n",
      "Epoch 65/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 4.6939 - acc: 0.3343 - prec: 0.3371 - rec: 0.3213 - val_loss: 2.3485 - val_acc: 0.3645 - val_prec: 0.3704 - val_rec: 0.3204\n",
      "Epoch 66/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 4.4425 - acc: 0.3140 - prec: 0.3186 - rec: 0.3000 - val_loss: 2.3284 - val_acc: 0.3720 - val_prec: 0.3740 - val_rec: 0.3241\n",
      "Epoch 67/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 4.9118 - acc: 0.2643 - prec: 0.2681 - rec: 0.2553 - val_loss: 2.3102 - val_acc: 0.3813 - val_prec: 0.3762 - val_rec: 0.3319\n",
      "Epoch 68/100\n",
      "68/68 [==============================] - 1s 22ms/step - loss: 4.3735 - acc: 0.3140 - prec: 0.3155 - rec: 0.3012 - val_loss: 2.2925 - val_acc: 0.3869 - val_prec: 0.3849 - val_rec: 0.3411\n",
      "Epoch 69/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 4.4903 - acc: 0.3112 - prec: 0.3111 - rec: 0.2971 - val_loss: 2.2759 - val_acc: 0.3869 - val_prec: 0.3856 - val_rec: 0.3411\n",
      "Epoch 70/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 4.1843 - acc: 0.3250 - prec: 0.3248 - rec: 0.3121 - val_loss: 2.2579 - val_acc: 0.3813 - val_prec: 0.3842 - val_rec: 0.3448\n",
      "Epoch 71/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 4.3177 - acc: 0.3389 - prec: 0.3471 - rec: 0.3314 - val_loss: 2.2446 - val_acc: 0.3832 - val_prec: 0.3912 - val_rec: 0.3522\n",
      "Epoch 72/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 4.2940 - acc: 0.3232 - prec: 0.3259 - rec: 0.3092 - val_loss: 2.2326 - val_acc: 0.3813 - val_prec: 0.3940 - val_rec: 0.3540\n",
      "Epoch 73/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 4.4621 - acc: 0.3214 - prec: 0.3236 - rec: 0.3101 - val_loss: 2.2193 - val_acc: 0.3832 - val_prec: 0.3994 - val_rec: 0.3577\n",
      "Epoch 74/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 4.4585 - acc: 0.3168 - prec: 0.3187 - rec: 0.3032 - val_loss: 2.2090 - val_acc: 0.3832 - val_prec: 0.4014 - val_rec: 0.3558\n",
      "Epoch 75/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 4.4146 - acc: 0.3103 - prec: 0.3121 - rec: 0.3003 - val_loss: 2.1980 - val_acc: 0.3907 - val_prec: 0.4058 - val_rec: 0.3669\n",
      "Epoch 76/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 4.5250 - acc: 0.3204 - prec: 0.3231 - rec: 0.3078 - val_loss: 2.1896 - val_acc: 0.3925 - val_prec: 0.4065 - val_rec: 0.3669\n",
      "Epoch 77/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 4.3611 - acc: 0.3361 - prec: 0.3372 - rec: 0.3216 - val_loss: 2.1817 - val_acc: 0.3981 - val_prec: 0.4056 - val_rec: 0.3650\n",
      "Epoch 78/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 4.3209 - acc: 0.3306 - prec: 0.3346 - rec: 0.3227 - val_loss: 2.1740 - val_acc: 0.4037 - val_prec: 0.4086 - val_rec: 0.3669\n",
      "Epoch 79/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 4.2679 - acc: 0.3379 - prec: 0.3452 - rec: 0.3269 - val_loss: 2.1666 - val_acc: 0.4019 - val_prec: 0.4075 - val_rec: 0.3687\n",
      "Epoch 80/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 4.2365 - acc: 0.3177 - prec: 0.3219 - rec: 0.3083 - val_loss: 2.1601 - val_acc: 0.4000 - val_prec: 0.4085 - val_rec: 0.3705\n",
      "Epoch 81/100\n",
      "68/68 [==============================] - 8s 122ms/step - loss: 4.1973 - acc: 0.3435 - prec: 0.3469 - rec: 0.3318 - val_loss: 2.1544 - val_acc: 0.3963 - val_prec: 0.4063 - val_rec: 0.3687\n",
      "Epoch 82/100\n",
      "68/68 [==============================] - 1s 19ms/step - loss: 4.2600 - acc: 0.3444 - prec: 0.3476 - rec: 0.3318 - val_loss: 2.1494 - val_acc: 0.3981 - val_prec: 0.4059 - val_rec: 0.3687\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 2s 24ms/step - loss: 4.3100 - acc: 0.3471 - prec: 0.3523 - rec: 0.3339 - val_loss: 2.1439 - val_acc: 0.4093 - val_prec: 0.4065 - val_rec: 0.3705\n",
      "Epoch 84/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 4.2678 - acc: 0.3269 - prec: 0.3289 - rec: 0.3174 - val_loss: 2.1393 - val_acc: 0.4037 - val_prec: 0.4062 - val_rec: 0.3724\n",
      "Epoch 85/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 4.0444 - acc: 0.3527 - prec: 0.3541 - rec: 0.3427 - val_loss: 2.1358 - val_acc: 0.4093 - val_prec: 0.4048 - val_rec: 0.3724\n",
      "Epoch 86/100\n",
      "68/68 [==============================] - 1s 21ms/step - loss: 3.9563 - acc: 0.3444 - prec: 0.3503 - rec: 0.3350 - val_loss: 2.1317 - val_acc: 0.4112 - val_prec: 0.4088 - val_rec: 0.3742\n",
      "Epoch 87/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 4.2219 - acc: 0.3223 - prec: 0.3249 - rec: 0.3076 - val_loss: 2.1278 - val_acc: 0.4131 - val_prec: 0.4079 - val_rec: 0.3724\n",
      "Epoch 88/100\n",
      "68/68 [==============================] - 2s 28ms/step - loss: 4.2685 - acc: 0.3536 - prec: 0.3620 - rec: 0.3451 - val_loss: 2.1242 - val_acc: 0.4168 - val_prec: 0.4061 - val_rec: 0.3724\n",
      "Epoch 89/100\n",
      "68/68 [==============================] - 2s 23ms/step - loss: 3.9576 - acc: 0.3462 - prec: 0.3518 - rec: 0.3363 - val_loss: 2.1206 - val_acc: 0.4168 - val_prec: 0.4092 - val_rec: 0.3761\n",
      "Epoch 90/100\n",
      "68/68 [==============================] - 2s 26ms/step - loss: 4.0675 - acc: 0.3370 - prec: 0.3416 - rec: 0.3280 - val_loss: 2.1184 - val_acc: 0.4131 - val_prec: 0.4133 - val_rec: 0.3761\n",
      "Epoch 91/100\n",
      "68/68 [==============================] - 2s 22ms/step - loss: 4.1475 - acc: 0.3517 - prec: 0.3531 - rec: 0.3395 - val_loss: 2.1158 - val_acc: 0.4150 - val_prec: 0.4154 - val_rec: 0.3816\n",
      "Epoch 92/100\n",
      "68/68 [==============================] - 1s 20ms/step - loss: 4.1772 - acc: 0.3453 - prec: 0.3472 - rec: 0.3322 - val_loss: 2.1131 - val_acc: 0.4131 - val_prec: 0.4171 - val_rec: 0.3816\n",
      "Epoch 93/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 4.0287 - acc: 0.3656 - prec: 0.3655 - rec: 0.3518 - val_loss: 2.1113 - val_acc: 0.4131 - val_prec: 0.4240 - val_rec: 0.3834\n",
      "Epoch 94/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 4.0772 - acc: 0.3444 - prec: 0.3495 - rec: 0.3382 - val_loss: 2.1090 - val_acc: 0.4075 - val_prec: 0.4212 - val_rec: 0.3797\n",
      "Epoch 95/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 4.0240 - acc: 0.3471 - prec: 0.3547 - rec: 0.3388 - val_loss: 2.1059 - val_acc: 0.4131 - val_prec: 0.4236 - val_rec: 0.3834\n",
      "Epoch 96/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 4.0036 - acc: 0.3462 - prec: 0.3457 - rec: 0.3300 - val_loss: 2.1032 - val_acc: 0.4112 - val_prec: 0.4258 - val_rec: 0.3852\n",
      "Epoch 97/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 3.8935 - acc: 0.3720 - prec: 0.3785 - rec: 0.3613 - val_loss: 2.1019 - val_acc: 0.4150 - val_prec: 0.4260 - val_rec: 0.3871\n",
      "Epoch 98/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 4.0859 - acc: 0.3527 - prec: 0.3524 - rec: 0.3378 - val_loss: 2.1003 - val_acc: 0.4150 - val_prec: 0.4291 - val_rec: 0.3889\n",
      "Epoch 99/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 4.0901 - acc: 0.3610 - prec: 0.3571 - rec: 0.3460 - val_loss: 2.0988 - val_acc: 0.4150 - val_prec: 0.4295 - val_rec: 0.3908\n",
      "Epoch 100/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 4.0535 - acc: 0.3508 - prec: 0.3572 - rec: 0.3403 - val_loss: 2.0976 - val_acc: 0.4187 - val_prec: 0.4316 - val_rec: 0.3908\n",
      "Time taken : 1.56 mins\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "logadadelta = CSVLogger('cnnadadelta.log', separator=',', append=False)\n",
    "historyadadelta = modeladadelta.fit(X_train, Y_train, epochs=epoh, batch_size=16,  validation_split = 0.33,callbacks=[logadadelta]) \n",
    "print(f'Time taken : {(time.time() - start_time) / 60:.2f} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "68/68 [==============================] - 8s 29ms/step - loss: 9.8137 - acc: 0.3600 - prec: 0.3608 - rec: 0.3598 - val_loss: 8.4395 - val_acc: 0.3832 - val_prec: 0.3887 - val_rec: 0.3887\n",
      "Epoch 2/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 6.4357 - acc: 0.4291 - prec: 0.4321 - rec: 0.4277 - val_loss: 2.6965 - val_acc: 0.5215 - val_prec: 0.5190 - val_rec: 0.5158\n",
      "Epoch 3/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 3.3770 - acc: 0.5396 - prec: 0.5447 - rec: 0.5345 - val_loss: 3.2577 - val_acc: 0.4505 - val_prec: 0.4481 - val_rec: 0.4459\n",
      "Epoch 4/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 2.9241 - acc: 0.5359 - prec: 0.5373 - rec: 0.5272 - val_loss: 3.9336 - val_acc: 0.4729 - val_prec: 0.4760 - val_rec: 0.4751\n",
      "Epoch 5/100\n",
      "68/68 [==============================] - 2s 24ms/step - loss: 2.0716 - acc: 0.6160 - prec: 0.6174 - rec: 0.6113 - val_loss: 1.4521 - val_acc: 0.6523 - val_prec: 0.6610 - val_rec: 0.6358\n",
      "Epoch 6/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 1.7348 - acc: 0.6667 - prec: 0.6713 - rec: 0.6557 - val_loss: 1.8207 - val_acc: 0.6000 - val_prec: 0.6076 - val_rec: 0.5945\n",
      "Epoch 7/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 1.2628 - acc: 0.7063 - prec: 0.7150 - rec: 0.6976 - val_loss: 1.9517 - val_acc: 0.5944 - val_prec: 0.5956 - val_rec: 0.5801\n",
      "Epoch 8/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 1.2886 - acc: 0.7376 - prec: 0.7396 - rec: 0.7198 - val_loss: 2.0001 - val_acc: 0.5682 - val_prec: 0.5720 - val_rec: 0.5525\n",
      "Epoch 9/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 1.1505 - acc: 0.7366 - prec: 0.7473 - rec: 0.7294 - val_loss: 1.2094 - val_acc: 0.7215 - val_prec: 0.7408 - val_rec: 0.7054\n",
      "Epoch 10/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 1.0210 - acc: 0.7707 - prec: 0.7762 - rec: 0.7579 - val_loss: 1.4130 - val_acc: 0.6748 - val_prec: 0.7048 - val_rec: 0.6783\n",
      "Epoch 11/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.7866 - acc: 0.8131 - prec: 0.8192 - rec: 0.8046 - val_loss: 1.1772 - val_acc: 0.7402 - val_prec: 0.7590 - val_rec: 0.7329\n",
      "Epoch 12/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.7529 - acc: 0.8278 - prec: 0.8367 - rec: 0.8178 - val_loss: 1.4370 - val_acc: 0.7196 - val_prec: 0.7286 - val_rec: 0.7122\n",
      "Epoch 13/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.7098 - acc: 0.8269 - prec: 0.8349 - rec: 0.8196 - val_loss: 1.5015 - val_acc: 0.6953 - val_prec: 0.7023 - val_rec: 0.6893\n",
      "Epoch 14/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.6117 - acc: 0.8527 - prec: 0.8572 - rec: 0.8427 - val_loss: 1.2656 - val_acc: 0.7364 - val_prec: 0.7403 - val_rec: 0.7214\n",
      "Epoch 15/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.6898 - acc: 0.8297 - prec: 0.8372 - rec: 0.8168 - val_loss: 1.2615 - val_acc: 0.7514 - val_prec: 0.7613 - val_rec: 0.7445\n",
      "Epoch 16/100\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5894 - acc: 0.8591 - prec: 0.8713 - rec: 0.8498 - val_loss: 1.2197 - val_acc: 0.7421 - val_prec: 0.7610 - val_rec: 0.7366\n",
      "Epoch 17/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.5177 - acc: 0.8858 - prec: 0.8930 - rec: 0.8767 - val_loss: 1.2032 - val_acc: 0.7551 - val_prec: 0.7711 - val_rec: 0.7500\n",
      "Epoch 18/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.5778 - acc: 0.8812 - prec: 0.8919 - rec: 0.8758 - val_loss: 1.3415 - val_acc: 0.7607 - val_prec: 0.7723 - val_rec: 0.7537\n",
      "Epoch 19/100\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.5570 - acc: 0.8803 - prec: 0.8857 - rec: 0.8768 - val_loss: 1.2581 - val_acc: 0.7514 - val_prec: 0.7691 - val_rec: 0.7476\n",
      "Epoch 20/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.5404 - acc: 0.8766 - prec: 0.8832 - rec: 0.8712 - val_loss: 1.2496 - val_acc: 0.7402 - val_prec: 0.7600 - val_rec: 0.7311\n",
      "Epoch 21/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.4620 - acc: 0.9052 - prec: 0.9115 - rec: 0.9026 - val_loss: 1.4060 - val_acc: 0.7383 - val_prec: 0.7478 - val_rec: 0.7269\n",
      "Epoch 22/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.4583 - acc: 0.9134 - prec: 0.9174 - rec: 0.9072 - val_loss: 1.2405 - val_acc: 0.7514 - val_prec: 0.7630 - val_rec: 0.7513\n",
      "Epoch 23/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.4348 - acc: 0.9208 - prec: 0.9228 - rec: 0.9136 - val_loss: 1.2148 - val_acc: 0.7645 - val_prec: 0.7702 - val_rec: 0.7508\n",
      "Epoch 24/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.4455 - acc: 0.9217 - prec: 0.9256 - rec: 0.9179 - val_loss: 1.2400 - val_acc: 0.7607 - val_prec: 0.7628 - val_rec: 0.7458\n",
      "Epoch 25/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.4140 - acc: 0.9153 - prec: 0.9199 - rec: 0.9145 - val_loss: 1.2664 - val_acc: 0.7533 - val_prec: 0.7609 - val_rec: 0.7476\n",
      "Epoch 26/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.4187 - acc: 0.9208 - prec: 0.9276 - rec: 0.9179 - val_loss: 1.1996 - val_acc: 0.7533 - val_prec: 0.7622 - val_rec: 0.7500\n",
      "Epoch 27/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.4558 - acc: 0.9088 - prec: 0.9126 - rec: 0.9049 - val_loss: 1.2276 - val_acc: 0.7533 - val_prec: 0.7706 - val_rec: 0.7476\n",
      "Epoch 28/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.4173 - acc: 0.9245 - prec: 0.9276 - rec: 0.9210 - val_loss: 1.2069 - val_acc: 0.7589 - val_prec: 0.7696 - val_rec: 0.7537\n",
      "Epoch 29/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.4177 - acc: 0.9273 - prec: 0.9278 - rec: 0.9253 - val_loss: 1.2671 - val_acc: 0.7589 - val_prec: 0.7677 - val_rec: 0.7471\n",
      "Epoch 30/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.3836 - acc: 0.9319 - prec: 0.9333 - rec: 0.9299 - val_loss: 1.2376 - val_acc: 0.7682 - val_prec: 0.7820 - val_rec: 0.7702\n",
      "Epoch 31/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.4017 - acc: 0.9291 - prec: 0.9308 - rec: 0.9256 - val_loss: 1.1905 - val_acc: 0.7570 - val_prec: 0.7768 - val_rec: 0.7592\n",
      "Epoch 32/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.4158 - acc: 0.9291 - prec: 0.9306 - rec: 0.9245 - val_loss: 1.2118 - val_acc: 0.7514 - val_prec: 0.7709 - val_rec: 0.7482\n",
      "Epoch 33/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3543 - acc: 0.9475 - prec: 0.9513 - rec: 0.9454 - val_loss: 1.2322 - val_acc: 0.7439 - val_prec: 0.7639 - val_rec: 0.7390\n",
      "Epoch 34/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.4189 - acc: 0.9309 - prec: 0.9322 - rec: 0.9263 - val_loss: 1.2078 - val_acc: 0.7589 - val_prec: 0.7708 - val_rec: 0.7592\n",
      "Epoch 35/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3490 - acc: 0.9411 - prec: 0.9456 - rec: 0.9403 - val_loss: 1.2043 - val_acc: 0.7720 - val_prec: 0.7813 - val_rec: 0.7642\n",
      "Epoch 36/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.3389 - acc: 0.9521 - prec: 0.9543 - rec: 0.9492 - val_loss: 1.3060 - val_acc: 0.7645 - val_prec: 0.7683 - val_rec: 0.7574\n",
      "Epoch 37/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3332 - acc: 0.9475 - prec: 0.9528 - rec: 0.9475 - val_loss: 1.2450 - val_acc: 0.7607 - val_prec: 0.7728 - val_rec: 0.7555\n",
      "Epoch 38/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.3414 - acc: 0.9604 - prec: 0.9620 - rec: 0.9547 - val_loss: 1.2545 - val_acc: 0.7533 - val_prec: 0.7623 - val_rec: 0.7537\n",
      "Epoch 39/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.3494 - acc: 0.9401 - prec: 0.9422 - rec: 0.9371 - val_loss: 1.3730 - val_acc: 0.7570 - val_prec: 0.7587 - val_rec: 0.7482\n",
      "Epoch 40/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.3657 - acc: 0.9475 - prec: 0.9511 - rec: 0.9467 - val_loss: 1.2132 - val_acc: 0.7607 - val_prec: 0.7786 - val_rec: 0.7605\n",
      "Epoch 41/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3221 - acc: 0.9650 - prec: 0.9659 - rec: 0.9614 - val_loss: 1.2437 - val_acc: 0.7439 - val_prec: 0.7602 - val_rec: 0.7445\n",
      "Epoch 42/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3356 - acc: 0.9503 - prec: 0.9545 - rec: 0.9493 - val_loss: 1.2066 - val_acc: 0.7664 - val_prec: 0.7867 - val_rec: 0.7610\n",
      "Epoch 43/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3201 - acc: 0.9586 - prec: 0.9622 - rec: 0.9577 - val_loss: 1.2527 - val_acc: 0.7589 - val_prec: 0.7728 - val_rec: 0.7482\n",
      "Epoch 44/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.3551 - acc: 0.9521 - prec: 0.9522 - rec: 0.9513 - val_loss: 1.2025 - val_acc: 0.7682 - val_prec: 0.7889 - val_rec: 0.7574\n",
      "Epoch 45/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.3286 - acc: 0.9540 - prec: 0.9557 - rec: 0.9521 - val_loss: 1.2120 - val_acc: 0.7589 - val_prec: 0.7757 - val_rec: 0.7592\n",
      "Epoch 46/100\n",
      "68/68 [==============================] - 1s 11ms/step - loss: 0.2995 - acc: 0.9632 - prec: 0.9631 - rec: 0.9586 - val_loss: 1.2449 - val_acc: 0.7645 - val_prec: 0.7750 - val_rec: 0.7629\n",
      "Epoch 47/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.3142 - acc: 0.9613 - prec: 0.9620 - rec: 0.9594 - val_loss: 1.2361 - val_acc: 0.7776 - val_prec: 0.7946 - val_rec: 0.7757\n",
      "Epoch 48/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.3176 - acc: 0.9586 - prec: 0.9612 - rec: 0.9539 - val_loss: 1.2403 - val_acc: 0.7589 - val_prec: 0.7676 - val_rec: 0.7518\n",
      "Epoch 49/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.3280 - acc: 0.9586 - prec: 0.9594 - rec: 0.9567 - val_loss: 1.2960 - val_acc: 0.7794 - val_prec: 0.7907 - val_rec: 0.7776\n",
      "Epoch 50/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2970 - acc: 0.9678 - prec: 0.9677 - rec: 0.9668 - val_loss: 1.2682 - val_acc: 0.7645 - val_prec: 0.7745 - val_rec: 0.7647\n",
      "Epoch 51/100\n",
      "68/68 [==============================] - 1s 10ms/step - loss: 0.3106 - acc: 0.9650 - prec: 0.9676 - rec: 0.9631 - val_loss: 1.2352 - val_acc: 0.7645 - val_prec: 0.7738 - val_rec: 0.7647\n",
      "Epoch 52/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.3142 - acc: 0.9622 - prec: 0.9623 - rec: 0.9614 - val_loss: 1.2075 - val_acc: 0.7738 - val_prec: 0.7820 - val_rec: 0.7665\n",
      "Epoch 53/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.3032 - acc: 0.9586 - prec: 0.9594 - rec: 0.9585 - val_loss: 1.2648 - val_acc: 0.7813 - val_prec: 0.7890 - val_rec: 0.7757\n",
      "Epoch 54/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2977 - acc: 0.9669 - prec: 0.9678 - rec: 0.9669 - val_loss: 1.2753 - val_acc: 0.7664 - val_prec: 0.7780 - val_rec: 0.7629\n",
      "Epoch 55/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.3141 - acc: 0.9586 - prec: 0.9593 - rec: 0.9540 - val_loss: 1.2346 - val_acc: 0.7514 - val_prec: 0.7662 - val_rec: 0.7518\n",
      "Epoch 56/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.3021 - acc: 0.9641 - prec: 0.9656 - rec: 0.9603 - val_loss: 1.2917 - val_acc: 0.7645 - val_prec: 0.7777 - val_rec: 0.7684\n",
      "Epoch 57/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.3074 - acc: 0.9540 - prec: 0.9556 - rec: 0.9522 - val_loss: 1.2183 - val_acc: 0.7664 - val_prec: 0.7745 - val_rec: 0.7629\n",
      "Epoch 58/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.3145 - acc: 0.9622 - prec: 0.9632 - rec: 0.9623 - val_loss: 1.2466 - val_acc: 0.7664 - val_prec: 0.7727 - val_rec: 0.7629\n",
      "Epoch 59/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.3072 - acc: 0.9659 - prec: 0.9668 - rec: 0.9631 - val_loss: 1.3496 - val_acc: 0.7551 - val_prec: 0.7714 - val_rec: 0.7555\n",
      "Epoch 60/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.3066 - acc: 0.9641 - prec: 0.9658 - rec: 0.9631 - val_loss: 1.2263 - val_acc: 0.7645 - val_prec: 0.7806 - val_rec: 0.7665\n",
      "Epoch 61/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.2980 - acc: 0.9687 - prec: 0.9696 - rec: 0.9669 - val_loss: 1.2352 - val_acc: 0.7664 - val_prec: 0.7711 - val_rec: 0.7647\n",
      "Epoch 62/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.3165 - acc: 0.9613 - prec: 0.9613 - rec: 0.9613 - val_loss: 1.3968 - val_acc: 0.7495 - val_prec: 0.7653 - val_rec: 0.7463\n",
      "Epoch 63/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.3177 - acc: 0.9586 - prec: 0.9622 - rec: 0.9586 - val_loss: 1.2214 - val_acc: 0.7720 - val_prec: 0.7880 - val_rec: 0.7684\n",
      "Epoch 64/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2930 - acc: 0.9678 - prec: 0.9706 - rec: 0.9678 - val_loss: 1.2129 - val_acc: 0.7738 - val_prec: 0.7853 - val_rec: 0.7702\n",
      "Epoch 65/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.2919 - acc: 0.9558 - prec: 0.9567 - rec: 0.9559 - val_loss: 1.2390 - val_acc: 0.7757 - val_prec: 0.7858 - val_rec: 0.7757\n",
      "Epoch 66/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.2591 - acc: 0.9742 - prec: 0.9768 - rec: 0.9733 - val_loss: 1.2234 - val_acc: 0.7720 - val_prec: 0.7814 - val_rec: 0.7629\n",
      "Epoch 67/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.2789 - acc: 0.9687 - prec: 0.9695 - rec: 0.9686 - val_loss: 1.2583 - val_acc: 0.7738 - val_prec: 0.7873 - val_rec: 0.7721\n",
      "Epoch 68/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.2809 - acc: 0.9724 - prec: 0.9740 - rec: 0.9714 - val_loss: 1.3356 - val_acc: 0.7477 - val_prec: 0.7538 - val_rec: 0.7463\n",
      "Epoch 69/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.3010 - acc: 0.9678 - prec: 0.9686 - rec: 0.9660 - val_loss: 1.2123 - val_acc: 0.7682 - val_prec: 0.7779 - val_rec: 0.7647\n",
      "Epoch 70/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2778 - acc: 0.9742 - prec: 0.9752 - rec: 0.9733 - val_loss: 1.2308 - val_acc: 0.7664 - val_prec: 0.7810 - val_rec: 0.7684\n",
      "Epoch 71/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.2535 - acc: 0.9779 - prec: 0.9779 - rec: 0.9760 - val_loss: 1.2889 - val_acc: 0.7645 - val_prec: 0.7688 - val_rec: 0.7610\n",
      "Epoch 72/100\n",
      "68/68 [==============================] - 1s 12ms/step - loss: 0.2563 - acc: 0.9696 - prec: 0.9701 - rec: 0.9666 - val_loss: 1.3118 - val_acc: 0.7664 - val_prec: 0.7800 - val_rec: 0.7684\n",
      "Epoch 73/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2627 - acc: 0.9742 - prec: 0.9768 - rec: 0.9705 - val_loss: 1.2324 - val_acc: 0.7626 - val_prec: 0.7745 - val_rec: 0.7574\n",
      "Epoch 74/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2528 - acc: 0.9797 - prec: 0.9806 - rec: 0.9798 - val_loss: 1.2664 - val_acc: 0.7664 - val_prec: 0.7825 - val_rec: 0.7629\n",
      "Epoch 75/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2705 - acc: 0.9669 - prec: 0.9688 - rec: 0.9660 - val_loss: 1.2808 - val_acc: 0.7850 - val_prec: 0.8016 - val_rec: 0.7776\n",
      "Epoch 76/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2940 - acc: 0.9650 - prec: 0.9677 - rec: 0.9651 - val_loss: 1.2798 - val_acc: 0.7832 - val_prec: 0.7858 - val_rec: 0.7684\n",
      "Epoch 77/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2824 - acc: 0.9724 - prec: 0.9732 - rec: 0.9715 - val_loss: 1.2783 - val_acc: 0.7626 - val_prec: 0.7691 - val_rec: 0.7555\n",
      "Epoch 78/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.2548 - acc: 0.9761 - prec: 0.9770 - rec: 0.9752 - val_loss: 1.2302 - val_acc: 0.7720 - val_prec: 0.7908 - val_rec: 0.7721\n",
      "Epoch 79/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2672 - acc: 0.9687 - prec: 0.9686 - rec: 0.9678 - val_loss: 1.2451 - val_acc: 0.7701 - val_prec: 0.7846 - val_rec: 0.7684\n",
      "Epoch 80/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.2622 - acc: 0.9770 - prec: 0.9770 - rec: 0.9770 - val_loss: 1.2261 - val_acc: 0.7682 - val_prec: 0.7770 - val_rec: 0.7702\n",
      "Epoch 81/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2509 - acc: 0.9853 - prec: 0.9853 - rec: 0.9825 - val_loss: 1.2551 - val_acc: 0.7720 - val_prec: 0.7835 - val_rec: 0.7721\n",
      "Epoch 82/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.2643 - acc: 0.9751 - prec: 0.9752 - rec: 0.9752 - val_loss: 1.2307 - val_acc: 0.7626 - val_prec: 0.7744 - val_rec: 0.7610\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 1s 19ms/step - loss: 0.2519 - acc: 0.9770 - prec: 0.9786 - rec: 0.9758 - val_loss: 1.2232 - val_acc: 0.7645 - val_prec: 0.7792 - val_rec: 0.7592\n",
      "Epoch 84/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.2630 - acc: 0.9696 - prec: 0.9706 - rec: 0.9697 - val_loss: 1.2138 - val_acc: 0.7701 - val_prec: 0.7777 - val_rec: 0.7629\n",
      "Epoch 85/100\n",
      "68/68 [==============================] - 1s 18ms/step - loss: 0.2638 - acc: 0.9715 - prec: 0.9724 - rec: 0.9715 - val_loss: 1.2179 - val_acc: 0.7645 - val_prec: 0.7751 - val_rec: 0.7665\n",
      "Epoch 86/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2603 - acc: 0.9751 - prec: 0.9751 - rec: 0.9751 - val_loss: 1.2612 - val_acc: 0.7570 - val_prec: 0.7730 - val_rec: 0.7555\n",
      "Epoch 87/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2584 - acc: 0.9751 - prec: 0.9751 - rec: 0.9743 - val_loss: 1.2326 - val_acc: 0.7682 - val_prec: 0.7781 - val_rec: 0.7721\n",
      "Epoch 88/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2427 - acc: 0.9770 - prec: 0.9770 - rec: 0.9770 - val_loss: 1.2790 - val_acc: 0.7626 - val_prec: 0.7718 - val_rec: 0.7592\n",
      "Epoch 89/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2441 - acc: 0.9751 - prec: 0.9752 - rec: 0.9733 - val_loss: 1.2346 - val_acc: 0.7757 - val_prec: 0.7898 - val_rec: 0.7739\n",
      "Epoch 90/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2608 - acc: 0.9742 - prec: 0.9751 - rec: 0.9715 - val_loss: 1.2271 - val_acc: 0.7701 - val_prec: 0.7884 - val_rec: 0.7702\n",
      "Epoch 91/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2547 - acc: 0.9705 - prec: 0.9715 - rec: 0.9688 - val_loss: 1.2388 - val_acc: 0.7589 - val_prec: 0.7774 - val_rec: 0.7592\n",
      "Epoch 92/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2375 - acc: 0.9779 - prec: 0.9779 - rec: 0.9779 - val_loss: 1.3005 - val_acc: 0.7682 - val_prec: 0.7740 - val_rec: 0.7629\n",
      "Epoch 93/100\n",
      "68/68 [==============================] - 1s 15ms/step - loss: 0.2596 - acc: 0.9705 - prec: 0.9705 - rec: 0.9697 - val_loss: 1.2476 - val_acc: 0.7570 - val_prec: 0.7695 - val_rec: 0.7555\n",
      "Epoch 94/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2479 - acc: 0.9770 - prec: 0.9769 - rec: 0.9769 - val_loss: 1.2212 - val_acc: 0.7626 - val_prec: 0.7719 - val_rec: 0.7555\n",
      "Epoch 95/100\n",
      "68/68 [==============================] - 1s 17ms/step - loss: 0.2511 - acc: 0.9705 - prec: 0.9723 - rec: 0.9695 - val_loss: 1.2386 - val_acc: 0.7551 - val_prec: 0.7752 - val_rec: 0.7555\n",
      "Epoch 96/100\n",
      "68/68 [==============================] - 1s 16ms/step - loss: 0.2645 - acc: 0.9761 - prec: 0.9760 - rec: 0.9743 - val_loss: 1.1932 - val_acc: 0.7626 - val_prec: 0.7825 - val_rec: 0.7610\n",
      "Epoch 97/100\n",
      "68/68 [==============================] - 1s 14ms/step - loss: 0.2448 - acc: 0.9770 - prec: 0.9770 - rec: 0.9761 - val_loss: 1.2109 - val_acc: 0.7682 - val_prec: 0.7866 - val_rec: 0.7684\n",
      "Epoch 98/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2442 - acc: 0.9724 - prec: 0.9733 - rec: 0.9724 - val_loss: 1.2229 - val_acc: 0.7607 - val_prec: 0.7659 - val_rec: 0.7574\n",
      "Epoch 99/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2583 - acc: 0.9715 - prec: 0.9713 - rec: 0.9705 - val_loss: 1.2022 - val_acc: 0.7645 - val_prec: 0.7749 - val_rec: 0.7574\n",
      "Epoch 100/100\n",
      "68/68 [==============================] - 1s 13ms/step - loss: 0.2471 - acc: 0.9751 - prec: 0.9752 - rec: 0.9752 - val_loss: 1.2271 - val_acc: 0.7664 - val_prec: 0.7766 - val_rec: 0.7702\n",
      "Time taken : 1.66 mins\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "logsgd = CSVLogger('cnnsgd.log', separator=',', append=False)\n",
    "historysgd = modelsgd.fit(X_train, Y_train, epochs=epoh, batch_size=16,  validation_split = 0.33,callbacks=[logsgd]) \n",
    "print(f'Time taken : {(time.time() - start_time) / 60:.2f} mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Train---\n",
      "51/51 [==============================] - 0s 5ms/step - loss: 0.5224 - acc: 0.9105 - prec: 0.9141 - rec: 0.9053\n",
      "acc : 91.05%\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.5224 - acc: 0.9105 - prec: 0.9141 - rec: 0.9053\n",
      "loss : 0.52\n",
      "\n",
      "\n",
      "\n",
      "--- Data Test---\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3700 - acc: 0.7069 - prec: 0.7131 - rec: 0.6906\n",
      "acc : 70.69%\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1.3700 - acc: 0.7069 - prec: 0.7131 - rec: 0.6906\n",
      "loss : 1.37\n"
     ]
    }
   ],
   "source": [
    "#Tes Data Latih\n",
    "print(\"--- Data Train---\")\n",
    "scores = modeladam.evaluate(X_train, Y_train)\n",
    "print(\"%s : %.2f%%\" % (modeladam.metrics_names[1], scores[1]*100))\n",
    "scores = modeladam.evaluate(X_train, Y_train)\n",
    "print(\"%s : %.2f\" % (modeladam.metrics_names[0], scores[0]))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"--- Data Test---\")\n",
    "scores = modeladam.evaluate(X_test, Y_test)\n",
    "print(\"%s : %.2f%%\" % (modeladam.metrics_names[1], scores[1]*100))\n",
    "scores = modeladam.evaluate(X_test, Y_test)\n",
    "print(\"%s : %.2f\" % (modeladam.metrics_names[0], scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Train---\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 1.9926 - acc: 0.4312 - prec: 0.4352 - rec: 0.3953\n",
      "acc : 43.12%\n",
      "51/51 [==============================] - 0s 5ms/step - loss: 1.9926 - acc: 0.4312 - prec: 0.4352 - rec: 0.3953\n",
      "loss : 1.99\n",
      "\n",
      "\n",
      "\n",
      "--- Data Test---\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.3123 - acc: 0.3966 - prec: 0.4001 - rec: 0.3706\n",
      "acc : 39.66%\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 2.3123 - acc: 0.3966 - prec: 0.4001 - rec: 0.3706\n",
      "loss : 2.31\n"
     ]
    }
   ],
   "source": [
    "#Tes Data Latih\n",
    "print(\"--- Data Train---\")\n",
    "scores = modeladadelta.evaluate(X_train, Y_train)\n",
    "print(\"%s : %.2f%%\" % (modeladadelta.metrics_names[1], scores[1]*100))\n",
    "scores = modeladadelta.evaluate(X_train, Y_train)\n",
    "print(\"%s : %.2f\" % (modeladadelta.metrics_names[0], scores[0]))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"--- Data Test---\")\n",
    "scores = modeladadelta.evaluate(X_test, Y_test)\n",
    "print(\"%s : %.2f%%\" % (modeladadelta.metrics_names[1], scores[1]*100))\n",
    "scores = modeladadelta.evaluate(X_test, Y_test)\n",
    "print(\"%s : %.2f\" % (modeladadelta.metrics_names[0], scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Train---\n",
      "51/51 [==============================] - 0s 5ms/step - loss: 0.5353 - acc: 0.9143 - prec: 0.9158 - rec: 0.9139\n",
      "acc : 91.43%\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.5353 - acc: 0.9143 - prec: 0.9158 - rec: 0.9139\n",
      "loss : 0.54\n",
      "\n",
      "\n",
      "\n",
      "--- Data Test---\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4098 - acc: 0.7167 - prec: 0.7258 - rec: 0.7181\n",
      "acc : 71.67%\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4098 - acc: 0.7167 - prec: 0.7258 - rec: 0.7181\n",
      "loss : 1.41\n"
     ]
    }
   ],
   "source": [
    "#Tes Data Latih\n",
    "print(\"--- Data Train---\")\n",
    "scores = modelsgd.evaluate(X_train, Y_train)\n",
    "print(\"%s : %.2f%%\" % (modelsgd.metrics_names[1], scores[1]*100))\n",
    "scores = modelsgd.evaluate(X_train, Y_train)\n",
    "print(\"%s : %.2f\" % (modelsgd.metrics_names[0], scores[0]))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"--- Data Test---\")\n",
    "scores = modelsgd.evaluate(X_test, Y_test)\n",
    "print(\"%s : %.2f%%\" % (modelsgd.metrics_names[1], scores[1]*100))\n",
    "scores = modelsgd.evaluate(X_test, Y_test)\n",
    "print(\"%s : %.2f\" % (modelsgd.metrics_names[0], scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_data = pd.read_csv('cnnadam.log', sep=',', engine='python') \n",
    "plot_history_model(log_data, \"Adam\", epoh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABFEAAAFnCAYAAABuAtawAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAACHj0lEQVR4nOzdd3hUZd7G8e8zk9576ITeewApIhbsHXvFhq67uuqqq1usu+prXXsXUREFxYYFaQJio/feCZAAgYT0ZOZ5/zgTCBBaSDIp9+e6znVmznnOOb+JEcidpxhrLSIiIiIiIiIicngufxcgIiIiIiIiIlIbKEQRERERERERETkKClFERERERERERI6CQhQRERERERERkaOgEEVERERERERE5CgoRBEREREREREROQoKUURERI6RMSbFGGONMesr8Z7rffdMOYZrhvmueb+y6hARERGRQ1OIIiIiNYox5idfMGCNMdOP0LaRMcZTpv1t1VWniIiIiNQ/ClFERKQmG2iMaXGY89egv8tEREREpJroH54iIlJTrQAMcO1h2lwLeIFV1VKRiIiIiNRrClFERKSm+hgo4RAhijGmJ9AZmAxsqca6RERERKSeUogiIiI1VQYwAWhtjOlfzvnrffsPjnQjY0wPY8xoY0yaMabIGJNhjPnGGDPkCNddYYz5zRiTa4zJNMZMMMacdBTPcxljrjfGTDHG7DTGFPomjn3DGNPsSNdXFmNMK2PM275nF/o+wyRjzGWHueY0Y8zXxphtvq/VTmPMUmPMSGPMGeW0v9QYM9EYs8MYU+z72i70fda+VfsJRURERKqXQhQREanJRvr215U9aIwJAK4EcoBxh7uBMeY6YBZwBRAKLAA8wLnAj8aYRw9x3VPAaKAvkAWs8b2eAgw9zPMigB+A94GTgQJgKZAA3ArMN8b0PlzNlcEXEC0AbvY9exGwBzgV+NQYM8IYYw645hZgInAeEOi7ZivQGOe/wZ8OaP8fYAxwGk6voQVAJtAK57NeWUUfT0RERMQvFKKIiEhN9jWwG7jMGBNc5vhZQCLwubU271AXG2M6AW8DbuD/gGRrbW+gEXA7znwqDxljzjngujOAv/vODwca+65rALwFPHmYml8DhgC/AV2ttY2ttT2AON91scCYAz5PpTLGJAGfAOHAh0ADa22qtbY5cDFOsDMM52tQeo0beMr39i9AkrW2l7W2s7U2GidAGlumfQLwAE54MhRo6HtGeyASJ6yZXFWfUURERMQfFKKIiEiNZa0txOnpEIvTO6LU0Q7luRcIAmZYax+w1hb77mutta/j9BYB+NcB1z3g24+w1r5trbW+6wpwgofV5T3MF9pcC6QD51trF5X5LEXW2n8A3wApwCVHqP14/AkntFkH3GitzSlTxxfAE763D/jCE3BCqThgt7X2VWutp+wNrbV/WGtHlTnUGiecWmytHVf6NfK19Vprp1hrv6n0TyYiIiLiRwpRRESkpttvSI8xJhZnKM4mYOoRrj3Lt//fIc4/79v3NcbE+O4fDpzoO/7ygRf4woKDjvtc7Nt/Ya3dfog2n/v2gw9xvjKUfu5XrLUl5Zx/GacHSROcyXkBtuP0UIk5sGfOIWzy7dsaY1KPp1gRERGR2iLA3wWIiIgcjrX2F2PMauBMY0wiztCRYOCjsr0fDmSMiQaSfW8XH6LZcpwwIQBoC/wBtMHpYWGBZYe4bukhjnf17c8wxvx8iDYxvn3jQ5yvDO18+3I/t7V2tzFmM06PmHbAAmutxxjzP5xeOOONMQtx5kf5FfjJWrvzgHukGWM+wZlr5g9jzO8488X8Ckyz1u6p/I8lIiIi4l8KUUREpDb4EHgUZ6LS0slKjzSUJ7LM6/TyGviCg504YUtp+wjfPstaW3SIe5d7P/YFJC182+GEHeH88Sj9LIeqs/RcCvt/nf4JbMYZstSVfaFQiTFmHHC3tbbsctLDcIKam4ETfBtAvjHmQ+B+a21WxT+GiIiISM2i4TwiIlIbfIDTM+QenB/UZ1lrlx/hmrI9IZLLa+CbDyT+gPal84dEG2OCDnHvcu9X5tq7rbXmCNvgI9R/PEo/y6HqLHtu79fJN5fJq9baTjg9Za4A3gHygMuA78p+Tay1hdba/1prWwAtceaqGQ0YnAl5R1fS5xERERGpERSiiIhIjWetXQ/MAJr7Dh2pFwq+HhClPTE6H6JZO5xemRZY6Tu2CmcJZAN0OMR1HQ9xfMkRnlddVvj25dbhG+rU5IC2+7HWbrHWfmqtvQWnR0ou0A1nlZ7y2q+z1n5grb2KfXPKnGWMaVrBzyAiIiJS4yhEERGR2uJ/OEvmTuLoezh879vfdYjzpcd/s9buBrDW5gKl85n8+cALjDGmvOM+n/n2lxljqnLOkyMp/dx/McaUN3T3zzjh0WYOPV/MXtbaDb62AA2P4vlzgPxjaC8iIiJSKyhEERGRWsFa+4W19jRr7ZADJzk9jGeBIuBEY8yTxphAcIIQY8ytwE2+dv894Lr/8+1vNMbc5AtOMMYE46xs0/YQNc7Hmb8lEphkjBlwYBtjTDdjzNPlnatErwOZOPOyvOtbcaj0+Rewb0nnp0qXMjbGdDTGvG2MOaH08/qOu4wx1+F8ZgvM9R0/1RjznDGmdN6U0vaBOJPThuIMAzrU5LwiIiIitY4mlhURkTrLWrvEGHML8B7OD/a3+lb6acK+HhKPW2u/PeC6740xzwL34swJ8pgxJg0nSIgE7scJaMpzKxAFXAD8bIzZBmwEgnBCjWhfuymV8ykPZq3NMMZcAXyJszT0xcaY5UAi+4ZEjQReK3NZEM4EsTcDe4wxa3BWLmruuw7gEWvtat/rSJw5au4xxuwC1uEMgWqBM8GuF/iLVukRERGRukQ9UUREpE6z1n4A9AE+AQqA7kAg8C1whrX2oUNcdx9wDTALiMVZ+vgP4FTg88M8Lx+4CLgY+Np3uAfORK3rgLeAs3CGJlUZa+1EnDlM3sXpldINJ8CZClxhrR12wBLRK3EClE+ALThhSHecIOVLnK/Vo2XazwDu8J3LxAmYOgPZOMOt+llrR1TNpxMRERHxD7P/v59ERERERERERKQ86okiIiIiIiIiInIUFKKIiIiIiIiIiBwFhSgiIiIiIiIiIkfB7yGKMeZa35KK84wxxcYYa4wZXE67eGPMrcaY8caYdcaYQmNMhjHmqypeJlJERERERERExP8Tyxpj1uMsn5gBFOOsXnCytfanA9rdBrwObMZZ0WAr0ApnBQQ3cK21dlS1FS4iIiIiIiIi9UpNCFFOBVZaazcZY54F/kb5IcopQCjwvbXWW+Z4f+AnIBdoYK0tPNIzExISbEpKSqV9BhERERERkdpkzpw5O6y1if6uQ6S2CfB3AdbayUfZbsohjv9ijJkKnA50AWYf6V4pKSnMnn3EZiIiIiIiInWSMWaDv2sQqY38PidKJSn27Uv8WoWIiIiIiIiI1Fm1PkQxxjQGTgG2AYv8XI6IiIiIiIiI1FG1OkQxxgQAI3HmSnnQWus5TNvhxpjZxpjZ27dvr7YaRURERERERKRuqLUhijHGAG8ApwIjrLXvH669tfYta22qtTY1MVHzJ4mIiIiIiIjIsam1IQrwEnATMAa4xc+1iIiIiIiIiEgdVytDFGPMC8BfgC+Aqw83jEdEREREREREpDLUuhDFGPMMcBcwHrjcWqsVeURERERERESkytWqEMUY8wRwL/ADcIm1tvgIl4iIiIiIiIiIVIoAfxdgjLkZGOh7m+rbP2CMGeZ7/Y619mdjzA3Ag0AhsAB40Jlbdj/vW2vXV23FIiIiIiIiIlIf+T1EwQlQrj/g2BllXv8E/Aw0970PBv5+iHv9BKyvvNKqT0Gxh115RezOK2Z3XjFZ+b7X+cW+Y0XEhQdx95C2BLprVQciERERERERkTrB7yGKtXYYMOwo2j0CPFK11fjPCxNX8ub0tUdsV+K1/OPsDtVQkYiIiIiIiIiU5fcQRRwJEcEkRQYTExZITGgQ0WGBxIYFEhMWRHRoIC5jePbHFbw1fS19UuI4rWPyUd/b47UsTsuiS+NoXK6DhkCJiIiIiIiIyFFQiFJD3DKoJbcMannYNi4DT36/nL+NXcD4OwbSNC7siPct8Xi585N5fLdoG9f3a86jF3SurJJFRERERERE6hVNrlGL3HJiS07rkERWfjF/GT2PohLvYdt7vJZ7xizgu0XbABj1+0Y2ZeZVR6kiIiIiIiIidY5ClFrE5TI8e2k3GseEsmDTbp78ftkh23q9lvs+W8DXC7YQERxA3xZxlHgtL01eVY0Vi4iIiIiIiNQdClFqmZiwIF65qgeBbsOImev5YfHWg9p4vZZ/fLGIcXPTCAtyM+KG3jx9SVcCXIbP525m7fYcP1QuIiIiIiIiUrspRKmFejSL5YGznBV67vtsIRt25u49Z63loa8X88msTYQEunj3+t70TomjeXw4l6Y2xWvhf5PUG0VERERERETkWClEqaVuHJDCGZ2S2VNQwp8/nktBsQdrLY9+s5SPfttIUICLd67rTb9W8XuvueOU1gS5XXyzcAvLt2X7sXoRERERERGR2kchSi1ljOHpS7rRNC6UxWnZ/PfbZTzx3TLe/2U9QW4Xb13bi4FtEva7plFMKFf1bYa18MLElX6qXERERERERKR2UohSi0WHBvLaVb0Icrv48LcNvD1jHQEuw2tX92Rwu6Ryr7l9cCtCAl1MWJLOos1Z1VyxiIiIiIiISO2lEKWW69Ikmn+d68yP4nYZXrmqB6d1TD5k+6SoEK7vlwLA8xNXVEeJIiIiIiIiInVCgL8LkON37QnNiQoJpElsKKkpcUdsf+tJrfjotw1MXbGdORsy6dX8yNeIiIiIiIiI1HfqiVIHGGO4sEfjowpQAOLCg7hpYAsAnvtRc6OIiIiIiIiIHA2FKPXUTSe2JCokgF/W7OSXNTv8XY6IiIiIiIhIjacQpZ6KDg1k+KCWADz/40qstX6uSERERERERKRmU4hSjw0b0IK48CBmb9jFtJXb/V2OiIiIiIiISI2mEKUeiwgO4E8ntQKcuVG8XvVGERERERERETkUhSg1RXEBTH8WSoqq9bHXnNCcpMhgFqVlcf2IP0jPLqjW54uIiIiIiIjUFgpRaopv/gpTHodPr4bi/Gp7bGiQm/9d0Z248CBmrNrBmf+bzo9LtlXb80VERERERERqC4UoNUW/P0NYPKz6EUZdCoU51fbo/q0S+OGvJ3JimwR25RUz/MM5/OOLReQXeaqtBhEREREREZGaTiFKTdGwKwz7DiIawPoZ8NHFUJBVbY9Pigph5A19+Pe5HQlyu/j4942c8/IMFqdVXw0iIiIiIiIiNZlClJokqT3c8B1EN4VNv8PI8yEvs9oe73IZbhrYgi//PIA2SRGs3Z7LRa/N5K3pazTprIiIiIiIiNR7ClFqmvhWTpAS2wK2zof3z4E96dVaQsdGUXxzx0Cu69ecYo/lie+W8+eP52KtghQRERERERGpvxSi1EQxzeCG7yGhHWQshffPhqy0ai0hJNDNYxd05t3rU4kMCeD7xduYoAlnRUREREREpB5TiFJTRTV0eqQ06AI7V8OIMyFzXbWXcWqHZO47ox0A//1uGYUlmmxWRERERERE6ieFKDVZeAJc/w00ToXdG2HE2ZCxrNrLuKpPM9omR7ApM5/3fl5f7c8XERERERERqQkUotR0obFw3ZfQfADs2QLvnQkbf6/WEgLcLv59bkcAXpmyiow9BdX6fBEREREREZGaQCFKbRAcCdd8Du3OgYLd8MEFsOL7ai3hxDaJnNo+idwiD89NWFmtzxYRERERERGpCRSi1BaBoXDZB9DzOijJh0+uhnkfVWsJ/zynA4Fuw5g5m1icllWtzxYRERERERHxN4UotYk7AM57CQbdB9YDX/0ZZjwP1bT0cMvECK7vl4K18Ng3S7XksYiIiIiIiNQrClFqG2PglH/BWU8DBiY/Cj88CF5vtTz+jlPbEBcexB/rM/l+sZY8FhERERERkfpDIUpt1fdWuORdcAXC76/DuFugpKjKHxsdGsg9Q9oC8MR3yygoPvKSxx6veqyIiIiIiIhI7acQpTbrPBSuHgtBEbD4Mxh9ORTmVPljr+jdlHbJkWzelc+7P68rt02Jx8uX89I483/T6frIBJZs0RwqIiIiIiIiUrspRKntWp0Mw8ZDWAKsmeKs3JOXWaWPDHC7eOg8Z8njV6euJj1735LH+UUe3p+5jpOe+Ym7Pp3P8m17nBV9ftSKPiIiIiIiIlK7KUSpCxr1gJt+hOhmkDYb3jsTstKq9JEDWicwpGMyeUUenpmwgl25Rbw4aRUD/m8Kj3yzlLTd+bRMCOeR8zoSFuRmyvIMFmzaXaU1iYiIiIiIiFQlUx9XWElNTbWzZ8/2dxmVL3sLfHgxbF8G0U3h2i8hoXWVPW7djlxOf2EaxR5LaKCbfN/8KN2axvCnk1oypGMD3C7Dk98v481pazmlfRLvDetdZfWIiIiIiMjRMcbMsdam+rsOkdpGPVHqkqhGcMN30KQ3ZG2C986ALfOq7HEtEsK5YUALAPKLPZzUNpHRt5zAl7f358zODXG7DADDT2xJaKB6o4iIiIiIiEjt5vcQxRhzrTHmbWPMPGNMsTHGGmMGH6Z9J2PMl8aYTGNMrjHmd2PMpdVXcQ0XFgfXfQWtT4O8HfD+ebBuepU97m+nt+XJi7vw3Z0nMvLGPvRrFY8xZr828RHBXNe/OQAvTl5VZbWIiIiIiIiIVCW/hyjA48DNQCMg/XANjTHdgd+AM4CvgNeABGCMMeYvVVtmLRIUDleMhs6XQNEe+GgoLPumSh4VHODmyj7N6Ngo6rDt1BtFREREREREaruaEKLcBDSz1iYDnxyh7etAOHCetfYGa+19QHdgGfC0MaZhlVZamwQEwcVvQ5/h4CmCMdfBnJF+K6dsb5SX1BtFREREREREaiG/hyjW2snW2k1HameM6QicAEy21k4qc/0e4AkgFLiqygqtjVwuOOtpGPwgWC98cyf89BT4aTLh0t4ok5dnsHDzbr/UICIiIiIiIlJRfg9RjsEg335iOedKj51UTbXUHsbA4Afg3BfAuOCnJ+Gbv4KnpNpLiY8I5rp+vrlRJqk3ioiIiIiIiNQutSlEKV2rd/WBJ6y16UBOmTZyoNQb4bIPISAE5o6EMddCUV61l3HLIPVGERERERERkdqpNoUopTOXZh/ifDYQXU211E4dznVW7gmJgRXfwQcXQF5mtZaQcAy9UXIKS/hqfhppu/OrozQRERERERGRw6pNIUrpurkVmtDDGDPcGDPbGDN7+/btlVhWLdPsBLjpR4huCpv/gHdPh10bqrWEI/VGycov5uXJqxj4f1P46yfzGfL8ND78bQNer3/mchERERERERGB2hWiZPn2h+ptElWmzUGstW9Za1OttamJiYmVXlytktjOCVKSOsHOVU6Qsm1RtT2+bG+Usiv1ZOYW8dyPKxj41BSem7iS3XnFNI0LJa/Iw7+/XMzV7/zOpszqH4IkIiIiIiIiArUrRCmdC+WgeU+MMclABOXMlyKHENUIbvweUk6EnG3w3lmw9qdqe3xpb5RJyzKYujyDJ79bxsD/m8LLU1azp7CEfi3j+fiWvky/72RevaonceFB/Lp2J2f8bzof/rpevVJERERERESk2tWmEGW6bz+knHNDDmgjRyMkGq75HDpdDEV74KOhsOCTanl02d4oN7w/izenryWvyMPgdol8dls/Rg8/gf6tEjDGcE7Xhky8exDndG3o9Er5aol6pYiIiIiIiEi1qzUhirV2KfAbcKox5rTS48aYSOAfQD7wsZ/Kq70CgmHou9DvL+AtgS9uhenPgK36nh63DGpJZEgAAKd3TObrvwzg/Rv6kJoSd1Db+IhgXr2qJ69d3ZP4Mr1SPvh1PbYaahUREREREREx/v4B1BhzMzDQ9zYV6ARMALb5jr1jrf3Z17Y78DPgBj4BdgAXAa2AO6y1rxzNM1NTU+3s2bMr6yPUHb+9AT88AFjoNQzOfg7cAVX6yE2ZeXi8lpSE8KO+ZmdOIQ9/vYTxC7cCcM+Qttx5apuqKlFEREREpM4xxsyx1qb6uw6R2qYmhCjvA9cfpskN1tr3y7TvDPwHOAkIBhYDz1hrxx7tMxWiHMbSr2HcLVBSAG3OgEveg+AIf1dVri/npXH3mPlYC29fl8qQjsn+LklEREREpFZQiCJSMX4PUfxBIcoRbPwdRl8B+ZnQsDtcPRYikvxdVblenbqaZyasICI4gC//3J/WSZH+LklEREREpMZTiCJSMbVmThSpRs36wk0TIaY5bJ0P75wKO1Yd8TJ/uH1wK87p2pCcwhJu+WAOWfnF/i5JRERERERE6iiFKFK+hNZw8yRo1AN2b4R3ToP1P/u7qoMYY3jmkq60bxDJuh25/PWTeXi0/LGIiIiIiIhUAYUocmgRSTDsW2h7FhTshg8uhPk1bwGksKAA3r4uldiwQH5asZ3nflzh75JERERERESkDlKIIocXFA5XjIITbgdvMXz5J5j8OHi9/q5sP03jwnjlqp64XYbXflrD+IVb/F2SiIiIiIiI1DEKUeTIXG4480k4+1kwbpjxLHx+ExTn+7uy/QxoncA/zu4AwH1jF7J0S7afKxIREREREZG6RCGKHL0+t8BVYyAoEpaMg5HnQc52f1e1nxsHpHBxz8bkF3sY/uFsduUW+bskERERERERqSMUosixaXMa3DQBopvC5lnwzimQsczfVe1ljOGJi7rQtUk0m3flc+PIWczbuMvfZYmIiIiIiEgdoBBFjl1yJ7h5MjTq6azc8+7psHqyv6vaKyTQzZvX9iIxMph5G3dz0Wu/cPmbvzJ1RQbWauUeERERERERqRiFKFIxkcnOyj0dzofCbBh1Kfzxtr+r2qthdCjf3Xkit53UisjgAH5fl8kNI2Zx1osz+GLeZoo9NWtiXBEREREREan5TH38zXxqaqqdPXu2v8uoG7xemPofmPGc8773zXDm/4E7wL91lZFdUMzHv2/kvZ/XkbGnEIDGMaHcfGILrujdjNAgt58rFBERERGpXsaYOdbaVH/XIVLbKESRyrHgU/j6L+ApgpaD4dL3ITTW31Xtp7DEw5fz0nhz+lrWbs8FoGlcKE9d3JUBrRP8XJ2IiIiISPVRiCJSMQpRpPJs+gM+uQpyt0N8G7jqU4hv5e+qDuL1Wn5cms7/Jq1k+bY9AFzRuykPnt2B6NBAP1cnIiIiIlL1FKKIVIzmRJHK07QP3DIFkjvDzlXw9imwbrq/qzqIy2U4s3MDvrljIPee3pYgt4tPZm3i9BemMXFpur/LExERERERkRpKIYpUrphmcOMEaHc2FOyGDy+C2SP8XVW5At0u/nJKG769cyA9msWQnl3ILR/M5i8fz2VnTqG/yxMREREREZEaRiGKVL7gCLj8IxjwV/CWwPi74Lv7wVPi78rK1SY5ks9u689D53YkNNDN+IVbOe35aXw1P01LIouIiIiIiMheClGkarjcMOQxuPB1cAfBH2/CRxdDXqa/KyuX22W4cWALfrx7EANax7Mrr5i/fjKf60fMYsPOXH+XJyIiIiIiIjWAQhSpWt2vguvHQ3gSrJvmzJOSsdzfVR1S07gwPrqpL/83tAtRIQFMX7md01+YzsuTV1FY4vF3eSIiIiIiIuJHClGk6jXrC8OnQsNusGsdvHMarPjB31UdkjGGy3s3Y8q9g7m4R2MKS7w8N3ElZ704g1/W7PB3eSIiIiIiIuInWuJYqk9RHnx1Oyz5AjBw2sMw4C4wxt+VHdYva3bwry8Xs3a7M6zn4p6N+cfZHUiICAbAWsuWrAKWbslm2VZnW7M9h+5NY7jvjPYkRgb7s3wRERERkYNoiWORilGIItXLWpjxLEz5j/O+y6Vw/ssQGOrfuo6gsMTDW9PW8vLU1RSVeIkODeScrg1Zuz2HZVv3kJVfXO51kcEB3HN6W649oTkBbnX8EhEREZGaQSGKSMUoRBH/WDYexg2H4lxo1AOu+BiiGvm7qiNavyOXf3+1mBmr9h/WExceRIeGkXRsGEWHhlE0jgnljWlrmLpiOwDtG0Ty2AWd6dMizh9li4iIiIjsRyGKSMUoRBH/SV8Co6+A3RshooETpDTp5e+qjshay+RlGazenkO75Eg6NooiKTIYc8CwJGstk5Zl8Og3S9i8Kx+Ai3o05sGz25MUGXJQ223ZBazOyGFNRg5hwQFc2qvJQfcUEREREakMClFEKkYhivhX7k4Ycx1s+Bncwc7Qnm6X+7uqSlVQ7OH1n9bw+rQ1FJV4iQwO4LbBrTCGvaHJmu255BSW7Hfdv8/tyE0DW/ipahERERGpyxSiiFSMQhTxv5Ii+P5+mDPCeT/gr3Dqw+By+7euSrZhZy6PfbOUycszyj0fGxZI66QIGkSH8s2CLbhdho9v7kvflvHVXKmIiIiI1HUKUUQqRiGK1Bx/vA3f/x2sB9qcAUPfgZAof1dV6SYvS+fL+VtIiAiidVIErRMjaJ0UQXzEvlV8nvx+GW9OW0tCRDDf3jmQ5KiQw9xRREREROTYKEQRqRiFKFKzrJ0GY6+H/F2Q0A6uHA3xrfxdVbUr8Xi57r0/+GXNTno2i+GT4f0ICtDqPiIiIiJSORSiiFSMfiqTmqXlSXDLFEhsDztWwNunwJqp/q6q2gW4Xbx8ZQ8aRYcwd+NuHh+/1N8liYiIiIiI1HsKUaTmiWsJN02EtmdBwW746GL45WWoZ72m4iOCef2aXgS5XXz42wY+n7PZ3yWJiIiIiIjUawpRpGYKiXKWPB50H1gv/Pgv+PwmKMr1d2XVqlvTGB69oBMA//hiEYvTsvxckYiIiIiISP2lEEVqLpcLTvkXXP4RBEXA4s/h3dMhc52/K6tWV/ZpxuWpTSks8fKnUXPYnVfk75JERERERETqJYUoUvN1OA9ungzxrSF9Mbw1GFZP9ndV1erRCzrRtUk0mzLz+esn8/F469fQJhERERERkZpAIYrUDkntnQln257pzJMy6hL4+X/1Zp6UkEA3r13dk9iwQKat3M7/Jq30d0kiIiIiIiL1jkIUqT1CouGK0XDSA848KZMehs9ugMIcf1dWLZrEhvHylT1xGXh5ymq+XrDF3yWJiIiIiIjUKwpRpHZxueDkB50wJSgSlnwBb58MGcv8XVm1GNgmgX+e0xGA+8YuYP6m3f4tSEREREREpB5RiCK1U/uzYfhUSOwAO1bC26fAgk/9XVW1uHFAClf2cSaaveWD2WzNyvd3SSIiIiIiIvWCQhSpvRLawC2ToesVUJwHXwyHb/4KxQX+rqxKGWN49PzOnNAyju17Crl55Gzyikr8XZaIiIiIiEidpxBFaregcLjoDTjvRXAHw5z34d0hkLnW35VVqaAAF69f3Yvm8WEs2ZLN38YswKsVe0RERERERKpUrQxRjDGBxpjbjDGzjDGZxpjdxph5xpi/GWNC/V2fVDNjoNcwuHkixLaAbQvhzcGwbLy/K6tSseFBvHt9byJDAvh+8TZe0Io9IiIiIiIiVapWhijA58DrQCjwAfA+EAQ8C/xgjKmtn0uOR8NucOs0aH8uFGbBp1fDhH9CSZG/K6syrZMiePWqnrhdhpenrOar+Wn+LklERERERKTOqnVhgzGmL3AeMBXoaq29y1p7F9AVmAYM8m1SH4VEw+Ufwen/BVcA/PoKjDgTdq33d2VVZlDbRB4617diz2cLmbtxl58rEhERERERqZsC/F1ABbTw7X+01npLD1prPcaYCcBJQIJfKpOawRjo/xdo2gc+uxHS5sAbg+D8l6DThf6urkpc1685qzL28NFvGxn+wRzevq4XPZrF+rssEREREakD5syZk+J2u4e7XK6zrLX6R6bUVdYYs764uPjpXr16fX+oRsba2jUZpTGmK7AApyfKaaVBijHGDUwGTgBaW2s3H+oeqampdvbs2dVRrvhbXiZ8fQcs982PknoTnPEEBIb4t64qUOzxMmzEH8xcvROA7k1jGNY/hbO7NCQooNZ1OhMRERGRKmSMmWOtTT1Suzlz5qQEBgaOS05OjomJidkTFBRUbIypjhJFqpW1ltzc3LD169cHFBYWntOrV69V5bWrdT9ZWWsXAq8BJwMLjTEvGGNeABYCXYBrDhegSD0TFucM7znrGXAHwex34Z1TYXvdm4Q10O3ijWt6ceuglkSHBjJ/027u+nQ+/Z+awvMTV5KeXbeXfhYRERGRyud2u4cnJyfHJCcnZwYHBytAkTrLGENERERegwYNXAEBAQ8esl1t64lSyhjzD+Bx9gVBFngTeMRam15O++HAcIBmzZr12rBhQ3WVKjXF1gUw9gbIXAOBYXDOc9D9Kn9XVSXyizx8OT+Nkb+sZ/m2PQAEuAxndWnIRT0aERUSSKDbRYDbEOR2Eeh2ERjgItBtSAgPxuXSX44iIiIiddnR9kRZuHDhvHbt2rmCg4OLq6MuEX8rKioKWL58uadr1669yjtf60IU38o77wIXA3cB4wEPcAbwEpAD9LLWZh7qHhrOU48V7oHx98CiMc77blfC2c9CcIR/66oi1lp+X5fJyF/WM2HJNrxH8b97s7gw/nF2e87o1AD9pkFERESkbjraEGXBggXru3btukP/LpT6wlrLwoUL47p169ayvPO1cWLZm4BhwF+stSPKHB/t+x/7Y+Bu4N/VX5rUeMGRcPFb0HIwfHcvLBgNm2fDpSOgQRd/V1fpjDGc0DKeE1rGk7Y7n1G/bWDuxl0UeyzFHi9FJV6KPd6973MKS9iYmcdtH83lhJZx/PvcjnRqFO3vjyEiIiIifqQAReoT3/f7Iac+qY0hypm+/bRyzv3k2/eonlKkVjIGelwNjXvBZzdAxlJ4+1Q480lIvdE5Xwc1jgnl/jPbH7ZNicfL6D828vzElfy2NpNzX/6ZK3o35W+ntyMhIriaKhUREREREamZat3EskDpT3LlLWNceqywmmqR2iypPdwyBXoNA08hfHsPjB0GBVn+rsxvAtwuru2Xwk/3nswNA1JwG8PoPzZx8jM/8db0NRSWePxdooiIiIiIiN/UxhDlF9/+78aYoNKDviWOS4fwlNdLReRggaFw3osw9F0IioSlX8IbJ0LaHH9X5lfRYYE8fF4nfrhrECe3S2RPYQlPfLec01+YzoQl26htcymJiIiIiPhL48aNuzRu3LhK5g4YP358pDGm1z333NOoovd46aWX4o0xvV566aX4yqytrqqNIcprwGqcYT2LjTGvGGNexFni+FJgAfCOH+uT2qjLJXDrNGjYDXZvgHfPgF9fhXoeFrROimDEDX14/4betEoMZ8POPG79cA5Xvv0bS7bU3x47IiIiIlK/9O7du50xplffvn3b+ruW6rBixYogY0yvoUOHpvi7lpqm1oUo1trdQF/gOZxljW8BbsX5LE8CJ1pr8/xWoNRe8a3gponQ90/gLYYJ/4CPL4fcHf6uzO8Gt0vih7sG8ch5HYkJC9w7X8r9ny0gI7vA3+WJiIiIiFSZpUuXBs2ZMyfCGMOsWbMily5dGnTkq6SuqnUhCoC1NtNae6+1tp21NthaG2Kt7WCt/Ye1do+/65NaLCAYznoKrvgYQmJg1QR4fQCs1QixQLeLYQNaMO3ek7lpYAvcxjBm9mYGP/sTL09eRUGx5ksRERERkbrnrbfeSrDWcuONN6Zba3n77bfLm59T6olaGaKIVLn258CfZkKz/pCzDT64ACY9Cp5if1fmd9Fhgfz73I5MvOckhnRMJq/Iw3MTV3LKsz/x9YIt/i5PRERERKTSeL1exo4dG5+QkFD88ssvp8XHx5eMGTMm3uv1HtR2xowZYSeccELb0NDQHrGxsd0uvvjilK1bt5a7Iu7ChQuDhw8f3qRdu3YdIyMju4eGhvZo165dx8cffzypvHt7vV4ef/zxpObNm3cODg7umZKS0vmJJ55IPFzt06ZNCxsyZEir2NjYbkFBQT1TUlI633///Q0LCgoOuxzpSy+9FN++ffsuAOPGjYs3xvQq3VasWBFUkfrrktq4xLFI9YhuAtd/AzOehWn/Bz8/D+tnwNB3IDbF39X5XYuEcN6+LpVfVu/g8W+XsWxrNneOnkdWXhHX9kvxd3kiIiIiIsftm2++idyyZUvQjTfemBEaGmrPP//8zBEjRiSNHz8+8vzzz987CmLmzJmhZ5xxRrvi4mJz7rnnZjZs2LB48uTJ0SeffHLb4uJiExgYuN9ki5988kns2LFj4/v3779n8ODB2Xl5ea7p06dHPfTQQ01XrVoV8sEHH2ws2/7OO+9s/OqrrzZo1KhR0XXXXZeRl5fnevLJJxv37Nkzp7y6R4wYEXvLLbe0CA0N9Q4ZMmR3QkJCyaxZsyKeeeaZRrNnzw6fNGnSaper/D4VqampeTfccEPGiBEjktq1a5d/9tln7y49Fx8f76lI/XWJQhSRw3EHwOAHIOVEGHcLbJ7lrN5z3ovQ+WJ/V1cj9G+dwPg7BvL+L+t5fPxSHv1mKe0aRNGnRZy/SxMREREROS4jRoxIALj++ut3lu5HjBiRNGLEiPiyIcodd9zRPD8/3/Xpp5+uuuyyy7IBSkpK0k466aQ2K1asCG3UqFFR2fsOHz5850MPPZQeEhKyN1wpKSnh1FNPbT1q1KjEf/7zn9vatWtXBLBgwYLg119/vUHz5s0L586duzQuLs7rO55+wgkndDyw5i1btgT85S9/SWnSpEnRzJkzlzdu3Lik9NwNN9zQ9P3330967733Ym+++eZd5X3m/v3758fHx6ePGDEiqVOnTnnPP//8Qd3Nj6X+ukYhisjRSBkAt/0MX98By8fDZzfAmilw5lMQHOHv6vzO7TLcNLAF27LyeXvGOm4fNYev/zKQRjGh/i5NRERERKpIygPf9vJ3DYez/qlz5hzP9ZmZma4JEybEpKSkFAwaNCgP4KSTTspr3rx54Q8//BC7a9eujbGxsd4VK1YEzZs3L7xnz545pQEKQEBAAI899tiW0047LerAe6ekpBw0T0BAQAA33XTTjunTp0dPmDAhsl27djsBRo4cGe/1ernzzju3lQYoAN26dSu86KKLdo4ePXq/YT1vvvlmfF5enuvxxx/fXDZAAXj++efTRo4cmTR27Ni4Q4UoR+NY6q9rFKKIHK2wOLj8I5j1Dkz4J8z7EDbMhIvfgSY1+u+PavP3M9uzdGs2M1fv5LaP5jDm1n6EBLr9XZaIiIiIyDEbMWJEXEFBgeuSSy7JLHv8kksu2fncc881ev/99+PuvvvuHbNnzw4F6Nu370FDawYPHpwbEBBgDzxeUlLCs88+mzh69Oj4NWvWhObl5bms3dds69atgaWvFy9eHApw8sknH3T/AQMG5BwYosyePTscYNq0aRHz5s0LO/Ca4OBg79q1a0OO4ktwSMdSf11zzCGKMSYWaAisttYWlTk+DDgPKABesdb+WllFitQYxkCfW6B5f/j8FshYAu8OgcEPwon3gKt+BwYBbhevXNmT8175mYWbs/jnF4t59tKuGHPYuatEREREpBY63p4eNd2oUaMSAG644Yb9elTceOONO5977rlGH330Ufzdd9+9Iysryw2QmJhYcuA93G43MTExBx2/9tprm3/yyScJjRo1Kjr77LMzk5OTSwIDA+2GDRuCxo0bF19YWLh3wpKcnBw3QMOGDQ/q/dGgQYOD7r1r1y43wLvvvpt8qM+Wl5d3XIvMHEv9dU1FeqI8DVwG7P0PYoy5B3gGKP1Jaagxpq+1dsHxlyhSAyV3glumwOTH4LdXYep/YPUkuPjNej/pbGx4EG9dm8rFr8/k87mb6dI4imEDWvi7LBERERGRo7Zw4cLgefPmhQN06tSpS3lt5s6dG7Fo0aLg6OhoD8D27dsP+vna4/Gwe/fugKSkpL0ByMaNGwM+/fTThPbt2+fPmTNnWVhY2N4uHG+//XbsuHHj4sveIyIiwgNO744GDRp4yp7btm3bQc+MjIz0AqxYsWJR27ZtK31ekmOtv66pSDo0CJhorS0AMM6vmO8FVgMdgJOBIuDvlVWkSI0UGAJnPgHXfgERDWDTb/D6QFjwCdiDeuzVKx0bRfHMJd0AePzbZfy65tDDITfvyuPpH5Zz6nM/MWLmuuoqUURERETkkN56660EgH79+mVfdtllOw7c+vfvn13aLjU1NR/g999/P2iyxJ9++im8pKRkv27Zq1atCrbWcuKJJ2aXDSAAfv3114Pu0blz53yAqVOnHnRu5syZBx1LTU3NBZg2bVr4sXzmstxutwXweDwHdSk/1vrrmoqEKA2Asj/pdPUde9Fau8JaOw34AuhXCfWJ1HytToHbf4UO50HRHvjiVvjsRsiv8DxNdcJ53Rpx60kt8Xgtf/54Lmm78/ee83gtU5dncNP7szjx6am89tMa1mzP5dFvlvLBr+v9V7SIiIiI1Hsej4exY8fGBwQE2M8//3zdp59+uuHAbdy4cWsDAwPt2LFj41u3bl3Uo0eP3Llz50aMGTNm7ySyJSUlPPTQQ40OvH/Lli2LAObMmRPu9e6dJ5Zp06aFffzxx4kHtr/22mszXS4XL730UoPMzMy9P8MvWLAg+Isvvjio18dtt922IzQ01PvII480Wb58edCB59PS0gLmzp172DlREhMTPQDbtm07aG6TY62/rqnIcJ4Dg5fBgAUmlTm2GSdYEakfwuLgsg9h3kfw/d9hyTjY+Cuc9xK0Pd3f1fnN/We0Z+mWbGas2sGtH87mzWtT+Wp+Gh//vpHNu5xQJcjt4qwuDWgeH85Lk1fx0FdLCAl0c1lqUz9XLyIiIiL10ZdffhmVkZEReNppp+1u2LDhQXOOACQnJ3tOOeWU3RMmTIj96quvol5++eUNp512Wvurr7669ejRozMbNmxYPGXKlGiAxMTE/eYyadGiRfHJJ5+cNXXq1Oju3bu379+/f87mzZuDJk6cGHPSSSdlTZw4MaZs+x49ehT86U9/2vbqq6826NKlS6ezzz57V15enuvrr7+O69Onz57p06dHl23ftGnTkjfeeGPdLbfc0rJbt26dBw8enNWiRYvC7Oxs17p160JmzZoVcf/992/p2bPntkN9DaKjo72dO3fOmzVrVuTQoUNTWrZsWWiM4e9//3vGsdZf11SkJ8p6oE+Z9xcCm6y1K8scawjU71/DS/1jDPS8Fm6bAU16w56t8PGl8OXtkL/b39X5hdtlePnKHjSLC2NxWjYDnprC0z+sYPOufJrGhfL3M9vz64On8OIVPbhnSFv+dU4HAB74fCHfLDhoOfpyeb2WL+el8emsjRQUe458gYiIiIjIYYwYMSIB4Prrrz/sEr2l50eMGBE/YMCA/AkTJqzo1atXzg8//BA7evTohA4dOuRNnTp1ZWBg4EFj/ceOHbvu6quv3p6RkRH0/vvvJ61evTrk2Wef3XDnnXdmlPesl156Ke2xxx7b5Ha77ciRI5NmzpwZ9eCDD6bdd9996eW1v+6663bPmDFj2VlnnbVr3rx54e+++27ShAkTYnNzc1133XXX1htuuCGzvOvK+uCDD9b169cv+8cff4x59tlnGz3zzDONduzY4a5I/XWJscc4d4Mx5kHgv8A4IA+4GnjaWvtgmTbzgGxr7UmVWGulSU1NtbNnz/Z3GVKXeT3w22sw5T9QUgCRDeG8F6HtGf6uzC+Wb8tm6Gu/kFfs4ZR2SVzTrzkntUnE5Tp41Z6XJ6/iuYkrCXAZXr+mF0M6HnJScRanZfGvLxczf9NuAJIigxk+qCVX9W1GWJBWcBcRERE5FGPMHGtt6pHaLViwYH23bt12VEdNIjXFggULErp165ZS3rmKhCghwCicHigG+AG4xFqb5zvfC5gFPGKtfaziZVcdhShSbXascnqibP7Ded/tKmcy2tBY/9blB+nZBQAkRx1+SXprLf/3wwremLaGILeLd4elcmKb/YdWZhcU8/yPK/ng1/V4rROexEcEs2xrNgBx4UHcNLAF1/ZrTlRInV2iXkRERKTCFKKIHFqlhih7LzQmCrDW2j0HHE8AGgPrrbVZFbp5FVOIItVKvVKOmbWWR75ewshfNxAS6OKDG/vSp0Uc1lq+mr+F/3y7jB05hbhdhmH9U7jrtDZEBAcwdUUGL01evbdnSmRIADf0T+GGAS2IDT9oTi0RERGRekshisihVUmIUpspRBG/KLdXypMQGuPXsmoqr9fy988XMnbOZiKCA3ji4i58/PsGflvrDN9MbR7L4xd2pkPDqP2us9byy5qdvDxl1d62YUFu7j29HTcMSMFZlV1ERESkflOIInJohwtRjnliWWNMC2PM2caY8DLHAowxjxhj5hhjZhpjLj2OekXqpoQ2cOMPcPp/ICAEFnwMr/WDVZOOfG095HIZnhralXO7NiSnsIQ7R8/jt7WZxIUH8cwlXRlza7+DAhQAYwwDWifwyfB+fHZbPwa3SySvyMNj45fy0FdLKPF4y3maiIiIiIjIkVVkdZ7HgfeBgjLHHgMeAjoDJwCjjTGDjrs6kbrG5Yb+d8CtM6BxKuzZAqOGwtd3QkG2v6urcdwuwwuXd+fMTg0wBq7q24wpfzuJS1Obljsp7YFSU+J4/4Y+vHxlD4ICXHz42waGfziH3MJyV6oTERERERE5rIqEKP2BSdZaD4Axxg3cCswBEoDWQCZwb2UVKVLnJLaFGyfAaY+COwjmjoTX+8Pan/xdWY0T6Hbx+jU9WfDw6TxxURdiwo59bpPzujXi45v7EhsWyJTlGVz25q97J7oVERERERE5WhUJUZKAjWXepwKxwKvW2j3W2nXAl0CP4y9PpA5zB8DAu+DW6dCwO2Rtgg8ugPH3QGGOv6urUYwxx73KTmpKHONuH0BKfBhLtmRz0aszWb5NvX9EREREROToVSREKQbK/jQzGLDAlDLHduD0ShGRI0nqADdPglP+Ba5AmP2ueqVUkRYJ4Yy7fQCpzWPZklXAJa//yvSV2/1dloiIiIiI1BIVCVHWAieXeX8psNJaW7Z3ShOcIEVEjoY7EAbdB8N/ggZdYPcGp1fK13dA/m5/V1enxIUH8dHNffdOWHvD+7P45I+N1MeVykRERERE5NhUJER5B+hujPnDGDMNZ9jOiAPa9AGWHm9xIvVOg85wy1Q45d++uVI+gFf7wvJv/V1ZnRIS6OalK3rwp8Gt8HgtD4xbxMnP/sRT3y9n/qbdClRERERERKRcFQlR3gReAFrhrMbzJvBc6UljzGCgLaB1W0Uqwh0Ig+6F236GJn0gZxt8chWMvQFyNPSksrhchr+f2Z6nh3YlPjyI9TvzeGPaGi58dSYDnprCI18v4fe1O/F4FaiIiIiIiIjDVPZvXI0xQUAokGutrZHriKamptrZs2f7uwyRI/N6YNY7MOlRKM6F0Fg48/+g62VgjrzErxwdj9cya30mPyzexg+Lt7GtzMo9CRFBXNyzCfcMaUtIoNuPVYqIiIhUHmPMHGtt6pHaLViwYH23bt00VYPUKwsWLEjo1q1bSnnnKtIT5bCstUXW2qyaGqCI1CouN/S9FW7/FVqeDPm74IvhMOpS2LXe39XVGW6X4YSW8Txyfid+eeAUvri9P7cOakmzuDB25BTx1vS1XPn2b2RoWWQREREROQ4vvfRSvDGm10svvRTvzzpWrFgRZIzpNXTo0JSquP8999zTyBjTa/z48ZEVvcfQoUNTjDG9VqxYEVSZtR2vCocoxphOxpinjDHfG2Nm+vZPGGM6VWaBIgLENodrv4ALXoOQaFg9EV49AWa+CJ5if1dXp7hchh7NYnnw7A5Mu28wn/+pH41jQpm3cTfnvzKTRZuz/F2iiIiIiNRzWVlZrrCwsB7GmF5/+9vfGvq7nupSE0KoCoUoxpiHgfnA/cAZQD/f/gFggTHmocoqUER8jIEeV8NfZkPnS6AkHyY+BG+dDJvn+Lu6OskYQ6/mcXz1F2dZ5G3ZBVzyxi98vWBLpdw/K7+YTZl5lXIvEREREak/3n///dj8/HyXMYZPPvkkwev1+rukeuOYQxRjzJXAw8Aa4HogBWcOlBTgOmA18LAx5opKq1JE9olIgkvehas/h5jmkL4I3jkVvrsPCrL9XV2dlBARzKhb+nJ5alMKS7zcOXoez0xYjvc4Jp3dsDOXU5+bxinP/cS8jbsqsVoRERERqes+/PDDhODgYHvVVVdt37JlS9C3335b4WEzcmwq0hPlTmAT0Mda+6G1dqO1ttC3/wjoC2wG/lqZhYrIAdqcBrf/BgPuAuOCP96CV/vA0q9BS/RWuuAAN08N7cLD53XEZeDVqWu49aM55BQe+/RP2/cUcu27f7Ajp5Bij+W+zxZSUOypgqpFRERE5Fh99dVXkcaYXsOHD29S3vlRo0ZFG2N63XvvvQ0BRo4cGXP22We3bNKkSZfg4OCe0dHR3QcPHtx6ypQp4VVR35IlS4Lnzp0bcfLJJ+++6667MgBGjBhxyOEtb7zxRlzbtm07BgcH92zUqFGXv/3tbw1LSkrKXaXi66+/jhw6dGhKSkpK59DQ0B6RkZHd+/bt2/azzz6LKq/9rl27XMOGDWuakJDQLTQ0tEf37t3bf/XVV4cNdN588824Xr16tYuIiOgRGhrao2vXru3feeed2CN97qFDh6b89a9/TQH461//mmKM6WWM6dW4ceMuFa2/IgIqcE1n4G1rbbm/8rbWZhljPgduOa7KROTIgsJgyKPQ5VL45q+QNhvGXAttz4Sz/g9iU/xdYZ1ijOGGAS1onRTBn0fNZeLSdC5+bSbvXNebZvFhR3WPPQXFDBvxBxsz8+jSOJrcohJWZ+Tw4uRV/P3M9lX8CURERETkSM4999w9iYmJxV999VXc66+/vtnt3n+FxtGjR8cDDBs2LBPg0UcfbRwSEuLt16/fnsTExOLNmzcHTZw4MebMM8+M+vbbb1cMGTIktzLre+utt+KttVx99dWZqampBe3atcv//vvvY3ft2rUxNjZ2v3E9zz77bMJ9993XPDY2tuSKK67Y7nK5+OCDDxJnz55dbsDzzDPPNNi8eXNQz549cxo2bFickZEROGHChJjLLruszTvvvLP2xhtv3NuFuqSkhCFDhrSZM2dORNeuXXMHDhy4Z/369cGXXnppm969e+8p7/433XRT0/feey+padOmhRdccMHOgIAAO2XKlOhbbrml5aZNmzY/+uij6Yf63BdeeOHurKws9+TJk2NOPfXU3V27ds0HiImJ2ftbzWOpv6IqEqJY4EjrfFb6qj8ichgNOsNNP8Ls92DyY7DyB1j7Ewy6F/rfCQHB/q6wTjmxTSJf/WUgN4+cxcr0HM55aQb/uagzF3RvfNjrCoo9DP9gDku2ZNMiIZwRN/Rmw848Ln3jF96ctoYzOjWge9OY6vkQIiIiIlIut9vNBRdckPnOO+8kf/fdd5HnnXfe3kAgKyvLNXny5OguXbrkdu7cuRDg+++/X9WuXbuisvdYsGBB8IABAzo+9NBDjYcMGbKysmrzer2MHTs2Pjo62nPJJZdkAVx66aU7//Of/zQZOXJk7F133bWztO327dvdDz/8cNPo6GjPrFmzlrZq1aoYYNOmTVt79uzZsbz7v/POOxsO/CxpaWkBvXr16vjwww83LhtCvPjiiwlz5syJOOecc3Z9/fXXa10uJwZ45ZVX4u+4446UA+89ZsyYqPfeey/prLPO2jVu3Lh1ISEhFmDPnj1pJ554Ytsnnnii8bBhwzJbtGhR7soZ11577e5du3a5J0+eHHP++efvvvPOO3ce2OZY6q+oioQoi4BLjTGPWmszDzxpjIkDLgUWHG9xInIMXG7ocwt0OB9+/BcsGgNT/gMLPoGzn4VWJ/u7wjqlRUI4X/x5APeOWcCPS9P56yfzmbZyO49d0JmI4IP/aPV4LfeMmc+va3eSGBnMBzf2ISEimISIYG4+sSVvTV/LfWMX8M0dAwkJPFJOLSIiIlIDPBLdy98lHNYjWRVefeH666/PfOedd5JHjRoVVzZEGTVqVExBQYHrsssu2/uz8IE/tAN069atsG/fvtnTp0+PLigoMKWBwfH6+uuvI7du3Rp05ZVXbi+954033pj5xBNPNPnwww8TyoYoo0ePjsnLy3PdcccdW0sDFICmTZuW3HLLLRlPPvnkQb8BLO+zNG7cuOSss87a9f777yetWLEiqLTNp59+Gm+M4cknn0wrDVAAbr/99p3PP/98g3Xr1oWUvc8bb7yR5HK5eO+99zaU/XpERkZ6H3jgga1XXnll648//jj2n//8Z0ZFvz7HUn9FVSRE+R/wKTDLGPN/wHQgHUgGBgF/BxqgOVFE/CMyGYa+DT2vhW//BjtWwocXQuehcPp/IarerIBW5aJCAnnz2l6M/mMTj41fwri5aczZsIsXr+ixX48Say0Pf72Y7xZtIzI4gA9u7EPTuH3Df+4Z0pZJS9NZlZHDS5NXcb+G9YiIiIj41cCBA/NatGhR8P3338cWFBRsLP2h/9NPP41zu91cf/31e0OUdevWBf773/9uOH369Kj09PSgoqKi/eYbSU9PD2jevHm5vSuO1XvvvZcAcN111+19fosWLYp79+695/fff49cvHhxcGkPmYULF4YCDBo0KOfA+wwaNCjnySefPOj+O3bscD/00EMNJkyYEJOWlhZcWFi432fZtGlTYGkIsWLFitC4uLiSLl26FJZt43K5SE1NzTkwRFmwYEF4RESE5+mnn04+8Lnbt28P8N0z5MBzx+JY6q+oYw5RrLVjjTGtgceA18tp4gX+Za397HgKE5Hj1GIQ3DYTfn0Fpj0Niz+HlT/CKf+E3reAuyIZqhzIGMNVfZvRp0Usd4yez7Kt2Vzy+i/cPaQtt53UCrfL8OLkVXz020aCAly8c30qHRruP69VSKCbZy7tyiVv/MobvmE93TSsR0RERGq64+jpURtccsklmc8880yjzz77LPqaa67ZvXXr1oCZM2dGnXDCCdlNmzYtAdi6dWtA3759O+zYsSMwNTU157TTTsuKioryuFwuvvvuu5gVK1aEFhQUlDuJ67HKzMx0TZw4MaZRo0ZFp59++n7ByBVXXLHz999/j3zrrbfiX3rppS0Ae/bscQMkJycftBJCw4YNDwp18vPzTf/+/dutWrUqtHPnznmXX3759piYGI/b7ebnn3+OnDVrVkRBQcHeLie5ubnuli1b5pdXa2Ji4kHPzMrKcns8HvPCCy8c8re6eXl5FZ4a5Fjrr6gK/RRlrX3SGDMWuBroCkQB2ThDeD621q4+3sJEpBIEBMGJ9zi9UH54EFZ8Cz884AzxOf8laNjN3xXWGa2TIvnyz/15+ocVvPvzOp6ZsIIZq7YzsHUC/5u0CpeBl6/sQd+W5U+c3qt5HDcNaME7P6/jvs+cYT3BAVUzrCe3sIQPf9tAi4RwzujUoEqeISIiIlLbDRs2bOczzzzTaPTo0XHXXHPN7pEjR8Z6PB5z+eWX7+0F8uqrr8Zv37498MEHH0x74okntpW9fvbs2eErVqwIrax63nvvvbiCggLXli1bgtxud7lDqcaOHRv/wgsvbHG73URGRnrA6QlzYLutW7cGHnhs1KhRMatWrQq98sort3/88ccby567+uqrm82aNSui7LHw8HBPZmbmQfeBfT1LyoqIiPCGh4d70tLSFh3+k1bMsdZfURX+VbQvKHm0MooQkSoW2xyu/BhWfA/f3gtb58NbJ0O/P8PgB51VfuS4BQe4+fe5HRnUNpG/jVnAb2sz+W2t83fsfy/qcsTA4t4z2jFleQYr051hPfedUfnDeiYtTeehrxazJavAV1dnru7bvNKfIyIiIlLbdezYsah79+65U6ZMic7KynKNHTs2LiQkxHvNNdfsnZx07dq1wQAXXXTR7rLX5uXlmaVLl1bqP7JHjRqVAHDhhRfuDAoKOmiOlXnz5oWvWrUq9Ouvv4666KKLsktXr5k+fXrEJZdcst/qutOnTz8oUCj9LOedd17Wgefmzp17UPt27drlz549O2LRokXBZYf0eL1e5syZc1D7rl275v78889RGzZsCKzo8Ca3220BPB7PQb17jrX+ijpiVxZjzEMV3P5dWUWKSCVpdxb8+Tfo+yewXvjlJXjtBFg92d+V1SkntU3kh7tO5JT2SQDcd0Y7ruzT7IjXhQS6efqSrhgDb0xby8LNuyutpq1Z+dz24Rxu/mA2W7IKaO5bkvmfXyzmo982VNpzREREROqSyy67bGdBQYHrv//9b/K8efMiTj755Kyyywg3bdq0CGDatGl7f0j3er3cfffdjXfu3Flp4+cXLFgQPH/+/PAOHTrkffHFF+s//fTTDQdujz32WBrAe++9Fw9w5ZVX7g4NDfV+8MEHSWvWrNnbYyQtLS3g7bffTjrwGaWfZebMmfsFDk888UTi8uXLD+pRc/nll++01vLggw829nr3raz82muvxa9du/aguU1uv/32DGst119/ffNdu3YdlEXMmTMnJC0t7bBfs/j4eI/vMxzUA+ZY66+oo/mP+kgF722Bxyt47REZYwxwLXALzpAiN7ABmGatvb2qnitS6wVHwllPQZdL4Zs7IX0xfHQxdL0czngCwhP8XWGdkBARzLvXp5KdX0J0WLm9HMuVmhLHjQNa8O7P67hv7EK+vmPAcQ3r8XgtH/y6nmcnrCC3yEN4kJu/nd6O6/o1Z+SvG3h8/FL+9eVirLVc2y+lws8RERERqYuuv/76Xf/+97+bvvDCCw2ttVx11VX7rVB70003Zb7yyisNH3zwwWbTp0+PTE5OLv7jjz8i1q9fH9K7d++cyhpC8tZbbyUAXHnllQct61vqkksuybrjjjuKJ02aFLNz5053YmKi59FHH910//33N+/du3fH888/P9MYw/jx42M7duyYN3369Oiy119++eW7H3nkkeLXXnutwfLly0NatWpVuGjRorDZs2dHnHTSSVnTpk3br/1f//rXHR9//HH8t99+G9ujR4/2AwcO3LN+/frgiRMnxvTv3z/7l19+2W8iwCuuuCLrp59+Sn/zzTeT27Rp0+XEE0/MatiwYXF6enrg8uXLQ5cuXRo2adKk5Y0bNz5oPpVSgwcPzgkODrZvvfVWcmZmZkBCQkJJTExMyT/+8Y/tx1p/RR3NpConV3A7pTIKLI8xxg2MAkYC4cC7OJPcLgcuq6rnitQpTXrB8J/gtEcgIAQWfgqv9IZ5H0GZJFkqzhhzTAFKqXtPb0eLhHBWpO/hunf/4L/fLmXkL+uZvCyd5duyySk85N8r+1mclsVFr83k0W+Wklvk4fSOyUy85yRuHNiCALeLmwa24KFzOwLw76+W8MGv64+5VhEREZG6rFGjRiUDBw7MLikpMVFRUZ5LLrlkv6Eibdu2LRo/fvyKnj175kydOjV6zJgxCQkJCcXTp09f1rRp08JD3fdYeDwePvvss/iAgAB70003ZR6qXUBAABdddFFmQUGB67333osFuO+++3a89tpr6xISEoo//vjjxAkTJsRce+21O1599dVNB14fFxfnnThx4orBgwdnzZ49O3LUqFGJAN9///2Knj175pX3vEmTJq267rrrMjZt2hT83nvvJaWlpQWNHTt2Vd++fXPLq/GNN97YPGrUqNUdOnTImzp1asw777yT/Msvv0TGxMSUPPXUUxt79+5d7kS1pRo0aOB599131zRt2rTw448/TnzmmWcavfrqqw0qUn9FGWsrZbnqamWMeQB4ErjXWvvcAecCrLWH/QkjNTXVzp49uypLFKldMtfC+Lth7U/O+8apcPbT0Ljc+aqkGsxan8lVb/9Gsaf8P6NjwgJpEBVCoNvJwi1Ou9I/0q2F5duy8VpoGB3Co+d34vRDzMny/sx1PPLNUgAePb8T1/dPqdwPIyIiIjWOMWaOtTb1SO0WLFiwvlu3bjuqoyaRmmLBggUJ3bp1SynvXK1b49QYEw48CPx0YIACcKQARUTKEdcSrv0SFo6BiQ9B2mx4+xTocQ2c+jBEHDRkUqpY75Q4Jt8zmPmbd7N5Vx6bd+WTtit/7+vdecXszjv8fFwuAzcNbMHdQ9oSEXzoP+6HDWiBMYaHv17Cw18vwVrLsAEtKvsjiYiIiIjUerUuRAFOx1lS+XNjTBRwAdAE2AJ8b63N8GdxIrWWMdDtcmh/Nkx/Fn591Rnas/RrGPwA9BkO7mMfmiIV1yw+jGbxB0/qbq1lR04R6dkFeMv0JjQ4k5Qb31zlCRHBNIg+aE6vcl3fPwVj4KGvlvDIN0uxwA0KUkRERERE9lMbQ5TS8QWxwAqgbP/0XGPMrdbaUdVflkgdERwJQx6FHtfChAdh1Y8w4R8wZySc9X/Q6mR/V1jvGWNIjAwmMTK4Uu97Xb8UDM78KI9+s5T3Zq4jOTKEpKhgkiJDSI4KITkqmOSoEJrHh9EkVktji4iIiByPe+65p9GR2sTExJQ89NBD6ixQQ9S6OVGMMW8AtwIe4HvgXmAbcCbwBhAB9LbWzj/guuHAcIBmzZr12rBBS3qKHJWVE+CHB5x5UwDanQOnPw7xrfxbl1SZUb9v4NFvllJUcvgJhk9qm8itg1rSr1U8prT7i4iIiNQKmhOlZjDGHHESwkaNGhWlpaUtqo56xHG4OVFqY4jyFs6yxluA1tba/DLnbsUJUt6z1t50qHtoYlmRY1RSCL+95gzzKcoBVyD0vRUG3QehMf6uTqpAfpGH9OwCZ9tTSIbvdcaeQrZlFbBg824Kip2QpXPjKG45sSVnd2m4d6JbERERqdkUoogcWp2aWBYoXVJqUtkAxecbnBBFS4qIVKaAYBh4N3S7EqY8DvNGwa+vwPyP4eR/QK8bwF0b/ziRQwkNcpOSEE5KQni553flFvHRbxsY+et6Fqdl89dP5vP0Dyu4YUAKV/RpdtiJbEVEREREaqva+CvDlb59VjnnSo+FVlMtIvVLZAO44FW4dRo0Hwj5mfDdvfDGAFg9yd/VSTWKDQ/ijlPb8PPfT+HJi7vQMjGctN35/OfbZfR7cjJ/G7OA135azQ+Lt7Ji2x4Kij3+LllERERE5LjVxl8V/uTbdyjnXOmxjdVTikg91bAbDBsPy8fDj/+C7cvho6HQeogzKW1yJ39XKNUkJNDNlX2acXlqUyYvz+Dt6Wv5Y30mn8/dvF87Y6BRdCgtE8NpkRDOwNYJDGqbSEig20+Vi4iIiIgcu1oXolhrVxljfgJONcacbK2dCmCMCQQe8TX73E/lidQfxkCH86DN6fD7mzD9GVg90emR0v0qZ5hPdBN/VynVxOUyDOmYzJCOySxOy2L+pt2s25HLuh25rN+Ry8bMPNJ255O2O58Zq3bwwa8biAwOYEjHZM7p2pCBbRIIDlCgIiIiUhNZazWJvNQbvnljD7nCQq2bWBbAGNMe+AVnJZ7Pga3AqUBXYCpwurW25FDXa2JZkSqQuwOmPQ2z3wVvCbiDnclnT7wHQmP9XZ34WbHHy+Zd+azbkcPSLdn8sGQbi9Oy956PDAng9I4NOLdbQwa0SiAooDaONhUREak9jnZi2YULF85r166dKzg4uLg66hLxt6KiooDly5d7unbtWu5cq7UyRAEwxrQC/gOcBkQBG4BRwFPW2sLDXasQRaQKZa6FyY/DknHO+5AYOPFv0Gc4BIb4tTSpWdbvyOXbRVsZv3Ary7buC1RiwwIZ1r8Fw/qnEB0W6McKRURE6q6jDVHmz5//RMOGDa9ITk7OrI66RPxtx44dMWlpadO7det2Y3nna22IcjwUoohUg7Q5MPFhWD/DeR/VBE75J3S9HFwatiH7W7M9h28XbmX8wi2sTM8BICI4gOv6NeemgS2Ijwj2c4UiIiJ1y9GGKHPmzEkJDAwcl5ycHBMTE7MnKCioWEN7pC6y1pKbmxu2fv36gMLCwnN69eq1qrx2ClFEpOpY68yRMvFhyFjiHEvqBKc9Am2GOPOqiJRhreX3dZm8MmU1P6/eAUBooJur+jZj+KCWJEcdvjeTxmyLiIgcnaMNUcAJUtxu93CXy3WWtVbjtKWussaYdcXFxU/36tXrh0M1UogiIlXP64GFY2DqfyFrk3Os+QA47VFo2tu/tUmNNXfjLl6dsprJyzMACHK7uKx3Ewa3TWJ7TiHp2QWkZxeSkV1A+p4CtmUVkpVfxOkdG/DI+Z1IjFTvFRERkUM5lhBFRPZRiCIi1ae4AGa9AzOehfxdzrEO58GpD0NCG//WJjXW4rQsXp26mu8Xbzvqa2LCAnn0/E6c362ReqaIiIiUQyGKSMUoRBGR6leQBTNfhF9fg5J8MG7ocQ0MfgCiGvm7OqmhVqbv4Z0Za9mWXUhyZDDJUSEkR4fsex0VQlGJl39+uYgZq5yhQKd1SOaJizqTdIRhQCIiIvWNQhSRilGIIiL+k70Vpj0Fcz8E64GAEOh9Mwy4CyIS/V2d1FLWWsbM3sR/xi9jT2EJUSEBPHxeJy7u2Vi9UkRERHwUoohUjEIUEfG/HatgyuOw9CvnfWA4nHAb9L8DQjV3mVTM1qx8Hhy3iJ9WbAfg5HaJPHlxVxpEq1eKiIiIQhSRilGIIiI1x9YFMPUJWOmbDDs42glSTrgNgiP9W5vUStZaPp+bxmPfLCG7oITwIDeD2iYyoHUCA1sn0Dw+TL1TRESkXlKIIlIxClFEpObZ9AdM+Q+sm+a8D42DgXc7Q32Cwvxbm9RK6dkF/OvLxUxcmr7f8cYxoQxsncCANgn0bxVPfHgQO3KK2LQrj8278tm8K49Nmc4+PbuAbk1iuGVQS9omK9QTEZHaTSGKSMUoRBGRmmvddCdM2fS78z6iAQy6F3peBwFavlaO3YaducxcvZOZq3cwc80OducV73c+OMBFYYn3iPcZ3C6R4YNa0q9lvHqyiIhIraQQRaRiFKKISM1mLayaCFP/4wz3AYhuCifdD92uBHegf+uTWsvrtSzdms3Pq3cwc/UO/liXSWGJl5iwQJrEhtI0NowmsaE0iQ2jaVwo0aFBfDU/jTGzN1FQ7AQtnRtHMXxQK87u3IAAt8vPn0hEROToKUQRqRiFKCJSO1gLy8fDlP/C9mXOsbiWMPhB6DwUXG7/1ie1XmGJh2KPJSI44LDtMnOL+Oi3DYz8ZT07c4sAZ1jQNSc0p1FMCAEuFwFuQ6DbEOByEeh2Eeg2NI0LI1lLLYuISA2hEEWkYhSiiEjt4vXAki+cCWgz1zjHEtvD4AegwwXgUm8AqR4FxR7GzU3jnRlrWbsj94jtA1yG205qxV9OaU1IoEI/ERHxL4UoIhWjEEVEaidPCSz8BH76P8ja6ByLb+NMQNv1Mg3zkWrj9VomLUtn4tJ0Ckq8lHi8FHssJV4vxb7XhSVeFmzaDUCLhHCeuKgL/VrF+7dwERGp1xSiiFSMQhQRqd1KimDeh/Dz//aFKdFNof+d0PNaCAz1a3kipWavz+TBcYtYlZEDwOWpTXnw7PbEhAX5uTIREamPFKKIVIxCFBGpGzzFsPhzmPE87FjhHAtPhBNuh943QUi0f+sTwZl35Y2f1vLq1NUUebwkRATx8HmdOLdrQ63yIyIi1UohikjFKEQRkbrF63UmoJ3xHGyd7xwLjoa+w6HvnyBcQyjE/1Zn5PCPcYv4Y30mAKe0T+Ly3k0JC3ITGugmJNBNqO91aKCbsGA3wQGaR0VERCqPQhSRilGIIiJ1k7WwZorTM2XDz86xwHDofSP0uwMik/1bn9R7Xq/l09mbeOK7ZewpKDls2wCX4dLUptx7elviI4KrqUIREanLFKKIVIxCFBGp+zb+BtOfhdUTnfcBIdDzehhwJ0Q38W9tUu9lZBfw2k9r2Lwrj/xiD/lFHvKLvRTsfe0hu6AYayEqJIC7TmvLtf2aE+g+8kpUizZn8fnczWzZnU+Rx0thsZcij5eiEi+FJR6KSrx4LfRqHstpHZI5qV3iEZd4luOTV1RCcIAbt0vDt0TEvxSiiFSMQhQRqT+2zHPClOXjnfeuQOh+lbOiT1wL/9YmchirM/bw2PhlTF+5HYDWSRE8dG5HBrVNPKhtbmEJXy/Ywse/b2RRWtYxPSfI7aJfq3hO65jMkA7JNIgOOWx7r9eyPaeQzbvy2Lwrf++Wtjufzbvy2FNQwq2DWnLziS2PqY66anFaFpe9+Ssnt0/i1at6+rscEannFKKIVIxCFBGpf9KXOHOmLB4HWDBu6HQh9L8DGvXwd3Ui5bLWMmV5Bo+PX8r6nXkAnNYhiX+d05GUhHCWbMni49838tX8LeQUOsODokMDGdqzCb1TYgkOdBEc4CYowEVwgIugABdBbheFJV6mr9zOxKXpzNm4i7L/LOjSOJpezWPJKyohO7+ErPxisguKnX1+MXsKSziaf0b8+9yO3DSwfgeVJR4vF742k8Vp2QCMurkvA1on+LkqEanPFKKIVIxCFBGpv3ascuZMWTQGvL45KVJOdMKU1kPAdeThEiLVrbDEw/sz1/PylNXkFJYQ6Da0Topk2dbsvW1Sm8dyVd9mnN2lISGBRz8h7c6cQiYvz2DS0nRmrNpBfrHniNfEhQfRNDaUJrFhNI4NpYlvaxwTxpwNu/jHF4sA+M+FnbnmhOZHVUfpfDFea7m4RxNCg2r/pLpvT1/Lf79bhttl8HgtnRpF8c1fBuLSsB4R8ROFKCIVoxBFRCRrM/z+Bsx+H4r2OMcS2kG/P0PXyyHw8EMaRPwhY08Bz/ywgrFzNgMQGRLA0J5NuLJPM9o1iDzu+xcUe/h51Q7W7sghMiSQ6NBAokr3oQFEhwYSERxAwBHmZvng1/U89NUSAJ65pCuXpjY9bPv07ALuGTOfmat3Ak5Ic32/FK7r15zY8KDj/lz+sCkzj9NfmE5+sYfXr+7Jo98sZVt2Ac9f1o2Le2peJhHxD4UoIhWjEEVEpFRBFsz9AH57HbLTnGPhidBnOPS6ASIOnn9CxN+Wbc1m8658BrZOqLE9Nkp7YbgM/O+KHpzfrVG57SYtTee+zxawK6+Y+PAgmsSGsmCzM69LWJCbK3o34+YTW9AoJvSga4s9XhalZfHb2p38vjaT9OwCTu/UgMt7N6VxOe2ri7WW60fMYvrK7ZzXrREvX9mDz+Zs5t6xC2gYHcLUewcfU28hEZHKohBFpGIUooiIHMhTDEu+gF9egm3OUATcwdDlUjjhNmjQxb/1idRCL09exXMTV+J2GV69qidndm6w91xBsYcnv1vGyF83AHBimwSeu6wbiRHB/LY2kzemrWGab1LdAJfh/O6NuHlgS/KKSvh9XSa/rd3JnA27yCs6ePiRMTC4bSJX9GnGKe2TjmpVo0Pxei2/rdvJsq17uDS1CVEhgUe85st5adz16XyiQwOZdM9JJEYG4/Faznv5Z5Zuzea+M9rx55NbV7gmEZGKUogiUjEKUUREDsVaWDfN6ZmycgLg+/My5UToexu0Owtc+g2yyNF6dsIKXpm6mkC34a1rUzm5fRKr0vdwx+h5LN+2h0C34f4z2nPTwBYHzRWydEs2b05fwzcLtuA9xD9dWiaGc0LLePq2iCMmLIjP52zmh8XbKPJ4AUiKDOay1KZc3rspTePCjqpmay3Ltu7hq/lpfDV/C9uyCwBomxzBu9f3Pux9MnOLOO35aWTmFvH00K5c1nvfUKaZq3dw9Tu/ExEcwLT7BhMfEXxU9YiIVBaFKCIVoxBFRORo7FwDf7wF8z6CohznWExz6Hsr9LgGQqL9W59ILWCt5b/fLuOdn9cRFODihv4pjPx1PQXFXlokhPPSFT3o0uTw/y9tyszj7Rlr+XJeGg2iQ+jbIp6+LePo0yKOpMiD5y/KzC1i3NzNjP5jI2u25wJO75QujaNpmRBOs/hwUuLDaB4fTvP4MOLDgzDGkLY73wlO5m1hRfqevfdrEhtKgMuwfmce8eFBvHVdL3o1jyu31r+NWcDnczdzQss4Rt9yAsbsHwwNG/EHP63YznX9mvPYBZ2P9cspInJcFKKIVIxCFBGRY1GQBfNGORPR7naGHhAYDt2ucOZOSWrv3/pEajhrLQ99tYQPf9uw99glvZrw6PmdCA8OqNLnzlq/i0/+2Mj4RVspKvGW2y4iOICkqGDW+gIXgJiwQM7t2pALuzemV/NY9hSW8OdRc5mxagdBbhdPX9KVC3s03u8+P6/awTXv/k5QgIsJdw2iRUL4Qc9asW0PZ704HWMMP949iFaJEZX7oUVEDkMhikjFKEQREakIrwdW/uAM9Vk/Y9/xFic5YYqG+ogcktdreWz8UiYs2cYDZ7Xngu6Nj3xRJcouKGbZlmw2ZOaxYWcu63fmsXFnHut35rKnwFnuPDjAxWkdk7moe2MGtU0kKGD/uVRKPF4eG7+UD3zzuNx5SmvuOq0tLpchv8jDGf+bzsbMvCPOefLguIWM/mMTQzom8/Z19fNnmbyiEgJcroO+xiJStRSiiFSMQhQRkeOVvhRmvQ0LPoHiPOdYdDPofRP0vA7Cyu/qLyI1i7WWXXnFbNmdT/P4MCKPYuLYkb+s59FvluC1cE7Xhjx3aTf+N2kVb0xbQ7vkSMbfOfCwk9lmZBcw+NmfyCvy8OnwE+jbMv6QbbfvKaSg2HPU87nUZEUlXqYsz+DzuZuZujyDiJAA7jilDdee0Fxhikg1UYgiUjEKUUREKkv+bpj/sTN3yq51zrGAEOh4AfS8Hpr3dyZjEJE65acVGfzl43nkFJbQoWEUK9P34LWWcX/qT49msUe8/n+TVvK/Savo1iSaL24fsN+kuiUeLz+t2M4nszYxdUUGwEGrG1W1gmIPn8/dTNqufNwug8sY3K4ymzEEuA2NYkJ988yEERxwcE88ay0LN2fx+dzNfL1gC7vzig9q0ywujPvPbMc5XRoeNIeMiFQuhSgiFaMQRUSksnm9sHqSE6asnrjveHwbp2dK96sgPMF/9YlIpVuZvocb35/F5l35AAzrn8Ij53c6qmvzikoY/MxPZOwp5MUrunNB98Zsyszj01mbGDtnE+nZhQC4DHgtBLoNb1zTi1M7JFfZ5wEn9Phu0Tae+mEZmzLzj/o6l4HGsaGkxIfTMiGcFgnh5Bd7GTd3M6sycva2a98gkqE9m3BBj0Ys2pzFk98vZ7XvfLemMfzz7A70aaGefCJVRSGKSMUoRBERqUqZ65wVfeZ9BDnbnGOuQGh/ttM7peXJ4FLXdZG6YEdOIfeNXcCeghLev7EPEccwUe4nf2zkgXGLaBQdQqukCGas2rH3XIuEcC7v3ZShPZvw5rQ1zupGbhdvX5/KSW0Tq+KjMH/Tbh4fv5Q5G3YBzpLO53ZthLXgsRav11LitXitxeO1FJZ42JSZz/qduWzKzDvkMtTx4UGc370RQ3s2oVOjqP16m5R4vIyZvZnnJ65kR44THA3pmMzfz2xP6yRNuitS2RSiiFSMQhQRkergKYFVP8Lckc7e+lYGiW4GPa6G7ldDTFP/1igifuPxWs5+ccbe5ZSDA1yc3aUhV/RuSp8WcXvDBmstj3y9hJG/biA4wMWIYb3p37ryeral7c7n6R+W89X8LQAkRARxz5B2XJbahIDDzO1SVlGJl42Zeazfkcu6Hbms3ZFLUYmXMzs3YHC7xMPOEQOQW1jCW9PX8tb0teQXe3C7DG2SImgWF0bz+DCaxYXRLD6cZnFhNI4J1RwqIhWkEEWkYhSiiIhUt+wtzjLJ8z6A3Rt9Bw20Ohl6XAvtz4GAYL+WKCLVb3FaFi9NXsXANglc0K0x0WHlT2zr9Vr++eViRv+xkdBANyNv7HPYYS/ZBcV88sdGflySTkigm9jwIOLCAokLDyYuPNB5Hx7EL6t38vaMtRSWeAkKcHHzwBb8aXCro5pgtypkZBfwwqRVjJm9Cc8huraUDh1qlxxJ+wZRtGsQSYeGkaTEhx916FOZMnOLWJm+h1Xpe1izPZeokACax4eTkhBG8/hw4sODNNeL1BgKUUQqRiGKiIi/eL2wbhrM+xCWfQOeIud4aBx0vRx6XgvJRzengojUL16v5f7PF/LZnM2EB7n54Ka+9Gq+/yS2mzLzGDFzPZ/O2khukeeo731et0bcf0a7GrMKUHZBMet35LIxM48NO/PY5NtvzMxja1Z+uUOHggJctEmKoF2DSFolRpAcFUKDqBAaRAeTHBVy3MFQQbGHJVuyWLrVCUxWpeewKmMPO3KKDntdZHAAzX2BSpukCK7rl0JceNBx1XI0PF5L2q581mzPYc32HIID3Vzdp9l+kxhL/aMQRaRiFKKIiNQEeZmwaCzM/RDSF+073riXMxlt56EQHOm/+kSkxvF4LfeMmc9X87cQGRzAqFv60rVJDPM27uKdn9fx/aKtewOGfi3jub5/c8KCAsjMLdq35RWRmePsI4MDuP3k1geFMTVZUYmXDTtzWb5tDyu27WH5tmyWb9uzd4LfQwkPcpMc7QQrTWJDaRobRrP4MJrGhdE0NoyEiH09Rjxey+qMHBZs2s38zbtZsGk3K7btoaSc9CY8yE3r5EjaJkXQKimC3MIS1u/MY8NOZ2jTnoKS/do3iArhlat6kJpSeRPoFpZ4mLg0nZXbnN4wa7bn7B1SVdbVfZvxnws7q2dMPaYQRaRi6kSIYoz5ErgA2GmtPeLAYIUoIlJjWQtb5zthyqLPoDDLOR4YDp0vhl7DnGBF/+gVEZzJWO/8ZB7fLdpGdGggbZIimO2bDDbAZTivWyNuGtiCzo2j/Vxp9dpTUMzK9D0s27qHTbvySM8qYFt2AenZhWzLKiC/+PA9c0ID3TSNCyUiOIDl2/aQd0BPHpeBtsmRdGoUTbsGEbRJjqRtciSNokMOGUpYa9mVV8z6nbls2JnLR79tZM6GXbhdhntPb8etg1oed8+Q3XlF3DRy9t4JgctqEBVCqyRnLplxc9MoLPFy88AW/POcDtUSpOQWlpCxp5CkyGDCj2HSZak6ClFEKqbWhyjGmCuBj4AiIFchiojUGUV5sOxrmDMSNv6y73hSR2dln66XQZiW/xSp74o9Xm4fNZeJS9MBiAwJ4Kq+zRjWP4WG0aF+rq7msdaSXVBCenYBW7MK2LzLGRq0OTOfjZnO66z84v2uaRIbSremMXRvEkO3pjF0bhxFWNDxBQHFHi/P/riCN6etBWBwu0Sev6x7hYf3bN6Vx/Xv/cGa7bk0iAphaK/GtEqMoFViBC0Tw/cbwjR1RQbDP5hNscdy5ymtuef0dsf1WcAZ4jR3wy6WbMkmPbuAjD2FpGcXsN23Lx1SFhEcwA0DUrhpYAtiwqp+KJMcmkIUkYqp1SGKMSYJWAKMAi4EIhSiiEidtGOVs7LP/NGQ51v61B3sTELb/WpnUlqX2781iojfFJZ4eG3qGuLCg7ikVxP9pv84ZeUXs8kXprRrEElCRNVN9j1leTr3jFnA7rxiGkSF8PJVPeh9jMN7lm7JZtiIP8jYU0i75Ejev7H3EQO0HxZv5c8fz8Pjtdx/ZjtuH9z6mJ7p9VqWb9vDz6u3M2PVDmatz6Sg2HvI9sEBLmLDgtiWXQA4Ycqw/k6YElsN88LIwRSiiFRMbQ9RxgK9gU44YYpCFBGp20qKYMV3MPcDWDMF8P0ZHtnQmYy2+9WQ2NavJYqIyLHZsjufO0bP2zu852+nt+W2Qa2OanjPzNU7uPXDOeQUltC3RRxvXZdKdOjRTZz75bw07h4zH2vh4fM6csOAFodtn5VXzI9LtzFj1Q5+WbPjoIl0OzSMondKLI1iQkmKdCbxTYoMJikqhKiQAIwxzF6fyYuTVzFjlfMLgfAgN8MGpHDzwJYHhSmlc96syshhdUYOu/KKaJ0UsXclpohqCAszsgt46vvlLEzLwuO1+20lXovH68VrIT48iMaxoTSJDaVxTKjvtbMMd3JUCO4aOImvQhSRiqm1IYoxZijwGXCmtXaCMWY9ClFEpD7ZvQkWfgLzP4bMtfuON+kN3a+CThdDaIzfyhMRkaNX7PHy3I8reWPaGgD6tIjj4h6NObl9EslRIeVe89X8NO4du4Bij+Wcrg15/rJuBAccW6/ET/7YyAPjnAnNn7q4C1f0aXZQXdNXbmfc3DQmLkvfb4LaBlEhDGyTwIltEujfKoHEyKPvsTNnQyb/m7R/mHL1Cc0JDnDtXe1ow868cifwLdU0LpT2DaJo38BZ4rpZXBgxYYFEhQYSGRxwXHPMWGsZM3sT//12GdkHTAh8rAJchn+f25Hr+6cc130qm0IUkYqplSGKMSYep+fJRGvttb5j61GIIiL1kbWw8TeYPwqWfAFFOc5xdzC0OxO6XAZthkBA1XVHFxGRyjF1eQb3jJnPrrx987J0bhzFKe2TOaV9El0bR2MMvD1jLU98txyAGwe04F/ndKhwaDBi5joe/WYpxsALl3Xngu6NWLIlm8/nbubr+VvYmev0ODEGBrRK4LQOSQxsk0irxPDjnpR2zoZdvDh5FdNXbj/onDHQLC6M1okRtE6OICY0iNUZOSzfls2q9ByKPIcePuQyEBUaSHRoIDGhgcSEBXFCy3gu6tGYBtHlh1KlNuzM5cFxi/hlzU4ATmmfxN2ntSUs2E2Ay+AuuxmDMYYdOYWk7cpn8648Nu/O973OJ213Ptv3FPLiFd25oHvj4/paVTaFKCIVU1tDlFHAEKCjtXaH79h6DhOiGGOGA8MBmjVr1mvDhg3VVK2ISDUqyoVl42H+R7BuBnuH+4TEQKcLnSE/TU8Al8uPRYqIyOHszClkwpJ0pixP5+fVO/abayQhIpi2yRF7f8D/1zkduPnElsf9zNd+Ws3TP6zA7TK0SAhndUbO3nOtkyIY2rMJF/ZoVGWTFc/duIsv5qYRExZI66QIWic5k+KGBJbfs6bY42X9Dmd56+Xbslm+dQ9bswrIyi8mK7+YnMLye48YAwNbJzC0ZxPO6NSA0KB99y/xeHlv5jqen7iSgmIvceFBPHxeR87v1ui4wqIC32pQh/os/qIQRaRial2IYow5D/gauNpa+3GZ4+tRTxQRkX2y0mDxZ7BwDKQv3nc8uil0ucQJVJI6+K8+ERE5ooJiD7+u3cmUZRlMWZ5B2u58AALdhucu68753RpV2rOe+3EFL09ZDUBsWCAXdG/MxT0b06VxdLUsg1yZij1esn2BSlZ+MVt2F/Dtoi1MWpqxtwdLeJCbs7s0ZGivJkSFBPLAuIUs3JwFwAXdG/HQuR2Jr8JJhf1NIYpIxdSqEMUYEw6sBBZYa88+4Nx6FKKIiJQvfYkTpiz6DLI37zue3Bm6XOqEKtFN/FefiIgckbWWlek5/Lx6Bz2axdCzWWyl3//bRVsJDnBzUttEggLqXq/F3XlFfLNwK5/P2cz8TbsPOt8oOoT/XNSZU9onV39x1UwhikjF1LYQJQVYdxRNs6y1MYc6qRBFROotrxc2/uIEKku/hIKsfeeaD3AClY4XQNixLa8pIiJS26zZnsO4uZv5Ym4aW7MLuKZvc+4/sx2RIUe3ulFtpxBFpGJqW4gSD/zfIU5fDgQCHwF51to7D3UfhSgiIkBJIaye5AQqK3+AkgLnuCvQmYi208XQ9gwIifJvnSIiIlXI67UUe73HvLJRbacQRaRialWIcjgaziMichwKsmH5eCdQWTcNrG8SQ3cwtD7V6Z3S9kwtmSwiIlJHKEQRqRiFKCIisr896c5Qn6VfwYZf2LvCjysQWp0MHS+EdmdpyI+IiEgtphBFpGIUooiIyKHtSYdlX/sClZn7eqi4AiBlIHQ4D9qfC5EN/FuniIiIHBOFKCIVU2dClGOhEEVEpAJytjtDfpZ+CetmgPXsO9ekD3Q41wlU4lv5rUQRERE5OgpRRCpGIYqIiBy7vExnMtpl38CaKfsmpQVI6gTtz3EmpW3UE1x1b4lMERGR2k4hikjFKEQREZHjU5jjrPKzfDysnACF2fvOhSVA69Og7enQ6lRNTCsiIlJDKEQRqRiFKCIiUnlKCp2hPit/gFUTYPfGfeeMG5qdAG1Od3qpJLYHY/xXq4iISD2mEEWkYhSiiIhI1bAWtq9wwpSVP8LGX/efRyW6mdNDpc0Z0OJECAz1X60iIiL1jEIUkYpRiCIiItUjfzesneoEKqsnQu72fecCQqDFIKeXSpvTIba538oUERGpDxSiiFSMQhQREal+Xi9smQerfnR6qmyZt//5+NbQ6hRnHpWUgRAc4Z86RURE6iiFKCIVoxBFRET8b0+6MzntqgmwZur+k9O6AqFpX2h1MrQ+FRp004o/IiIix0khikjFKEQREZGaxVMCaXOcpZPXTHZeW+++86Fx0PIkaHmyE6zENPNfrSIiIrWUQhSRilGIIiIiNVv+Llg33QlVVk+BrI37n49vvS9QSTkRQqL8U6eIiEgtohBFpGIUooiISO1hLWSu9fVSmQrrZ+w/9Me4oXEvZ5LaFoOgaR+t+iMiIlIOhSgiFaMQRUREaq/SoT9rpzqhyuZZ+y+j7A52gpQWJzmhSuOe4A70X70iIiI1hEIUkYpRiCIiInVHQTZs/NUZ/rNuOmxbBJT5ey4w3AlVmg+A5v2dXiuBIX4rV0RExF8UoohUjEIUERGpu/IyYf3P+0KVHSv2P+8OgsapTqDSvL8TsARH+qdWERGRaqQQRaRiFKKIiEj9kZMBG36BDTOdffoS9uupYtzQsOu+nirN+kFYnN/KFRERqSoKUUQqRiGKiIjUX3mZsOl3J1RZPxO2Lth/ThWAxA7QvJ8TrDTtC9FNwBj/1CsiIlJJFKKIVIxCFBERkVKFObD5D9jwq9NTZfMs8BTu3yayITTp7Qz9adIHGnbTvCoiIlLrKEQRqRiFKCIiIodSUghb5vmG//zqBCwFWfu3cQU6Q4Ca9IGmvZ29equIiEgNpxBFpGIUooiIiBwtrxd2rnbClE1/OD1VMpax37wqoN4qIiJS4ylEEakYhSgiIiLHoyAL0ubApllOuLJ51sG9VdxBkNwZGnSG5C6+fScIifZPzSIiUu8pRBGpmAB/FyAiIlKrhURDq1OcDXy9VVb5eqr84YQr25fDlrnOVlZMMydcKQ1YkjpBXAtwuav/c4iIiIjIEaknioiISFUryIKtCyF9sbNtW+wMAzpw0lqAgFBI6gDJHX0BSycnXAmPr/66RUSkzlJPFJGKUU8UERGRqhYSDS1OdLZSnhJnfpX0xbBtEWQshfQlkJ1Wfq+ViGQnXEns4OyTOkJSewiOrN7PIiIiIlKPKUQRERHxB3eAE4IktYcul+w7nr8L0n2BSsYS334Z5KQ729qf9r9PdDPnHontfSFLe0hsB0Hh1fpxREREROoDhSgiIiI1SWgspAxwtlJeL2RtcsKUjKXOHCsZS2H7Ssja6GyrfixzE+PMt1IaqiS0hbiWzhaRpOWXRURERCpIIYqIiEhN53JBbHNna3fmvuOeEshcC9uXQcbyffudq2D3Bmdb+cP+9woM9wUqLfYFK/GtIL61M2RIAYuIiIjIISlEERERqa3cAZDY1tk6XrDvuKcYdq7ZF6pkrnHClsy1vuFCi5ztQEERvlCl9b5gJa6VcywsTgGLiIiI1HsKUUREROoad+C++VY6HXAuLxN2rYPMdfuClZ1rnElu8zNh20JnO1BwNMSl7Ou9EttiX4+WiAZObxkRERGROk4hioiISH0SFudsjXsdfC4v0xeqrN4XrGSucQKXwizYusDZDuQO9g03Sjl4i24KIVFV+pFEREREqotCFBEREXGUBixNUvc/bu2+gCVzra8nS+nr9ZC7HXasdLbyhEQ7qwhFN4GYpk6wUrqPauzMxaKeLCIiIlILKEQRERGRwzMGwuOdrWnvg88X5jiT2O5a72yZ6/a9ztoEBVlQcIh5WABcARDZCKIbO6FK6T6qkXM8soETtLj1zxYRERHxL/1rRERERI5PcAQkd3K2A1kLuTucMCVrE+z27bM2w+6NkJ0GeTv3LdV8SMZZnjmyobNFNYTwJIhIhPDSLQnCE5yeL5oEV0RERKqAQhQRERGpOsY4QUdEIjTuWX6b4gInTMlOg6w0yN7s7Pds9W3bICcDctKdbev8wz/THeSEKmHxTqgSluDbH/C+tI1CFxERETlKClFERETEvwJDfEsqtzp0G0+xE6Ts2QZ7tuwLVnIznJ4uZV8X5ewLZY6GK9AXsCQ6Q5bKhi6hsc5+vy0OAoIr57OLiIhIraIQRURERGo+d6AzV0p0Y6CclYXKKspzJrvN2wG5O337Hb5jO53XpcfydjqhS842ZztagWEQEgOhMb59bJnXMU7vluAoZ793870PitREuiIiIrWUQhQRERGpW4LCIKi5s+zy0SguKBOqlAYvB26Z+78vznO2PVsqUKDxBSxRB+x9QUtwpPN+v71vC4mCYF87d2AFni0iIiLHQyGKiIiI1G+BIc7yy9FNjq69tU7vlfzdULAb8neVee3bF2Q7qxIV+vZlt6IcKMxytuOqO+yAHi9REBTh28IgKBwCw519UJjvdRgEhjqvA0OdewSG+o6HOfPJaH4YERGRQ1KIIiIiInIsjNnXM4Smx3691+MLV8oGLdn7ApfCPb4tu8zrPWUCGV/bvb1htlbiZ3PvC2ECfUFMUPi+sCUwFAJC9u0DQpwQKiB03/nA0HLehzjzyLiDnX1AsAIbERGplWpdiGKMaQxcBpwDtAOSge3ANOAJa+1iP5YnIiIicngut28OldiK36O0N8yBPV6KcqAo15kXpjh33+uiHF/oku8cK873bXn7tqI88BZXTi+Zo+UOcgIWd5DvdZAvaPHtyx5zB+4LX/a29x0vPX/QsaB9x92BzuYq732Ab1/e+wCFPSIisletC1GAO4C/A6uA74FMoDNwBXCxMeYsa+1UP9YnIiIiUrXK9oaJblx59y0p8oUveb6wpezrPCgpcLbiAijJL39f2m5vUJPvnCspAk8hlPg2bzF4ipytpjMuJ1RxBTibO2Dfa1fAvrBlbwDje+1yH3Deve8a43beG5dv795/v/d1wCHeB+y7tuz9yh7f756ug59Reny/ekr3xrlP6cYB7w9q4y7/vIhIHVMbQ5Q/gEHW2hllDxpjLgXGAK8BHfxRmIiIiEitFuDr+XE8vWSOlte7L1TxFO3bH/a1b+8pdJa9Lin0vS454FiR89pT+rrE2XuLfcdLzxfvO+YtKfO+ZN9x6wHrq9VTWPVfl7qmvIClNHzB+F6bowxqygQ2e88dEETtvfaA9qXPLX3m3j1l3h9Ym+vgc2VrLu/4ofYdL4Rmfav7qy8iVaDWhSjW2nGHOD7WGLMSaG+MSbDW7qjm0kRERETkaLlc4PLNmVKTWevMY+Mt8W3F+96Xhi5eT5nXJftCmL3XlL2+7HuPE9J4fUFN2ffeEt9rb5nXpdceop098L6HO+Y5eF/62nrLbPaA9wdudl/91rv/9bD/6/osvrVCFJE6otaFKEdQ7NuX+LUKEREREakbjHGG6Ljr2j+bq8EhAxYL2DIhzYHvDwxyDgxryoZO3oNfl73eW/ZY6XMOs98v/Cl9X6a2/dodeM1h7tu0T/V+7UWkytSZvw2MMb2ATsBsa+1uP5cjIiIiIlK/lQZQIiJ1iMvfBVQGY0wE8D5gcSadLa/NcGPMbGPM7O3bt1dneSIiIiIiIiJSB9T6EMUYEwSMxVmh5xFr7ZTy2llr37LWplprUxMTE6u1RhERERERERGp/Wp1iGKMCQA+Bc4EnrPWPubnkkRERERERESkjqq1IYovQBkNXAi8bK29178ViYiIiIiIiEhdVitDFGOMG/gQuAR4w1p7p59LEhEREREREZE6rtaFKMYYF84kslcA7wK3+7UgEREREREREakXauOaYw8D1wC7gS3Aw8aYA9v8T8sci4iIiIiIiEhlqo0hSnPfPgb49yHavI8TsoiIiIiIiIiIVIpaN5zHWjvMWmuOsK33d50iIiIiIiIiUrfUuhBFRERERERERMQfjLXW3zVUO2PMdmCDnx6fAOzw07Ol/tD3mVQ1fY9JddD3mVQHfZ9JVaup32PNrbWJ/i5CpLaplyGKPxljZltrU/1dh9Rt+j6TqqbvMakO+j6T6qDvM6lq+h4TqVs0nEdERERERERE5CgoRBEREREREREROQoKUarfW/4uQOoFfZ9JVdP3mFQHfZ9JddD3mVQ1fY+J1CGaE0VERERERERE5CioJ4qIiIiIiIiIyFFQiCIiIiIiIiIichQUolQDY8xAY8xEY0yWMWaPMWaqMeYUf9cltYsxprEx5m5jzCRjzCZjTJExJs0Y87ExpvMhrulkjPnSGJNpjMk1xvxujLm0umuX2s33PWSNMTsOcV7fZ3LMjOM6Y8wM39+POcaYJcaY18ppq+8xOWbGmEBjzG3GmFm+753dxph5xpi/GWNCy2mv7zMplzHmWmPM277vn2Lf34mDD9P+mL6XjDFNjTEfGmMyjDH5xpiFvu9dUxWfR0SOj+ZEqWLGmDOAb4EcYDRQCFwOJAEXWWu/9mN5UosYY54C/g6sAn4CMoHOwNlAEXCWtXbq/7d378Fy1+Udx98fQ6SUW8o9WFQKHSFaDQ2KUKbAEALWohUFS6FFCtYRB8SBWoQZoQ7VTpWhAjpMK5gKFLmVEWsrCIUCNaCMXIRWLq1puUYYy0WU+9M/ft/V7bLnsHtyTk6Seb9mMt/Z5/f89nw38yR79tnf7/vty18I3ACsA3wVeAw4APg14OiqOmsVTl9rqCQHA+fT1djTVbXZwPGFWGcaU5I5wHnAwcCtdP+nvUhXN3v015k1pqlKcgWwP3AXcHUL7wMsAK4H9qqql1ruQqwzTSDJcuB1wI+A54HX0NXPdUNyFzJGLSXZBrgZ2BK4FFgO7Au8BTitqo6fgZckaSXYRJlBSV4N3EPXMHlrVd3V4vOB2+h+Ydyuqn42a5PUGiPJAcCjVXXDQPxA4GLgB1W1Y198GbALsKSqrm6xDeneqF9PV3sPr6Lpaw2UZAu6Dx8XAL8HbDCkiWKdaWxJTgA+AxxfVacNHFunql7oe2yNaWxJdgFuAq4FFvc1S+YA1wB70Pch2DrTZJLsDdxTVfcn+RxwHBM3UcaqpSQXAr8PHFFV57bYXOBKYE9gUVXdOnOvTtK4vJ1nZi2m61pf0GugALT/OM8E5tNdRSC9oqr6h8EGSotfQtes2yHJZgBJFgBvB67pvYG33KeATwPrAX+wSiauNdkXgKeBk4YdtM40FUnWBz4BXDfYQAEYaKBYY5qqbdt4Va+BAlBVL9J9OAXwPVMjqaprqur+V8obt5aSbAy8F7i310Bp+c8DnwQC/PF0vQ5J08Mmysz67TZ+a8ixXmyPVTQXrd2eb2Pvw4e1p5WS5L3A+4APVdXTE6RZZ5qKJcBGwGVJNmprDXwiyWHt6qd+1pim6t/buCTJz3/fbVei7Et3e/VNLWydabqMW0u7AnP5xe1m/ZbRfZFh7UmrmXVmewJrue3beN+QY/cN5EhTkmQR8Ebglqp6vIUnrL2qWpHkJ1h7mkCSTemuQjm/qq6cJNU601QsauOvAHcDW/UdezrJh6rqgvbYGtOUVNUdbZHio4A7kvQ+wC6hq7lDq+qBFrPONF3GraXJ8l9M8kOsPWm145UoM2ujNj455FgvtvEqmovWQkk2AJYCRbfobM9ktdeLW3uayBl07w8fe4U860xT0VtX52TgFmAHYB7dmgDPA0vbwoxgjWklVNVH6G5H3BE4tv3ZkW4dsf7bY60zTZdxa2mU/PXaGimSVhM2UWZWb1uyYav3uqKvVkpbuPgSuh16Tqmqf+k/3EbrTGNJsj/d/drHVtXQLY3709tonWkcvd89VgAHVdXdVfVEVV0EnEB3lezRLcca05QkeVWSL9N9wXAk3SL/mwKH0N2qeFOSTXrpbbTOtLLGrSVrT1oD2USZWU+0cdi3FxsP5EgjS7IOcBGwH932d58aSJms9qD75sPa0//TFvw8G/jnqvr7EU6xzjQVvZq4esjudF9v46KBXGtM4zoC+ABwYlV9uaoeraofV9WFwDF0u6T0rrazzjRdxq2lUfJ/1haalbSacE2UmdW/7sn3Bo5Ntl6KNKHWQLmQbsvZM6vq+CFpE665k2RLYAOsPb3c5sDWwNZJhn4r1uJPVNU8rDNNzT1tHPahtBdbr43WmKZqvzb+65Bj17VxpzZaZ5ou49bSZPlz6HaZsvak1YxXosys69u4z5Bj+wzkSK+ovaGeR3cp8tlVdcwEqdaepuIp4JwJ/vyEbjeLc4CvtHzrTFNxXRt3HHKsF/ufNlpjmqp127jZkGO92LNttM40XcatpWV0a0EtHpK/K7A+1p602kmVt+DNlLZmxb103+6+taruavH5wG3Ai8B2Qy5nll6mbdH4d8ChdB9kP1iT/ANOsgzYBVhSVVe32IbAzXSXMW9fVQ/N9Ly1dkiyHNigqjYbiFtnGluSa+m27dy7qq5tsbnA5cA7gQ9X1dktbo1pbElOBP4C+Cbw7qp6rsXn0F3NeSDw0ao6o8WtM40kyeeA44C9quq6IcfHqqUkF9ItrH1EVZ3bYnPpancvYFFV3TqTr0nSeGyizLAk+wH/SPct7oV033q8n26BswOq6muzOD2tQZL8OfBJ4HHgTOClIWl/3dvmuO1ucSMwB/gq8BjwHmA74OiqOmvGJ621xiRNlIVYZxpTkh2Ab9Nd2n4Z8DCwN/Bm4Fq6Dx8vtNyFWGMaU5J5wHfpbpO4F7iK7surxcAC4HZgt6r6actfiHWmCSQ5Eti9PdwZeCNwJfBIi32pqm5suQsZo5aSbAN8h+6zwaXAD+luR3sL3bp3w27bljSLbKKsAkl2B06h60qHbkvHTw3spiJNKslS4LBXSNu2qpb3nfMm4FS6b3zXBe4EPltVl8zQNLWWmqiJ0o5ZZxpbku3o6mYx3eKJ/w1cAPxlVT07kGuNaWxt950Tgf3prgAoug+olwOfqaqnBvKtMw01wu9gh1fV0r78sWopyWuBTwP7AhvSNf6+SHfrth/WpNWMTRRJkiRJkqQRuLCsJEmSJEnSCGyiSJIkSZIkjcAmiiRJkiRJ0ghsokiSJEmSJI3AJookSZIkSdIIbKJIkiRJkiSNwCaKJEmSJEnSCGyiSJK0BktySpJKsudsz0WSJGltZxNFkiRJkiRpBDZRJEmSJEmSRmATRZIkSZIkaQQ2USRJ6pPk4CQ3JHkyydNJbk5y0JC8pW0tku2TnJxkeZJnktyZ5PAJnnurJF9Mcn+S55I8mORvk2w9Qf4b2s+5P8mzSR5K8o0k+0yQ/0dJvt/m8UCSU5PMWbm/EUmSJPWsM9sTkCRpdZHkdOBY4D+BC4AXgN8BLkqyTVWdNuS0M4CdgIvb4/cD5yaZV1Wn9z33VsDNwGuBbwLnAwuAI4F3JHl7VT3Ql78X8HXgl9r4H8AWwG7AIcC3BuZxDLAY+BpwDfAu4CS69/oTpvDXIUmSpAGpqtmegyRJsy7JO4B/Ai4BDq2q51r8l+maEouAbavqwRZfChwGPAzsVFUrWnxL4DZgHvD6vvhXgD8E/qyq/qrv5x4FfAG4tKoObLH1gP8CNgX2rKpvD8z1NX3zOAU4Gfhf4G1VdV+LbwLcC7wa2LT3eiRJkjR13s4jSVLnKOAl4MP9DYeq+ilwKjAXOGDIeWf0GiUtfwXweborSN4HkGRd4CDgAeD0gfPPBu4D3pNkwxZ7N7AV8DeDDZT2Mx4cMo/P9xooLefHwBXABsAbJn7ZkiRJGpW380iS1Hkb8ARwdJLBY5u3cVgz4sYhsX9r45v7zlsXWFZVz/cnVtVLSW4EtgfeBCwDdm6Hrxpj/rcOifWaLfPGeB5JkiRNwCaKJEmdTejeF0+eJGf9IbFHh8R+1MaNBsYVQ3L74728jdv40CRzGfTkkNgLbXRxWUmSpGlgE0WSpM6TwJNVte2Y520O3D0Q26LvOfvHLSd4ji0H8h5v49BdeyRJkjQ7XBNFkqTOd4DXJZk/5nm7D4n9VhvvaOPdwDPArknm9icmeVXLfxG4s4W/28YlY85FkiRJM8gmiiRJnbOAAF/qW+D155IsSLLFy0/jmP54253no8CzwGUAVfUs3a4/vwocPXD+B4FfBy6vqqda7Aq6W3n+JMmuQ+biFSqSJEmzwNt5JEkCquobST4L/Clwb5Kr6BoZWwG/AfwmsCu/WO+k5zbg9iQXt8cHtXOOq6pH+vI+DuwBnJZkb+B2YAHwrvZzPtY3l2eSHEy35fINSa4AfgBsBuwG3AJ8YHpeuSRJkkZlE0WSpKaqPp7kBuAjwDvptgdeQdfAOAr4/pDTjgEOAQ4H5tNtV3xSVZ078NyPJNmFbuHa/YF9gMeAc4BTBrctrqrrk+wMnAQsBn6XbhHb7wHnTcsLliRJ0lhSVbM9B0mS1jhJlgKHAdtW1fLZnY0kSZJWBddEkSRJkiRJGoFNFEmSJEmSpBHYRJEkSZIkSRqBa6JIkiRJkiSNwCtRJEmSJEmSRmATRZIkSZIkaQQ2USRJkiRJkkZgE0WSJEmSJGkENlEkSZIkSZJGYBNFkiRJkiRpBP8H8dan6TRnnNgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_data = pd.read_csv('cnnadadelta.log', sep=',', engine='python') \n",
    "plot_history_model(log_data, \"Adadelta\", epoh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_data = pd.read_csv('cnnsgd.log', sep=',', engine='python') \n",
    "plot_history_model(log_data, \"SGD\", epoh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       1.00      0.44      0.62         9\n",
      "           3       0.68      0.72      0.70        78\n",
      "           4       0.64      0.71      0.67       140\n",
      "           5       0.77      0.72      0.74       177\n",
      "\n",
      "    accuracy                           0.71       406\n",
      "   macro avg       0.68      0.60      0.62       406\n",
      "weighted avg       0.71      0.71      0.71       406\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "y_prob = modeladam.predict(X_test) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "rounded_labels=np.argmax(Y_test, axis=1)\n",
    "mat = confusion_matrix(rounded_labels, y_pred)\n",
    "class_names = ['Opposite','Related', 'No Alignment', 'Spesific', 'Similar', 'Equivalent']\n",
    "plot_confusion_matrix(conf_mat=mat, class_names = class_names, cmap=plt.cm.Blues, colorbar=True)\n",
    "print(classification_report(rounded_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.19      0.13      0.15        78\n",
      "           4       0.38      0.48      0.42       140\n",
      "           5       0.47      0.47      0.47       177\n",
      "\n",
      "    accuracy                           0.40       406\n",
      "   macro avg       0.17      0.18      0.18       406\n",
      "weighted avg       0.37      0.40      0.38       406\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "y_prob = modeladadelta.predict(X_test) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "rounded_labels=np.argmax(Y_test, axis=1)\n",
    "mat = confusion_matrix(rounded_labels, y_pred)\n",
    "class_names = ['Opposite','Related', 'No Alignment', 'Spesific', 'Similar', 'Equivalent']\n",
    "plot_confusion_matrix(conf_mat=mat, class_names = class_names, cmap=plt.cm.Blues, colorbar=True)\n",
    "print(classification_report(rounded_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "           2       1.00      0.56      0.71         9\n",
      "           3       0.71      0.71      0.71        78\n",
      "           4       0.68      0.65      0.67       140\n",
      "           5       0.73      0.79      0.76       177\n",
      "\n",
      "    accuracy                           0.72       406\n",
      "   macro avg       0.69      0.62      0.64       406\n",
      "weighted avg       0.72      0.72      0.71       406\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "y_prob = modelsgd.predict(X_test) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "rounded_labels=np.argmax(Y_test, axis=1)\n",
    "mat = confusion_matrix(rounded_labels, y_pred)\n",
    "class_names = ['Opposite','Related', 'No Alignment', 'Spesific', 'Similar', 'Equivalent']\n",
    "plot_confusion_matrix(conf_mat=mat, class_names = class_names, cmap=plt.cm.Blues, colorbar=True)\n",
    "print(classification_report(rounded_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mlxtend.plotting import plot_confusion_matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import classification_report\n",
    "# y_prob = model.predict(X_train) \n",
    "# y_pred = y_prob.argmax(axis=-1)\n",
    "# rounded_labels=np.argmax(Y_train, axis=1)\n",
    "# mat = confusion_matrix(rounded_labels, y_pred)\n",
    "# class_names = ['Weak','Comparison', 'Neutral', 'Point']\n",
    "# plot_confusion_matrix(conf_mat=mat, class_names = class_names, cmap=plt.cm.Blues, colorbar=True)\n",
    "# print(classification_report(rounded_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_encoded(x):\n",
    "#     x = token.texts_to_sequences(x)\n",
    "#     x = pad_sequences(x, maxlen = max_kata, padding='post')\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kalimat = []\n",
    "# kalimat1 = input(\"Inputkan kalimat : \")\n",
    "# kalimat.append(kalimat1)\n",
    "# #kalimat = ['The common phrasebased translation systems, such as ( Och et al., 1999; Koehn, 2004 ), do not use an explicit sentence length model.']\n",
    "# x = get_encoded(kalimat)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_prob = model.predict(x) \n",
    "# y_classes = y_prob.argmax(axis=-1)\n",
    "# y_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = y_classes\n",
    "# if (result == 3): \n",
    "#     print(\"PUse\")\n",
    "# elif (result == 2):\n",
    "#     print(\"Neut\")\n",
    "# elif (result == 1):\n",
    "#     print(\"Coco\")\n",
    "# else:\n",
    "#     print(\"Weak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_dict = dict({})\n",
    "# for idx, key in enumerate(word_model.wv.vocab):\n",
    "#     my_dict[key] = word_model.wv[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
